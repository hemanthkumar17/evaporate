Virtual memory is a computer memory management technique that enables a computer to compensate for shortages of physical memory by temporarily transferring pages of data from random access memory (RAM) to disk storage. According toStallings, W. (2018)It provides a way for programs to use more memory than the computer has physically installed, which makes it possible for large applications to run on computers with limited memory resources. This article explores what virtual memory is, how it works, and its advantages and disadvantages. Virtual memory is a technique that allows a computer to use more memory than it has physically available. It uses a combination of hardware and software to enable the operating system to create the illusion of a large, contiguous address space in the form of virtual memory. When a program needs to access a piece of memory, the operating system first checks to see if it is already in physical memory. If it is, the program can access it quickly. If it is not,the operating system transfers the required pages of data from RAM to disk storage making room in physical memory for the new data. Virtual memory works by creating a virtual address space that is larger than the physical memory available on a computer. It does this by breaking up the virtual address space into smaller units called pages. These pages are typically 4 KB or 8 KB in size. Each page is mapped to a physical page frame in RAM or to a page file on disk.\nWhen a program accesses a piece of memory,According toPatterson, D. A., & Hennessy, J. L. (2017).the operating system translates the virtual address into a physical address by consulting a page table. If the page is already in physical memory, the operating system returns the corresponding physical address. If the page is not in physical memory, the operating system retrieves the required page from disk and places it in an available page frame in RAM. The page table is updated to reflect the new mapping, and the program can access the data as if it were in physical memory. Virtual memory offers several advantages over physical memory: Larger Address Space: Virtual memory allows programs to use a larger address space than physical memory, which makes it possible to run larger applications. Memory Protection: Virtual memory provides memory protection by assigning each program its own virtual address space. This ensures that a program cannot access memory that it is not supposed to. Memory Sharing: Virtual memory allows multiple programs to share the same physical memory. This reduces the amount of memory required by each program and enables more programs to run simultaneously. Simplified Memory Management: Virtual memory simplifies memory management by providing a uniform address space for all programs. This eliminates the need for complex memory management schemes, such as overlays and swapping. Virtual memory also has some disadvantages: Slower Access Times: Accessing data in virtual memory is slower than accessing data in physical memory because it involves disk I/O operations.\nis an important component of data centers and other IT infrastructure. It is responsible for distributing power to the various components of the IT infrastructure. A PDU receives power from a power source and distributes it to the devices that are connected to it. PDUs are essential for IT infrastructure because they help to ensure that power is distributed efficiently and safely. A power distribution unit, or PDU, distributes continuous network power to multiple devices. A PDU doesn't condition or generate power but relays AC power from a generator, an uninterruptible power supply (UPS), or utility power source to networking hardware, servers, telecom equipment, and other critical devices. A Power Distribution Unit (PDU) works by receiving power from a power source and distributing it to the devices that are connected to it. The power source can be a utility power grid or an uninterruptible power supply (UPS). The PDU receives the power and then distributes it to the devices through multiple outlets or receptacles. The PDU is connected to the power source through a power cord that is plugged into a wall outlet or a UPS. The PDU then distributes the power to the devices through its outlets. The outlets can be either a standard electrical outlet or a specialized outlet designed for IT equipment. PDUs may have different types of outlets depending on the type of PDU. For example, a basic PDU may have standard electrical outlets, while a high-density PDU may have multiple outlets designed specifically for IT equipment.\nSome PDUs also have different types of outlets to accommodate different types of power cords. PDUs may also have additional features such as surge protection, overload protection, and monitoring capabilities. Surge protection helps to protect the connected devices from power surges or spikes. Overload protection helps to prevent the PDU from being overloaded, which can cause damage to the PDU and the connected devices. Monitoring capabilities allow administrators to monitor the power consumption of the connected devices. This information can be used to identify areas of energy waste and implement energy-saving measures. Some PDUs also have remote management capabilities, which allow administrators to monitor and control power supply to connected devices from a remote location. In summary, a PDU works by receiving power from a power source and distributing it to the devices that are connected to it through multiple outlets or receptacles. The PDU may also have additional features such as surge protection, overload protection, and monitoring capabilities. Choosing the right type of PDU is important to ensure efficient and safe power distribution in IT infrastructure. There are several types of PDUs available in the market. Each type of PDU is designed to meet specific requirements. Some of the most common types of PDUs are: Basic PDU: A basic PDU is a simple power strip that distributes power to multiple devices. It does not have any intelligence or monitoring capabilities. Metered PDU : A metered PDU is a basic PDU with monitoring capabilities. It provides information about the power consumption of the connected devices.\nSwitched PDU: A switched PDU is a PDU with remote power control capabilities. It allows administrators to remotely turn on or off the power supply to connected devices. Smart PDU: A smart PDU is a PDU with advanced monitoring and control capabilities. It provides detailed information about power consumption and allows administrators to control power supply to individual devices. High-Density PDU: A high-density PDU is a PDU with a higher number of outlets. It is designed to provide power to a large number of devices in a small space. 1. PDU2000-32-1PH-11/2-B1 The PDU (PDU2000-32-1PH-11/2-B1) distributes AC power to cabinets. It has the following features: Supports 220, 230, or 240 V AC per power supply. Provides 13 outlets: Eleven IEC60320 C13 outlets: The maximum output current of a single outlet is 10 A. Two IEC60320 C19 outlets: The maximum output current of a single outlet is 16 A. Provides output overcurrent protection. 2. PDU32-3PH-12/9-B-12+9 The PDU (PDU32-3PH-12/9-B-12+9) distributes AC power to cabinets. It has the following features:Supports 380 V AC per power supply. Provides 21 outlets: Twelve IEC60320 C13 outlets: The maximum output current of a single outlet is 10 A. Nine IEC60320 C19 outlets: The maximum output current of a single outlet is 16 A. Provides output overcurrent protection. Rack-Mount PDU: A rack-mount PDU is designed to be mounted in a server rack. It is available in different sizes and configurations to fit different types of server racks. Why is PDU important?\nEffectively distributing power: Power distribution units (PDUs) make the guarantee that power is distributed equally across all connected devices, lowering the possibility of power surges or overloading specific equipment. This reduces equipment damage and downtime. Increased dependability: A PDU can assist in ensuring that crucial equipment stays online and accessible by supplying backup power in the event of a power outage or other power-related concerns. Savings on costs: A PDU's sophisticated power management features can aid in optimizing energy use and cutting down on electricity expenses. PDUs offer several benefits to IT infrastructure. Some of the benefits are: Increased Efficiency: PDUs help to distribute power more efficiently. They reduce the number of power cords required and help to reduce clutter. Improved Safety: PDUs are designed to distribute power safely. They have built-in safety features such as surge protection and overload protection. Remote Management: Most PDUs have remote management capabilities. This allows administrators to monitor and control power supply to connected devices from a remote location. Energy Management: PDUs provide detailed information about power consumption. This information can be used to identify areas of energy waste and implement energy-saving measures. Scalability: PDUs can be easily scaled up or down depending on the needs of the IT infrastructure. Benefits of Intelligent PDUs Intelligent power distribution units are an important part of an integrated monitoring system that protects mission-critical equipment.\nAn efficient unit will allow you to monitor and control the power at your individual outlets, switch power on or off, remotely shut down the power during an emergency power outage, and also allocate power efficiently. Also, keep in mind that PDUs that were designed to meet your specific needs will be able to provide you with customized features to attend your remote sites' requirements. In a nutshell, the most important advantages PDUs offer include: You'll know you where your energy is going. The best PDUs will be able to give you monitoring capabilities as well as power consumption data. This way, your network techs will be able to know where energy is being consumed the most. You'll have better control over your power. Your technicians will be able to remotely toggle even individual outlets. This makes it possible to turn off non-essential gear when an outage happens and UPS runtime needs to be maximized. Also, you can remotely restart your equipment after an issue to restore normal operations. You'll save money on network components. The most modern PDUs will allow you to daisy-chain network connectivity, so you'll only need a single port and one IP address for multiple PDUs. Up-to-the-minute information about remote equipment. Advanced PDUs will have intuitive web interfaces that be accessed by your techs when they need information about remote sites, such as alarms. Not only that, but you'll also get immediate notification about alarms and issues through whichever method you prefer, like email or text messages.\nVeeam Provisory & Replication operates both the virtualization subcaste as well manages physical machine backup. It backs up VMs at the image- position using a hypervisor's shots to recoup VM data. Backups can be full( a full dupe of VM image) or incremental( saving only the changed blocks of data since the last backup job run). Provisory supplements are created using the erected- in changed block shadowing( CBT) medium. The available backup styles include forward incremental- ever backup, forward incremental backup, and rear incremental backup. also, there's an option to perform active full and synthetic full backups.Veeam Provisory & Replication provides automated recovery verification for both backups andclones. The program starts a VM directly from a backup or replica in the isolated test terrain and runs tests against it. During the verification, the VM image remains in a read-only state. This medium can also be used for troubleshooting or testing patches and upgrades. Backup storage Veeam Provisory & Replication supports software- defined storehouse technology. It allows organizing a scalable backup depository from a collection of miscellaneous storehousebias. Backups can be stored on- demesne, transferred to out-point depositories via the WAN,( saved to tape recording media for long- term retention, or transferred to pallstorehouse. pall storehouse support is available on an structure- as-a-Service( IaaS) model. Veeam's technology, Cloud Connect, provides integrated and secured backup to the pall through Veeam- powered service providers.\nVeeam Provisory & Replication is storehouse- agnostic, but it also has specialized storehouse integrations with some storehouse systems similar as Cisco HyperFlex, EMC VNX, EMC VNXe, HP 3PAR, HP StoreVirtual, Nimble, NetApp, IBM, Lenovo Storage V Series. In addition, through a separate Universal Storage API and plug- in, Veeam also provides storehouse integrations with INFINIDAT and Pure storehouse. It usesstorehouse system shots as a source for backups and recovery of VMware VMs with disks abiding on storehouse volumes. Veeam Provisory & Replication also have make in direct NFS agent which allows to pierce NetApp shots directly from NAS storehouse bypassing hosts for backup, restore & storehouse checkup operations. Backup from Storage Snapshots for HPE, NetApp, EMC and Nimble Generate ultra-fast backups with low RPOs Scale-out Backup Repository Produce a single virtual pool of backup storehouse to which backups can be assigned, offering the freedom to fluently extend provisory storehouse capacity. Veeam Cloud Connect Get backups off point without the cost and complexity ofstructure and maintaining anout- point structure; secure you cloud backup to a service provider SureBackup Automatically test and verify every backup and every virtual machine (VM) for recoverability Recovery Veeam delivers lightning-fast, reliable restore for individual files, entire VMs and application items -- ensuring you have confidence in virtually every recovery scenario.\nThe SmartPartition sets target values for cache partitions. Based on the values, cache resources are dynamically and separately allocated to services to avoid malicious cache contention and ensure critical services' performance. It means that the SmartPartition feature is a performance feature that ensures that the cache size of the partition is secured by configuring different sized cache partitions for different levels of users to ensure service performance for the business applications that are located in that partition. The SmartPartition feature ensures critical service and high-level user quality of service by allocating the cache resources of the engine's various systems on demand. A. Isolation of multi-service resources: the business application independent use of the allocation of the cache partition, to avoid the interaction between different types of applications to protect the overall quality of the storage system. B. To ensure business reliability: by limiting the provision of non-critical applications to the cache resources, to provide critical applications to the cache resources to ensure the performance of critical applications to ensure business reliability. SmartPartition allocates cache resources to services (the actual control objects are LUNs and file systems) based on partition sizes, thereby ensuring the QoS of mission-critical services. 1. Multi-service system to ensure that the core business performance. 2. VDI scene to ensure that the important customer service quality requirements. 3. Multi-tenant scenarios in cloud computing systems LUNs 1. After configuring the SmartPartition feature for a LUN, you can configure other value-added features other than SmartDedupe & SmartCompression. 2.\nA CIFS share is to share a file system or its quota tree among authentication users, including local and domain authentication users. The users have the permissions granted by the storage system on the CIFS share. Homedir shares are a type of CIFS shares. A Homedir share is to share a file system to a specific user as an exclusive directory. The user can only access the exclusive directory named after its user name. Accessing Shared Files Across Protocols The storage system allows users to configure both NFS sharing and CIFS sharing for a file system. The user mapping function allows users to access shared files across protocols (CIFS-NFS) through clients on different platforms and obtain precise permission control. Procedure Command Check the license file. show license NOTE: SmartMulti-Tenant depends on the NAS license, which covers NAS Foundation, SmartQuota, and SmartMulti-Tenant. Create a storage pool. create storage_pool NOTE: When you run the create storage_pool command to create a storage pool, the system automatically creates a disk domain in the background. You can also run the create disk_domain and create storage_pool commands to create a disk domain and a storage pool, respectively. If you run the create disk_domain command to create a disk domain separately, the default redundancy policy is disk redundancy. If you want to create a storage pool with enclosure redundancy, add redundancy_strategy=enclosure in the command to create a disk domain. Create a vStore. create vstore general (Optional) Create a bond port. create bond_port (Optional) Create a VLAN.\nHi there! In this article, we will learn about NAS vs SAN. Background: Network Attached Storage ( NAS stands for Network Attached Storage ; NAS can be used if you want to store data in a cartelized location where the data can be accessed by any device on the network.It consists of multiple hard drives combined in a RAID configuration for protection and redundancy. The NAS has a network interface card (NIC) connected to a router or switch.Therefore, other devices on the network can access data stored on the NAS.Devices such as desktops, laptops, and servers can access this data.Devices on the network access the NAS as a shared drive. Usually, NAS is used in small and medium enterprises.The main disadvantage of a NAS is that it does not tolerate failures.For example, if the NAS power supply fails, another device will not be able to access the data. Benefits of NAS: One volume is used by several hosts (clients). Provides a fault-tolerant system. Allows administrators to use simple and inexpensive load balancing. Disadvantages of NAS: Not supported by all applications. A backup solution costs more than a storage system. Any reduction in the local network can slow down the storage access time. Storage Area Network ( SAN stands for Storage Area Network .\nHow does the OceanStor Dorado All-Flash Storage ensure high availability and data reliability in mission-critical environments? The OceanStor Dorado All-Flash Storage ensures high availability and data reliability in mission-critical environments through various features such as: Dual-Controller Architecture : The storage system employs dual controllers, which provide redundancy and failover capabilities. If one controller fails, the other takes over to ensure continuous operation. RAID Technology: The use of RAID technology allows data to be distributed across multiple drives, providing data redundancy and protection against drive failures. Hot-Swappable Components: Critical components like drives, power supplies, and cooling fans are hot-swappable, enabling maintenance and replacement without interrupting storage operations. Snapshot and Replication: Advanced snapshot and replication capabilities enable data backup and disaster recovery, ensuring data availability even in the event of a failure or disaster. SmartCache and SmartPartition: These features optimize data access and distribution, ensuring high performance and efficient utilization of resources. Data Deduplication and Compression: Data reduction technologies reduce storage capacity requirements and enhance data efficiency. By incorporating these features, the OceanStor Dorado All-Flash Storage provides a reliable and high-performing storage solution for mission-critical environments, minimizing downtime and ensuring data integrity and availability. Hello Friend The OceanStor Dorado All-Flash Storage ensures high availability and data reliability in mission-critical environments through a number of features, including: 5-layer reliability: The OceanStor Dorado All-Flash Storage system features a 5-layer reliability architecture that protects data from all levels, from the component level to the cloud level. This ensures that even in the event of a failure, data remains accessible and protected.\nThere are many great hard drives for laptops on the market, so it can be tough to decide which one is right for you. Here are a few of the best hard drives for laptops in 2023: Samsung 870 QVO SATA SSD: This is a great option for those who want a fast and reliable hard drive. It has a read speed of up to 560 MB/s and a write speed of up to 530 MB/s. WD Blue SN550 NVMe PCIe SSD: This is a great option for those who want a fast and portable hard drive. It has a read speed of up to 2,400 MB/s and a write speed of up to 1,950 MB/s. Seagate Barracuda 510 SATA SSD: This is a great option for those who want a budget-friendly hard drive. It has a read speed of up to 540 MB/s and a write speed of up to 520 MB/s. Samsung 980 PRO NVMe PCIe SSD: This is the best hard drive for laptops if you want the absolute fastest speeds. It has a read speed of up to 7,000 MB/s and a write speed of up to 5,300 MB/s. If you are looking for Huawei, the following are some good hard drives and laptops from Huawei: Hard Drives Huawei 2TB Portable SSD: This is a great option for those who want a portable and fast hard drive. It has a read speed of up to 540 MB/s and a write speed of up to 520 MB/s.\nIt is also shockproof and dustproof, making it perfect for taking on the go. Huawei 512GB NVMe PCIe SSD: This is a great option for those who want a fast and reliable hard drive. It has a read speed of up to 2,500 MB/s and a write speed of up to 1,900 MB/s. It is also very energy-efficient, making it a great choice for laptops. Laptops Huawei MateBook 14: This is a great all-around laptop that offers a good balance of performance, battery life, and portability. It has a 14-inch display, an Intel Core i5 processor, 8GB of RAM, and a 512GB SSD. Huawei MateBook X Pro: This is a high-end laptop that offers excellent performance and a premium build quality. It has a 13.9-inch display, an Intel Core i7 processor, 16GB of RAM, and a 1TB SSD. Huawei MateBook D 15: This is a budget-friendly laptop that offers good value for money. It has a 15.6-inch display, an AMD Ryzen 5 processor, 8GB of RAM, and a 256GB SSD. When choosing a hard drive for your laptop, there are a few factors you'll need to consider, such as: Speed: The speed of the hard drive will determine how fast your laptop boots up, how quickly your applications load, and how smoothly your games run. Capacity: The capacity of the hard drive will determine how much data you can store on your laptop. Price: Hard drives can range in price from a few dollars to hundreds of dollars.\nCan you explain the concept of multi-site active-active replication in the context of the Huawei HyperMetro DR Solution and its benefits? Sure. Multi-site active-active replication is a disaster recovery (DR) solution that uses two or more geographically dispersed sites to replicate data. In this configuration, the data is replicated synchronously between the sites, which means that any changes made to the data on one site are immediately replicated to the other sites. The Huawei HyperMetro DR Solution is a multi-site active-active replication solution that can be used to protect mission-critical applications and data. The solution uses Huawei's own high-speed replication technology to ensure that data is replicated synchronously between the sites. Centralized management: The Huawei HyperMetro DR Solution provides a centralized management console that can be used to manage all aspects of the DR solution from a single location. Automatic failover: The Huawei HyperMetro DR Solution can automatically failover to the secondary site in the event of a disaster. Remote replication: The Huawei HyperMetro DR Solution can be used to replicate data to remote sites, which can provide additional protection from disasters. The benefits of using multi-site active-active replication in the context of the Huawei HyperMetro DR Solution include: Increased availability: Multi-site active-active replication can help to increase the availability of applications and data by providing an up-to-date copy of the data at all times. Reduced downtime: In the event of a disaster, multi-site active-active replication can help to reduce downtime by automatically failing over to the secondary site.\nWhen can the use of SmartThin LUNS be considered in dorado storage? SmartThin LUNS (Logical Unit Numbers) are a feature of the SmartThin software-defined storage platform. LUNS are logical representations of physical storage devices, such as hard drives or solid-state drives, that are used to allocate and manage available storage capacity. With SmartThin LUNS, administrators can create, resize, and delete virtual disk volumes within the software-defined storage infrastructure. These virtual volumes can be provisioned to servers or applications as needed, providing flexible and scalable storage solutions. By using SmartThin LUNS, organizations can optimize their storage utilization by dynamically allocating space based on actual needs, rather than pre-allocating fixed amounts of storage. This helps reduce costs and improves overall storage efficiency. In addition, SmartThin LUNS offer advanced data protection capabilities such as snapshots, replication, and thin provisioning. This allows for better management of storage resources while ensuring high performance and availability for critical applications. Overall, SmartThin LUNS provide an efficient and easy-to-use way to manage and allocate storage capacity in a software-defined storage environment. The use of SmartThin LUNs can be considered in Dorado storage when there is a need to optimize storage capacity utilization and reduce the physical storage space required. SmartThin LUNs allow for thin provisioning, which means that the actual physical storage space is allocated dynamically as data is written. This allows for more efficient use of available storage resources and can help reduce costs associated with purchasing and managing physical storage.\nTo ensure sufficient capacity for storing data, which types of capacity need to be planned? The following types of capacity need to be planned: Data retention and archiving capacity: Organizations should also plan for long-term data retention and archiving. This requires allocating enough capacity to store historical data that might not require frequent access but needs to be preserved for compliance or analytical purposes. Redundancy Capacity: Building redundancy into the storage infrastructure ensures high availability and resiliency. This may involve replicating data across multiple storage devices or implementing redundant systems to mitigate risks of data loss or downtime. Storage Capacity: This refers to the amount of storage space required to store data. It can be measured in bytes (e.g., gigabytes, terabytes) or as a percentage of total available storage. Backup and Recovery Capacity: Adequate capacity needs to be allocated for backups and disaster recovery processes. This involves determining how much additional storage space and resources are needed to handle regular data backups and ensure quick data restoration during emergencies. Scalability Capacity: Planning for scalability is important to accommodate future growth and expansion. It involves estimating the potential increase in data volumes and ensuring systems can scale up to meet the demands without impacting performance. To ensure sufficient capacity for storing data, the following types of capacity need to be planned: Storage capacity: This refers to the physical capacity required to store data, including both primary storage (such as hard disk drives, and solid-state drives) and secondary storage (like tape drives, and cloud storage).\nWhich technologies can be used together with SmartPartition to increase cache read/write performance? SmartCache SmartQoS 1- SmartCache: SmartCache is a technology that optimizes data access and retrieval by storing frequently accessed data in a fast cache, thereby improving overall system performance. 2- SmartQoS: SmartQoS helps improve the user experience by ensuring important network activities are given priority over others, resulting in better performance and resource management on the network. 1- SmartCache: SmartCache is a technology used in computer systems and storage devices to improve overall system performance by dynamically managing the caching of data. It uses intelligent algorithms and advanced caching mechanisms to predict and store frequently used data in a cache, which can be accessed much faster than from the original storage location. The purpose of SmartCache is to minimize the time it takes to retrieve data from slower storage devices, such as hard disk drives (HDDs), by keeping frequently accessed data in a faster cache. The cache can be located closer to the CPU or on another storage device specifically designed for caching, such as a solid-state drive (SSD). SmartCache continuously monitors and analyzes data access patterns to identify frequently accessed data. It then intelligently predicts which data should be stored in the cache based on these patterns. By storing the most accessed data in the cache, SmartCache reduces the latency associated with retrieving data from slower storage devices. SmartCache is commonly used in systems that require high performance and low latency, such as database servers, virtualized environments, and high-traffic websites.\nWhat is Virtualization Technology? Virtualization What isStorage Virtualization? torage virtualization is \"the process of presenting a logical representation of the physical storage resources\" of a host computer system, \"treating all storage media (hard disk, optical disk, magnetic tape, etc.) in an enterprise as a single storage pool.\" A \"storage system\" is also known as a storage array, disk array, or filter. Storage systems typically use dedicated hardware and software along with disk drives to provide very fast and reliable storage for computing and data processing. Storage systems are complex and can be thought of as a special-purpose computer designed to provide storage capacity along with advanced data protection features. Disk drives are only one element in a storage system, along with the hardware and special firmware within the system. Storage systems can provide either block-access storage or file-access storage. Block access is typically provided over Fiber Channel, iSCSI, SAS, FICON, or other protocols. File access is often granted using the NFS or SMB protocols . In the context of a storage system, two main types of virtualization are possible: Block virtualization, used in this context, refers to the abstraction (separation) of the logical storage (partition) from the physical storage so that it can be accessed independently of the physical storage or heterogeneous structure. This separation allows storage administrators more flexibility in managing storage for end users. File virtualization solves the problems of NAS by removing dependencies between the data accessed at the file level and where the files are physically stored.\nThis enables you to optimize storage usage and server consolidation, as well as non-disruptive file migrations. When is Storage Virtualization Needed? Myth #1: Virtualization only applies to external storage Myth #2: Virtualization is only about storage networks Myth #3: Virtualization is only about disk storage NAS Virtualization: NAS virtualization refers to the process of virtualizing Network-Attached Storage (NAS) systems. It involves creating virtual instances of NAS devices or abstracting the underlying NAS infrastructure to provide virtualized storage resources to multiple users or applications. Key Aspects and Benefits of NAS Virtualization: Resource Consolidation: NAS virtualization allows for the consolidation of multiple physical NAS devices into a single virtualized storage infrastructure. This enables better utilization of storage resources and reduces the need for maintaining and managing multiple separate NAS devices. Flexible Provisioning: Virtualized NAS environments provide the ability to provision storage resources dynamically to meet changing demands. Administrators can allocate storage capacity, create virtual NAS volumes, and adjust storage parameters based on the requirements of different users or applications. Multi-Tenancy: NAS virtualization enables the creation of isolated storage environments for different users or departments within an organization. Each user or group can have their dedicated virtual NAS instance with separate access controls, security settings, and storage quotas. Improved Storage Management: Virtualized NAS environments often come with centralized management interfaces or tools that simplify the administration and monitoring of storage resources. Administrators can easily allocate, monitor, and optimize storage resources across the virtualized NAS infrastructure.\nHello, everyone! As digitization advances in various industries, data has become critical to the efficient operation of enterprises, and customers impose increasingly demanding requirements on the stability of storage systems. Although many enterprises have highly stable storage systems, it is a big challenge for them to ensure data recovery from damage caused by natural disasters. The Geo-Redundant Solution rises up to this challenge. With intra-city and remote DR centers, this solution can ensure service continuity and data reliability and availability in the event of disasters. 3DC is the core of the Geo-Redundant Solution. 3DC refers to the production center, intra-city DR center, and remote DR center. Production center A DC where production services are running. Intra-city DR center A DR center in the same city as the production center or in a neighboring city. It can independently run services or work with the production center to form an active-active system, achieving zero data loss and automatic failover. Remote DR center A DR center in a remote city. If the production and intra-city DR centers fail concurrently due to a natural disaster, the remote DR center can take over services and use backup data to recover services. This section uses DR Star (HyperMetro+HyperReplication) as an example to describe the data replication principles of 3DC. In the figure, LUN_bb is the remote LUN in the HyperMetro pair and the primary LUN in the B-C remote replication pair. LUN_cc is the secondary LUN shared by the two remote replication pairs.\nInitial synchronization after the DR Star trio is created: Initial synchronization from LUN_aa to LUN_bb is manually or automatically started. After the data status of LUN_bb is Consistent, manually start or enable the system to automatically start data synchronization to fully synchronize data from LUN_bb to LUN_cc. To ensure the availability of backup data in storage system C, 3DC processes host data as follows: 1. The host writes data to both LUN_aa and LUN_bb. 2. When replication starts between site B and site C, storage system B checks the data status of the remote LUN (LUN_bb) in the HyperMetro pair and generates a snapshot t1_snap for LUN_bb at the point in time if the data status is Consistent. 3. Storage system C generates a snapshot t2_snap for LUN_cc at this point in time. Note: If asynchronous remote replication fails and LUN_cc must be used for running services, the system automatically restores data using snapshot t2_snap to ensure the availability of data in storage system C. 4. Data is periodically synchronized from snapshot t1_snap of LUN_bb to LUN_cc in the background. If a fault occurs at site B, the A-C remote replication pair switches to the active state (the B-C remote replication pair becomes the standby one), thereby continuing implementing remote data protection.\nTechnology is transforming healthcare. Technology is transforming patient care and enhancing health. Huawei, a global IT leader, is leading this transition. Huawei's IoT technologies are transforming healthcare. These solutions empower doctors and improve patient outcomes. Huawei's IoT technologies are changing healthcare in this post. Huawei is transforming patient care and healthcare delivery with cutting-edge technologies. Let's examine how. Huawei's IoT solutions enable healthcare professionals to collect and analyze real patient data. Wearables and remote monitoring tools have changed healthcare by giving doctors real-time vital signs. Continuous monitoring has changed the game by giving vital data for patient care decisions. This technology revolutionizes healthcare with seamless connectivity. It helps detect anomalies and intervene quickly, improving patient outcomes and saving lives. This simplified patient care is invaluable. Healthcare is rapidly using remote patient monitoring. Technology lets doctors remotely monitor patients. This technology lets patients get care at home.Remote patient monitoring is an advantage of Huawei's IoT healthcare solutions. This function lets doctors monitor patients without frequent visits. Modern technology allows doctors to remotely check patients' health. This has transformed healthcare by making it more efficient and effective and giving patients peace of mind. Technology allows healthcare providers to stay in touch with patients and provide care wherever they are. Remote patient monitoring systems transformed healthcare with Huawei's IoT devices. Real-time data transmission lets healthcare workers prevent hospital readmissions. Patients can now receive tailored care and continuous monitoring at home. Technology and healthcare have enabled this.\nRoutine checkups and treatments no longer require patients to visit to hospitals or clinics. They can now receive the same care at home. This has enhanced patient care and accessibility. Predictive analytics and AI are two of the most fascinating and fast-growing IT sectors. These cutting-edge technologies are changing company operations and decision-making, and they could do a lot more. Huawei's IoT solutions use predictive analytics and AI to transform patient care. Huawei's cutting-edge solutions could transform healthcare. IoT devices can analyze massive patient data to find trends and anticipate health hazards. This information can help healthcare practitioners improve patient outcomes. Huawei's cutting-edge innovation is transforming healthcare. AI algorithms provide accurate disease diagnosis, individualized treatment regimens, and early warning sign identification, revolutionizing healthcare. Patient care and results have increased greatly. Huawei's IoT solutions give hospitals advanced smart asset tracking and management capabilities. These devices let medical facilities track their equipment in real time. Any organization must minimize equipment downtime, avoid theft, and optimize inventory management. Companies can optimize operations and earnings by addressing these concerns. By assuring resource availability, healthcare professionals may boost efficiency, cut costs, and spend more time on patients. Healthcare management cannot ignore this. Providers may streamline processes and provide high-quality care with the right resources. Healthcare organizations must invest in and make available the proper resources. This will improve care, patient satisfaction, and loyalty. Data communication is essential in today's digital world. However, as more sensitive data is exchanged online, Protecting patient data is paramount in healthcare.\nDear All, Today we are going to learn about Network-Layer Performance Tuning Network-Layer Performance Tuning Network-layer performance tuning refers to the process of optimizing the performance of the network infrastructure that supports data storage, such as storage area networks (SAN) or network-attached storage (NAS). The network infrastructure can have a significant impact on the performance of data storage systems. Network-layer performance tuning involves optimizing the network settings to reduce latency, increase throughput, and improve reliability. This can involve various techniques, such as tuning network adapters, configuring network switches, and optimizing network protocols. Link Bandwidth Bottlenecks Multipathing Software Storage multipathing software is a technology that enables redundant connections between servers and storage devices in a storage area network (SAN). It provides an alternative path for data to travel in case one of the paths fails due to a network or storage component failure. The role of storage multipathing software in network performance issues is to improve network reliability and availability. It can help to prevent network downtime caused by a single point of failure and enable failover to an alternate path if one path is unavailable. This can result in increased uptime for the network and the storage infrastructure, and reduce the risk of data loss or corruption. Storage multipathing software can also help to optimize network performance by load balancing the data traffic across multiple paths. This can improve network throughput and reduce latency by distributing the data traffic more evenly across the available network paths.\nDear all, This post is talking about an Overview of HyperCDP. This post can help you learn about HCIP Storage V5.0. With an ever-increasing amount of data, traditional data backup solutions are facing the following challenges: Large amount of backup data and rapid data growth Small backup window Requirement for zero impact on production system performance Ever higher requirements on the recovery point objective (RPO) and recovery time objective (RTO) Currently, Huawei OceanStor Dorado V6 storage systems provide writable snapshots. A single LUN supports up to 1,000 snapshots, but this does not meet the requirements of mission-critical applications for continuous data protection. To address these requirements, Huawei provides HyperCDP, which creates high-density snapshots on a storage system to provide continuous data protection. HyperCDP creates high-density snapshots on Huawei OceanStor Dorado V6 to provide continuous data protection. HyperCDP has the following advantages: HyperCDP provides intensive and persistent data protection. A single LUN supports 60,000 HyperCDP objects. The minimum interval is 3 seconds. HyperCDP provides data protection at an interval of seconds, with zero impact on performance and while occupying little space. Scheduled tasks are supported. You can specify HyperCDP schedules by day, week, month, or a specific interval. HyperCDP consistency groups are supported. A HyperCDP object cannot be directly mapped to a host for read and write. You can create a duplicate, convert it into a writable snapshot, and map it to the host. HyperCDP objects cannot be directly mapped to hosts for read and write.\n5. Set the capacity and tuning information of the file system. 6. If a HyperMetro vStore pair has been created for the selected vStore, you need to configure HyperMetro for the newly created file system. Specify Remote Storage Pool for creating a remote file system. The system will create a remote file system on the remote device of the HyperMetro vStore pair and add the local and remote file systems to a HyperMetro pair. 7. Configure shares for the file system. 8. Set a quota for the file system. 9. Configure data protection for the file system. 10. (Applicable to 6.1.6 and later versions) If an antivirus server has been configured for the vStore you selected, you can configure the antivirus service for the file system. 11. Select Advanced in the upper right corner and set the audit log items of the file system. The system records audit logs of operations on the file system. The audit log items include Create , Delete , Read , Write , Open , Close , Rename , List folders , Obtain properties , Set properties , Obtain security properties , Set security properties , Obtain extension properties , and Set extension properties . 12. Set advanced attributes of the file system. 13. Set the WORM (Write Once Read Many) properties of the file system. The WORM file system ensures that a file enters the protected state after being written. In this case, the file cannot be modified, moved, or deleted, but can be read for multiple times. 14.\nThe audit log items include Create , Delete , Read , Write , Open , Close , Rename , List folders , Obtain properties , Set properties , Obtain security properties , Set security properties , Obtain extension properties , and Set extension properties . 10. Set advanced attributes of the file system. 11. Set the Write Once Read Many (WORM) properties of the file system. The WORM file system ensures that a file enters the protected state after being written. In this case, the file cannot be modified, moved, or deleted, but can be read for multiple times. 12. Click OK . Confirm your operation as prompted. After creating a WORM file system, you need to share it with clients. Users can store files that need to be protected in the WORM file system to prevent data tampering. For operations on sharing file systems, see SmartMulti-Tenant Feature Guide for File . Windows-based Client 1. Access a WORM file system shared in CIFS mode. 2. Set a file in the WORM file system to the locked state. 3. (Optional) Shift the file whose size is 0 bytes from the locked state to the appending state. 4. (Optional) Add contents to the end of the file. 5. (Optional) Set a file in the appending state to the locked state. Linux-based Client 1. Access a WORM file system shared in NFS mode. 2. (Optional) Run the touch -a -t time file command to set the protection period of a file. 3.\nHello everyone! In the new digital era, data is key for an enterprise to survive and succeed. Protecting service data that grows explosively is one of the most pressing challenges. Currently, basic data backup and recovery policies have the following problems: Massive service data cannot be backed up and recovered quickly. The backup process affects service continuity. More devices and management tools are required as the backup environment is increasingly complex, raising data protection costs. Therefore, enterprises are seeking for a solution that backs up and recover data quickly without affecting services or raising costs. To meet the customer requirements, Huawei has developed HyperVault. Based on file systems, HyperVault enables data backup and recovery within a storage system and between different storage systems. Data backup involves local backup and remote backup. With file systems' snapshot or remote replication technology, HyperVault backs up the data at a specific point in time to the source storage system or backup storage system based on a specified backup policy. Data recovery involves local recovery and remote recovery. With file systems' snapshot rollback or remote replication technology, HyperVault specifies a local backup snapshot of a file system to roll back it or specifies a remote snapshot of the backup storage system for recovery. HyperVault has the following characteristics: Time-saving local backup and recovery: A storage system can generate a local snapshot within several seconds to obtain a consistent copy of the source file system, and roll back the snapshot to quickly recover data to that at the desired point in time.\nHello, everyone! SmartVirtualization is a heterogeneous virtualization feature developed by Huawei. When a local storage system (OceanStor storage system) is connected to another type of Huawei storage system or a third-party storage system, this feature enables the local storage system to use and manage storage resources of the peer storage system as local storage resources despite of the different software and hardware architectures. SmartVirtualization resolves the incompatibility issues among different storage systems, so users can manage heterogeneous storage systems and use storage resources from both legacy and new storage systems, protecting customer investments. Procedure Command Check the license file. show license NOTE: The SmartMulti-Tenant feature requires the NAS feature license. The NAS feature license grants the privilege to use NAS Foundation, SmartQuota, and SmartMulti-Tenant. Create a target connected through an iSCSI link. create iscsi target Map external LUNs to the local storage system. - NOTE: When using SmartVirtualization to centrally manage storage resources, configure the heterogeneous storage systems and establish mappings between the local storage system and heterogeneous storage systems, so that external LUNs can be identified on the local storage system. After connecting to the local storage system, the heterogeneous storage system regards the local storage system as a host. You need to map the external LUNs on the heterogeneous storage to the local storage so the external LUNs can be identified by the local storage. Query information about a remote device. show remote_device general Scan for external LUNs. scan remote_lun View information about remote LUNs. show remote_lun general (Optional) Create an eDevLUN (without masquerading).\nHello, everyone! The requirements for XaaS in public and private clouds emerge with the soaring development of cloud services. As the number of end users increases constantly, one physical storage system may be used by multiple enterprises or individual users. The following challenges arise: The logical resources of enterprises or individual users who use the same storage system may interfere with each other or unauthorized access may occur, impairing data security. IT service providers need to pay extra costs to manage users. Data migration without affecting services is required. Developed to deal with these challenges, the multi-tenancy technology allows storage resource sharing among vStores and at the same time simplifies configuration and management, as well as enhances data security. Huawei's SmartMulti-Tenant allows vStores to create multiple virtual storage systems in one physical storage system. With SmartMulti-Tenant, vStores can share hardware resources and safeguard data security and confidentiality in a multi-protocol unified storage architecture. 1. Creating a Bond Port 2. Creating a VLAN 3. Creating a Logical Port Creating a LUN (Optional) Creating a LUN Group Creating a Host (Optional) Creating a Host Group (Optional) Creating a Port Group Creating a Mapping Configuring the Host Connectivity Using the Storage Space on an Application Server Creating a LUN (Optional) Creating a LUN Group Creating a Host (Optional) Creating a Host Group (Optional) Creating a Port Group Creating a Mapping This section describes how to create a mapping between hosts/host groups and LUNs/LUN groups so that hosts can use the storage space provided by LUNs. 7.\nConfiguring the Host Connectivity This section describes how to configure the connectivity between a host and a storage system to ensure that the host can properly use the storage resources allocated by the storage system. 8. Using the Storage Space on an Application Server After a connection is established between a storage system and an application server, the application server must discover the newly-added logical disk (that is, the storage space specified by a mapped LUN) to use it as a common disk for data reads and writes. Procedure Command Check the license file. show license Create a storage pool. create storage_pool NOTE: When you run the create storage_pool command to create a storage pool, the system automatically creates a disk domain in the background. You can also run the create disk_domain and create storage_pool commands to create a disk domain and a storage pool, respectively. If you run the create disk_domain command to create a disk domain separately, the default redundancy policy is disk redundancy. If you want to create a storage pool with enclosure redundancy, add redundancy_strategy=enclosure in the command to create a disk domain. Creating a vStore create vstore general Configure storage service ports (iSCSI connection). Storage systems use logical ports to establish iSCSI connections with hosts. The home ports of logical ports can be physical Ethernet ports, bond ports, or VLANs. (Optional) Create a bond port.create bond_port (Optional) Create a VLAN.create vlan general Enter the vStore view. change vstore view Create a LUN. create lun (Optional) Create a LUN group.\nwhat is Benefits of Upgrading to PCIe 5.0? Dear Engr_Shakeel, Upgrading to PCIe 5.0 (Peripheral Component Interconnect Express 5.0) offers several benefits compared to its predecessor, PCIe 4.0, and older versions. PCIe is a high-speed interface used to connect various components inside a computer, such as graphics cards, storage devices, network cards, and more. Here are the key benefits of upgrading to PCIe 5.0: Higher Data Transfer Rates : PCIe 5.0 doubles the data transfer rate compared to PCIe 4.0. With a raw data rate of 32 GT/s (gigatransfers per second) per lane, PCIe 5.0 provides up to 64 GT/s for a 16-lane slot, enabling faster communication between components and reducing data bottlenecks. Increased Bandwidth : The higher data transfer rates in PCIe 5.0 result in increased bandwidth. This means that more data can be transmitted simultaneously, leading to improved system performance, particularly in tasks that involve large data transfers, such as 4K/8K video editing, high-resolution gaming, and data-intensive computations. Improved Gaming and Graphics Performance : PCIe 5.0 is particularly beneficial for graphics cards, as it allows for faster data exchange between the GPU and the CPU/memory. This results in reduced latency and improved frame rates in games, leading to a smoother and more responsive gaming experience. Faster NVMe SSDs : PCIe 5.0 significantly enhances the performance of NVMe (Non-Volatile Memory Express) SSDs. These storage devices can take full advantage of the increased bandwidth, resulting in faster read and write speeds, shorter load times, and improved overall system responsiveness.\nDear All, Today we are going to learn about Storage Virtualization. What is Storage Virtualization? Storage virtualization is the process of abstracting physical storage resources and presenting them as logical storage units to be used by applications or other systems. It allows for better utilization, management, and flexibility of storage infrastructure. Here are some storage virtualization vendors and the solutions they offer: IBM: IBM offers the IBM Spectrum Virtualize solution, which includes the IBM SAN Volume Controller (SVC) and IBM Storwize family of storage systems. These solutions provide virtualization capabilities to pool and manage storage resources from various vendors, enabling centralized management and improved storage efficiency. Dell EMC: Dell EMC provides the Dell EMC UnityVSA and PowerMax storage systems with storage virtualization features. These solutions enable the creation of virtual pools of storage resources and offer advanced data services, such as data reduction, replication, and snapshot capabilities. NetApp: NetApp offers the NetApp ONTAP storage operating system, which includes storage virtualization features. ONTAP allows for the creation of flexible storage architectures, data tiering, and efficient data management across NetApp storage systems, including All Flash FAS, AFF, and FAS series. HPE: Hewlett Packard Enterprise (HPE) provides the HPE StoreVirtual VSA (Virtual Storage Appliance), which virtualizes internal and external storage resources into a shared storage pool. It offers features like thin provisioning, snapshots, and replication to simplify storage management. Hitachi Vantara: Hitachi Vantara offers the Hitachi Virtual Storage Platform (VSP) series, which incorporates storage virtualization capabilities.\nThe VSP family provides advanced storage services, such as dynamic tiering, data protection, and data mobility across heterogeneous storage systems. VMware: VMware offers software-defined storage solutions, including VMware vSAN (Virtual SAN) and VMware vSphere Virtual Volumes (VVols). These solutions enable the virtualization of local and shared storage resources and provide policy-driven storage management and data services within virtualized environments. DataCore: DataCore provides the SANsymphony software-defined storage platform. It virtualizes storage resources from multiple vendors, pooling them into a unified storage pool. SANsymphony offers features like automated tiering, thin provisioning, and data replication. Huawei: Huawei offers the Huawei OceanStor storage systems, which include storage virtualization capabilities. The OceanStor solutions enable the virtualization of heterogeneous storage resources, delivering centralized management, data protection, and high-performance storage services. These are just a few examples of storage virtualization vendors and solutions. Other notable vendors in this space include Pure Storage, Fujitsu, Cisco, and Infinidat. Each vendor offers its own set of features, capabilities, and integration options, allowing organizations to choose a storage virtualization solution that best fits their specific needs and infrastructure requirements. How it works Storage virtualization works by abstracting the physical storage resources and presenting them as logical storage units to be utilized by applications or systems. Here's a general overview of how storage virtualization works: Virtualization Layer: Storage virtualization is facilitated by a virtualization layer, which can be implemented as software or hardware. This layer acts as an intermediary between the physical storage infrastructure and the systems accessing the storage.\nStorage Pool Creation: The virtualization layer aggregates and combines the available physical storage resources from various storage devices or arrays into a virtual storage pool. This pool can span multiple storage devices, types, and vendors, creating a unified and flexible storage infrastructure. Provisioning of Virtual Volumes : From the virtual storage pool, virtual volumes are created and presented to the systems or applications requiring storage. These virtual volumes appear as logical units and can be provisioned to meet specific capacity and performance requirements. Abstraction and Mapping: The virtualization layer handles the abstraction and mapping of the logical storage units to the physical storage resources. It maintains a mapping table that tracks which logical volumes correspond to which physical storage devices or arrays. Storage Management and Services: The virtualization layer provides management capabilities and services for the virtual storage pool. This includes functions like storage provisioning, data protection (such as snapshots and replication), performance optimization, and monitoring. Simplified Management: Storage virtualization simplifies storage management by centralizing the control and administration of the storage infrastructure. Administrators can perform tasks like capacity allocation, performance optimization, and data protection from a single management interface, regardless of the underlying physical storage devices. Flexibility and Scalability: Storage virtualization offers flexibility and scalability by decoupling the logical storage units from the physical hardware. It allows for easy expansion and addition of storage capacity without disrupting the applications or systems relying on the virtual volumes. Data Migration and Mobility: Storage virtualization facilitates data migration and mobility by abstracting the physical storage details.\nHello, everybody, In todays article we are going to discuss about one of the most pressing future concerns for every enterprise: effective backup and archiving. Why will this be such a pressing matter? Well, let me give you some context. Did you know that, by 2030, the amount of data generated globally will leap by a factor of 20, reaching 1 Yottabyte annually? If youre wondering what that means, the answer is the storage capacity of four trillion 256 GB mobile phones! Yes, youve read that right! So, whats the solution? Keep on reading and youll find out! According to Huaweis Global Industry Vision (GIV) report , over 80% of that massive data will be unstructured, consisting mainly of videos and images. Storing mass data brings new challenges to the table, as traditional scale-up array storage cant keep up with the growth of mass data. Also, the long-term retention of mass data which is necessary in order to comply with the regulations, increases the IT costs very much. Data silos negatively impact the user experience, as the read and write performance models of different applications interfere with each other. Also, as unstructured data gains more and more importance in the analysis of key enterprise services, the need and the demand for speed, security and reliability increases. We talked about the issues, now lets talk about the solution. Introducing OceanStor Pacific scale-out object storage provides comprehensive data infrastructure services (like production, backup and archiving) for unstructured data like mass video and images.\nIt has multi-load support, lossless multi-protocol interworking, cross-site multi-active mode, deduplication, compression, tiering/replication to cloud, ransomware protection capabilities and Write Once Read Many (WORM). Pretty cool, right? Besides, its features include the following: Object storage uses Representational State Transfer (RESTful) interfaces which eliminates security risks and, thus, makes it a great fit for public network transmission. Object storage can directly access files through Uniform Resource Locators (URLs) and, in tests, has shown a read speed of 10 m in a system of billions of files, all while traditional file storage has reached the second level. Object storage is designed based on tenants in order to effectively share storage resources, to simplify configuration and management and to improve data security. Economical large-scale deployment It improves space utilization to 91.6% through high-ratio Erasure Coding technology It maximizes data center storage value with an ultra-high-density capacity design It increases available capacity Reliable cross-site multi-active mode It supports up to 12 active sites and can tolerate two sites simultaneously failing It enhances security with WORM, enterprise-level misdeletion prevention, encryption, antivirus and ransomware protection features.\nHi All, Today we are going to understand what Erasure Coding (EC) is, what are its advantages and application scenarios. We will be starting to learn EC from basic to expert. In the realm of data storage and transmission, ensuring the integrity and availability of data is of paramount importance. Erasure coding, a technique widely employed in storage systems and data communication, plays a crucial role in enhancing data reliability and resilience. In this article, we will delve into the concept of erasure coding, its working principle, advantages, and applications across various domains. Erasure coding: a method of adding redundancy to data for error detection and recovery. Protects against data loss caused by disk failures, network errors, or other faults. Divides data into blocks and generates additional redundant blocks. Distributes these blocks across multiple storage devices or network nodes. Enables reconstruction of the original data even if some blocks are lost or inaccessible. Widely used erasure coding technique. Adds parity blocks to provide redundancy and enable data recovery. Offers configurable trade-offs between storage overhead and fault tolerance. Utilizes XOR operations to create redundancy. Efficient in terms of computation and storage overhead. Commonly used in distributed storage systems. Maintains the original data blocks along with redundant blocks. Facilitates direct access to the original data without decoding. Enables efficient data reconstruction. Reduces the risk of data loss due to hardware failures or network errors. Enables data recovery and reconstruction, maintaining data integrity. Reduces the storage overhead compared to traditional replication methods.\nDear All, Today we are going to learn about memory swapping Introduction: In the realm of computer systems, memory management plays a critical role in ensuring optimal performance. One important technique used by operating systems to efficiently manage memory resources is memory swapping. This article dives into the concept of memory swapping, explains how it works, highlights its advantages, and explores how it enhances system performance. What is Memory Swapping? Memory swapping, also known as virtual memory swapping or paging, is a technique employed by operating systems to extend the available physical memory by utilizing secondary storage, typically the hard disk. It enables the efficient management of memory by temporarily transferring pages of data between the physical memory (RAM) and the secondary storage (disk). How Memory Swapping Works: When a computer system executes multiple processes or applications concurrently, each process requires a certain amount of memory to operate. However, physical memory is often limited. To overcome this constraint, the operating system divides the memory into fixed-size blocks called pages. Similarly, the secondary storage is divided into corresponding blocks called page frames. When a process requires more memory than the available physical memory, the operating system uses memory swapping to transfer the least recently used (LRU) pages from physical memory to the secondary storage, freeing up space for the required pages. The swapped-out pages are stored in page frames on the disk, creating a memory hierarchy. When a process accesses a page that has been swapped out, a page fault occurs.\nThe operating system then retrieves the required page from the secondary storage and swaps it back into the physical memory, replacing a less frequently used page if necessary. This swapping process allows for efficient memory utilization while providing the illusion of a larger memory space to the running processes. Advantages of Memory Swapping: a. Increased memory capacity: Memory swapping allows the system to utilize secondary storage as an extension of physical memory, effectively increasing the available memory capacity. This enables the execution of larger and more memory-intensive applications without relying solely on the physical memory constraints. b. Multi-programming support: Memory swapping facilitates the execution of multiple processes concurrently. Each process can have its own dedicated virtual memory space, even if the total memory requirement exceeds the physical memory limit. This capability improves the system's ability to multitask efficiently. c. Flexibility and resource management: With memory swapping, the operating system has more control over memory allocation and can dynamically allocate memory resources to different processes based on their demand. It enables efficient usage of memory, ensuring that processes get the necessary resources when required. How Does Memory Swapping Improve Performance? a. Increased system responsiveness: Memory swapping helps prevent system slowdown or crashes caused by insufficient memory. By swapping out the least recently used pages to disk, the operating system ensures that the active processes have enough memory to execute, thereby maintaining system responsiveness. b.\nHow does Huawei SmartVirtualization facilitate storage consolidation, resource optimization, and simplification of storage management tasks? Storage Consolidation: Huawei SmartVirtualization enables the consolidation of multiple physical storage resources into a unified virtual storage pool. This means that various individual storage systems from different vendors or different models can be combined into a single virtual storage environment. By doing so, it creates a centralized and efficient storage infrastructure that is easier to manage and utilize. Resource Optimization: With storage consolidation in place, SmartVirtualization allows for better resource utilization and optimization. The virtualized storage pool can dynamically allocate storage capacity and performance resources to different applications and workloads based on their specific requirements. This flexibility prevents overprovisioning, reduces wasted resources, and ensures that storage is used efficiently, which can lead to cost savings and improved performance. Simplification of Storage Management Tasks: SmartVirtualization simplifies storage management through centralization and abstraction. Administrators can manage the entire storage infrastructure from a single management interface, regardless of the underlying physical storage devices. This reduces the complexity of managing multiple storage systems and simplifies tasks such as provisioning, data migration, and monitoring. Additional features that might be offered by Huawei SmartVirtualization include: Data Tiering: SmartVirtualization can intelligently move data between different tiers of storage based on its access frequency and importance. Frequently accessed data can be placed on high-performance storage, while less frequently accessed data can be moved to lower-cost, slower storage, optimizing the overall storage performance and cost.\nThin Provisioning: This feature allows administrators to allocate more logical storage capacity to applications than is physically available. It helps prevent overprovisioning and ensures that storage resources are used efficiently. Data Migration and Load Balancing: SmartVirtualization can dynamically move data between physical storage devices to balance the workload and optimize performance. It ensures that no single storage device is overwhelmed while others remain underutilized. Snapshots and Clones: SmartVirtualization typically offers snapshot and cloning capabilities, enabling fast and space-efficient backups, data protection, and testing environments. Friend, Huawei SmartVirtualization is a storage virtualization technology that can help to facilitate storage consolidation, resource optimization, and simplification of storage management tasks. Here are some of the ways that Huawei SmartVirtualization can help with storage consolidation: Data deduplication: Huawei SmartVirtualization can deduplicate data across multiple storage devices, which can help to reduce the amount of storage space required. Thin provisioning: Huawei SmartVirtualization can thin provision storage, which means that only the actual amount of data used is allocated. This can help to reduce storage costs and improve storage utilization. Virtual RAID: Huawei SmartVirtualization can create virtual RAID arrays across multiple storage devices, which can improve performance and reliability. Here are some of the ways that Huawei SmartVirtualization can help with resource optimization: QoS: Huawei SmartVirtualization can provide QoS (Quality of Service) policies, which can help to ensure that critical applications have the resources they need. Storage tiering: Huawei SmartVirtualization can tier data based on its access frequency, which can help to optimize performance and storage costs.\nCan you provide examples of use cases where Huawei SmartMigration has been employed for data center consolidation or technology refresh projects? Data Center Consolidation: Large enterprises often acquire other companies or accumulate data centers over time due to growth. Data center consolidation involves merging these scattered data centers into a central, more efficient facility. Use cases for this can include: a. Mergers and Acquisitions: When two companies merge, they may choose to consolidate their data centers to eliminate redundancies and reduce costs. b. Legacy Data Center Optimization: Replacing older, inefficient data centers with a modern, more energy-efficient facility. c. Cost Reduction: Consolidating data centers can lead to significant savings in terms of maintenance, energy consumption, and operational costs. Technology Refresh: Regularly updating and upgrading the technology in a data center is crucial to staying competitive, maintaining security, and improving efficiency. Some use cases include: a. Hardware and Infrastructure Upgrade: Refreshing outdated server hardware, storage systems, and networking equipment to improve performance and capacity. b. Virtualization and Cloud Adoption: Migrating physical servers to virtual machines or adopting cloud solutions to enhance flexibility and scalability. c. Security Enhancements: Upgrading security systems and implementing the latest cybersecurity measures to protect against emerging threats. While I don't have specific examples of Huawei SmartMigration being used in these scenarios, it's possible that Huawei or other vendors may have developed migration or consolidation solutions that help streamline and optimize the process.\nWhat are the advantages of using Huawei HyperSnap and HyperCDP for achieving near-zero data loss and rapid recovery in virtualized environments? I can highlight the general advantages these solutions aimed to offer for achieving near-zero data loss and rapid recovery in virtualized environments: Continuous Data Protection (CDP): HyperCDP likely provided continuous data protection capabilities, meaning it continuously captured changes to data in real-time, rather than relying on traditional periodic backup methods. This ensures that every data update is captured, reducing the potential for data loss to almost zero. Rapid Recovery: By using continuous data protection, HyperCDP could provide near-instantaneous recovery capabilities. In case of a data loss event, administrators could restore data to any previous point in time, minimizing downtime and reducing the impact on business operations. Application-Consistent Snapshots: HyperSnap likely supported application-consistent snapshots, which means that the snapshots taken were coordinated with the applications running on the virtual machines (VMs). This ensured that data was captured in a consistent state, avoiding potential data integrity issues during recovery. Space Efficiency: Both HyperSnap and HyperCDP might have employed space-efficient snapshot and replication technologies. These technologies ensure that the storage requirements for maintaining snapshots and recovery points are optimized, reducing the overall storage costs. Integration with Virtualization Platforms: These Huawei solutions were likely designed to seamlessly integrate with popular virtualization platforms like VMware vSphere and Microsoft Hyper-V. This integration simplifies deployment and management, as well as provides additional benefits like single-console management.\nMulti-Site Replication: HyperSnap and HyperCDP might have supported multi-site replication, allowing organizations to replicate data to remote locations for disaster recovery purposes. This feature enhances data protection by providing an additional layer of redundancy. Granular Recovery Options: The solutions might have offered granular recovery options, enabling administrators to recover individual files or application data from specific points in time without needing to restore the entire VM. Centralized Management: Huawei likely provided a centralized management interface to configure, monitor, and manage the data protection and recovery processes. This streamlined approach simplifies operations and enhances control over the backup and recovery environment. Data Encryption and Security: The solutions could have incorporated data encryption mechanisms to ensure data security during replication and backup processes. Automation and Policy-Based Management: Automation and policy-based management features might have been present, allowing administrators to define rules and policies for data protection, retention, and recovery, thereby reducing manual intervention. Before making any decisions about using Huawei HyperSnap and HyperCDP or any other data protection solutions, it's essential to conduct a thorough evaluation of the latest product features, performance, and compatibility with your specific virtualization environment. hello guy, Here are some of the advantages of using Huawei HyperSnap and HyperCDP for achieving near-zero data loss and rapid recovery in virtualized environments: Near-zero data loss: Huawei HyperSnap and HyperCDP use snapshot technology to create an exact copy of a virtual machine (VM) at a specific point in time. Rapid recovery: Huawei HyperSnap and HyperCDP can restore VMs from snapshots in a matter of seconds.\nHow does Huawei HyperReplication ensure data consistency and reliability in scenarios with high write-intensive workloads and frequent updates? Replication and Data Distribution: In distributed systems, data is often replicated across multiple nodes or data centers. This redundancy ensures that even if one node fails, data remains available from other replicas. Consistency Models: There are various consistency models that dictate how replicas are updated and synchronized. Some common models include: a. Strong Consistency: In this model, all replicas are updated synchronously, ensuring that all reads return the most recent write. This approach provides strong guarantees but may impact performance and availability, especially in geographically distributed systems. b. Eventual Consistency: This model allows replicas to be asynchronously updated, meaning that there may be a temporary inconsistency between replicas. Eventually, all replicas will converge to the same state. This approach prioritizes availability and performance over strict consistency. Conflict Resolution: In scenarios with frequent updates, conflicts may arise when concurrent writes are performed on different replicas. Conflict resolution mechanisms are essential to resolve these conflicts and ensure data consistency. Techniques like last-write-wins, timestamp ordering, or application-specific resolution rules can be used. Quorum-based Replication: Quorum-based approaches allow a subset of replicas to make decisions about updates. For example, a write operation might require a majority of replicas to agree before it's considered successful. This approach ensures data consistency even in the face of network partitions and failures. Failure Handling: To ensure reliability, systems must be capable of handling node failures gracefully.\nWhat are the key benefits of Huawei HyperMetro in terms of workload balancing, automatic failover, and improved application performance? Huawei HyperMetro is a feature offered by Huawei's OceanStor storage systems designed to enhance workload balancing, automatic failover, and improved application performance in a dual-controller (active-active) storage architecture. Here are the key benefits of Huawei HyperMetro: Workload Balancing: Load Distribution: HyperMetro enables both controllers (storage nodes) to actively serve data simultaneously, distributing the workload evenly between them. This load distribution prevents overloading of any single controller and optimizes resource utilization. Intelligent Data Migration: HyperMetro uses intelligent data migration algorithms to balance the data distribution between the controllers dynamically. This ensures that data is evenly distributed across the controllers based on access patterns, avoiding hotspots and optimizing performance. Automatic Failover: High Availability: HyperMetro provides high availability for applications by maintaining synchronized data between two active-active storage systems. In the event of a controller failure or link disruption, the system automatically fails over to the other controller seamlessly, ensuring uninterrupted service and minimal downtime. Transparent Failover: The failover process is transparent to the applications, meaning there is no disruption or impact on application operations during the failover process. This contributes to an improved user experience and higher service reliability. Improved Application Performance: Low Latency: With HyperMetro, data can be accessed from both controllers simultaneously, reducing data access latency. This leads to improved application response times and overall system performance.\nWhat are the deployment options available for the OceanStor Dorado 3000 series? All-Flash Storage Arrays: The OceanStor Dorado 3000 series primarily consists of all-flash storage arrays, offering high-performance storage capabilities for enterprises. These arrays utilize solid-state drives (SSDs) to provide low-latency access to data and support various data-intensive workloads. Block Storage: The OceanStor Dorado 3000 series is designed to provide block-level storage services. It can be used as a high-speed storage solution for applications requiring direct block-level access, such as databases and virtualized environments. File and Object Storage Protocols: In addition to block storage, the Dorado 3000 series can also support file and object storage protocols, such as NFS (Network File System) and S3 (Simple Storage Service). This makes it versatile for various use cases, including file sharing and unstructured data storage. Scale-Out Architecture: Some models within the Dorado 3000 series support a scale-out architecture, allowing enterprises to expand their storage capacity and performance by adding more nodes to the cluster. Scale-out architectures are particularly beneficial for handling large and growing datasets. High Availability Configurations: The Dorado 3000 series can be deployed in high availability configurations with multiple nodes. This ensures data availability and business continuity even in the event of hardware failures. Unified Storage: Some models may provide unified storage capabilities, which means they can offer both block and file-level storage services in a single system. This simplifies storage management and reduces the need for separate storage solutions for different workloads.\nHow does the OceanStor Dorado 3000 series ensure data reliability and availability? Redundant Components: The Dorado 3000 series is built with redundant components, such as power supplies, fans, and controllers. Redundancy helps to eliminate single points of failure and ensures the system can continue to operate even if a component fails. RAID Protection: The storage systems support various RAID (Redundant Array of Independent Disks) configurations, which distribute data across multiple drives in a way that provides data redundancy. This protects against data loss in case of a drive failure. SmartErase: This feature helps to prevent data leakage when drives need to be retired or replaced. It erases data on the drives securely to protect sensitive information. Snapshot and Replication: The OceanStor Dorado 3000 series supports snapshot and replication features. Snapshots allow point-in-time copies of data, which can be used for quick data recovery or testing. Replication enables data to be copied to a remote location, ensuring data availability even in the event of a site failure. Data Deduplication and Compression: These data optimization techniques help to reduce storage space usage and improve overall efficiency, enabling cost savings and better resource utilization. High Availability (HA) Configurations: The Dorado 3000 series can be deployed in high availability configurations with multiple nodes. In such setups, if one node fails, the workload automatically switches to another node, ensuring continuous access to data. Quality of Service (QoS): QoS features allow administrators to prioritize critical workloads and applications, ensuring they receive the necessary resources and performance levels even during heavy usage periods.\nData Integrity and Checksums: The storage systems implement data integrity checks and checksums to detect and correct errors in stored data, reducing the risk of data corruption. Data Scrubbing: Regular data scrubbing processes help to identify and correct latent disk errors, maintaining the health and reliability of stored data. Hello, dear. The OceanStor Dorado 3000 series ensures data reliability and availability through several features: 1. SmartMatrix architecture: The SmartMatrix architecture is a distributed RAID technology that ensures data reliability and availability by distributing data across multiple disks and nodes. This architecture ensures that data is always available even if one or more disks or nodes fail. 2. HyperMetro: HyperMetro is a disaster recovery solution that ensures data availability by synchronously replicating data between two data centers. In the event of a disaster, the system automatically switches to the secondary data center, ensuring uninterrupted access to data. 3. SmartErase: SmartErase is a data erasure technology that ensures data security by securely erasing data from SSDs before they are decommissioned or repurposed. This technology ensures that sensitive data is not accessible to unauthorized users. 4. SmartPartition: SmartPartition is a data isolation technology that ensures data reliability and availability by isolating data from different applications and tenants. This technology ensures that data is always available and does not get corrupted or lost due to conflicts between different applications or tenants. 5. SmartQuota: SmartQuota is a storage management technology that ensures data availability by allocating storage resources based on user requirements.\nWhat is the process for installing and configuring the OceanStor Dorado All-Flash Storage? Pre-installation preparations: a. Verify that all the necessary hardware components have been delivered, including the OceanStor Dorado storage array, rackmount kits, power cords, and network cables. b. Ensure that the installation site meets the environmental requirements (temperature, humidity, ventilation, etc.) specified in the product documentation. c. Review the installation planning and ensure that any prerequisites, such as network configurations, are in place. Physical installation: a. Mount the OceanStor Dorado storage array in the designated rack using the provided rackmount kits. b. Connect the power cords to the power sources and the power supply units on the storage array. c. Connect the network cables to the appropriate ports on the storage controllers and the network switches. Power on and initial setup: a. After the hardware is installed and powered on, the storage system will boot up. b. Follow the on-screen instructions and connect to the system through the console or a web-based interface to perform the initial configuration. c. Set the system's basic network settings, such as IP addresses, subnet masks, and gateway information. RAID Configuration: a. Set up the RAID groups and storage pools based on your performance and capacity requirements. b. Choose the appropriate RAID level and disk group configuration to optimize performance and data protection. LUN (Logical Unit Number) creation: a. Create LUNs from the available storage pools to present them to the connected servers or hosts. b. Configure LUN properties, such as size, access control, and performance policies.\nNetwork configuration: a. Set up network interfaces, including Ethernet ports, Fiber Channel ports, and iSCSI ports. b. Configure networking options such as link aggregation and VLANs if needed. Data protection and backup: a. Implement data protection mechanisms, such as snapshots and replication, to ensure data resilience and disaster recovery capabilities. Performance optimization: a. Fine-tune storage performance parameters based on workload requirements. b. Monitor and manage the system to ensure optimal performance and resource utilization. Integration with existing infrastructure: a. If integrating the OceanStor Dorado All-Flash Storage with an existing environment, configure any necessary host initiators, zoning, and multipathing on the host side. Documentation and maintenance: a. Document the entire setup process, including configurations and network details. b. Establish a regular maintenance schedule for updates, patches, and monitoring. Hello, dear. To install and configure Huawei OceanStor Dorado All-Flash Storage, perform the following steps: 1. Connect the OceanStor Dorado All-Flash Storage to a computer or server. 2. Download and install drivers: Download and install drivers based on the storage device model and operating system version to ensure that the storage device can properly connect to and work with the computer or server. 3. Configure the storage controller: Configure the OceanStor Dorado All-Flash Storage controller on the computer or server, and set the access permission and data backup and restoration policies for the storage device. 4. Install and configure OceanStor Dorado: Install and configure OceanStor Dorado on computers or servers, and set the communication protocol, data format, and access permission between storage controllers and storage devices. 5.\nWhat are some real-world case studies or customer success stories involving the implementation of the OceanStor Dorado storage systems? High-Performance Databases: OceanStor Dorado systems could handle the demanding I/O requirements of high-performance databases, such as Oracle, Microsoft SQL Server, and SAP HANA, ensuring fast data access and low latency for critical applications. Virtualized Environments: Virtualized environments, like VMware or Hyper-V, require efficient and reliable storage solutions. OceanStor Dorado storage systems were suitable for these scenarios, offering excellent performance and integration with virtualization platforms. Big Data Analytics: When dealing with vast amounts of data in big data analytics applications, a storage system with high throughput and low latency becomes crucial. OceanStor Dorado could serve as a backend storage solution for big data workloads. Cloud Infrastructure: For private or hybrid cloud deployments, OceanStor Dorado storage systems provided the necessary scalability and performance to support cloud-based services and applications. Artificial Intelligence (AI) and Machine Learning (ML): AI and ML workloads often require storage systems capable of handling large datasets and delivering high-speed data processing. OceanStor Dorado could support AI/ML use cases effectively. Video Surveillance: In video surveillance applications, where constant data streaming and real-time access are essential, OceanStor Dorado storage systems could ensure reliable and speedy data storage. Backup and Disaster Recovery: For organizations requiring robust backup and disaster recovery solutions, OceanStor Dorado offered features like snapshotting, replication, and data protection capabilities.\nHow can I troubleshoot common issues or errors encountered on the OceanStor Dorado storage systems? Troubleshooting common issues or errors on OceanStor Dorado storage systems can help you identify and resolve problems to ensure the system's optimal performance and reliability. Here are some steps you can take to troubleshoot common issues: Check system logs: Access the system logs to look for any error messages, warnings, or alerts. The logs can provide valuable information about the root cause of the issue. Review system documentation: Consult the product documentation and user manuals provided by Huawei. They often contain troubleshooting guides and solutions to common problems. Vendor support: Contact Huawei's technical support team for assistance. They have experienced engineers who can help you diagnose and resolve complex issues. Monitor performance metrics: Use monitoring tools or the centralized management interface (eSight) to track the system's performance metrics. Analyzing performance data can help you identify bottlenecks and performance-related issues. Check hardware components: Ensure that all hardware components are properly installed and functioning correctly. Check for any faulty or failed components such as disks, controllers, or power supplies. Firmware and software updates: Check if the storage system's firmware and software are up-to-date. Upgrading to the latest versions can often resolve known issues and improve system stability. Network connectivity: Verify the network connectivity between the storage system and connected hosts. Misconfigurations or network-related issues can impact performance and data access. RAID configurations: Check the RAID configurations and rebuild status.\nIf there are any degraded or failed RAID groups, take appropriate actions to restore redundancy. LUN and storage pool settings: Review the LUN and storage pool configurations to ensure proper allocation and capacity utilization. Incorrect settings can lead to performance issues or storage space shortages. Data consistency and integrity: Verify data consistency and integrity using data scrubbing or data consistency check utilities. Backup and disaster recovery: Ensure that regular backups are performed, and disaster recovery plans are in place to protect against data loss. Environmental factors: Check the storage system's operating environment, such as temperature and humidity, to ensure it meets the recommended specifications. When troubleshooting common issues or errors on OceanStor Dorado storage systems, you can follow these steps: Identify the issue: Determine the nature of the problem or error encountered. This could be related to performance degradation, connectivity issues, data corruption, or specific error messages. Check system status: Use the management interface, such as OceanStor DeviceManager, to check the overall system status, including the health of components, storage pools, and volumes. Look for any alerts or warnings that could be related to the issue. Review logs and error messages : Access the system logs and error messages to gather more detailed information about the problem. Look for any specific error codes, descriptions, or timestamps that can help pinpoint the cause of the issue. Verify network connectivity : Ensure that the network connectivity between the storage system and the client servers is functioning correctly.\nHi guys!! I would like to share with you about What is Memory Swapping? How Memory Swapping Works? Advantages of Memory Swapping. How Does Memory Swapping Improve Performance? What is Memory Swapping? Memory swapping, also known as virtual memory swapping, is a technique used by operating systems to manage the memory resources of a computer system. It involves transferring data between the computer's RAM (Random Access Memory) and the hard disk or other storage devices to free up memory space for other processes or data. When a computer system runs multiple processes simultaneously and the available RAM becomes insufficient to accommodate all the data and programs, the operating system may employ memory swapping to optimize memory usage. How Memory Swapping Works? Memory swapping, also known as virtual memory swapping, is a mechanism used by operating systems to manage memory resources efficiently when the physical RAM (Random Access Memory) becomes insufficient to accommodate all the active processes and data. Here's a simplified explanation of how memory swapping works: Paging and Page Tables: The operating system divides the physical memory into fixed-size blocks called pages. Similarly, the secondary storage (such as a hard disk) is divided into blocks called page frames. Each process has its own virtual address space, which is divided into pages. The operating system maintains a data structure called a page table for each process, which maps the virtual memory addresses used by the process to the corresponding physical memory addresses.\nMemory Demand and Page Faults: When a process needs to access a page that is not currently present in the physical RAM, it triggers a page fault. A page fault occurs when the CPU encounters a virtual memory address that is mapped to a page that is not resident in RAM. Swapping Out (Paging Out): To free up memory space, the operating system identifies pages that are less frequently used or not currently needed by active processes. It selects one or more pages and swaps them out from the RAM to secondary storage, typically a hard disk. The content of the selected pages is written to the page frames on the secondary storage. Swapping In (Paging In): When a page fault occurs for a specific virtual memory address, indicating that the required page is not present in RAM, the operating system retrieves the corresponding page from the secondary storage and brings it back into RAM. The page is loaded into a free page frame, and the page table is updated to reflect the new location of the page in RAM. Page Replacement: If the RAM is already full and there are no free page frames available, the operating system uses page replacement algorithms to select a victim page that will be swapped out to make room for the incoming page. Common page replacement algorithms include the Least Recently Used (LRU) algorithm, where the least recently used page is chosen for replacement.\nAccessing Swapped Pages: When a process tries to access a page that has been swapped out, a page fault occurs, and the page is swapped back into RAM. The process resumes execution once the required page is available in the RAM. Memory swapping allows the operating system to efficiently manage memory resources by moving infrequently accessed or less critical pages to secondary storage. This helps maintain the illusion of a larger memory space for each process, even when the physical RAM is limited. However, swapping introduces additional overhead due to the slower access times of secondary storage compared to RAM, which can impact overall system performance. Therefore, the operating system employs various optimization techniques and algorithms to minimize the frequency of swapping and prioritize keeping frequently used pages in RAM. Advantages of Memory Swapping: Increased Memory Capacity: Memory swapping allows the operating system to use secondary storage, such as a hard disk, as an extension of the physical RAM. This effectively increases the available memory capacity beyond the physical limitations of the system. It enables running larger applications or multiple processes concurrently without running out of memory. Efficient Memory Management: Swapping helps optimize memory usage by moving less frequently accessed or inactive pages to secondary storage. This frees up valuable RAM space for active processes and data, allowing the system to prioritize resources based on demand. Swapping allows for better utilization of memory resources and can prevent or reduce the likelihood of out-of-memory errors. Process Isolation and Protection: Memory swapping enables process isolation and protection.\nEach process operates in its own virtual address space, isolated from other processes. Swapping ensures that one process cannot access or modify the memory of another process, enhancing system security and stability. Flexibility and Scalability: Memory swapping provides flexibility and scalability to accommodate varying memory demands. The system can adapt to changing memory requirements dynamically by swapping pages in and out as needed. This allows for the efficient allocation and deallocation of memory resources, supporting the execution of diverse workloads without requiring constant physical RAM upgrades. Virtual Memory Abstraction: Swapping is an integral part of virtual memory abstraction. It allows the operating system to present a uniform and consistent view of memory to applications and processes, regardless of the underlying physical resources. Applications can operate under the assumption of having a larger memory space, simplifying programming and improving portability. Page Sharing and Copy-on-Write: Swapping facilitates page sharing and copy-on-write techniques. Pages that are identical or read-only can be shared among multiple processes, reducing memory duplication and conserving resources. When a shared page needs modification, the copy-on-write mechanism ensures that only the modified portion is copied, minimizing memory overhead. Transparent to Applications: From the perspective of applications, swapping is transparent. The operating system manages the swapping process, ensuring that required pages are brought into memory when needed. Applications can focus on their tasks without explicit awareness of swapping operations. While memory swapping offers several advantages, it's important to note that excessive swapping or frequent swapping operations can impact system performance.\nSwapping involves slower disk access compared to RAM, which can introduce delays. Therefore, careful memory management, including optimizing page replacement algorithms and ensuring sufficient physical RAM, is essential to maintain an efficient balance between memory usage and performance. How Does Memory Swapping Improve Performance? Memory swapping, or virtual memory swapping, is primarily designed to help manage memory resources efficiently rather than directly improving performance. However, in certain scenarios, memory swapping can indirectly contribute to performance improvement in computer systems. Here are a few ways memory swapping can have a positive impact on performance: Larger Memory Capacity: Memory swapping allows the operating system to use secondary storage, such as a hard disk, as an extension of the physical RAM. This effectively increases the available memory capacity. With more memory available, the system can accommodate larger applications or a higher number of concurrent processes without running out of memory. This can prevent or reduce memory-related performance issues, such as out-of-memory errors or excessive swapping due to memory exhaustion. Effective Memory Utilization: By swapping out less frequently used or inactive pages to secondary storage, memory swapping optimizes memory usage. It frees up valuable RAM space for actively running processes and data. This efficient utilization of memory resources can reduce contention for limited physical memory and help ensure that active processes have sufficient memory to operate effectively. It can prevent or mitigate performance degradation due to memory constraints. Flexible Workload Management: Memory swapping allows for flexible workload management in systems with varying memory demands.\nIt dynamically adjusts the allocation of memory resources based on the needs of active processes. Swapping enables the system to adapt to changing memory requirements by bringing in required pages from secondary storage and swapping out less critical or inactive pages. This flexibility in managing memory helps maintain system responsiveness and performance even when memory demands fluctuate. Virtual Memory Abstraction: Memory swapping is an integral part of the virtual memory abstraction provided by modern operating systems. It allows applications and processes to operate under the assumption of having a larger memory space than physically available. This abstraction simplifies programming, as applications can address virtual memory without the need to explicitly manage physical memory. The virtual memory abstraction enhances portability, allowing applications to run on systems with varying memory capacities. It's important to note that memory swapping can introduce performance overhead due to the slower access times of secondary storage compared to RAM. Excessive swapping or frequent swapping operations can lead to increased disk I/O operations, which may impact overall system performance. Therefore, the efficient management of memory resources, including optimizing page replacement algorithms, maintaining an appropriate balance between physical RAM and virtual memory, and minimizing the frequency of swapping, is crucial to ensuring optimal system performance. Memory Swapping Example: The interchange of processes is known as swapping. Process exchange also use priority-based preemptive scheduling.\nHi everyone! This time, I will share with you an important article about Difference between DRAM SSDs and DRAM-less SSDs. What are DRAM SSDs? DRAM SSDs (Solid State Drives) refer to a type of storage device that combines the characteristics of both DRAM (Dynamic Random Access Memory) and traditional SSD technology. DRAM is a type of volatile memory commonly used in computers and other electronic devices. It is fast and provides low-latency access to data. However, DRAM requires a continuous power supply to retain data, meaning that it loses its stored information when power is removed. On the other hand, SSDs are non-volatile storage devices that use flash memory to store data. They are known for their faster data access speeds, improved durability, and lower power consumption compared to traditional hard disk drives (HDDs). DRAM SSDs are designed to leverage the benefits of both DRAM and SSD technologies. They use DRAM as the primary storage medium to provide extremely fast access to data. However, to overcome the volatile nature of DRAM and ensure data persistence, these SSDs include backup power sources and mechanisms to flush data from DRAM to non-volatile storage during power loss or system shutdown. The combination of DRAM and SSD technologies in DRAM SSDs aims to provide high-performance storage solutions for applications that require ultra-low latency and high-speed data processing, such as high-frequency trading, real-time analytics, and certain database workloads. What is DRAM-less SSDs?\nDRAM-less SSDs (Solid-State Drives) are a type of storage device that do not include a built-in DRAM (Dynamic Random Access Memory) cache. DRAM is commonly used in SSDs as a cache to improve performance and overall responsiveness. The following are key points to understand about DRAM-less SSDs: Cache Functionality: In traditional SSDs with DRAM, the cache serves as a temporary storage buffer that holds frequently accessed data. This allows for faster read and write operations, as data can be retrieved or written from the cache instead of directly accessing the slower NAND flash memory. DRAM-less Design: DRAM-less SSDs, as the name suggests, lack built-in DRAM cache. Instead, they rely on other techniques and technologies to manage and optimize storage performance without the use of a dedicated cache. Controller Optimization: DRAM-less SSDs typically employ advanced controller technologies and algorithms to efficiently handle data operations. These controllers utilize internal buffers, small on-chip caches, or host system memory to store metadata and manage data flow, ensuring efficient data access and transfer. DRAM-less SSDs may utilize various techniques to optimize the performance of the NAND flash memory itself. This can include using faster NAND flash chips, implementing advanced error correction mechanisms, and employing wear-leveling algorithms to extend the lifespan of the NAND flash. DRAM-less SSDs tend to be more cost-effective compared to SSDs with built-in DRAM caches. The absence of a dedicated cache reduces manufacturing costs, making DRAM-less SSDs more affordable for consumers.\nDRAM-less SSDs are often used in entry-level or budget-oriented systems, as well as in applications that prioritize cost efficiency over absolute performance. While DRAM-less SSDs may not offer the same level of performance as SSDs with a dedicated DRAM cache, they can still provide noticeable improvements over traditional hard disk drives (HDDs) in terms of data access speeds, power efficiency, and overall system responsiveness. It's important to note that the absence of a DRAM cache in DRAM-less SSDs may result in lower random-access performance, especially for small, random read or write operations. However, advancements in controller technologies and optimizations have allowed DRAM-less SSDs to offer competitive performance levels, particularly in sequential read and write operations. When considering an SSD purchase, it is recommended to review the specifications, performance benchmarks, and user reviews to assess the overall performance and suitability of a specific DRAM-less SSD for your intended use case and requirements. Difference between DRAM SSDs and DRAM-less SSDs: DRAM SSDs and DRAM-less SSDs differ primarily in terms of their internal architecture and performance characteristics. Internal Architecture: DRAM SSDs: These SSDs include both DRAM and NAND flash memory. DRAM serves as a cache layer, storing frequently accessed data for faster retrieval. The DRAM cache allows for high-speed read and write operations, resulting in improved overall performance. DRAM-less SSDs: As the name suggests, these SSDs do not have a DRAM cache. They rely solely on NAND flash memory for data storage and retrieval.\nPerformance: DRAM SSDs: The presence of a DRAM cache enables faster data access and transfer speeds. DRAM SSDs offer lower latency, high IOPS (Input/Output Operations Per Second), and better random read/write performance. They are well-suited for applications that demand high-speed data processing and low-latency operations. DRAM-less SSDs: Without a DRAM cache, these SSDs typically have lower performance compared to DRAM SSDs. However, advancements in NAND flash memory technology and controller optimizations have allowed DRAM-less SSDs to offer respectable performance, especially for sequential read/write operations. Cost and Capacity: DRAM SSDs: The inclusion of DRAM as a cache layer increases the cost of these SSDs. Additionally, the DRAM cache occupies a portion of the SSD's overall capacity. As a result, DRAM SSDs tend to be more expensive and offer slightly lower usable storage capacity compared to DRAM-less SSDs. DRAM-less SSDs: The absence of DRAM cache reduces the cost of these SSDs, making them more affordable. They also tend to offer slightly higher usable storage capacity compared to DRAM SSDs. Use Cases: DRAM SSDs: Due to their superior performance, DRAM SSDs are often used in scenarios that demand low latency and high IOPS, such as gaming, virtualization, and high-performance computing. They excel in applications that require quick data access and fast data processing. DRAM-less SSDs: These SSDs are commonly used in budget-oriented systems, laptops, and consumer-grade devices where cost efficiency is prioritized over absolute performance. They are suitable for general-purpose computing tasks and everyday storage needs.\nIt's important to note that SSD technology is continually evolving, and newer generations of SSDs may blur the performance gap between DRAM SSDs and DRAM-less SSDs. The choice between the two depends on specific requirements, budget constraints, and the intended use case. Why Not Use SSDs with DRAM or SSDs without DRAM? While DRAM SSDs and DRAM-less SSDs have their respective advantages and use cases, there are some considerations where their usage may not be appropriate or may not provide optimal performance. The following are a few reasons why you might want to avoid using DRAM SSDs or DRAM-less SSDs in certain scenarios: Cost: DRAM SSDs are typically more expensive compared to DRAM-less SSDs due to the inclusion of DRAM as a cache layer. If cost is a significant factor, and the application or workload doesn't require extremely high performance, opting for traditional SSDs or HDDs might be more cost-effective. Power Dependency: DRAM-based SSDs require continuous power to retain data stored in the DRAM cache. In case of power loss or system shutdown, the data in the cache may be lost. If data persistence during power outages or unexpected shutdowns is crucial for your application, you may prefer a non-volatile storage solution like traditional SSDs that use NAND flash memory. Capacity Considerations: DRAM caches in DRAM SSDs occupy a portion of the overall storage capacity, reducing the usable space available for data storage.\nHow Does PCIe 5.0 Affect SSD Storage? Hello, dear. PCIe 5.0 is the latest version of the Peripheral Component Interconnect Express (PCIe) interface, which is used to connect various components in a computer system, including storage devices like SSDs. PCIe 5.0 offers significant improvements over its predecessor, PCIe 4.0, in terms of speed and bandwidth. The increased speed and bandwidth of PCIe 5.0 can have a significant impact on SSD storage. With PCIe 5.0, SSDs can achieve much higher data transfer rates, which means faster read and write speeds. This can be particularly beneficial for applications that require high-speed data access, such as gaming, video editing, and data analysis. In addition, PCIe 5.0 can also increase the number of lanes available for SSDs, which can improve overall system performance. This is because more lanes mean more data can be transferred simultaneously, reducing the potential for bottlenecks and improving overall system responsiveness. Overall, PCIe 5.0 is a significant improvement over previous versions of the PCIe interface, and it can have a significant impact on the performance of SSD storage. As more systems adopt PCIe 5.0, we can expect to see even faster and more efficient SSDs that can take full advantage of this new technology. Hello, dear. PCIe 5.0 is the latest version of the Peripheral Component Interconnect Express (PCIe) interface, which is used to connect various components in a computer system, including storage devices like SSDs. PCIe 5.0 offers significant improvements over its predecessor, PCIe 4.0, in terms of speed and bandwidth.\nWhat are the SAN components must be redundant to avoid single points of failure and to improve system reliability? The SAN components that should be redundant to avoid single points of failure and improve system reliability are: Storage Controllers: Redundant storage controllers ensure continuous access to data in case of a failure in one controller. They help maintain system availability and prevent downtime. Power Supplies: Redundant power supplies ensure continuous power to the SAN, even if one power supply fails. This prevents power-related outages and helps maintain system reliability. Network Switches: Redundant network switches provide alternate pathways for data traffic in case of a network switch failure. This helps prevent network disruptions and ensures continuous connectivity. Network Interfaces: Redundant network interfaces allow data to be transferred through multiple paths, ensuring uninterrupted access to storage resources and preventing single-point failures. Disk Drives: Redundant disk drives ensure data availability even if one drive fails. RAID configurations with mirroring or parity mechanisms can be used to protect against disk failures and improve system reliability. Cables and Connectors: Redundant cables and connectors help avoid single-point failures in data transmission. Multiple paths for data transfer ensure continuous connectivity and minimize the impact of cable or connector failures. Overall, redundant components in the SAN infrastructure eliminate single points of failure, improve system reliability, and ensure continuous access to data and resources.\nInput/Output Operations Per Second (IOPS) Introduction In today's data-driven environment, effective data management and processing are critical. Storage systems play a critical role in guaranteeing efficient data access and retrieval, making it vital that you precisely measure their performance. Input/Output Operations Per Second (IOPS) and throughput are two significant measures to measure storage performance. IOPS and throughput, on the other hand, are fundamentally different and assess various aspects of performance. Understanding the difference between IOPS and throughput is becoming increasingly important as modern firms continue to adopt more digital technologies and rely on non-hardware storage systems such as cloud and virtualization for their data. What is IOPS? The IOPS figures indicate the number of read and/or write operations that thestorage device can do per second. Hard Disc Drives (HDD) typically have 512B or 4KB blocks, nevertheless,contemporary Solid-State Drives (SSD) expose storage capacity in pages connected in blocks that can approach 512KB in size. The IOPS numberreveals nothing about how much data a drive can handle. This amount is determined by the number of IOPS as well as the block size (the maximum number of bits/bytes that can be allotted to a single I/O operation). For example, given the same IOPS value, the drive with the larger block size may process (read or write) more data. Furthermore, IOPS can vary depending on whether data is accessed sequentially or randomly. IOPS is often higher for sequential writes on HDDs since the disc head may quickly access dependent blocks.\nRandom reads and writes, on the other hand, necessitate the movement of the disc head to find the required place. The IOPS value can also differ across read and write operations. IOPS can be classified into four groups for these reasons: Performance Characteristics of IOPS The most typical performance criteria measured are sequential operations and random operations. Sequential operations continuously access locations on storage devices and are typically associated with greater data transfer amounts, such as 128 kB. Random operations use non-continuous access to storage device locations and are often associated with modest data transfer volumes, such as 4kB. Let us explore the most prevalent characteristics: Parameter Description Total IOPS (Total IOPS) Random Read IOPS The number of random read I/O operations performed per second on average. Random Write IOPS Sequential Read IOPS A numberof sequential read I/O operations performed per second on average. Sequential Write IOPS Measuring IOPS Calculating IOPS can be difficult because there are so many variables that influence throughput and performance. The type of drive is also a factor to consider, therefore an SSD will be evaluated differently than an HDD of the same size. Because some RAIDs have a performance penalty, the type of RAID used in an array will also be a factor. Because it must distribute parity across all discs, RAID 6 has a much higher penalty than RAID 0.\nIn general, IOPS are calculated as follows: IOPS vs. Throughput in Storage Networks Throughput quantifies the quantity of data transferred per unit of time, whereas IOPS monitors the number of individual I/O operations. Typically, throughput is measured in bytes per second or gigabits per second. Despite their similarities, IOPS and throughput indicate different aspects of storage system performance: Latency vs. Bandwidth: IOPS is associated with latency, or the time it takes to perform eachI/O transaction, whereas throughput is concerned with the overall amount of data transferred. IOPS is necessary for responsive applications, whereas throughput is required for fast data transmission rates. Small vs. Large I/O Sizes: IOPS is more sensitive to small-sized I/O activities like random read/write. Throughput is a superior metric for measuring performance for large-scale sequential processes like video streaming or massive file transfers. These activities entail the continual transport of massive amounts of data. Workload Variability: IOPS and throughput needs differ based on the workload, for example, a database application requires high IOPS for quick transaction processing and a video streaming service requires high throughput for smooth playing. Assessing the workload characteristics is essential for choosing the best storage option. It's worth noting that IOPS and throughput are not mutually exclusive measurements, and both contribute to overall storage performance. Based on the unique application needs, storage systems must be optimized to achieve an acceptable balance of IOPS and throughput.\nHi, everyone, In today's article, we are going to discuss about data governance framework. A data governance framework is the backbone of an organization's data governance program, outlining a clear roadmap for how to effectively manage data quality, security, discoverability, accessibility, and usability. It acts as a blueprint, providing a structured approach to implementing data governance policies and procedures, ensuring that data is trustworthy and reliable throughout its entire lifecycle. With a well-defined data governance framework in place, organizations can enhance their decision-making capabilities and gain a competitive edge by leveraging the full potential of their data assets. It is different from Data Management in some ways as Data governance and Data management are two essential components of effective data utilization. While data governance involves the creation of policies and standards for managing data, data management focuses on the practical aspects of handling data daily. From acquisition to archival, the entire data lifecycle is governed by data governance, which ensures that data is handled in a way that complies with established policies. Meanwhile, data management handles the nuts and bolts of storing, organizing and retrieving data, so that it can be used to generate insights and drive business outcomes. When combined, these two processes help organizations unlock the full potential of their data and gain a competitive advantage in today's data-driven economy. A data governance framework is crucial for businesses as it enables the definition and documentation of standards and norms, accountability, ownership, roles and responsibilities related to data management.\nIn addition, a data governance framework establishes various critical components such as: - Key quality indicators (KQIs) - Key data elements (KDEs) - Key performance indicators (KPIs) - Metrics to measure data risk and privacy - Policies and processes governing data management - A shared business vocabulary and semantics - Rules to ensure data quality Having a data governance framework in place ensures that data is managed effectively, following standardized procedures, and is of high quality, which ultimately facilitates informed decision-making and supports business success. Figure 1: Data Governance The primary objective of a data governance framework is to maximize the return on data. A data governance framework can help businesses capture crucial opportunities to leverage their data assets while minimizing risks associated with inappropriate data use or exposure. To assess data governance readiness and maturity, the following critical factors should be considered: Figure 2: Defining the Critical Factors Collaboration and planning are key to successful data governance. People are responsible for determining technology requirements, defining policies and processes and driving data governance outcomes to support strategic objectives. It is important to consider if people are committed to data governance if their roles and responsibilities are clearly defined, if they possess the necessary skills and data literacy, and if a change management plan is in place to support alignment and buy-in. Data management should be integrated throughout the enterprise to ensure that critical business processes rely on trusted data.\nIt is important to consider if data definitions, rules and goals are realistic and appropriate, if business processes have been modernized, and if business rules have been reviewed to integrate data governance and deliver meaningful results. These are the subject matter experts in business and IT who provide the necessary context. Contributors include business leaders, process owners, stewards, IT architects, analysts and systems experts. It is important to identify data governance stakeholders and determine where contributors maintain expertise across the organization. : Platforms, tools and subject matter expertise enable reliable data governance processes. Even when some governance is already in place, platform technology enablers can improve outcomes. Data profiling, lineage, and metadata tools can automate and scale data governance processes, accelerating time to value. Figure 3: Data Governance Framework Here's a step-by-step guide to building an effective data governance framework: - Review and question the purpose of data governance, ensuring it covers all data assets. - Periodically revisit and update your data governance structure to accommodate growing data volumes, new data streams, and access points. - Foster organization-wide data sharing and collaboration. - Identify and standardize data domains across the organization. - Correspond domains to different functions generating data, such as finance, marketing, sales, and others. - Ask questions such as which are the prominent data domains in our organization, what data they generate, where is that data now and who consumes that data. - Assign data owners to each domain and understand its data consumption pattern.\n- Ensure each domain creating data is responsible for managing it and ensuring its security, integrity and privacy. - Determine who is creating and consuming data within each domain and what the current dependencies are to get access to domain data. - Standardize data domain definitions, data flow rules and workflows, access policies and more by documenting everything. - Address questions such as where does data originate from, what does it mean, how does it flow through the organization, and does it help domains meet their goals and support the organizations business outcomes. - Set up a modern data workspace that uses active metadata to keep your documentation relevant, fresh and useful. - Set up processes to conduct frequent data security and risk assessments for each domain. - Ensure that existing data access policies and security checks for data from each domain are up-to-date. - Determine who is allowed to access what data and why, and whether policies mitigate risks without creating data discovery, access, and collaboration bottlenecks. By following these steps, you can build a decentralized, community-led data governance framework that works for everyone in your organization. While data governance initiatives can provide significant benefits, they also encounter numerous obstacles that need to be overcome to achieve success. These obstacles include: One of the biggest challenges in launching a data governance program is aligning stakeholders across the organization. There must be in agreement on what constitutes key data assets, as well as their definitions and formats.\nThis may be easier for data that falls under regulatory policies such as customer data, but other datasets, such as product-specific data that falls under master data management (MDM), maybe more challenging to agree upon. : Effective data governance programs require sponsorship at both the executive and individual contributor levels. Chief Data Officers (CDOs) and data stewards are critical in communicating and prioritizing data governance within an organization. The Chief Data Officer can oversee and enforce accountability across data teams, ensuring that data governance policies are adopted. Data stewards can promote policy awareness to data producers and consumers, encouraging compliance across the organization. In the absence of proper tools and data architecture, companies will struggle to implement an effective data governance program. For instance, teams may uncover redundant data across different functions, but data architects must devise suitable data models and architectures to integrate and merge data across storage systems. Teams may also need to adopt a data catalog to inventory data assets or develop a metadata management process to ensure that the underlying data remains relevant and up-to-date. Data is essential for any organization to succeed, but without good data governance, it can be difficult to extract value from it. Unfortunately, most governance programs today are ineffective, either because they lack the support of top-level executives or rely too heavily on technology solutions. As a result, organizations miss out on opportunities and waste resources. For example, data processing and cleanup can consume over half of an analytics team's time, limiting scalability and frustrating employees.\nBy investing in good data governance, organizations can ensure that their data is readily available, of high quality and relevant, and that employees can focus on value-added tasks. Huawei has developed a comprehensive framework for Data Governance that covers a wide range of principles, processes, tools, standards and architecture. The data governance process comprises various stages such as generation, integration, analysis and consumption, and it requires well-defined rules to ensure effective management. Huawei's approach to data governance involves the creation of dedicated organizations responsible for data management and reporting to the corporate data management department. Huawei has also established cross-domain joint operation teams and a unified data management framework that classifies data by domain. To ensure data quality and operational efficiency, Huawei has implemented a framework based on ISO 8000 standards, which is reviewed twice a year. Additionally, data owners release quality reports periodically to drive continuous improvement in their respective domains. Huawei has also developed a unified information architecture that outlines the definitions and compositions of the information architecture in policy documents. Huawei's Data Governance framework is built on a set of principles that guide the management and use of data. These principles include accountability, transparency, security, compliance and ethics. Huawei's Data Governance framework includes a set of processes that enable organizations to manage their data effectively. These processes include data classification, data retention, data access and authorization, data quality management, data privacy and security. Huawei's Data Governance framework provides a range of tools that enable organizations to manage their data effectively.\nDear Community Members, Data migration is a critical process that allows organizations to transfer data from one storage system to another, enabling hardware upgrades, capacity expansion, or technology advancements. Huawei, a global leader in information and communication technology solutions, offers a comprehensive suite of storage products and solutions. In this article, we will delve into the data migration techniques offered by Huawei and explore how they ensure a seamless transition while optimizing data performance. Host-based data migration is a technique in which the data movement is managed by servers or hosts. Huawei provides several host-based migration methods, including file-based data migration and block-based data migration. enables administrators to move files from source to target storage systems while maintaining metadata and permissions intact. The FMS solution and efficient file tracking mechanisms, reducing migration time and resource utilization. are enterprise-level storage systems designed to provide high-performance, reliability, and scalability for data-intensive workloads. The File Migration Service (FMS) is a software feature included in these storage solutions, enabling administrators to perform efficient file-based data migration. FMS allows organizations to migrate files between different storage systems while preserving file metadata, permissions, and access control lists (ACLs). It optimizes the migration process by leveraging parallel processing and intelligent tracking mechanisms, reducing the time and resources required for data transfer. This feature is particularly useful when organizations need to upgrade their storage infrastructure, consolidate file data, or perform technology refreshes without disrupting business operations.\nHuawei's storage systems support block-based migration techniques, such as and , which facilitate the seamless transfer of data between storage arrays. LUN Copy allows for the direct cloning of logical units (LUNs) between storage systems, while SmartMigration optimizes data movement by dynamically adjusting migration speed based on real-time performance analysis. Huawei's array-based data migration techniques are designed to transfer data between storage systems within the same or different storage arrays. These methods provide high-performance migration capabilities with minimal host involvement. Huawei's Remote Replication feature, including technologies like and , enables efficient data mirroring and snapshot-based replication between source and target storage systems. This technique is suitable for disaster recovery scenarios, ensuring data integrity and availability during migration. Huawei's Data Mobility Manager simplifies data migration within heterogeneous storage arrays. DMM enables migration across different storage platforms without disrupting business operations. DMM utilizes intelligent algorithms to optimize data migration, reducing migration time and resource consumption. Network-based data migration techniques leverage dedicated migration networks to transfer data between storage systems. Huawei's solutions in this area focus on minimizing network impact and maximizing migration efficiency. SmartMigration for SAN allows administrators to transfer large volumes of data across storage area networks (SANs) with minimal disruption to production environments. The solution utilizes agents on hosts to optimize data migration and provide detailed progress tracking. Huawei's SmartMigration for NAS streamlines the migration process for The solution minimizes service downtime and offers extensive compatibility with various NAS protocols.\nWhat is a data fabric and How Does a Data Fabric Work? Hello, dear. A data fabric is a unified architecture that enables organizations to manage and access data from multiple sources, locations, and formats in a seamless and efficient manner. It provides a single view of data across the entire organization, regardless of where the data resides or how it is stored. A data fabric works by integrating various data management technologies, such as data integration, data quality, data governance, and data security, into a single platform. It uses a combination of data virtualization, data caching, and data orchestration techniques to provide real-time access to data from multiple sources. Data virtualization is a key component of a data fabric, which allows users to access and query data from multiple sources as if it were stored in a single location. Data caching is used to store frequently accessed data in memory, which improves performance and reduces latency. Data orchestration is used to manage the flow of data between different systems and applications. In summary, a data fabric provides a unified view of data across the organization, improves data accessibility and performance, and enables organizations to make better decisions based on accurate and timely data. Hello, dear. A data fabric is a unified architecture that enables organizations to manage and access data from multiple sources, locations, and formats in a seamless and efficient manner. It provides a single view of data across the entire organization, regardless of where the data resides or how it is stored.\nHello Everyone, Today we're discussing FTP versus SFTP. Both protocols will get data from one place to another, but which one is right for my business needs? We're here to simplify your decision-making process an easier headache. First, we'll look at FTP and SFTP to find out what the differences are. Then a quick review of what each protocol is best for. First FTP known as File Transfer Protocol . Is the standard for transferring files between a client and a server on a computer network. In short, FTP opens a control channel and a data channel to transfer data between two computers over an Internet connection. What about SFTP ? SFTP is known as the SSH or Secure Shell File Transfer Protocol. SFTP is functionally similar to FTP, but is a separate protocol entirely. SFTP requires authentication by the server. Your data transfer takes place over a secure SSH channel. Let's go over some reasons to use FTP. FTP is best for uploading and downloading extremely large files. FTP is the fastest way to get your files transferred from 1 remote location to another. FTP is user friendly, ideal for daily business operations requiring file transfers between multiple stakeholders. FTP is ideal for backup of large amounts of business data. Web developers use FTP to move their code to the server hosting the website they're working on. And why you might use SFTP . SFTP is idea for sensitive data that requires encryption. Use SFTP for compliance with federal regulations.\nBusinesses that need firewall friendly file transfer choose SFTP. SFTP is popular on Linux and Unix. SFTP delivers more detailed metadata with your files. Consider your needs and choose FTP or SFTP. Then the process to get going is simple. First you select the FTP client you want to use. Many clients support both FTP and SFTP. Then download and install the client software. Cyberduck Installation: Cyberduck isfree software, but it still costs money to write, support, and distribute it. Here are the installation steps for windows desktop: Select the appropriate protocol you want to use, enter the server address, username and password. Connect to the FTP server and transfer files. Here is a brief comparison between the two: Feature FTP (File Transfer Protocol) SFTP (Secure File Transfer Protocol) Security Less secure as it transfers data in plain text format. More secure as it encrypts data during transmission. Authentication Relies on username and password for authentication. Utilizes public-key cryptography for authentication. Port Uses port 21 for control connection and port 20 for data connection. Uses port 22 for both control and data connections. Data Encryption Does not provide inherent encryption. Encrypts data using SSH (Secure Shell) protocol. Firewall May face issues with firewalls and NAT (Network Address Translation) due to multiple connections. Easily traverses firewalls and NAT as it uses a single secure connection. File Access Limited file access controls and permissions. Offers robust file access controls and permissions, similar to SSH. Connection Stability Prone to connection drops and interruptions.\nWhich protocol do Huawei switches use for dynamic routing within a network? a) BGP (Border Gateway Protocol) b) RIP (Routing Information Protocol) c) OSPF (Open Shortest Path First) d) VRRP (Virtual Router Redundancy Protocol) The correct answer is c) OSPF (Open Shortest Path First). Huawei switches commonly utilize OSPF as the protocol for dynamic routing within a network. OSPF is an interior gateway protocol (IGP) designed for use within an autonomous system (AS). It is widely used in enterprise networks to enable routers to exchange routing information and calculate the shortest paths for data transmission. OSPF operates based on the Link State Routing Protocol (LSRP) algorithm, which allows routers to build and maintain a database of network topology information. By exchanging link state advertisements (LSAs), routers can determine the best paths and calculate the shortest routes to various destinations within the network. One of the key advantages of OSPF is its scalability. It can support networks of various sizes and complexities, including large networks with multiple areas. OSPF allows for efficient routing updates and convergence, ensuring that routers can adapt to network changes quickly and effectively. By utilizing OSPF, Huawei switches can dynamically update and adjust their routing tables based on changes in network topology or link availability. This enables efficient and reliable data transmission within the network, as OSPF can automatically reroute traffic along the most optimal paths. Overall, OSPF is a robust and widely adopted routing protocol for dynamic routing within networks.\nWhich Huawei switch series is optimized for small to medium-sized businesses and provides basic Layer 2 features? a) S1700 Series b) S2700 Series c) S5700 Series d) S9300 Series Answer: a) S1700 Series The Huawei S1700 Series is a range of switches specifically designed to cater to the needs of small to medium-sized businesses. These entry-level Ethernet switches offer basic Layer 2 features and are optimized for simple network deployments. The S1700 switches are known for their cost-effectiveness and ease of use, making them ideal for businesses with limited IT resources. These switches provide essential networking capabilities while keeping the complexity to a minimum, ensuring that small to medium-sized businesses can set up and manage their networks efficiently. Although they offer basic Layer 2 features, the S1700 switches still provide essential functionality for business networking. They support features such as VLANs (Virtual Local Area Networks), spanning tree protocol, link aggregation, and Quality of Service (QoS) capabilities. These features enable network segmentation, redundancy, and traffic prioritization, ensuring reliable and efficient data transmission within the network. In addition to their functionality, the S1700 switches are designed with simplicity in mind. They typically have a user-friendly web-based management interface that allows for easy configuration and monitoring. This interface simplifies the network setup process, making it accessible to non-technical users or IT staff with limited networking knowledge. Overall, the Huawei S1700 Series switches are an excellent choice for small to medium-sized businesses that require basic Layer 2 features, cost-effectiveness, and ease of use.\nA storage virtualization technology which stores and replicates data in a disk group (logical disk) consisting of multiple physical disks. The disk group provides higher storage performance than a single disk and supports data redundancy. It is used to combine multiple physical disk drives into a logical unit to enhance data storage performance, reliability, or both. RAID achieves this by distributing or replicating data across the drives in different configurations known as RAID levels. Here are some traditional RAID levels: Data is divided into blocks and written across the drives simultaneously, allowing for parallel access. RAID 0 does not offer fault tolerance, so the failure of a single drive can result in data loss. RAID 1 mirrors data by writing identical copies of data across two drives. It offers data redundancy, meaning if one drive fails, the other drive still contains a complete copy of the data. RAID 1 provides high data availability but does not offer performance improvements. RAID 5 stripes data across multiple drives, similar to RAID 0, but it also includes parity information. However, write performance can be slower due to the need to recalculate parity. A new type of RAID technology. Block virtualization divides disks into multiple chunks (CKs) of a fixed size and organizes them into multiple chunk groups (CKGs). When a disk fails, the disks of the CKG where the CKs in the faulty disk reside also participate in reconstruction. This significantly increases the number of disks involved in the reconstruction, improving the data reconstruction speed.\nHow can the performance of the OceanStor Dorado All-Flash Storage be optimized? Optimizing the performance of OceanStor Dorado All-Flash Storage involves various considerations, configurations, and best practices to ensure it operates at its full potential. Here are some key steps to optimize the performance of OceanStor Dorado All-Flash Storage: Proper Capacity Planning: Understand your workload requirements and plan the storage capacity accordingly. Overprovisioning or underprovisioning storage can lead to inefficiencies and affect performance. Firmware and Software Updates: Keep the OceanStor Dorado's firmware and management software up to date to access the latest performance improvements, bug fixes, and feature enhancements. RAID Configuration: Choose an appropriate RAID level that balances data protection and performance based on your workload needs. RAID 5 and RAID 6 offer better usable capacity but have lower write performance compared to RAID 1+0 (RAID 10). Cache Settings: Configure the cache settings on the storage system to optimize read and write performance. Larger cache sizes can improve read operations, while write cache settings impact write-intensive workloads. Storage Tiering: If your OceanStor Dorado supports storage tiering, set up automatic data movement between different tiers based on data access patterns. Frequently accessed data can reside on higher-performance tiers, while less-accessed data can be moved to lower-cost, larger-capacity tiers. QoS (Quality of Service): Utilize Quality of Service settings to prioritize workloads and ensure that critical applications get the necessary performance while preventing noisy neighbors from impacting performance.\nMultipathing and Load Balancing: Configure multipathing on the host side to establish multiple paths to the storage system, increasing both performance and reliability. Use load balancing mechanisms to distribute I/O evenly across the paths. Jumbo Frames and Network Configuration: If applicable, enable jumbo frames on your network switches and configure the appropriate MTU size on the storage and host network interfaces to reduce packet overhead and improve network performance. LUN Striping: For workloads that require high performance, consider striping (RAID 0) multiple LUNs across multiple RAID groups to spread the I/O workload. Monitoring and Analysis: Regularly monitor storage performance using the management software provided by Huawei or third-party tools. Analyze performance metrics to identify bottlenecks and potential areas for improvement. Backup and Maintenance Scheduling: Schedule backups and routine maintenance during periods of low workload to avoid performance impacts during peak usage times. Storage Network Bandwidth: Ensure that your storage network bandwidth is adequate to handle the I/O demands of your workloads. It's essential to tailor the performance optimization strategies to your specific use case and workload characteristics. To optimize the performance of OceanStor Dorado All-Flash Storage, consider the following best practices: Choose appropriate RAID levels based on workload requirements. Utilize storage tiering to place frequently accessed data on faster tiers. Configure and monitor cache settings for improved performance. Implement Quality of Service (QoS) to prioritize critical workloads. Use multipathing and load balancing techniques to distribute I/O. Ensure optimal network configuration for sufficient bandwidth. Keep the system up to date with firmware and software updates.\nCan the OceanStor Dorado storage systems be integrated with third-party backup and recovery solutions? Yes, the OceanStor Dorado storage systems can typically be integrated with third-party backup and recovery solutions. As enterprise-class storage systems, they are designed to support interoperability with various backup and recovery software from different vendors. This integration enables organizations to create comprehensive data protection strategies and enhance their disaster recovery capabilities. Here are some common methods of integration with third-party backup and recovery solutions: Snapshot Integration: OceanStor Dorado storage systems often support snapshot capabilities, allowing you to create point-in-time copies of data. Third-party backup solutions can utilize these snapshots to perform incremental backups, reducing the impact on production systems during backup operations. Volume Replication: Some third-party backup solutions may leverage volume-level replication features of OceanStor Dorado to replicate data to a secondary site or storage system. This approach provides an additional layer of data protection and disaster recovery. API Integration: OceanStor Dorado storage systems may expose APIs (Application Programming Interfaces) that third-party backup software can use to interact with the storage system directly. This enables backup and recovery operations to be more tightly integrated into the storage infrastructure. Backup Agents: In certain cases, backup agents installed on hosts or VMs may be used to facilitate data backup from the application or file system level. These agents communicate with the OceanStor Dorado storage to ensure data consistency during backups. Management Software Integration: Some third-party backup solutions offer centralized management consoles that can discover and integrate with storage systems like OceanStor Dorado.\nAre there any specific software requirements for managing the OceanStor Dorado storage systems? DeviceManager: Huawei's DeviceManager is the primary management software used to manage OceanStor Dorado storage systems. It provides a graphical user interface (GUI) that allows administrators to monitor, configure, and manage the storage arrays. DeviceManager is typically accessible via a web browser. CLI (Command-Line Interface): Administrators can also manage the OceanStor Dorado storage systems through the Command-Line Interface, which offers more advanced configuration and management options. Interoperability with Virtualization Platforms: If you are using the OceanStor Dorado storage systems in conjunction with virtualization platforms like VMware vSphere or Microsoft Hyper-V, you may need to ensure compatibility and proper configuration of storage integrations and plugins. Firmware Updates: It's essential to keep the storage system's firmware up to date to access the latest features, bug fixes, and improvements. Before performing firmware updates, carefully review the release notes and ensure compatibility with your specific OceanStor Dorado model. SNMP Management: Simple Network Management Protocol (SNMP) is often used for monitoring and managing storage systems. Ensure that SNMP is properly configured and supported on the OceanStor Dorado storage system. Supported Browsers: When using the web-based management interface (DeviceManager), make sure to use a supported web browser version. Commonly supported browsers include Chrome, Firefox, and Internet Explorer (or Microsoft Edge). Storage Networking: If your OceanStor Dorado storage system is connected to hosts or networks, ensure that the storage networking components (such as Fibre Channel switches or Ethernet switches) are properly configured and compatible with the storage system. Hello, dear.\nDear All, Today we are going to learn about ransomware. What is ransomware? Ransomware is a type of malicious software (malware) that encrypts files or locks a user's computer or device, effectively holding it hostage until a ransom is paid to the attacker. It is a form of cyber extortion that aims to extort money from individuals, businesses, or organizations. Encryption : Ransomware typically encrypts the victim's files using strong encryption algorithms, making them inaccessible and unusable without the decryption key. This can include important documents, photos, videos, databases, and other valuable data stored on the infected device. Ransom Note: Once the files are encrypted, ransomware displays a ransom note or a message on the victim's screen, informing them that their files have been locked or encrypted. The note includes instructions on how to pay the ransom and usually provides a deadline for payment. Ransom Payment: The attackers demand a ransom payment, often in a form of cryptocurrency such as Bitcoin, to provide the decryption key or release the locked system. The ransom amount varies, and the payment is typically required within a specified timeframe. Time Pressure: Ransomware creators often impose a sense of urgency by setting a short deadline for payment. They may threaten to delete or permanently encrypt files if the ransom is not paid within the given timeframe, increasing the pressure on the victim to comply. Delivery Methods: Ransomware can be delivered through various means, including malicious email attachments, infected websites, drive-by downloads, exploit kits, or social engineering techniques.\nIt can also spread within networks, targeting vulnerable systems connected to the infected device. Evolving Threat: Ransomware is a constantly evolving threat, with attackers developing new variants and techniques to evade detection and maximize their chances of successful infections. This includes the use of advanced encryption algorithms, obfuscation techniques, and evasion mechanisms. Impacts and Consequences: Ransomware attacks can have severe consequences for individuals and organizations. They can result in data loss, financial losses from ransom payments, operational disruptions, reputational damage, and legal and regulatory implications. It is important to note that paying the ransom does not guarantee the safe return of files or the removal of the malware. In many cases, victims who pay the ransom may not receive the decryption key or may be targeted again in the future. Prevention and mitigation strategies against ransomware include regularly backing up important files, keeping software and systems up to date with security patches, using reputable security software, exercising caution when opening email attachments or visiting unfamiliar websites, and educating users about safe online practices. In the event of a ransomware attack, it is recommended to report the incident to law enforcement agencies and seek assistance from cybersecurity professionals who may be able to help with decryption tools or recovery options. To avoid ransomware attacks, it's important to implement a combination of proactive measures and security best practices. Here are some essential steps you can take to help protect yourself and your organization: Regularly backup your important files and data to an offline or cloud-based backup solution.\nEnsure backups are performed automatically, and verify the integrity of backups periodically to ensure they can be restored if needed. Install updates and patches for your operating system, applications, and security software. Outdated software can have vulnerabilities that ransomware can exploit, so keeping everything up to date is crucial. Install reputable antivirus/anti-malware software and keep it updated. This can help detect and block known ransomware threats. Enable real-time scanning and automatic updates to ensure continuous protection. Exercise caution with email attachments and links: Be cautious when opening email attachments, especially from unknown or suspicious senders. Do not click on links in emails unless you are certain of their legitimacy. Be vigilant against phishing attempts that may trick you into downloading ransomware. Enable strong spam filters on your email accounts to help filter out malicious emails that may contain ransomware attachments or links. Use strong, complex passwords for all your accounts and avoid reusing passwords across multiple platforms. Consider using a password manager to generate and store unique passwords securely. Enable multi-factor authentication (MFA): Enable MFA for your accounts whenever possible. This adds an extra layer of security by requiring additional verification steps, such as a unique code sent to your mobile device, in addition to your password. Be cautious when visiting websites and avoid clicking on suspicious or untrusted links . Use browser extensions or security tools that can help block malicious websites and scripts. Restrict user privileges to only what is necessary for their job functions.\nImplement the principle of least privilege (PoLP) to minimize the potential impact of ransomware if a user's account is compromised. Regularly educate yourself and your employees about ransomware threats , phishing techniques, and safe online practices. Raise awareness about the risks of opening unknown attachments or clicking on suspicious links. Use a firewall to control inbound and outbound network traffic and prevent unauthorized access to your systems. Implement intrusion detection/prevention systems (IDS/IPS): Deploy IDS/IPS solutions to monitor network traffic and detect potential ransomware or other malicious activities. Develop and test an incident response plan that outlines steps to be taken in the event of a ransomware attack. This can help minimize the impact and facilitate a timely response to mitigate the attack. Stay updated on the latest ransomware trends , techniques, and prevention strategies through security resources, blogs, and industry news. This will help you stay ahead of evolving threats. To enhance protection against ransomware attacks, it is crucial to have dedicated ransomware protection on storage devices, complementing the existing security measures at the network level. Huawei offers an exceptional solution that ensures comprehensive and dependable safeguarding for both primary and backup storage, significantly bolstering your business's security posture. Primary storage: In the realm of primary storage, Huawei employs cutting-edge measures to ensure data integrity and protection. By implementing secure snapshot and Write Once Read Many (WORM) functionalities within the storage, a secure zone is established, effectively preventing any unauthorized tampering or deletion of data.\nthat is designed to encrypt or lock up files, documents, or other data on a victim's computer or network, making them inaccessible. Once the files have been encrypted, the ransomware program typically demands a ransom payment in exchange for providing the victim with the decryption key needed to unlock their files. alike, as they can result in the loss of critical data, financial losses, and other damages. Ransomware attacks typically spread through infected email attachments, compromised websites, or other means of exploiting vulnerabilities in computer systems. Anti-Ransomware Storage Solution: is a type of data storage system that includes built-in protection against ransomware attacks. This type of storage solution typically uses a combination of features to prevent, detect, and mitigate ransomware attacks. is to use behavior-based detection that can identify and stop ransomware before it can cause damage. These systems can analyze patterns of file access and usage, detecting anomalous behavior that may indicate a ransomware attack in progress. is to use data snapshots or backups that can be used to restore files in case they are encrypted or otherwise compromised by ransomware. These backups should be stored in a separate location from the primary data storage, so that they are not affected by the same ransomware attack. Some also incorporate advanced security features such as encryption, access controls, and monitoring to help prevent unauthorized access to sensitive data. Overall, an against ransomware attacks, minimizing the risk of data loss and ensuring business continuity.\nData Lifecycle Management (DLM) refers to the process of managing data throughout its lifecycle, from creation to disposal. It involves various stages, including data creation, storage, access, retention, archival, and eventual deletion. Huawei provides storage solutions that facilitate efficient data lifecycle management, ensuring data is stored, protected, and accessed in a manner that aligns with an organization's requirements and regulatory compliance. Huawei Storage solutions offer robust capabilities for effective data lifecycle management, enabling organizations to optimize storage resources, enhance data accessibility, and ensure regulatory compliance. In this article, we will explore the key principles and best practices of data lifecycle management with Huawei Storage. Data Classification: Effective data lifecycle management begins with data classification. Organizations should categorize their data based on factors such as criticality, sensitivity, regulatory requirements, and access frequency. Huawei Storage provides features like metadata tagging and policy-based classification to help classify and organize data efficiently. Storage Tiering: Storage tiering involves assigning different types of storage media based on the performance requirements of data. Huawei Storage solutions offer automated storage tiering capabilities, allowing organizations to optimize cost and performance by dynamically moving data between different storage tiers, such as high-performance SSDs, cost-effective HDDs, or cloud storage. Data Protection: Data protection is a crucial aspect of data lifecycle management. Huawei Storage provides comprehensive data protection features like snapshots, replication, and backup. Organizations can leverage these features to create point-in-time copies of data, replicate data across multiple sites for disaster recovery purposes, and perform regular backups to safeguard against data loss.\nData Retention and Archiving: As data ages, it may no longer require immediate access but must be retained for compliance or historical purposes. Huawei Storage solutions offer advanced data archiving capabilities, allowing organizations to move less frequently accessed data to cost-effective storage tiers or even integrate with external archival systems. This approach optimizes primary storage resources and reduces costs while ensuring data availability when needed. Data Deletion and Disposal: To comply with data protection regulations and maintain data privacy, organizations must securely delete data that is no longer needed. Huawei Storage provides secure data erasure mechanisms, including data shredding and encryption key management, to ensure data is effectively and permanently removed from storage media. Data Governance and Compliance: Data lifecycle management with Huawei Storage enables organizations to establish data governance practices and ensure compliance with industry-specific regulations such as GDPR, HIPAA, or PCI DSS. The storage solutions offer features like data encryption, access controls, and audit logs to facilitate compliance monitoring and reporting. Data Analytics and Insights: Huawei Storage solutions integrate with data analytics platforms to enable organizations to gain valuable insights from their stored data. By leveraging advanced analytics tools, organizations can uncover patterns, trends, and business intelligence that can drive informed decision-making and extract additional value from their data assets. Scalability and Future-Proofing: Huawei Storage solutions are designed with scalability in mind, allowing organizations to seamlessly expand their storage infrastructure as data volumes grow.\nIntroduction: In digital age, businesses are confronted with an expanding number of cyber threats that can compromise their sensitive information. Data breaches, intellectual property theft, and regulatory non-compliance have ended up major concerns for associations over various businesses. To defend their basic data, businesses are turning to Data Loss Prevention (DLP) solutions. This article points to an in-depth understanding of DLP and its significance in securing delicate information. DLP sources Key features of DLP software: DLP software envelops a range of devices and innovations planned to avoid unauthorized access, transmission, or leakage of sensitive data. Let's investigate a few of the key highlights ordinarily offered by DLP solutions: Content Discovery: DLP software employs advanced filtering techniques to find sensitive information inside an organization's network. It can recognize private data such as credit card numbers, social security numbers, or intellectual property, indeed if they are stored in various groups or areas. Data Classification: DLP solutions give the capacity to classify information based on its sensitivity and allot appropriate security policies. This feature enables associations to organize assurance efforts and center on safeguarding their most basic data. Data Monitoring and Control: DLP software continually screens data flow inside an organization's network, both at rest and in transit. It can identify and avoid unauthorized attempts to duplicate, print, e-mail, or exchange sensitive information. Real-time alarms and notices are activated when arrangement violations happen, permitting immediate activity to be taken.\nDLP System Encryption and Data Masking: DLP solutions offer encryption capabilities to ensure delicate information when it is being transmitted or stored. Encryption guarantees that indeed on the off chance that information falls into the off-base hands, it remains unreadable and unusable. Data veiling, on the other hand, replaces sensitive information with invented or mixed values, allowing the utilization of representative data while preserving security. Endpoint Protection: DLP software can be sent on endpoints such as portable workstations, desktops, and portable devices to expand information protection beyond the corporate arrangement. It implements data security approaches on personal devices, avoiding information leaks through removable media, cloud capacity, or unauthorized applications. User Behavior Analytics: DLP solutions utilize machine learning algorithms to examine client conduct and identify irregularities that may demonstrate information exfiltration attempts or insider threats. By building up pattern client activity, the DLP program can distinguish deviations and flag suspicious exercises for further examination. DLP framework How to Choose the Best DLP Software for Your Business? Selecting the right DLP software for your association can be a basic choice. Here are a few variables to consider when assessing different arrangements: Comprehensive Coverage: Look for a DLP solution that can secure information over different channels, counting mail, web activity, cloud capacity, and endpoints. It should offer a wide extent of discovery and anticipation capabilities to address your particular information security needs. Scalability and Flexibility: Ensure that the DLP software can scale nearby your organization's growth and suit advancing business necessities.\nIt should support distinctive sending choices, counting on-premises, cloud-based, or hybrid models. Integration Capabilities: Evaluate the software's compatibility together with your existing security framework, such as firewalls, SIEM (Security Information and Event Management) frameworks, and character and get to administration arrangements. Consistent integration will empower effective information observing and occurrence reaction. Ease of Use: A user-friendly interface and natural configuration alternatives can streamline the sending and management of the DLP arrangement. Look for highlights like pre-built policies and computerized workflows to streamline your information protection forms. Compliance Support: If your association works in controlled industries, ensure that the DLP software adjusts with significant compliance measures, such as GDPR (General Data Protection Regulation) or HIPAA (Health Insurance Portability and Accountability Act). The arrangement should provide announcing capabilities to encourage compliance reviews. DLP methods Application of DLP software: DLP software finds extensive application in diverse industries, including finance, healthcare, retail, and technology. Here are some scenarios where DLP solutions play a crucial role: Protection of Personal Data: With the increasing prevalence of data breaches and privacy concerns, organizations handling personal data need robust DLP solutions. These solutions help prevent unauthorized access, loss, or disclosure of personal information, ensuring compliance with data protection regulations. Intellectual Property Protection: Industries that heavily rely on intellectual property, such as technology and manufacturing, face the constant threat of data theft. DLP software assists in identifying and protecting sensitive documents, source code, trade secrets, and other proprietary information. Insider Threat Mitigation: Insider threats pose a significant risk to organizations.\nAs the volume of data continues to grow exponentially, organizations face the challenge of storing and managing large datasets efficiently. Traditional storage approaches often involve data replication, where multiple copies of the same data are stored on different machines. While replication provides data redundancy and fault tolerance, it comes at the cost of increased storage overhead. As a result, alternative techniques such as erasure coding have emerged to address these challenges. Here we will explore the concept of erasure coding and its application in a widely used distributed file system designed for storing and processing large datasets across multiple machines. We will delve into the benefits of HDFS erasure coding, discuss key concepts and erasure coding policies in HDFS, and provide implementation examples with sample code. Hadoop Distributed File System (HDFS) is designed to store and process large datasets reliably and efficiently across a cluster of machines. HDFS supports native erasure coding starting from version 3.x, making it an ideal choice for organizations dealing with big data workloads. By integrating erasure coding into HDFS, users can achieve efficient data storage, reduced storage costs, and high fault tolerance. Key Concepts Before diving into the implementation details, let's understand some key concepts related to HDFS erasure coding: HDFS supports multiple erasure coding policies, allowing users to choose the policy that best suits their requirements. Each policy defines the number of data blocks, parity blocks, and other parameters.\nSome commonly used erasure coding policies in HDFS include: RS-3-2 : This policy divides the data into three data blocks and encodes them with two parity blocks. It provides fault tolerance against the failure of any two blocks. RS-6-3: This policy divides the data into six data blocks and encodes them with three parity blocks. It provides fault tolerance against the failure of any three blocks. RS-10-4 : This policy divides the data into ten data blocks and encodes them with four parity blocks. It provides fault tolerance against the failure of any four blocks. These erasure coding policies offer different levels of fault tolerance and storage efficiency, allowing organizations to choose the appropriate policy based on their specific requirements. Let's now explore an implementation example of HDFS erasure coding using the Java API. We will focus on encoding and decoding files using the RS-3-2 erasure coding policy. To encode a file using HDFS erasure coding, we need to follow these steps: Create an ` object: Create an ` object: Create a ` ` object: Encode the file: To decode a file encoded with HDFS erasure coding, we can follow these steps: Create a ` ` object: Decode the file: We can then decode the file by calling the ` ` method of the ` ` object. In a real-world scenario, you would need to handle exceptions, ensure proper error recovery, and consider other aspects such as file striping and data placement policies.\nWhat is a Data Mesh Used For? Hello, friend! Data Mesh is a relatively new architectural paradigm for managing data in large, complex organizations. It is used to address the challenges of data management in modern enterprises, where data is distributed across multiple systems, teams, and domains. The primary goal of Data Mesh is to enable organizations to manage their data in a more decentralized and scalable way. It does this by breaking down data silos and creating a more modular and flexible data architecture. This allows teams to work more independently and efficiently, while still maintaining a high level of data quality and consistency. Some of the key benefits of Data Mesh include: 1. Improved data quality and consistency 2. Increased agility and flexibility in data management 3. Better alignment between data and business processes 4. Reduced data duplication and redundancy 5. Improved collaboration and communication between teams Overall, Data Mesh is a powerful tool for organizations looking to modernize their data management practices and stay competitive in today's fast-paced business environment. Hope this helps! Hello, friend! Data Mesh is a relatively new architectural paradigm for managing data in large, complex organizations. It is used to address the challenges of data management in modern enterprises, where data is distributed across multiple systems, teams, and domains. The primary goal of Data Mesh is to enable organizations to manage their data in a more decentralized and scalable way. It does this by breaking down data silos and creating a more modular and flexible data architecture.\nHello there , Today we will talk about,Configurations of VMware vCloud Director and data security. VMware vCloud Director phoenixNAP-Virtual Private Data Center Data Security Cloud VMware-vCloud Director Initial Network Configuration virtual private network Creating an Org VDC Network Networks Networking Click Add Org-VDC Network Follows These settings should be configured. Org VDC : In most use cases, already be set to the name of your virtual datacenter. If not, select yours from the drop-down the menu and select. Name : Create a name, which is you want to use to reference this network in the future. Description : Add an-optional of this network for description. Share this network with other VDCs : This will be only the affective, when you have multiple virtual data centers, which is not common use. Edge Gateway : Your organization/company already have an Edge Gateway deployed. Select and click on it to select it as the Edge Gateway that this network will connect to. Allow Guest VLAN ID - Create as Subinterface it is very important as Most clients/users will leave these unchecked. Gateway CIDR : This is the internal IP to which you will want your VMs to connect to, so it can reach the internet (such as 192.260.20.1). It is followed by the network mask that matches the subnet size you wish to use (e.g., 192.260.20.1 /24 ) Primary and Secondary DNS : Set these to the IPs of the domain name servers you wish to use. DNS Suffix : you need a specific-DNS suffix for your local VMs.\nStatic IP pool : required identification is a pool of IPs that will be reserved as static IPs, enter the IP range, such as example192.168.20.10 to192.168.20.100. Edge Gateway Configuration menu Configure-Services Edge-Gateway Services Managing NAT Rules NAT Creating a Source NAT Rule SNAT Rule Applied On : it is by default to your Edge-Gateways preconfigured external network. Original Source IP-Range : This will be the same internal-IP and subnet as defined earlier, as 192.168.20.0/24. Translated Source IP-Range : for external IP -traffic to show up as. You will be required to enter one of your usable public IPs here. Enabled : by default this should be enable. Enable logging : for traffic matching this rule should Enable optional logging. Creating Firewall Rules + some examples and rules to allow all traffic from VMs to reach the internet: Name : Egress-traffic Type : User Source : Internal IP range as (192.168.20.0/24) Destination : Any Service : Any Action : Accept For allowing inbound-traffic port 443 on a VMs: Name : https Type : User Source : Any Destination : IP of the VM serving https Service : tcp:443:any Action : Accept Creation VMsand Management with vCloud Director vApps. Creating a vApp vApps New-vAPP Creation of Virtual Machine Add VMs Enterprise-High Enterprise-All-Flash Type New Configured as follows: Name : name is required display for this VM within vCloud Director. Computer Name : hostname or computer name of the guest-system. Description : text description of the machine. Type : Set to New unless using a template.\nOS family : Set this to the OS family that best matches your desired guest OS. Operating System : Specific OS you plan on installing within the guest OS. Please note that this only configures the VM for compatibility, it does not actually install the guest OS selected. Boot image : If you have boot-image media uploaded to your catalog select it, it will be automatically connected to the VM-creation. Size : This will pre-populate with a list of common VM sizes based on the OS selected. If none of the shown , settings match your desired configuration, Customize to be taken to the detailed sizing settings, where you can manually configure by the following: Virtual CPUs: total number of virtual CPUs that will be assigned to VM. One V-CPU is approximately equal to one core physical CPU. Cores per socket : controls how many sockets the guest OS will see. It does not modify the number of cores, Memory : Virtual-RAM assigned to VM. Storage : allows you to add and configure the size of virtual hard disks. Networking : this will be connected to your routed network created earlier. If you need to change the network, IP allocation, add more virtual NICs, click on the Customize button. Ok Build Actions : Enables you to power on off, suspend, or reset the entire vApp.\nStorage systems play a vital role in today's businesses. They hold critical data and ensure that it is available when needed. However, storage systems can encounter problems that can impact their performance, leading to data loss or downtime. Monitoring and troubleshooting storage systems can help prevent these issues and minimize their impact. In this article, we'll discuss some best practices for monitoring and troubleshooting storage systems. The first step in monitoring and troubleshooting storage systems is to establish a monitoring process. Define which metrics to monitor and how often to check them. Use monitoring tools to track storage capacity, performance, and health indicators such as CPU usage, disk usage, and error rates. Establish a baseline for what normal performance looks like so that you can quickly identify deviations from it. Configuring alerts for critical events is essential in detecting problems early on. Set up alerts for events such as low disk space or high error rates, and ensure they are sent to the right people at the right time. Utilize an alerting system that can integrate with your existing tools and can send notifications to your team members via email, SMS, or other means. Use analytics tools to identify trends and anomalies in storage performance data, which can help you diagnose issues before they become critical. With data analytics, you can detect patterns in the data that may indicate performance issues or potential capacity concerns. Also, visualize the data to see changes and trends over time to assist in decision-making.\nwhat is DME architecture? DME (Distributed Media Engine) architecture is a scalable and distributed architecture designed to handle video processing and delivery tasks efficiently. It consists of multiple components working together to provide a comprehensive solution for media processing and streaming. Here are the key components and their roles in the DME architecture: DME Manager: The DME Manager is the central control plane of the architecture. It manages the overall system configuration, task scheduling, and resource allocation. It provides a web-based user interface (UI) or API for administrators to configure and monitor the DME system. DME Nodes: DME Nodes are the individual processing units within the architecture. Each DME Node can perform tasks such as video transcoding, packaging, and streaming. Nodes can be scaled horizontally to handle increased workloads, improving performance and efficiency. DME Nodes communicate with each other and the DME Manager to receive instructions and exchange information. Media Grid: The Media Grid is a distributed storage system that stores video assets, temporary files, and metadata used by DME. It provides high-speed access to media resources and facilitates efficient data transfer between DME Nodes. Transcoder: The Transcoder component performs the actual video transcoding, converting videos from one format to another. It supports various video codecs, formats, and adaptive streaming protocols, ensuring compatibility with different devices and platforms. Packager: The Packager component packages the transcoded videos into different formats suitable for streaming.\nwhat is DME three node deployment requirements Here are the requirements for a three-node DME deployment: Hardware: Three physical servers with the following minimum specifications: 2 x Intel Xeon E5-2620 v4 2.10 GHz CPUs 16 GB of RAM 4 x 960 GB SATA SSDs Operating system: Ubuntu 18.04 LTS Software: DME 1.0.0 or later Docker 19.03.13 or later Kubernetes 1.19.12 or later The three nodes should be connected to each other using a reliable network connection. The network connection should have a minimum bandwidth of 1 Gbps. The three nodes should be configured as follows: Node 1: This node should be the primary node. It should be the first node that is deployed and it should be the node that is used to manage the DME cluster. Node 2: This node should be a worker node. It should be deployed after the primary node and it should be used to store data and process requests. Node 3: This node should also be a worker node. It should be deployed after the primary node and the secondary node and it should be used to store data and process requests. The three nodes can be deployed in any order. However, it is important to ensure that the primary node is always available. If the primary node fails, one of the worker nodes can be promoted to be the primary node. Once the three nodes are deployed, they can be configured to form a DME cluster.\nHi, dear community members, The weekend is nearly here again and that cant be anything but good news, right? However, before we dive into our usual weekend activities, let me introduce you to Huaweis Data Management Engine, a very intelligent storage management platform, that you will learn more about in the following lines. So, grab a coffee or whatever your usual preferred beverage is, and relax by reading our weekly community blog post. Enjoy! Huaweis Data Management Engine So, whats the deal with this intelligent Data Management Engine (DME)? Well, the first thing you need to know is that this platform was designed for a variety of applications, while offering convergence, intelligence and openness. It actually simplifies storage management and Operations and Maintenance (O&M), while also improving the operation efficiency of data centers and automating storage throughout its lifecycle. Data Management Engine Features The key features of Huaweis DME are: Converged functions: multi-device storage management? No problem! Users just have to log in to a software interface to complete resource provisioning, intelligent O&M and data protection as well. Converged devices: unified storage network management which comes with End to End (E2E) automatic resource provisioning, topology generation and associated performance analytics that can only lead to an improved management efficiency. Intelligent provisioning: it provides a flexible service, which is Service Level Agreement-based and Ansible-based for all scenarios and it supports device or app-based resource provisioning.\nDear All, Today, We are going share an important topic on Optimization of Data Storage Performance with What are SSDs: Solid-state drives (SSDs) differ from conventional hard disk drives (HDDs) in that they do not employ rotating magnetic disks or mechanical read/write heads. Instead, SSDs are devoid of any moving components. Solid-state drives (SSDs) utilize integrated circuits for data storage and retrieval, resulting in faster data transfer rates, reduced latency, and enhanced system responsiveness. Solid-state drives (SSDs) are a popular choice for a wide range of applications, from consumer electronics to enterprise-level storage solutions. This is due to their superior reliability, durability, and energy efficiency when compared to traditional hard disk drives (HDDs). Solid State Drives (SSDs) have become an integral part of modern computing due to their compact size and advanced technology. They provide faster data access, improved performance, and more dependable data storage. Why SSDs are Preferred: Solid State Drives (SSDs) provide notably accelerated data access speeds owing to their nonexistence of mechanical components. Due to their lower latency, these devices exhibit a marked decrease in data retrieval time, thereby enhancing the overall system responsiveness. Due to the lack of mechanical components, SSDs can attain accelerated read and write speeds, thereby facilitating expedited boot times, swifter application loading, and rapid data transfers. SSDs are exceptionally well-suited for high-performance storage applications, including but not limited to database servers, virtualization platforms, and content delivery networks (CDNs). SSDs boast superior reliability and durability as a key technical advantage.\nSSDs are less susceptible to mechanical issues such as head crashes and motor failures than HDDs. The utilization of SSDs enhances their durability against physical impacts, vibrations, and temperature variations. These components are particularly well-suited for deployment in settings where robustness is important, such as in mobile gadgets, sturdy industrial machinery, and high-stakes operational scenarios. SSDs exhibit superior power efficiency when compared to their HDD counterparts, leading to a reduced energy footprint and minimized thermal output. The meticulous attention paid to energy efficiency not only leads to a significant reduction in operational costs but also plays a pivotal role in fostering a more ecologically conscious and sustainable computing ecosystem. SSDs are the top choice for a wide range of technical applications due to their exceptional reliability, durability, and energy efficiency. These applications span from high-performance computing clusters to mobile devices and embedded systems. SSDs have brought about a significant transformation in the storage industry by providing cutting-edge, high-speed, dependable, and power-saving storage alternatives that are tailored to meet the escalating requirements of contemporary computing systems. Factors impacting the performance of SSDs: Several factors can significantly impact the performance of solid-state drives (SSDs). Understanding these factors is essential for optimizing SSD performance. Here are some key factors that can influence SSD performance: NAND Flash Type: Crucial for SSD performance. NAND flash has different generations: SLC, MLC, TLC, and QLC Each generation has different performance, endurance, and cost SLC: highest performance, higher cost - QLC: higher capacities, lower performance SSD interface affects performance. Common interfaces: SATA, PCIe, NVMe.\nNVMe uses PCIe interface Higher data transfer rates Lower latency than SATA Improved SSD performance SSD controller and firmware manage data access, error correction, wear leveling, and other functions. Controllers and firmware can boost SSD performance They handle data operations efficiently They maximize the use of NAND flash memory SSD read/write speeds are important performance indicators Sequential and random speeds are measured Sequential read/write speeds: transfer rate of large, contiguous data blocks Random read/write speeds: performance when accessing smaller, random data blocks Faster read/write speeds = faster data access and improved performance. SSDs use over-provisioning Reserves capacity for maintenance and optimization Over-provisioning improves SSD performance, endurance, and reliability. It helps the controller distribute data more effectively and reduce write amplification. SSDs have a built-in cache for faster data access. Cache stores frequently accessed data Improves read and write speeds Cache size and efficiency affect SSD performance. Larger caches = improved performance High data locality benefits from larger caches Trim Support: Important feature for modern OS and SSDs. Trim informs SSD about unused data blocks Allows the drive to optimize performance Erases and prepares blocks for future writes Workload and queue depth affect SSD performance. SSD performance is impacted by the nature of the workload. The depth of the input/output (I/O) queue can also affect SSD performance. Databases and virtualization generate high I/O requests. SSDs with higher queue depths and concurrent request handling have better performance. Improved performance is a result of better handling of workloads.\nSystem configuration affects SSD performance Includes CPU, memory, and storage architecture Bottlenecks in other components can hinder SSD performance Examples include limited memory and slow CPU performance Solid-state drives (SSDs) have revolutionized the storage industry, offering faster, more reliable, and energy-efficient storage solutions. Huawei has made significant advancements in the field of SSDs. By leveraging cutting-edge technologies, innovative features, and a focus on performance enhancement, Huawei SSDs deliver exceptional results in various computing environments. Huawei utilizes advanced NAND flash memory technology, such as 3D NAND , in its SSDs to optimize data storage performance . 3D NAND increases storage density by stacking memory cells vertically , allowing for higher capacities and improved performance. Huawei SSDs incorporate intelligent algorithms that efficiently manage the data stored in the NAND cells, reducing read and write amplification and improving overall performance. Additionally, Huawei's advanced ECC algorithms enhance data integrity by detecting and correcting errors in the stored data. Huawei designs its SSD controllers and firmware in-house, enabling tight integration with the hardware and optimal performance. The customized controllers are specifically tailored to work seamlessly with the NAND flash memory, optimizing data transfer rates and reducing latency . Huawei's firmware includes advanced algorithms for wear leveling , which evenly distribute write operations across the NAND cells to extend the SSDs' lifespan and maintain consistent performance. The firmware also incorporates intelligent garbage collection techniques to optimize the utilization of available storage space and prevent performance degradation. Huawei SSDs achieve high sequential and random read/write speeds through several optimizations.\nThe use of high-speed interfaces , such as PCIe and NVMe, ensures faster data transfer rates and reduced latency. Huawei's controllers and firmware are designed to efficiently handle read and write operations, minimizing the time taken for data access . Additionally, the intelligent caching algorithm s implemented in Huawei SSDs store frequently accessed data in a cache, enabling faster retrieval and improving random read performance. Huawei SSDs employ intelligent caching algorithms to optimize performance further. These algorithms dynamically allocate a portion of the SSD's capacity as a cache , storing frequently accessed data. The caching algorithms intelligently monitor data access patterns and prioritize the most frequently accessed data for caching, resulting in faster read and write operations. Huawei's cache management techniques ensure that the cache is efficiently utilized, maximizing its impact on overall storage performance. This intelligent caching mechanism significantly reduces latency and improves the r esponsiveness of the SSDs, particularly in workloads with high data locality . Huawei SSDs prioritize endurance and reliability through various implementations. The wear-leveling algorithms incorporated in the SSD controllers evenly distribute write operations across the NAND cells, preventing premature cell wear and extending the lifespan of the SSD . This ensures consistent performance and long-term reliability. Huawei also integrates advanced ECC algorithms into the firmware, which detects and corrects errors in the stored data, enhancing data integrity and reducing the risk of data corruption.\nHi, As IT engineers, the most common tool we use should be the terminal emulator. The common terminal emulators include PuTTY, MobaXterm, SecureCRT, and Xshell. In this post, we are going to share the difference between them, and how could we select one from the PuTTY, MobaXterm, SecureCRT, and Xshell. PuTTY is a free and open-source terminal emulator, serial console, and network file transfer application. It supports the common connection protocols, such as SSH, Telnet, Rlogin, SCP, and so on. Besides, PuTTY also supports creating a serial connection to the device. Figure 1. PuTTY icon PuTTY is a lightweight software. Usually, the installation package size of the PuTTY is only about several K bytes. On the other hand, PuTTY supports almost all OSs, such as Windows, macOS, and Linux. The lightweight characteristics make the PuTTY can be installed on old PCs or some portable but poor-performance laptops. Besides, the lightweight also helps speed up the startup process. Figure 2. PuTTY main interface. As the most disadvantage of the PuTTY, it might be the PuTTY cannot manage sessions in one window with multi-tabs. That makes it hard to find the required one when you created tens of sessions. Figure 3. MobaXterm icon MobaXterm is an ultimate toolbox for remote computing. It provides almost all the important remote network tools. SSH, RDP, FTP, and VNC, as long as you can think, you can find it in MobaXterm. Besides the massive protocols, MobaXterm also supports installing extra plugins to extend its capability. Figure 4.\nMobaXterm main appearance In addition to the powerful features, the MobaXterm makes Linux file management easier. It provides a visual interface for the Linux OS so that the admins can find the files using the mouse, rather than typing the command. But, unlike the PuTTY, The supported OS by the MobaXterm is Windows. This makes it less universal than the PuTTY does. Figure 5. Sessions that can be created in MobaXterm On the other hand, the powerful functions make it bloated and resource-consuming, which makes it a suboptimal choice for old PCs and poor-performance laptops. The MobaXterm provides two editions, the home edition, and the professional edition. The home edition is free to use but has limited sessions and other capabilities, while you can purchase the professional edition to unlock the limitations. As an old terminal emulator, SecureCRT is probably the most used software in this category. SecureCRT is a commercial SSH and Telnet client and terminal emulator by VanDyke Software. At the old version, SecureCRT supports only the Windows system. Later, the VanDyke added the MacOS version (in v6.6) and then Linux version (in v6.7). Figure 6. SecureCRT icon The same as the MobaXterm, SecureCRT also provides a tabbed session management capability. Simply put, the SecureCRT can be seen as a weakened version of the MobaXterm, it supports fewer protocols. As a commercial software, SecureCRT requires the user to buy, but before that, you can have a 30-day evaluation license.\nIn today's digital era, data storage has become an integral part of every organization's IT infrastructure. With the exponential growth of data, managing storage resources efficiently and effectively has become increasingly challenging. To address this challenge, storage virtualization has emerged as a powerful technology that allows organizations to abstract and pool their storage resources, enabling greater flexibility, scalability, and efficiency. Huawei, a leading global provider of information and communications technology (ICT) solutions, offers innovative storage virtualization solutions that empower businesses to optimize their storage infrastructure and streamline their operations. In this article, we will explore the benefits and features of storage virtualization with Huawei and how it can revolutionize the way organizations manage their storage resources. Storage virtualization with Huawei brings significant efficiency gains to organizations by simplifying storage management, improving utilization, and reducing operational costs. By abstracting physical storage resources into a unified virtual pool, administrators can manage and allocate storage more effectively. This abstraction layer enables dynamic provisioning, where storage capacity can be allocated on-demand, eliminating the need for manual intervention and reducing provisioning time. Moreover, Huawei's storage virtualization solutions leverage intelligent data tiering and deduplication techniques, ensuring optimal utilization of storage resources and reducing overall storage footprint. One of the primary advantages of storage virtualization is its ability to enable seamless scalability. Huawei's virtualization solutions allow organizations to scale their storage infrastructure without disrupting ongoing operations. By decoupling physical storage from the virtualized layer, administrators can add or remove storage devices easily, ensuring flexibility and agility in adapting to changing business requirements.\nThis scalability ensures that organizations can meet their evolving storage needs, whether they are experiencing rapid data growth, expanding their operations, or embracing new technologies. Huawei's storage virtualization solutions provide centralized management capabilities, allowing administrators to monitor, configure, and control the entire storage infrastructure from a single console. This centralized management simplifies administrative tasks, reduces the risk of human error, and improves overall operational efficiency. Administrators can gain a comprehensive view of the storage environment, monitor performance, and implement policies to optimize data placement and resource allocation. Additionally, Huawei's solutions offer advanced analytics and reporting features, empowering organizations with insights into storage usage, performance, and trends, enabling informed decision-making and proactive management. Ensuring data protection and high availability is paramount for any organization's storage infrastructure. Huawei's storage virtualization solutions incorporate robust features to safeguard data integrity and minimize downtime. By leveraging advanced RAID technologies, data replication, and snapshot capabilities, organizations can create redundant copies of data, enable disaster recovery mechanisms, and protect against data loss or system failures. Moreover, Huawei's solutions support seamless data migration between different storage systems, ensuring data availability during system upgrades or hardware replacements. Huawei's storage virtualization solutions are designed to seamlessly integrate with cloud environments and leverage artificial intelligence (AI) capabilities. This integration enables organizations to extend their storage resources to the cloud, leverage cloud-based services, and implement hybrid storage architectures. Huawei's AI-powered storage systems can analyze and optimize data placement based on usage patterns, further enhancing performance and efficiency.\nwhat is difference between NOR and NAND? Nand vs Nor Flash Memory : Comparison table CHARACTERISTIC NOR FLASH NAND FLASH Memory Architecture A number of memory cells (typically 8 cells) are linked to a series that is identical to a NAND gate. Memory Range and Cell Size Good Bits Typically has 98% good bits at the time of shipping with additional bit failure during the course of the life of the part. That is the reason why it requires ECC or error correcting code functionality inside the device. Memory Capacity The memory capacity comes within a range of 1 GB to 16 GB. That is the reason why it is primarily used for the data storage application. Erase, Read and Write Operations This Nand vs Nor Flash Memory characteristic defines all three operational capacities. Here, each byte requires to be written along with 0 prior to erase, resulting in much slower operation. For example, Cypress NOR Flash needs ~520ms typically to erase down a similar 128KB data slot. Reading capacity is directly dependent on the size of block of data. The delay in reading is directly proportional to the increase in size. The data writing can only be done if there is an empty block. The writing operation is typically slower. The erase operation is quite straightforward in NAND flash. For example, the requirement of Cypress NAND Flash typically needs 3.5ms in order to erase a block of 128KB data. Here, the difference is around 150 times.\nThe reading capacity of NAND Flash can turn out to be faster for the sequential reads. Here also, the data writing can only be done in case a block is empty. Similar to read, the writing of data is also done over pages (generally 2 KB). For example, a page singly written down alone on S34ML04G2 NAND Flash typically takes up to 300S. Power Consumption During initial power turn on NOR Flash memories need more current. But, when put on standby mode, the power consumption is lower than NAND Flash. Require less current than NOR Flash during initial current flow. However, in standby mode it requires more current. Reliability Source: Nand vs Nor Flash Memory : Comparison table CHARACTERISTIC NOR FLASH NAND FLASH Memory Architecture A number of memory cells (typically 8 cells) are linked to a series that is identical to a NAND gate. Memory Range and Cell Size Good Bits Typically has 98% good bits at the time of shipping with additional bit failure during the course of the life of the part. That is the reason why it requires ECC or error correcting code functionality inside the device. Memory Capacity The memory capacity comes within a range of 1 GB to 16 GB. That is the reason why it is primarily used for the data storage application. Erase, Read and Write Operations This Nand vs Nor Flash Memory characteristic defines all three operational capacities. Here, each byte requires to be written along with 0 prior to erase, resulting in much slower operation.\nWhat is the SmartQoS and SmartPartition features? The SmartQoS and SmartPartition features are advanced capabilities offered by Huawei for optimizing network performance and resource allocation in their network equipment, such as routers and switches. SmartQoS: SmartQoS, or Smart Quality of Service, is a feature that allows for intelligent prioritization and management of network traffic based on different service requirements. It ensures that critical applications or services receive the necessary bandwidth and prioritization, guaranteeing a consistent quality of experience for users. Key aspects and benefits of SmartQoS include: Traffic Prioritization: SmartQoS classifies network traffic into different categories based on predefined policies or user-defined rules. It assigns appropriate priority levels to each category to ensure that critical applications, such as real-time communication or business-critical services, receive the necessary bandwidth and low latency. Bandwidth Management: SmartQoS enables bandwidth allocation and management, allowing administrators to control and prioritize network resources based on the specific needs of different applications or user groups. This helps prevent congestion, optimize network performance, and ensure a smooth user experience. Dynamic Adaptation: SmartQoS continuously monitors network conditions and adapts its traffic prioritization and resource allocation accordingly. It can dynamically adjust bandwidth allocations based on real-time network congestion, ensuring optimal performance during peak usage periods. SmartPartition: SmartPartition is a feature that enables the logical division and resource allocation of network devices, such as switches or routers, into multiple virtual devices or partitions. Each partition operates independently and has its own dedicated resources, providing enhanced security, isolation, and flexibility.\nKey aspects and benefits of SmartPartition include: Resource Isolation: SmartPartition allows network administrators to allocate specific resources, such as ports, bandwidth, and processing power, to each partition. This ensures that resources are dedicated to a particular group or application, preventing interference or resource contention between different partitions. Enhanced Security: By logically separating network devices into partitions, SmartPartition helps enhance security by isolating traffic and preventing unauthorized access between partitions. It helps protect sensitive data and critical applications from potential threats or breaches. Flexibility and Efficiency: SmartPartition enables network administrators to flexibly allocate resources based on the specific requirements of different user groups or applications. It provides a cost-effective solution by maximizing resource utilization and allowing multiple applications or user groups to share the same physical device without compromising performance or security. The SmartQoS and SmartPartition features are advanced capabilities offered by Huawei for optimizing network performance and resource allocation in their network equipment, such as routers and switches. SmartQoS: SmartQoS, or Smart Quality of Service, is a feature that allows for intelligent prioritization and management of network traffic based on different service requirements. It ensures that critical applications or services receive the necessary bandwidth and prioritization, guaranteeing a consistent quality of experience for users. Key aspects and benefits of SmartQoS include: Traffic Prioritization: SmartQoS classifies network traffic into different categories based on predefined policies or user-defined rules. It assigns appropriate priority levels to each category to ensure that critical applications, such as real-time communication or business-critical services, receive the necessary bandwidth and low latency.\nAI chatbots have emerged as interesting and versatile language models in the field of artificial intelligence. However, as users explore their capabilities, they often come across myths and misunderstandings about what they can truly achieve. In this piece, we will dispel these fallacies and disclose the truth about AI chatbots. By delving into common misconceptions, we aim to gain a clear understanding of their capabilities and limitations, enabling us to fully utilize these powerful tools. Join us as we explore the world of AI chatbots, separating fact from fiction and realizing their full potential. One common myth surrounding AI chatbots is the belief that they can solve all problems instantly. Users may expect miraculous solutions by simply asking a chatbot a question or seeking advice. However, it's important to note the limitations of AI chatbots. Despite their training, complex problems often require the expertise of human professionals or additional resources. Another misconception is that AI chatbots understand everything. People assume that these language models, trained on vast amounts of text, can comprehend any context and provide accurate information. However, AI chatbots may occasionally misinterpret context or deliver erroneous responses. It's crucial to verify their answers with credible sources to ensure accuracy. AI chatbots are sometimes mistaken as replacements for human interaction. Some believe that they can fully replicate human conversation and emotions. However, AI chatbots lack human emotions, experiences, and empathy. While they can engage in conversation and provide assistance, they work best as supplements to human connection.\nthere are variety of school of thoughts There is a myth that AI chatbots are inherently biased and unreliable. These chatbots learn from internet data, which can be biased in itself. Users should approach their responses with a critical mindset and consult multiple sources to interpret their answers accurately. Another misconception is the assumption that AI chatbots are always available. Their accessibility depends on server load, maintenance, and technological factors. Users should be aware that AI chatbots may occasionally be unavailable or respond slower than expected. In addition, there are some amusing myths surrounding AI chatbots. Some people believed that AI chatbots had the ability to predict the future. They thought that by asking the right questions, they could receive hints about future events. However, it turned out that their predictions were as accurate as the messages found in fortune cookies, leaving users perplexed and pondering the mysteries of the world. As these myths about AI chatbots' supposed powers are debunked, it becomes clear that they are flawed but enjoyable companions. The true power of AI chatbots lies in their ability to initiate interesting conversations and provide helpful information to the best of their abilities. While they may not predict the future or communicate with animals, they can still bring laughter with their unintentionally humorous responses. So, in the world of AI chatbots, let's embrace the enjoyment and appreciate the simple pleasures they offer. Despite their limitations, there is still room for creativity and amusing exchanges with these fascinating tools.\nWhat is Veeam Backup & Replication? Microsoft SQL Server. Oracle Database. Microsoft Exchange. Active directory. Microsoft OneDrive. Microsoft SharePoint. Licensing: Community Edition: The instance limit for backup jobs is 10 instances.It can be, for example, 10 virtual machines or 10 physical servers.Or 5 virtual machines and 5 physical servers.An important point is that this limitation does not apply to backup via VeeamZIP.Through VeeamZIP you can back up an unlimited number of virtual machines, however, VeeamZIP does not support incremental backups.In addition, to set up a scheduled launch, you need to write a small PowerShell script. Although the Community edition can correctly backup applications (Active Directory, Exchange, SharePoint, SQL), hot item-level restore via Veeam Explorers is not supported.At the same time, you can, for example, restore individual files via Object Explorer. Also, the free version does not support transaction log truncation when backing up SQL or Exchange. Lack of backup of cloud services (AWS, Azure, Google). Lack of RESTful API. Prerequisites: Windows Server 2022 - 2008 R2 SP1. Microsoft Windows 11 (version 21H2). Microsoft Windows 10 (versions 1803 to 21H1). Windows 8.1 - 7 SP1. 4-core processor. Minimum 4 GB RAM + 500 MB per backup job. In total, about 20 GB of free disk space is required for all overhead costs. Microsoft SQL Server 2008 - 2019 (Community Edition includes 2016 SP1 Express). Microsoft .NET Framework 4.7.2 (included in the distribution). Windows Installer 4.5 (included in the distribution). Microsoft Windows PowerShell 5.1 (included in the distribution). Installation: Let me specify different settings Install.\nDear All, Today we are going to learn about DLP. Introduction: Data is one of the most valuable assets for businesses today. As organizations increasingly rely on digital information, protecting sensitive data from unauthorized access, accidental leaks, or data breaches has become critical. This is where Data Loss Prevention (DLP) software plays a vital role. In this article, we will explore what DLP is, discuss its key features, and provide guidelines for choosing the best DLP software for your business. What is Data Loss Prevention? Data Loss Prevention (DLP) refers to a set of strategies, policies, and technologies designed to prevent the unauthorized disclosure or leakage of sensitive data. DLP aims to identify, monitor, and protect data at rest, in motion, and in use across an organization's network and endpoints. The primary goal of DLP is to ensure that sensitive data, such as personally identifiable information (PII), intellectual property, financial records, and trade secrets, remains confidential and doesn't fall into the wrong hands. By implementing DLP solutions, organizations can proactively detect and mitigate potential data breaches or leaks, helping to comply with data protection regulations and safeguard their reputation. Key Features of DLP Software: Effective DLP software encompasses a range of features that work together to safeguard data. Here are some key features to consider when evaluating DLP solutions: a. Data Discovery and Classification: DLP software should have robust capabilities to scan and identify sensitive data across various repositories, including databases, file shares, and cloud storage.\nIt should also provide automated classification mechanisms to tag data based on predefined policies or machine learning algorithms. b. Content Monitoring and Filtering : DLP software should monitor and analyze data in real-time, both at rest and in transit, to identify policy violations. It should be capable of inspecting content within files, emails, and other communication channels to detect and block sensitive information from being shared. c. Endpoint Protection: DLP solutions should extend their coverage to endpoints such as laptops, desktops, and mobile devices. This includes monitoring and controlling data transfers to external devices, encrypting data, and preventing unauthorized applications or processes from accessing sensitive data. d. User and Entity Behavior Analytics (UEBA): Advanced DLP software incorporates machine learning algorithms to analyze user behavior and identify anomalous patterns that might indicate potential data breaches or insider threats. UEBA helps in detecting and responding to data exfiltration attempts in real-time. e. Incident Response and Reporting: DLP solutions should provide comprehensive incident management capabilities, including alerts, notifications, and remediation workflows. Detailed reporting and audit logs are essential for compliance purposes and post-incident analysis. How to Choose the Best DLP Software for Your Business: Selecting the most suitable DLP software for your business requires careful consideration of several factors. Here are some guidelines to help you make an informed decision: a. Understand Your Data: Begin by understanding the types of data your organization handles and the associated risks. Identify the most critical data and compliance requirements to determine which DLP features are essential for your environment.\nb. Scalability and Integration: Assess the scalability of the DLP solution to accommodate your organization's growth. Ensure it integrates seamlessly with existing security infrastructure, such as firewalls, SIEM systems, and identity and access management tools. c. Flexibility and Customization: Look for a DLP solution that allows customization to align with your organization's policies and industry-specific requirements. The software should provide flexibility in creating rules, policies, and exceptions to match your unique data protection needs. d. Ease of Deployment and Management: Consider the ease of deploying and managing the DLP software. A user-friendly interface, centralized management console, and automated policy enforcement capabilities will simplify the administration and reduce the burden on IT teams. e. Compliance and Regulations: Verify that the DLP software meets the compliance standards and regulations relevant to your industry, such as GDPR, HIPAA, or PCI DSS. Look for features like data encryption, data access controls, and auditing capabilities to ensure compliance with data protection laws. f. Vendor Reputation and Support: Evaluate the reputation and track record of the DLP software vendor. Consider factors such as customer reviews, industry recognition, and the availability of reliable customer support and maintenance services. Data Loss Prevention (DLP) software plays a crucial role in protecting sensitive data from unauthorized access, leaks, and breaches. By understanding the concept of DLP, exploring its key features, and following the guidelines for selecting the best DLP software for your business, you can effectively safeguard your valuable data and maintain compliance with data protection regulations.\nIn today's era where each and everything is almost stored on drive or cloud using the database management hence it supports the storage as well as is easy to access data whenever and wherever we want to recover. Data replication is a technique for the storage of data across multiple locations. It is a process to create and maintain multiple copies of data. It involves the process of data storage from a source location or system to multiple destination locations using database management. It supports the unwanted data destruction issue rectification while storing on servers or different locations causes the data to be more secure and reliable. Figure 1.1: Data replication The data replication is a process of maintaining and creating multiple copies of data over the database using the database management techniques. Figure 1.2: Data replication process It involves different types of data replication process. Full replication that involves the sending, creating or maintaining full data storage at different location or targets. Figure 1.3: Full replication Partial replication involves the partial storage, maintenance and sending of data at different targets where only a small set of data is replicated, not the complete one.\nFigure 1.4: Partial replication No replication involves the replication at a single system and hence a poor availability of data is replicated through it Figure 1.5: No replication There are two types of data replication that are as follows: Synchronous replication Synchronous replication is defined as the replication that takes place at real time and is preferred more as the chance of data lost is minimum. This approach is used by high end transactional applications causing the replication speed to increase and causing the overall structure to be expensive, but the latency of the application slows down. Asynchronous replication Asynchronous replication is a type of replication process in which the time delay is used. It is designed to work on distance and requires less frequency bandwidth. There is a lot of time required for the replication process. Snapshot replication Snapshot replication is a type of replication that involves the replication of data at a given time and it doesn't pay attention if the data is changed or not. This type of replication is used when there is frequent replication of data Merge replication Merge replication is a type of replication that involves the changes in the replication afterwards. In merge replication the data is combined from two different sources to formulate a single copy of the data file. Huawei provides several data replication techniques, depending on the requirements of the specific use case.\nSome of the common techniques are: Remote Replication: This technique involves replicating data from one storage system to another located at a remote site. It can be synchronous or asynchronous, depending on the distance between the sites and the availability of network bandwidth. Local Replication: This technique involves creating multiple copies of data within the same storage system. The data is copied to another set of disks, typically in the same or a different RAID group. Continuous Data Protection (CDP): This technique involves creating a continuous backup of data by replicating every change made to the data. It provides a recovery point objective (RPO) of zero, which means that there is no data loss in case of a failure. Snapshot Replication: This technique involves creating a point-in-time copy of data, which can be used to restore the data in case of a failure. Snapshots can be created manually or automatically, depending on the requirements. Cascading Replication: This technique involves replicating data from one storage system to another and then to a third storage system. It is useful in scenarios where there are multiple remote sites that need to be backed up. These techniques can be implemented using Huawei's storage systems and software, such as OceanStor Replication Director and OceanStor BCManager. It is recommended to consult with Huawei experts to determine the best replication technique for a specific use case.\nThere are several approaches for data replication that involves: Host based replication: it requires servers for data replication using software and is file based replication process involving the asynchronous replication that provides encryption, security to the file. It supports both synchronous and asynchronous replication Hypervisor based replication: it replicates the virtual machine from source to destination. It requires CPU resources as VMs are replicated from source to multiple hosts, but the only issue is it causes a sudden failure as a single data file is missing during replication the complete system fails. Network based replication: It requires switches and devices between the host and servers. It can store data in multiple data streams at multiple locations. It supports both synchronous and asynchronous replication. Array based replication: it requires an array to store data and it has built in software that enables the replication. It requires similar source and destination arrays. Figure 1.6: replication procedure Advantage of data replication includes: Data quality is improved as it can be easily read by replicated data instead of the remote The data availability is increased as there are multiple copies of the data. The database become scalable as the data can be easily read bg primary database. The execution of replication is faster, and queries are handled timely Disadvantages of data replication includes: The complexity of data storage increases that should be maintained and stored. The data become vulnerable as the data is continuously replicated and updated.\nThe storage and network size increase due to different storage of copies on the database The cost or expenditure of the network increases due to a higher number of copies of data. There are many applications of Data replication that are as follows: Load is balanced on different sites and servers that improves the performance. It helps to provide a traffic free server website. It creates a data warehouse that helps organizations to store their data in a meaningful manner and is centralized to analyze the report accurately. It enhances regional data redundancy as the data is stored at multiple locations so any outrage to access data at one point can help in getting the data from any other region. It provides a long term backup to data files during any mishap or issue the data can be easily accessible from any other location. It provides synchronized data and up to date data. It helps to increase productivity and collaboration with sites and organizations in storing data files. Figure 1.7: Applications In conclusion the data replication technique has many features and benefits in providing a centralised data library or replicating the data at different locations to help to store files and get easy access whenever we want. It creates a backup to important files and helps in management and access of files.It helps to manage data on servers in different manners and provide easy accessibility.\nDear All, Today we are going to discuss DRAM SSDs vs DRAM-less SSDs. Introduction: Solid-State Drives (SSDs) have revolutionized data storage by offering faster speeds and improved reliability compared to traditional hard disk drives. Within the realm of SSDs, there are two main categories: DRAM SSDs and DRAM-less SSDs. Understanding the differences between these two types is crucial when considering the best storage solution for specific use cases. In this article, we will explore the dissimilarities between DRAM SSDs and DRAM-less SSDs, discuss the reasons why they may not be suitable for certain scenarios, and shed light on what DRAM-less SSDs actually are. Difference between DRAM SSDs and DRAM-less SSDs: DRAM SSDs: DRAM (Dynamic Random Access Memory) SSDs incorporate DRAM as a cache component to accelerate read and write operations. These drives utilize a combination of NAND flash memory and DRAM, where the DRAM acts as a buffer or temporary storage area for frequently accessed data. By caching data in DRAM, DRAM SSDs can offer exceptional performance, low latency, and high input/output (I/O) operations per second (IOPS). This results in faster data access and improved overall system responsiveness. DRAM-less SSDs: DRAM-less SSDs, as the name suggests, do not include a dedicated DRAM cache. Instead, they rely solely on NAND flash memory for data storage and retrieval. Without the presence of DRAM, these SSDs compensate by implementing sophisticated firmware algorithms and utilizing the drive's controller to manage data access efficiently.\nWhile they may not match the performance levels of DRAM SSDs, DRAM-less SSDs are often more cost-effective, making them suitable for budget-conscious users or applications that do not require the highest performance levels. Why Shouldn't You Use DRAM SSDs or DRAM-less SSDs? DRAM SSDs: Despite their significant advantages, DRAM SSDs may not be suitable for every use case. The main drawback of DRAM SSDs is their higher cost compared to DRAM-less SSDs. This makes them less appealing for budget-oriented consumers or applications where cost is a crucial factor. Additionally, the reliance on volatile DRAM for caching means that if power is lost abruptly, data stored in the cache may be lost before it has been written to the NAND flash memory. Although modern DRAM SSDs include power-loss protection mechanisms, this remains a potential risk. DRAM-less SSDs: While DRAM-less SSDs offer a more cost-effective storage solution, they have limitations in terms of performance. Without a dedicated DRAM cache, the drives may experience reduced random I/O performance, longer access times, and decreased overall responsiveness. Therefore, applications that heavily rely on fast random data access, such as databases or virtual machines, may not perform optimally on DRAM-less SSDs. However, DRAM-less SSDs are still suitable for general consumer use, including everyday computing, web browsing, and media storage, where performance demands are typically lower. What are DRAM-less SSDs? DRAM-less SSDs are solid-state drives that do not feature a dedicated DRAM cache. Instead, they utilize various techniques and firmware algorithms to optimize data access and performance.\nDear All, Today we are going to learn about Veeam. 1. What is Veeam Backup used for? Veeam Backup is a comprehensive data protection and disaster recovery solution designed to meet the backup and recovery needs of modern enterprises. It offers a wide range of features and capabilities to ensure the availability and integrity of data in virtual, physical, and cloud environments. Veeam Backup is primarily used for creating backups of critical systems, applications, and data to protect against data loss, accidental deletions, hardware failures, ransomware attacks, and other types of disasters. It provides organizations with reliable and efficient backup methods to safeguard their information and minimize downtime in case of a failure. Veeam Architecture Key features of Veeam Backup include: Image-based backups: Veeam Backup uses image-based backups, which capture a snapshot of an entire system, including the operating system, applications, and data. This approach ensures fast and efficient backups, as well as quick recovery times. Incremental backups: Veeam Backup utilizes incremental backups, which only capture the changes made since the last backup. This approach reduces backup windows and minimizes the impact on system resources. Flexible recovery options: Veeam Backup offers various recovery options, such as full system restores, granular file-level restores, and application-specific item-level restores. It enables organizations to quickly recover their data at different levels of granularity, depending on their specific needs. Replication and off-site backups: Veeam Backup allows organizations to replicate their backups to off-site locations or cloud repositories, ensuring data redundancy and enabling efficient disaster recovery strategies.\nMonitoring and reporting: Veeam Backup provides monitoring and reporting capabilities to give administrators real-time insights into the status of their backups, performance metrics, and potential issues. This helps ensure the reliability and effectiveness of the backup process. 2. What is the type of Veeam Backup? Veeam Backup is a software-based backup solution. It is designed to work across various types of environments, including virtual, physical, and cloud infrastructures. Veeam Backup supports a wide range of platforms, such as VMware vSphere, Microsoft Hyper-V, Nutanix AHV, physical servers, workstations, and cloud platforms like Microsoft Azure, Amazon Web Services (AWS) and Huawei Cloud. The flexibility of Veeam Backup allows organizations to protect their data regardless of the underlying infrastructure. Whether an organization has a virtualized environment, physical servers, or a mix of both, Veeam Backup can adapt to the specific requirements and provide reliable backup and recovery capabilities. 3. What is Veeam Backup & Replication? Veeam Backup & Replication is a comprehensive data protection and disaster recovery solution offered by Veeam Software. It combines backup and replication functionalities into a single platform, providing organizations with a unified solution for data availability and resilience. Veeam Backup & Replication builds upon the features of Veeam Backup and extends them with additional capabilities: Backup: Veeam Backup & Replication offers image-based backups, incremental backups, flexible recovery options, and monitoring/reporting features, as described earlier. Replication: In addition to backups, Veeam Backup & Replication enables organizations to create replicas of their critical systems and data.\nIOPS ( ) is a measure of the that a storage device can perform in . It is a for evaluating the systems, particularly in applications that require , such as databases or virtualized environments. The IOPS performance characteristics of a storage system can vary significantly depending on several factors, including the type of storage technology used, the storage controller's architecture, the size of the I/O request, and the workload's nature. Different types of storage technology have different IOPS performance characteristics. have and and NVMe storage have , resulting in higher IOPS. As a result, SSDs and NVMe storage are becoming increasingly popular in data center and cloud computing environments. The storage controller's architecture can significantly impact IOPS performance. Modern storage systems typically use a to manage data redundancy and improve I/O performance. The and can significantly impact IOPS performance, with write-back policies providing higher performance at the cost of potential data loss in the event of a power failure. The size of the I/O request can also impact IOPS performance. can be processed more However, small I/O requests require more processing overhead, resulting in lower IOPS. The optimal I/O request size varies depending on the storage technology and workload. The nature of the workload can also impact IOPS performance. Workloads with a . Measuring IOPS involves running a benchmark test that simulates the storage system's workload and tracks the number of .\nThere are several tools available to perform these tests, including When measuring IOPS, it is essential to The workload should include a mix of read and write operations, with a distribution that matches the application's I/O profile. IOPS and throughput are to evaluate storage system performance. measures the number of i , while measures the While high , other factors such as the size of the and the system can also impact throughput. As a result, it is essential to evaluate both IOPS and throughput when assessing storage system performance.\nInput/Output Operations Per Second Amount of data transferred in one second Measures the number of input/output operations that a storage device can perform in one second Measures the amount of data that can be transferred between a storage device and a host system in one second Useful for evaluating storage systems that require high levels of random access input/output operations, such as databases or virtualized environments Useful for evaluating storage systems that transfer large amounts of data, such as streaming or backup applications The number of read/write operations completed in one second The amount of data transferred in one second Factors that impact performance Storage technology, storage controller architecture, I/O request size, and workload nature I/O request size, latency of the storage system, and network bandwidth High IOPS can translate into high throughput, but other factors can impact throughput High throughput does not necessarily translate into high IOPS, as it depends on the size of the I/O request and the latency of the storage system Critical metric for evaluating the performance of storage systems in random access workloads Important metric for evaluating the performance of storage systems in sequential workloads IOPS and throughput are two different metrics used to evaluate storage system performance, with IOPS being more important for random access workloads and throughput being more important for sequential workloads. While high IOPS can translate into high throughput, other factors such as I/O request size and latency can also impact throughput.\nHi All, Today I am going to show how to add OS Dorado to your host. OceanStor Dorado V6 is a high-performance, all-flash storage system developed by Huawei. As an AI language model, I can provide you with a general overview of the host configuration process for the OceanStor Dorado V6. However, please note that specific configuration steps may vary depending on your environment and requirements. It's always recommended to consult the official documentation or contact Huawei support for detailed and up-to-date information. Following is the main flow chart that needs to be followed. Here is a general outline of the host configuration process for OceanStor Dorado V6: Physical Connectivity: Connect the OceanStor Dorado V6 storage system to the host server using Fibre Channel (FC), iSCSI, or other supported protocols. Ensure that the physical connections between the storage system and the host server are properly established. Network Configuration: Configure the network settings on both the storage system and the host server to ensure they are in the same network segment. Assign IP addresses, subnet masks, gateways, and other necessary network parameters. Host Configuration: Install the appropriate drivers or software components on the host server to enable communication with the OceanStor Dorado V6 storage system. Configure multipathing software, such as PowerPath, MPIO, or native multipathing, to ensure redundant paths to the storage system and improved performance. Enable and configure any required features like jumbo frames or flow control based on your network environment and best practices.\nComputer memory is usually divided into internal memory or external memory. Internal memory , also known as \"main memory or main memory,\" refers to memory that stores small amounts of data that can be quickly accessed while the computer is running. External memory , also known as \"secondary memory,\" is a storage device that can persist or store data. They can be embedded or removable storage devices. For example, hard or solid state drives, USB flash drives, and optical discs. There are basically two kinds of internal memory: ROM and RAM. ROM stands for read-only memory. It is non-volatile, which means it can retain data even without power. It is used mainly to start or boot up a computer. RAM ,which stands for random-access memory, which temporarily stores data while the central processing unit (CPU) is executing other tasks. With more RAM on the computer, the less the CPU has to read data from the external or secondary memory (storage device), allowing the computer to run faster. RAM is fast but it is volatile, which means it will not retain data if there is no power. It is therefore important to save data to the storage device before the system is turned off. There are two main types of RAM: Dynamic RAM (DRAM) and Static RAM (SRAM). DRAM (pronounced DEE-RAM), is widely used as a computers main memory. Each DRAM memory cell is made up of a transistor and a capacitor within an integrated circuit, and a data bit is stored in the capacitor.\nSince transistors always leak a small amount, the capacitors will slowly discharge, causing information stored in it to drain; hence, DRAM has to be refreshed (given a new electronic charge) every few milliseconds to retain data. Pronounced DEE-RAM, DRAM is widely used as a computers main memory. Each DRAM memory cell is made up of a transistor and a capacitor within an integrated circuit, and a data bit is stored in the capacitor. Since transistors always leak a small amount, the capacitors will slowly discharge, causing information stored in it to drain; hence, DRAM has to be refreshed (given a new electronic charge) every few milliseconds to retain data. The main advantages of DRAM are its simple design and low cost in comparison to alternative types of memory. The main disadvantages of DRAM are its high volatility and high power consumption relative to other options. SRAM (pronounced ES-RAM) is made up of four to six transistors. It keeps data in the memory as long as power is supplied to the system unlike DRAM, which has to be refreshed periodically. As such, SRAM is faster but also more expensive, making DRAM the more prevalent memory in computer systems. Pronounced S-RAM, SRAM is made up of four to six transistors. It keeps data in the memory as long as power is supplied to the system unlike DRAM, which has to be refreshed periodically. As such, SRAM is faster but also more expensive, making DRAM the more prevalent memory in computer systems.\nSRAM does not need to be refreshed because it operates on the principle of switching the current flow in one of two directions rather than holding a charge in place within a storage cell. SRAM is generally used for cache memory, which can be accessed more quickly than DRAM. SRAM is not commonly used for consumer applications and is more expensive than DRAM. Synchronous DRAM (SDRAM) \"synchronizes\" the memory speed with the CPU clock speed so that the memory controller knows the exact clock cycle when the requested data is ready. This allows the CPU to execute more instructions at a given time. Typical SDRAM transfers data at speeds up to 133 MHz. Rambus DRAM (RDRAM) is named after Rambus, the company that makes it. It was popular in the early 2000s, primarily for video game devices and graphics cards, with transfer speeds of up to 1 GHz. Double Data Rate SDRAM (DDR SDRAM) is a type of synchronous memory that nearly doubles the bandwidth of a single data rate (SDR) SDRAM operating at the same clock frequency by using a method called \"double pumping.\" This method allows data to be transferred on the rising and falling edges of the clock signal without increasing the clock frequency. DDR1 SDRAM has been replaced by DDR2, DDR3, and more recently DDR4 SDRAM. Although these modules work the same way, they are not backward compatible. Each generation delivers higher transfer rates and faster performance.\nhow DME (Distributed Media Engine) simplifies storage management in video processing and delivery environments through various mechanisms and features. While the specific implementation and capabilities may vary based on the version and configuration of DME, here are some ways DME achieves storage management simplification: Centralized Storage Configuration: DME provides a centralized management interface where administrators can configure and manage storage resources. This centralization simplifies the setup and configuration of storage systems used by DME. Integration with Storage Systems: DME integrates with storage systems, enabling seamless communication and interaction. It supports various storage technologies and protocols, allowing administrators to leverage existing storage infrastructure and simplify integration efforts. Automated Ingestion and Organization: DME automates the ingestion of video assets into the storage system. It can handle tasks such as metadata extraction, file naming conventions, and organization of video files in appropriate directories or folders. This automation streamlines the storage of video assets, simplifying organization and retrieval. Dynamic Resource Allocation: DME dynamically manages storage resources based on the workload and processing requirements. It optimizes storage utilization, allocating resources as needed to ensure efficient storage management. This dynamic allocation simplifies the manual configuration and optimization of storage capacity. Automated File Movement: DME includes features to automatically move or replicate video files across storage tiers or locations. It can intelligently manage file movement based on predefined policies or rules. This automated file movement simplifies data management, enables tiered storage strategies, and ensures efficient access to video assets. Data Retention and Archiving: DME facilitates automated data retention and archiving processes.\nHow Operations and Maintenance (O&M) is simplified with DME DME (Distributed Media Engine) can simplify Operations and Maintenance (O&M) processes in several ways, making the management and upkeep of the video processing and delivery system more efficient. Here are some ways in which DME simplifies O&M: Centralized Management: DME provides a centralized management interface, often a web-based user interface (UI), that allows administrators to configure, monitor, and control the entire system from a single location. This centralized management simplifies the overall administration and reduces the need to access and manage individual nodes separately. Automation of Routine Tasks: DME automates various routine tasks involved in video processing and delivery workflows. This automation reduces manual effort and human errors, streamlines operations, and simplifies the management of repetitive tasks. For example, DME can automate the transcoding, packaging, and streaming processes, minimizing the need for manual intervention. Monitoring and Alerting: DME includes monitoring capabilities that track the health, performance, and status of the video processing system. It collects metrics and provides real-time monitoring dashboards, allowing administrators to easily identify and diagnose issues. Additionally, DME can generate alerts or notifications when predefined thresholds or anomalies are detected, enabling proactive maintenance and issue resolution. Resource Optimization: DME optimizes resource utilization, including CPU, memory, and storage, to achieve efficient and scalable video processing. By dynamically allocating and managing resources based on workload demands, DME simplifies resource management, reduces manual configuration, and ensures optimal performance. Diagnostics and Troubleshooting: DME includes diagnostic tools and logging mechanisms that facilitate troubleshooting and issue resolution.\nCan Huawei's SAN solution integrate with existing storage management frameworks or third-party applications? Yes, Huawei's SAN solution can integrate with existing storage management frameworks or third-party applications. It supports a variety of integration methods, including: SNMP: Huawei's SAN solution supports SNMP (Simple Network Management Protocol), which allows it to be managed by a variety of SNMP-based management frameworks. CIFS: Huawei's SAN solution supports CIFS (Common Internet File System), which allows it to be accessed by Windows-based applications. NFS: Huawei's SAN solution supports NFS (Network File System), which allows it to be accessed by Unix-based applications. iSCSI: Huawei's SAN solution supports iSCSI (Internet Small Computer System Interface), which allows it to be accessed by iSCSI-based applications. FCoE: Huawei's SAN solution supports FCoE (Fibre Channel over Ethernet), which allows it to be accessed by Fibre Channel-based applications. In addition to these standard integration methods, Huawei also offers a variety of proprietary integration methods that can be used to integrate its SAN solution with specific third-party applications. Here are some of the benefits of integrating Huawei's SAN solution with existing storage management frameworks or third-party applications: It can simplify the management of storage resources. It can improve the performance of storage applications. It can increase the availability of storage resources. It can provide a single point of management for all storage resources. It can help to reduce the cost of storage management.\nDME (Distributed Media Engine) can contribute to increasing the operational efficiency of data centers in several ways. While the specific benefits and impact may vary depending on the specific deployment and use cases, here are some ways DME can enhance operational efficiency: Scalability and Resource Optimization: DME enables horizontal scaling by distributing video processing tasks across multiple nodes. This allows data centers to scale their processing capacity based on demand, optimizing resource utilization and avoiding underutilization or overprovisioning of resources. Automation and Workflow Efficiency: DME automates various video processing tasks, such as transcoding, packaging, and streaming, reducing the need for manual intervention. Automated workflows can streamline operations, reduce human errors, and improve overall efficiency by minimizing manual effort and intervention. Load Balancing and Performance Optimization: DME includes load balancing mechanisms to evenly distribute processing tasks across multiple nodes. This load balancing helps optimize resource utilization, prevents overloading of specific nodes, and ensures efficient processing of video workloads. Resource Consolidation and Cost Reduction: By leveraging DME's capabilities, data centers can consolidate video processing tasks onto a single platform, reducing the need for multiple specialized systems. This consolidation can lead to cost savings in terms of infrastructure, maintenance, and operational expenses. Intelligent Resource Management: DME incorporates intelligent resource management features, such as dynamic resource allocation and auto scaling. These capabilities enable efficient utilization of computing resources, allowing data centers to allocate resources based on workload demands, improving performance, and reducing resource wastage.\nhow does DME DME (Distributed Media Engine) primarily focuses on video processing and delivery rather than providing comprehensive storage lifecycle automation. While DME may include some storage-related functionalities within its workflows, it may not cover the entire lifecycle of storage management. However, in the context of video processing and delivery, DME may incorporate automation and integration with storage systems to support specific tasks and stages of the storage lifecycle. Here are some examples: Storage Provisioning: DME can integrate with storage systems to automate the provisioning of storage resources required for video assets, temporary files, and metadata. This can include automatically creating and configuring storage volumes or file systems as needed. Data Ingestion: DME may provide automated mechanisms to ingest video assets into the storage system, ensuring proper organization, metadata association, and file structure. This can include automated metadata extraction, file naming conventions, and tagging. Storage Monitoring: DME can incorporate monitoring capabilities to track storage health, capacity utilization, and performance. This may involve collecting metrics, generating alerts, and providing visibility into storage-related metrics through DME's management interface. Automated File Movement: DME may include features to automatically move or replicate video files across storage tiers or locations based on predefined policies or rules. This can optimize storage utilization, enable tiered storage strategies, and ensure efficient data access. Data Retention and Archiving: DME may facilitate automated data retention and archiving processes, allowing video assets to be archived or removed from primary storage based on retention policies or predefined criteria.\nThis can help manage storage capacity and ensure compliance with data retention requirements. It's important to note that while DME may offer some automation capabilities related to storage management, its primary focus is on video processing and delivery rather than comprehensive storage lifecycle automation. For more advanced storage automation and lifecycle management, dedicated storage management solutions or platforms may be more appropriate. For specific details on the storage automation capabilities within DME and its support for the entire storage lifecycle, I recommend referring to the official documentation, release notes, or contacting Cisco support for the most accurate and up-to-date information.DME (Distributed Media Engine) primarily focuses on video processing and delivery rather than providing comprehensive storage lifecycle automation. While DME may include some storage-related functionalities within its workflows, it may not cover the entire lifecycle of storage management. However, in the context of video processing and delivery, DME may incorporate automation and integration with storage systems to support specific tasks and stages of the storage lifecycle. Here are some examples: Storage Provisioning: DME can integrate with storage systems to automate the provisioning of storage resources required for video assets, temporary files, and metadata. This can include automatically creating and configuring storage volumes or file systems as needed. Data Ingestion: DME may provide automated mechanisms to ingest video assets into the storage system, ensuring proper organization, metadata association, and file structure. This can include automated metadata extraction, file naming conventions, and tagging. Storage Monitoring: DME can incorporate monitoring capabilities to track storage health, capacity utilization, and performance.\ncan we use DMe for data migration from one storage to any other storage. DME (Distributed Media Engine) is primarily designed for video processing and delivery rather than general-purpose data migration. While DME may have some capabilities related to data movement within its media processing workflows, it may not be the ideal solution for large-scale or complex data migration tasks. Data migration typically involves moving large volumes of data between storage systems, often with specific requirements such as data integrity, minimal downtime, and compatibility with different storage formats or protocols. DME's primary focus is on video transcoding, packaging, and streaming rather than comprehensive data migration functionalities. For general-purpose data migration tasks, it is recommended to explore dedicated data migration tools, platforms, or services that are specifically designed to handle such requirements. These solutions often provide features like data validation, error handling, data transfer optimization, and support for various storage systems and protocols. When considering data migration, factors such as the specific source and destination storage systems, network connectivity, data integrity, migration speed, and downtime constraints should be taken into account. It is advisable to consult with data migration specialists or explore appropriate data migration solutions to ensure a smooth and successful migration process. While DME may not be the primary choice for data migration, it can still play a role in the context of video-related data migration scenarios, such as transcoding video files to a different format or repackaging videos for compatibility with a new streaming platform.\nhow DME achieve end-to-end automatic resource provisioning? DME (Distributed Media Engine) aims to achieve end-to-end automatic resource provisioning through its intelligent resource management capabilities. While the specific implementation and features may vary based on the version and configuration of DME, here are some general principles and mechanisms that DME may utilize: Workload Monitoring: DME continuously monitors the incoming video processing workload and resource utilization across the system. It collects metrics related to processing tasks, such as CPU usage, memory usage, and network bandwidth. Resource Capacity Planning: Based on workload patterns, historical data, and performance metrics, DME analyzes resource capacity and identifies potential bottlenecks or areas of inefficiency. It considers factors such as available processing power, storage capacity, and network bandwidth. Dynamic Resource Allocation: DME dynamically allocates system resources, such as CPU, memory, and storage, based on the current workload and processing requirements. It can adjust resource allocation in real-time to optimize performance and capacity utilization. Auto Scaling: DME supports auto scaling capabilities, allowing the system to automatically adjust the number of processing nodes based on workload demands. When the workload increases, DME can provision additional nodes to handle the increased processing requirements. Similarly, when the workload decreases, DME can scale down resources to optimize resource utilization and cost. Load Balancing: DME incorporates load balancing mechanisms to evenly distribute processing tasks across multiple nodes. Load balancing algorithms consider factors such as node capacity, current workload, and network conditions to ensure efficient utilization of available resources.\nhow dme resolves 70% of performance and capacity issues automatically DME (Distributed Media Engine) includes features and mechanisms that aim to automatically resolve performance and capacity issues to a certain extent. While the exact details and capabilities may vary based on the specific version and configuration of DME, here are some general approaches used to address performance and capacity issues: Dynamic Resource Allocation: DME can dynamically allocate system resources, such as CPU and memory, based on the workload and processing requirements. By intelligently distributing resources among the nodes and tasks, DME can optimize performance and capacity utilization. Load Balancing: DME incorporates load balancing mechanisms to distribute processing tasks evenly across multiple nodes. This helps prevent overloading of individual nodes and ensures efficient utilization of available resources. Load balancing algorithms may consider factors such as node capacity, current workload, and network conditions. Task Scheduling and Prioritization: DME's task scheduler prioritizes and schedules video processing tasks based on factors such as priority level, deadline, and available resources. By efficiently managing the task queue and allocating resources accordingly, DME aims to maximize throughput and minimize processing delays. Auto Scaling: DME supports auto scaling capabilities, allowing the system to automatically adjust the number of nodes based on workload demands. When the workload increases, DME can provision additional nodes to handle the increased processing requirements. Similarly, when the workload decreases, DME can scale down the resources to optimize resource utilization and cost.\nhow DME connects/integrates with mainstream information technology Service Management (ITSM) platforms. DME (Distributed Media Engine) is primarily focused on video processing and delivery, and it does not have direct built-in integrations with mainstream IT Service Management (ITSM) platforms. However, it is possible to establish integrations between DME and ITSM platforms using custom solutions or by leveraging other software components. Here are a few approaches to integrating DME with ITSM platforms: Custom Development: You can develop custom scripts or applications that interact with both DME and your ITSM platform's APIs. These scripts can automate certain tasks, such as creating ITSM tickets for video processing requests, updating ticket statuses based on video processing progress, or extracting relevant metadata from DME for ITSM reporting. Middleware or Orchestration Tools: Utilize middleware or orchestration tools, such as enterprise service bus (ESB) or workflow automation platforms, to facilitate the integration between DME and ITSM. These tools can act as intermediaries, allowing data exchange and workflow coordination between the two systems. Webhooks or Event-Based Integrations: DME may support event notifications or webhooks, which can trigger actions in your ITSM platform. For example, you can configure DME to send notifications to your ITSM platform when video processing tasks are completed or encounter errors. This can initiate automated ticket creation or updates within the ITSM system. Manual Workflow Integration: In some cases, integration may involve manual steps. For example, you can establish communication channels or workflows between DME operators and ITSM teams to ensure proper coordination and handover of video processing tasks.\ncan you help me to get OceanStor SNS3096 Configuration to connect OceanStor 2600 V5 and Server's. configuring an OceanStor SNS3096 switch to connect an OceanStor 2600 V5 storage system and servers. Here's a step-by-step process you can follow: Connect the SNS3096 switch to your network infrastructure. Ensure that the switch is powered on and connected to your management network. Access the SNS3096 switch's management interface. You can do this by entering the switch's IP address into a web browser or by using a console cable and terminal emulator software. Log in to the switch using the appropriate credentials. Configure the switch's management IP address, subnet mask, gateway, and DNS settings. This will allow you to access the switch's management interface easily. Verify the connectivity between the SNS3096 switch and the OceanStor 2600 V5 storage system. Ensure that both devices are connected to the same network and can communicate with each other. Configure the switch ports that will connect to the servers. Depending on your requirements, you may need to configure VLANs, port channels, or other settings to ensure proper connectivity and traffic segregation. Connect the servers to the appropriate switch ports. Make sure the server network interfaces are properly configured with the correct IP addresses, subnet masks, and gateway settings. Validate the connectivity between the servers and the storage system. You can do this by verifying that the servers can access the storage system's management interface and that they can communicate with each other.\nDear, Firstly, it uses RAID technology to protect against data loss due to disk failures. The system supports RAID 1, 5, 6, 10, 50, and 60, which provide different levels of data protection and performance. Secondly, the system uses advanced data protection features such as snapshot, remote replication, and backup to ensure data consistency and integrity. Snapshots allow users to create point-in-time copies of data, which can be used for data recovery in case of accidental deletion or corruption. Remote replication allows users to replicate data to a remote site for disaster recovery purposes. Backup allows users to create a copy of data that can be used for recovery in case of a catastrophic failure. Thirdly, the system uses data verification techniques such as checksum and data scrubbing to ensure data integrity. Checksum is a mathematical function that generates a unique value for each block of data, which can be used to verify the integrity of the data. Data scrubbing is a process that periodically reads and verifies the data on the disk to detect and correct any errors. Lastly, the system uses advanced hardware and software components to ensure data consistency and integrity. The system uses high-quality components such as SSDs, controllers, and power supplies, which are designed to provide high reliability and availability. The system also uses advanced software features such as data compression, deduplication, and thin provisioning, which help to optimize storage utilization and reduce the risk of data corruption. Thanks.\nCan you provide information about the high availability features and redundancy mechanisms in the OceanStor Pacific 9950? Dear friend, The OceanStor Pacific 9950 is a high-performance distributed storage system with multiple high availability and redundancy mechanisms to ensure high reliability and stability of the system. The following table lists the high availability and redundancy mechanisms of the OceanStor Pacific 9950. 1. Primary node and backup node: OceanStor Pacific 9950 has two primary nodes and one backup node. Each primary node has a complete copy of the storage system to ensure high availability and fault tolerance of the system. When the active node is faulty, the standby node automatically switches to the active node to ensure data security and reliability. 2. Redundant hardware and software: OceanStor Pacific 9950 uses redundant hardware and software to ensure high availability of the system. Each node uses different hardware and software components to ensure system reliability and stability. 3. Data replication and backup: The OceanStor Pacific 9950 supports data replication and backup to ensure that data can be safely transferred or backed up to the backup node or standby hardware when the primary node fails. 4. Failover and automatic recovery: The OceanStor Pacific 9950 uses failover and automatic recovery mechanisms to ensure that when the primary node fails, the system can automatically switch to the backup node or standby hardware and quickly restore data. 5. Multi-tenant and load balancing: OceanStor Pacific 9950 supports multi-tenant and load balancing to ensure system availability and performance.\nWhat steps should be taken if I experience data corruption or integrity issues on the Huawei OceanStor Dorado 18800K V6? Dear friend, 1. Identify the bottleneck: The first step is to identify the bottleneck. This can be done by monitoring the system performance and identifying the component that is causing the slowdown. You can use tools like Huawei eSight or third-party monitoring tools to monitor the system performance. 2. Check the storage system configuration: Check the storage system configuration to ensure that it is optimized for performance. This includes checking the RAID level, disk type, and disk layout. You should also ensure that the storage system is properly configured for the workload. 3. Check the network configuration: Check the network configuration to ensure that it is optimized for performance. This includes checking the network bandwidth, latency, and packet loss. You should also ensure that the network is properly configured for the workload. 4. Check the host configuration: Check the host configuration to ensure that it is optimized for performance. This includes checking the host operating system, drivers, and firmware. You should also ensure that the host is properly configured for the workload. 5. Check the application configuration: Check the application configuration to ensure that it is optimized for performance. This includes checking the application settings, database configuration, and application workload. 6. Tune the system: Once you have identified the bottleneck, you can tune the system to improve performance. This may involve adjusting the storage system configuration, network configuration, host configuration, or application configuration. 7.\nHow can I diagnose and resolve performance bottlenecks on the Huawei OceanStor Dorado 18800K V6? Dear friend, 1. Identify the bottleneck: The first step is to identify the bottleneck. This can be done by monitoring the system performance and identifying the component that is causing the slowdown. You can use tools like Huawei eSight or third-party monitoring tools to monitor the system performance. 2. Check the storage system configuration: Check the storage system configuration to ensure that it is optimized for performance. This includes checking the RAID level, disk type, and disk layout. You should also ensure that the storage system is properly configured for the workload. 3. Check the network configuration: Check the network configuration to ensure that it is optimized for performance. This includes checking the network bandwidth, latency, and packet loss. You should also ensure that the network is properly configured for the workload. 4. Check the host configuration: Check the host configuration to ensure that it is optimized for performance. This includes checking the host operating system, drivers, and firmware. You should also ensure that the host is properly configured for the workload. 5. Check the application configuration: Check the application configuration to ensure that it is optimized for performance. This includes checking the application settings, database configuration, and application workload. 6. Tune the system: Once you have identified the bottleneck, you can tune the system to improve performance. This may involve adjusting the storage system configuration, network configuration, host configuration, or application configuration. 7.\nOceanCyber 300, Ransomware Detection Performance of NAS File System, Detection and analysis performance is 50TiB/hr as per specification datasheet. How can we test it and show to our customers while doing POC? Dear friend, 1. Create a test environment: Set up a test environment that closely resembles the customer's production environment. This environment should include the same hardware, software, and network configurations. 2. Install OceanCyber 300: Install OceanCyber 300 on the test environment and configure it according to the customer's requirements. 3. Generate Ransomware: Create a test ransomware file that can be used to simulate a ransomware attack. You can use a publicly available ransomware sample or create your own. 4. Run the test: Run the ransomware file on the test environment and observe how OceanCyber 300 detects and responds to the attack. You can also run other types of attacks to test the system's overall security capabilities. 5. Measure performance: Measure the detection and analysis performance of OceanCyber 300 during the test. You can use performance monitoring tools to measure the system's throughput and response time. 6. Document the results: Document the results of the test, including the performance metrics and any issues or challenges encountered during the test. 7. Present the results: Present the results of the test to the customer, highlighting the system's performance and capabilities. You can also provide recommendations for optimizing the system's performance and improving its security posture. Thanks. Dear friend, 1. Create a test environment: Set up a test environment that closely resembles the customer's production environment.\nWhat are the key features and benefits of HyperSnap and HyperCDP in terms of data protection and disaster recovery? Friend, Key features of HyperSnap Point-in-time snapshots: HyperSnap can create point-in-time snapshots of data, which are copies of data at a specific point in time. This allows organizations to recover data to a previous state in the event of a disaster or corruption. Continuous data protection: HyperSnap can also be used to continuously protect data. This means that data is always protected, even if a disaster occurs. Deduplication: HyperSnap can deduplicate data, which can help to reduce the amount of storage space required for backups. Compression: HyperSnap can compress data, which can also help to reduce the amount of storage space required for backups. Encryption: HyperSnap can encrypt data, which can help to protect data from unauthorized access. Remote replication: HyperSnap can replicate data to remote locations, which can help to protect data from disasters that affect a single location. Key features of HyperCDP Synchronous replication: HyperCDP can replicate data synchronously, which means that data is copied to the remote location in real time. Asynchronous replication: HyperCDP can also replicate data asynchronously, which means that data is copied to the remote location at regular intervals. Remote mirroring: HyperCDP can also use remote mirroring to create a real-time copy of data on the remote location. Data deduplication: HyperCDP can deduplicate data, which can help to reduce the amount of storage space required for backups.\n[Symptom Description] After a user logs in to the SNS3664 switch using a serial cable, no output is displayed. [Analysis] 1. The device picture provided by the customer is different from the front panel in the product documentation. The front panel of the SNS3664 is as follows that customer provided: The front panel of the SNS3664 in the product documentation is as follows: 2. Actually customer connect the serial cable to the management network port of the SNS3664, not serial port. In the following figure, the red area indicates the serial port. [Root Cause] The front panel of the newly delivered SNS3664 is changed, but the product documentation is not updated. As a result, the customer connects the serial cable to an incorrect port, causing the login failure. [Solution] Connect the serial cable delivered with the device to the correct port. The login is successful. [Appendix] The default IP address 10.77.77.77 is configured for the management interface of the SNS3664. You can use the default IP address to log in to the device. Set a static IP address for the management interface. 1. Log in to the appliance as the administrator. 2. Run the ipaddrset command to set the Ethernet IP address. If you want to use an IPv4 address, enter an IP address in dotted decimal notation as prompted. Ethernet IP Address: [192.168.74.102] If you are using an IPv6 address, enter the network information with colon-separated symbols as prompted. 3. Follow the prompts to complete the rest of the network information.\nHello friend, Here are some of the best practices for optimizing storage configuration on Huawei systems: Use the right type of storage: Different types of storage have different performance characteristics. Use the right RAID level: RAID (redundant array of independent disks) is a technology that can be used to improve the performance and reliability of storage. There are different RAID levels, each with its own advantages and disadvantages. Defragment your storage regularly: Defragmentation is the process of rearranging the files on your storage so that they are stored in contiguous blocks. Use a caching solution: A caching solution can be used to improve the performance of your storage by storing frequently accessed data in memory. Monitor your storage usage: It is important to monitor your storage usage so that you can identify potential problems early on. Back up your data regularly: It is important to back up your data regularly so that you can restore it if it is lost or corrupted. By following these best practices, you can optimize the storage configuration on your Huawei systems and improve their performance and reliability. BR, Hello friend, Here are some of the best practices for optimizing storage configuration on Huawei systems: Use the right type of storage: Different types of storage have different performance characteristics. Use the right RAID level: RAID (redundant array of independent disks) is a technology that can be used to improve the performance and reliability of storage. There are different RAID levels, each with its own advantages and disadvantages.\nHello friend, There are several options for integrating Huawei storage systems with virtualization platforms such as VMware or Hyper-V: Huawei OceanStor Storage VMotion: Huawei OceanStor Storage VMotion is a software solution that can be used to migrate virtual machines (VMs) between Huawei storage systems and VMware vSphere environments. This can be useful for disaster recovery or for migrating VMs to a new storage system. Huawei OceanStor Storage Hyper-V Integration: Huawei OceanStor Storage Hyper-V Integration is a software solution that can be used to integrate Huawei storage systems with Microsoft Hyper-V environments. This can be useful for managing VMs that are stored on Huawei storage systems from within the Hyper-V console. Huawei OceanStor Storage iSCSI Target: Huawei OceanStor Storage iSCSI Target is a software solution that can be used to create iSCSI targets on Huawei storage systems. This can be used to connect VMs that are running on VMware or Hyper-V environments to Huawei storage systems. BR, Hello friend, There are several options for integrating Huawei storage systems with virtualization platforms such as VMware or Hyper-V: Huawei OceanStor Storage VMotion: Huawei OceanStor Storage VMotion is a software solution that can be used to migrate virtual machines (VMs) between Huawei storage systems and VMware vSphere environments. This can be useful for disaster recovery or for migrating VMs to a new storage system. Huawei OceanStor Storage Hyper-V Integration: Huawei OceanStor Storage Hyper-V Integration is a software solution that can be used to integrate Huawei storage systems with Microsoft Hyper-V environments.\nWhat is the concept of File System WORM and Secure Snapshot in Huawei Ransomware Protection Storage Solution? In Huawei's Ransomware Protection Storage Solution, two key concepts contribute to enhancing data protection against ransomware attacks: File System WORM (Write Once Read Many) and Secure Snapshot. Here's an overview of these concepts: File System WORM (Write Once Read Many): File System WORM is a data storage mechanism that ensures files can only be written once and are subsequently read-only. Once data is written to a WORM-enabled file system, it cannot be modified, deleted, or overwritten. This immutability helps safeguard data from unauthorized modifications, including ransomware encryption attempts. By implementing File System WORM, Huawei's Ransomware Protection Storage Solution offers an additional layer of protection for critical and sensitive data. It prevents ransomware from altering or encrypting files stored in the WORM-enabled file system, ensuring data integrity and resilience against attacks. Secure Snapshot: Secure Snapshot is a feature that enables the creation of point-in-time, read-only copies of data stored in the Ransomware Protection Storage Solution. These snapshots capture the state of the data at a specific moment and provide a recovery point for restoring data in the event of a ransomware attack or data loss. Secure Snapshots ensure that even if the primary data is compromised or encrypted by ransomware, organizations can roll back to a previous snapshot to restore clean, uninfected versions of the data. By maintaining regular, secure snapshots, businesses can significantly reduce the impact of ransomware attacks and quickly recover critical data.\nDear All, Today we are going to learn about No I/O Forwarding and I/O Forwarding between engines in 18000 series storages. The host sends a write request to the owning engine of a LUN. The cache works in write back mode, stores the dirty data, and sends a response to the host. The engine destages the dirty data from the cache to disks. The disks return a write response to the owning engine of the LUN. The host sends a read request to the owning engine of the LUN. The requested data is not hit in the cache. The requested data is read from the disks. The disks return a read response to the owning engine of the LUN. The requested data is cached. The engine sends a read request response to the host. The host sends another read request to the owning engine of the LUN. The requested data is hit in the cache. The cache sends a response to the host. The link between the host and owning engine of a LUN (engine 0) is down. The host sends a write request to engine 0. This request is forwarded by engine 1. The cache works in write back mode, stores the dirty data, and sends a response to the host. The engine destages the dirty data from the cache to disks. The disks return a write response to engine 0. The host sends a read request to engine 0. This request is forwarded by engine 1.\nThe storage architecture of the system is a key component of data transmission and access to important information. It provides the foundation for data access across the enterprise. Depending on your operational and business needs, a specific storage architecture may be required to enable your workforce to reach its full potential. So what is IT storage architecture and how does it function in the daily tasks you need to accomplish? To help you understand storage optimization, we've outlined the details of storage architecture and what you need to know to make an informed decision about the design and maintenance of one of your enterprise's most critical components. Networked storage architecture refers to the physical and conceptual organization of the network that supports data transfer between storage devices and servers. It provides the back end for most enterprise operations and allows users to get what they need. The storage architecture can be set up to determine priorities such as cost, speed, scalability, or security. Because different enterprises have different requirements, the content in the IT storage architecture can be an important factor in the success and ease of use of day-to-day operations. The two main types of storage systems offer similar functionality, but differ widely in execution. These storage types include network-attached storage (NAS) and storage area networks (SANs). A NAS system connects a computer to a network to transfer file-based data to other devices.\nThese files are typically held on multiple storage drives arranged in a redundant array of independent disks (RAID), which helps improve performance and data security. This user-friendly approach shows up as a network mounted volume. Security, administration, and access are relatively easy to control. NAS is popular in small operations because it allows local and remote file sharing, data redundancy, 24x7 access, and easy upgrades. Plus, it's not very expensive and very flexible. The downside to NAS is that servers may need to be upgraded to keep up with increasing demand. It can also struggle with delays for large files. For small file sizes, it's unlikely to be noticeable, but if you're dealing with large files like video, this delay can interrupt many processes and slow you down significantly. The SAN creates a storage system that can handle consolidated block data. It bypasses many of the limitations caused by TCP/IP protocols and LAN congestion, making it access faster than NAS systems. Part of the reason for the speed increase has to do with the way the files are served. NAS uses Ethernet to access files and then serves them over incredibly high-speed Fibre Channel for fast access. NAS improves accessibility and looks like an external hard drive to the user. Because of its complexity, a SAN is usually reserved for large enterprises that have the money and IT department to manage it. The low latency and high speed of a SAN is a significant advantage for organizations with high demand files such as video.\nIt also distributes and prioritizes bandwidth fairly across the network, making it ideal for businesses with high-speed traffic, such as e-commerce sites. Other benefits of SANs include scalability and block-level access to files. The biggest drawbacks of SANs are their cost and maintenance challenges, so they are often used by large companies. multi-level In these storage systems, you can find a variety of settings. Different fabrics affect the performance of any given storage system. Components of these settings include: Front-end interface: Typically connected to the access layer of the server infrastructure, this interface allows users to interact with data. Master node: A master node is a node that uses information from outside the system to communicate with a compute node. It manages compute nodes and monitors resource and node status. Typically, they are located in servers that are more powerful than compute nodes. Compute nodes: Compute nodes help run operations such as calculations, file operations, and rendering. Consistent file systems: Compute nodes can easily access file types and provide better performance with parallel file systems shared across server clusters. High-speed fabric: Creating communication between nodes requires a fabric that provides low latency and high bandwidth. Gigabit Ethernet and Infiniband technology are the main choices. Here are some of the architectural styles you may find. 1. Multi-layer model With a multi-tiered data center, HTTP-based applications can take advantage of separate layers of Web, application, and database servers. It allows for significant separation between layers for increased security and redundancy.\nIn terms of security, if one layer is compromised, the other layers are usually secure with the help of a firewall between them. As for redundancy, if one server fails or requires maintenance, other servers in the same tier can remain operational. 2. Cluster architecture In a clustered system, data remains behind a single compute node. They don't share memory. The input and output (I/O) paths are short and direct, and the system interconnects have extremely low latency. This simple approach is actually the most versatile approach to be touted because it's very easy to add data services. One approach to clustered architectural models is to layer \"federation models\" on top of them to extend outwards to some extent. This bounces the I/O until it reaches the node containing the data. These federation layers require additional code to redirect the data, which increases the latency of the entire process. 3. Tightly coupled architecture These architectures distribute data among multiple nodes, run in parallel, and use a grid of multiple high-availability controllers. They have a lot of inter-node communication and handle many types of operations, but the master organizes input processing. These systems were originally designed to make the I/O path symmetrical across the node and limit the extent to which drive failures can cause imbalances in I/O operations. For more complex designs, tightly coupled architectures require more code. This limits the availability of data services, making them scarcer in the core code stack.\nHowever, the more tightly coupled the storage architecture is, the more it is able to provide predictable low latency. Because tight coupling improves performance, adding nodes and scaling can be difficult, which inevitably adds complexity to the overall system and leaves you vulnerable to errors. Storage Architecture 4. Loosely coupled architecture This type of system does not share memory between nodes. Data is distributed among them and there is a lot of inter-node communication on write, which makes it expensive for you to run while viewing cycles. The data transferred is transactional. Sometimes low latency is hidden in inherently low latency write locations, such as SSDs or NVRAM, but there is still more movement in loosely coupled architectures, resulting in additional I/O. Similar to the tightly coupled architecture, the architecture can follow the \"federation\" pattern and scale out. Typically, it needs to group nodes into subgroups with special nodes called mappers. This architecture is relatively simple to use and is suitable for distributed reading of data in multiple places. Because the data is located in multiple locations, multiple nodes can save it and access it faster. This makes the architecture particularly suitable for hyper-convergence of server and storage software and transactional workloads. Just as each node does not share memory, they do not share code that is separate from other nodes. This design has some effect. If data is widely distributed as it is written, you will see higher latency and lower I/O operations per second (IOPS) efficiency.\nIf you have less distribution, you may get lower latency, but you won't see as much read parallelism as you would otherwise. Finally, a loosely coupled architecture provides all three options -- low write latency, high parallelism, and high scalability -- if the data is sub-tiered and you don't write a large number of copies. 5. Distributed architecture Although it may look like a loosely coupled architecture, this approach works for non-transactional data. It does not share memory between nodes, data is distributed among them. Data is chunked on a node and occasionally distributed as a security measure. This type of system uses objects and non-POSIX file systems. This type of architecture is not as common as many others, but is used by very large enterprises because it can easily handle petabytes of storage. Its parallel processing model and speed make it ideal for search engines. It is incredibly scalable due to its chunking approach and independence from transactional data. Because of its simplicity, distributed, non-shared architectures are usually software-only and hardware-independent. Designing a storage architecture is often a balance of different functions. Improve one aspect, you may worsen another. You must determine which features are most important to your type of work and how you can make the most of them. You also need to balance the costs and needs of your organization. Here are some of the most common aspects of developing a storage architecture. 1.\nData mode Depending on the type of work you are doing, you may have random or sequential I/O requests. Which type of mode you use will most affect how disk components physically reach the area containing the data. Random : In random mode, data is written to and read from different locations on the disk platter, which affects the effectiveness of the RAID system. The controller cache uses patterns to predict which blocks of data need to be accessed next for reading or writing. If the data is random, then it has no mode of operation. Another problem with random mode is the increase in seek time. As data is distributed in blocks of data, the disk head needs to be moved each time a piece of information is requested. The robot arm and head must be physically moved there, which increases seek time and affects performance. Sequential : As you might imagine, Sequential mode works in an orderly manner. It is more structured and provides predictable data access. With this layout, the RAID controller can more accurately guess which blocks of data need to be accessed next and cache that information. It improves performance and keeps the arm from moving around. These sequential applications are typically built with throughput in mind. You'll see sequential patterns of large file types, such as video and backup, written to the drive in contiguous blocks. In random workloads, disk performance is related to rotational speed and the time it takes to access data.\nAs the disk moves faster, it provides more IOPS. All three major disk types (SATA, SAS, and SSD) offer similar levels of performance in sequential operations. However, in general, Sequential mode usually occurs in large or streaming files and is best suited for SATA drives. Random patterns can occur with small files or inconsistent storage requests, such as requests on virtual desktops. SAS and SSDs are usually the best choice for random mode. Here's how drives compare in terms of spin speed and access time. SATA : SATA drives have relatively large disk platters and may struggle to handle random workloads due to their slower speeds. Large disk sizes result in longer seek times. SAS : These drives have smaller platters and are faster. They can significantly reduce seek time. SSD : SSD drives are ideal for extremely high-performance workloads. It has no moving parts, so seek time is almost non-existent. 2. Layer In a data center storage architecture, you typically see multiple hardware layers that serve different functions. These layers typically include: Core layer: The first layer creates the high-speed packet switching required for data transmission. It connects to many aggregation modules and uses a redundant design. Aggregation layer: The aggregation layer is where traffic passes through and encounters services such as firewalls, network analysis, and intrusion detection. Access layer: This layer is where servers and networks are physically connected. It involves switches, cables, and adapters to connect everything and allow users to access data. 3.\nFor example, SSDs can have RAID 1+0 configurations for better performance, while SATA drives with RAID 6 configurations provide additional security during rebuilds and high capacity. Designing a storage architecture requires that we carefully examine the requirements of the business and environment. This may be self-evident, but meetings and discussions will help determine your needs. You also need to look for professional services to help with the details and build the architecture itself. Once you have identified the data pattern, you can start reviewing the following: Capacity Requirements Throughput IOPS Additional features, such as replication or snapshots If you don't have data on these areas, a closer look at your operating system and applications can help you get started. If you find that your data patterns are random, try balancing capacity and IOPS requirements. For sequential workloads, prioritize capacity and throughput. The megabytes per second (MB/s) rating for sequential data typically exceeds requirements. Of course, we can't put everything you need to know about storage architecture in one article, but here are some tips to help you easily create your ideal storage structure. Assess costs from the start: Designing from scratch with costs in mind allows you to make actionable long-term decisions. You don't want to end up with an architecture that needs to be restructured immediately because maintenance costs are too high or cannot meet the needs of the company. Realize the cost of a storage architecture and align it with your business budget.\nwhat is biplanar orthogonal backplane design for Huawei ssd enclosures? A biplanar orthogonal backplane design is a type of backplane design used in Huawei SSD enclosures. This design uses two separate planes, or layers, to route signals between the SSDs and the controller. The two planes are orthogonal to each other, meaning that they are perpendicular. This design has a number of advantages, including: Increased bandwidth: The two planes can each carry a full-duplex data stream, which doubles the bandwidth available for communication between the SSDs and the controller. Reduced crosstalk: The two planes are orthogonal to each other, which minimizes crosstalk between the signals on the two planes. Crosstalk is a type of interference that can occur between signals that are traveling on adjacent conductors. Simplified routing: The two planes can be routed independently of each other, which simplifies the routing process. The biplanar orthogonal backplane design is a relatively complex design, but it offers a number of advantages over other backplane designs. These advantages make it a good choice for high-performance SSD enclosures. Here are some additional details about the biplanar orthogonal backplane design: Planes: The two planes in a biplanar orthogonal backplane are typically made of copper or aluminum. The planes are separated by a dielectric material, such as FR4. Signals: The signals on the two planes are typically routed using a variety of techniques, including stripline and microstrip. Controller: The controller is the device that manages the communication between the SSDs and the rest of the system.\nWhat is the process of restoring data from a backup using OceanStor BCManager? Dear, 1. Log in to the DME console using your credentials. 2. Click on the \"Snapshots\" tab in the left-hand menu. 3. Click on the \"Create Snapshot\" button to create a new snapshot. 4. Select the source volume or disk that you want to create a snapshot of. 5. Choose the snapshot type (e.g., incremental or full) and set the snapshot schedule. 6. Configure the snapshot retention policy, which determines how long the snapshot will be retained. 7. Click on the \"Create Snapshot\" button to create the snapshot. 8. To manage snapshots, select the snapshot you want to manage and click on the \"Actions\" button. 9. From the dropdown menu, you can choose to restore, delete, or copy the snapshot. 10. You can also view the snapshot details, including the creation date, size, and status. 11. To monitor snapshot activity, you can view the snapshot logs and alerts. Thanks. Dear, 1. Log in to the DME console using your credentials. 2. Click on the \"Snapshots\" tab in the left-hand menu. 3. Click on the \"Create Snapshot\" button to create a new snapshot. 4. Select the source volume or disk that you want to create a snapshot of. 5. Choose the snapshot type (e.g., incremental or full) and set the snapshot schedule. 6. Configure the snapshot retention policy, which determines how long the snapshot will be retained. 7. Click on the \"Create Snapshot\" button to create the snapshot. 8.\nHow does OceanStor BCManager ensure data consistency and integrity during backup and recovery operations? Dear friend, OceanStor BCManager is a type of backup and recovery management software. It protects data by using technologies such as snapshot, replication, and migration. During backup and restoration, OceanStor BCManager takes the following measures to ensure data consistency and integrity: 1. Snapshot : OceanStor BCManager uses the snapshot technology to back up data. A snapshot is a read-only data copy that can be backed up without affecting the original data. During the backup, OceanStor BCManager creates a snapshot and then backs up the snapshot to ensure the consistency and integrity of backup data. 2. Data verification : During backup and restoration operations, OceanStor BCManager verifies backup data to ensure its integrity. During the backup, OceanStor BCManager calculates the checksum of the backup data and compares it with the checksum of the original data to ensure the integrity of the backup data. During the restoration, OceanStor BCManager verifies the restored data to ensure the integrity of the restored data. 3. Data encryption : During backup and restoration operations, OceanStor BCManager encrypts backup data to ensure backup data security. During the backup, OceanStor BCManager encrypts the backup data and stores the encrypted data in the backup media. During the restoration, OceanStor BCManager decrypts the restored data to restore the original data. Thanks. Dear friend, OceanStor BCManager is a type of backup and recovery management software. It protects data by using technologies such as snapshot, replication, and migration.\nHi, everyone, In this article we are going to discover everything there is to know about a very new, cost-effective and easy-to-use hybrid flash storage product: Huawei OceanStor 2200/2600 Hybrid Flash Storage . So, sit back, relax, read and get ready to be amazed by all its features and specifications! As you may already know, data storage is a big part of Huaweis Enterprise solutions and hybrid flash storage products have been developed for quite some time now. There are many products to choose from, depending on each enterprises needs and budget, of course. Among these, we will mention a few: OceanStor 18000 series , OceanStor 5000 Series , OceanStor 6000 Series and so on. You can find out more about them by visiting this link . So, what does this new product bring to the table? Well, first of all, you need to know that OceanStor 2200/2600 is an entry-level, very easy to use and intelligent hybrid flash storage system that was designed for small and medium enterprises. This is a new-generation product that was equipped with a premium hardware platform which runs powerful convergence capabilities and intelligent management software. It is also designed to adopt Huaweis proprietary algorithms in order to deploy data across different media in a very efficient way and to offer a simple configuration, thus by becoming a reliable solution for business continuity and data security as well.\nOceanStor 2200/2600 is a great option for different sectors like government, enterprise, education and healthcare , because it provides diversified services to users, like database OLTP/OLAP (Online Transaction Processing / Online Analytical Processing) and file sharing. Lets have a closer look at all the features of this new-gen hybrid flash storage product. It has Graphical User Interface based operations, one-click and three steps configuration, system rollout in a manner of minutes and, last but not least, fast service deployment. DME-IQ implements a 24/7 proactive cloud maintenance and also provide fault diagnosis, risk analysis and prediction, but also service evaluation using high-quality algorithms. And, one of the most important aspects: one person can successfully operate and maintain multiple devices. It has dual controllers which provide a maximum of 128 GB cache and the storage system uses 32 Gbit/s Fibre Channel and RDMA It implements multi-core processor optimization, cache adaptation and intelligent I/O scheduling algorithms which are oriented to all-flash storage It provides Smart series functions to supercharge performance and application efficiency The storage system of the OceanStor 2200/2600 provides SAN (Storage Area Network) and NAS (Network Attached Storage) features, it integrates SSDs (Solid State Drives) and HHDs (Hybrid Hard Drives) and also supports on-demand configuration One storage system can support a larger capacity and more tenants And it supports the free flow of data while also protecting customers existing investments. What else can you ask for, right? With all of these features, OceanStor 2200/2600 is one of the most efficient and recommended products for enterprises.\nDear friend, . You can check the system logs and alarms to identify the failed storage controller. After identifying the failed storage controller, you can replace it with a new one of the same model and firmware version. Then, you can rebuild the RAID group using the RAID management software provided by Huawei. Once the RAID group has been rebuilt, data access can be restored by restarting the affected hosts or applications. Finally, it is important to verify the integrity of the data by running data consistency checks or by comparing the data with backups. you can follow these steps: 1. Identify the failed storage controller: Check the system logs and alarms to identify the failed storage controller. 2. Replace the failed storage controller: Replace the failed storage controller with a new one. Make sure that the new controller is of the same model and has the same firmware version as the failed one. 3. Rebuild the RAID group: After replacing the failed storage controller, the RAID group needs to be rebuilt. This can be done using the RAID management software provided by Huawei. 4. Restore data access: Once the RAID group has been rebuilt, data access can be restored. This can be done by restarting the affected hosts or applications. 5. Verify data integrity: After restoring data access, it is important to verify the integrity of the data. This can be done by running data consistency checks or by comparing the data with backups. 6.\nPerform preventive maintenance: To prevent future storage controller failures, it is important to perform regular maintenance tasks such as firmware upgrades, disk replacements, and system backups. Thanks. Dear friend, . You can check the system logs and alarms to identify the failed storage controller. After identifying the failed storage controller, you can replace it with a new one of the same model and firmware version. Then, you can rebuild the RAID group using the RAID management software provided by Huawei. Once the RAID group has been rebuilt, data access can be restored by restarting the affected hosts or applications. Finally, it is important to verify the integrity of the data by running data consistency checks or by comparing the data with backups. you can follow these steps: 1. Identify the failed storage controller: Check the system logs and alarms to identify the failed storage controller. 2. Replace the failed storage controller: Replace the failed storage controller with a new one. Make sure that the new controller is of the same model and has the same firmware version as the failed one. 3. Rebuild the RAID group: After replacing the failed storage controller, the RAID group needs to be rebuilt. This can be done using the RAID management software provided by Huawei. 4. Restore data access: Once the RAID group has been rebuilt, data access can be restored. This can be done by restarting the affected hosts or applications. 5. Verify data integrity: After restoring data access, it is important to verify the integrity of the data.\nIs Hyper series management-software suite provides a complete data protection-strategy and DR solution for OceanStor18000, V5? Yes, the Hyper series management software suite provided by Huawei for OceanStor18000 V5 storage systems offers features and functionalities that contribute to a comprehensive data protection strategy and disaster recovery solution. These include snapshot technology, remote replication, high availability, and integration with third-party backup solutions. However, it is important to consider a holistic approach to data protection and DR by combining these features with other relevant technologies and best practices. Consulting with Huawei or their authorized partners is recommended for detailed guidance based on your specific requirements. Dear friend, Yes, the Hyper series management-software suite provides a complete data protection-strategy and DR solution for OceanStor 18000 V5. The suite includes HyperSnap, HyperReplication, and HyperMetro, which are designed to provide data protection, backup, and disaster recovery capabilities for the OceanStor 18000 V5 storage system. is a snapshot-based data protection solution that provides fast and efficient backup and recovery of data. It allows users to create point-in-time snapshots of data, which can be used to restore data in the event of data loss or corruption. is a remote replication solution that provides disaster recovery capabilities for the OceanStor 18000 V5 storage system. It allows users to replicate data to a remote site, providing a backup copy of data in the event of a disaster. is a high-availability solution that provides continuous access to data in the event of a storage system failure.\nRendering is the process of generating a 2D or 3D image from a model by an application. Rendering is primarily used for architectural design, video games and animated movies, simulators, TV effects, and design visualization. The technologies and features used vary from project to project. Rendering helps improve efficiency and reduce design costs. Rendering in computer graphics, the process of generating an image from a model in software. A model is a description of a 3D object or virtual scene that is strictly defined by language or data structure. It includes information such as geometry, viewpoint, texture, illumination and shadow. The image is a digital image or a bitmap image. Rendering is used to describe the process of calculating effects in video editing software to produce the output of the final video. Rendering is one of the most important research topics in 3D computer graphics, and it is closely related to other technologies in practice. In the graphics pipeline, rendering is the last important step, through which the final display effect of the model and animation can be obtained. Since the 1970s, with the increasing complexity of computer graphics, rendering has become an increasingly important technology. Rendering applications include computer and video games, simulations, movie or TV effects, and visual design, each of which is a combination of features and technologies. As a product, there are a variety of different rendering tool products, some integrated into larger modeling or animation packages, some stand-alone products, and some open source products.\nWhat are the recommended backup strategies for the all-flash OceanStor Hybrid Flash Storage? When it comes to implementing backup strategies for the all-flash OceanStor Hybrid Flash Storage system, it's important to ensure data protection, availability, and efficient recovery. Here are some recommended backup strategies: 1. Regular Full Backups: Perform regular full backups of the data stored on the OceanStor Hybrid Flash Storage system. Full backups capture all the data, ensuring comprehensive protection. Schedule the backups based on the criticality of the data and the recovery point objectives (RPO) of your organization. 2. Incremental or Differential Backups: To optimize backup storage space and reduce backup time, consider using incremental or differential backups. Incremental backups capture only the changes made since the last backup, while differential backups capture the changes since the last full backup. This approach can be useful for frequent backups and reducing backup windows. 3. Snapshot-Based Backups: Leverage the snapshot capabilities of the OceanStor Hybrid Flash Storage system. Snapshots provide point-in-time copies of data, allowing for fast and efficient backups. Schedule regular snapshots and use them as a foundation for backup operations. Additionally, ensure that snapshots are replicated to secondary storage or off-site locations for additional data protection. 4. Backup to Secondary Storage: Implement a backup-to-disk strategy by using secondary storage systems or backup appliances. This approach allows for faster backup and recovery times compared to traditional tape-based backups. Secondary storage can also provide additional features like deduplication and compression to optimize storage space. 5.\nData Storage is the collective methods and technologies used to retain digital information on a computer system, server, or another device. It allows users to store, access, and utilize data that is essential to their operations and ensures the integrity of the information stored. Different types of data storage exist, such as magnetic storage, optical storage, solid-state storage, and cloud storage, each offering different advantages and disadvantages. Data storage is a critical component of any system. It is the process of saving, organizing, and retrieving data from a system. It is a key part of the data management process, as it helps to maintain the integrity of the data, ensuring that it is securely stored for future use. Data storage solutions can range from simple databases to complex storage systems, such as big data and cloud storage. Data storage is important for businesses and individuals alike, as it helps to store and protect data from being lost or corrupted. Moreover, it can help to improve the efficiency of data processing, as well as provide access to data across multiple devices and locations. Ultimately, data storage is essential for any system, as it helps to ensure the secure storage and retrieval of data. Data storage is the term used to describe the process of storing digital data on a storage medium. This can include anything from magnetic disks, flash storage devices, or optical disks to tape storage, cloud storage, and more.\nData storage is essential for any digital system, as it is the place where information is stored and retrieved when it is needed. Data storage is also used to back up important information, in order to prevent data loss in the event of a system failure. chosen is dependent on the amount of data to be stored, the purpose of the data storage, and the cost of the data storage. In today's world, there are a variety of data storage options available, each offering a different level of security, cost, and storage capabilities. Direct Attached Storage (DAS) is usually located in an adjacent area and is directly connected to the computer that accesses it. Often, it is the only machine connected to it. DAS can also provide decent local backup services, but with limited sharing. DAS devices include floppy disks, compact disks (CDs), and digital video disks (DVDs) - hard disk drives (HDDs), flash drives, and solid state drives (SSDs). Network-based storage allows multiple computers to access it over a network for better data sharing and collaboration. Its offsite storage capabilities also make it better suited for backup and data protection. Two common network-based storage settings are network-attached storage (NAS) and storage area networks (SANs). File Storage File storage, also known as file-level or file-based storage, is a tiered storage method for organizing and storing data. In other words, data is stored in files, files are organized in folders, and folders are organized in a hierarchy of directories and subdirectories.\nBlock storage Block storage, sometimes referred to as block-level storage, is a technology for storing data in blocks. These blocks are then stored as individual blocks, each with a unique identifier. Developers prefer block storage for computing situations where fast, efficient, and reliable data transfer is required. Object storage Object storage, often referred to as object-based storage, is a data storage architecture for handling large amounts of unstructured data. This data does not conform to or cannot be easily organized into a traditional relational database with rows and columns. Examples include email, video, photos, web pages, audio files, sensor data, and other types of media and web content (text or non-text). Data storage is an integral part of any organization, as it allows them to store and access their data quickly and efficiently. It is often the backbone of any digital transformation and can be used to store both structured and unstructured data. There are a variety of data storage types available, each with its own unique advantages and disadvantages. For example, cloud storage is a popular choice due to its scalability, flexibility, and cost savings, but it can also be vulnerable to security breaches. On the other hand, traditional storage such as hard drives and tapes provide a secure and reliable option, but they can be expensive and inflexible. Ultimately, it is up to the organization to decide which data storage type is best suited for their needs.\nNAS Single storage device or RAI File storage system TCP/IP Ethernet network Limited users Limited speed Limited expansion options Lower cost and easy setup SAN Network of multiple devices Block storage system Fibre Channel network Optimized for multiple users Faster performance Highly expandable Higher cost and complex setup Data storage is an essential part of any modern business. It is the process of digitally storing information, such as documents, images, audio, and videos. Data is stored in a variety of formats, including hard drives, solid-state drives, RAID systems, and cloud-based solutions. Data storage solutions enable businesses to store and access large volumes of data quickly and securely. With the right data storage technology, businesses can ensure their data is protected, organized, and accessible when needed. Additionally, data storage solutions can help businesses reduce operational costs, improve scalability, and increase productivity. As businesses grow and evolve, it is important to have a reliable data storage solution in place to meet their needs. Data Storage Solutions Transform and enhance your business by integrating and updating your existing IT infrastructure with a comprehensive storage solution while reducing costs. All Flash storage Eliminate silos by simplifying data management on-premises or in the cloud with one platform system all-flash technology. Storage virtualization Reduce costs and complexity through storage virtualization. Virtualized storage allows you to centralize management to simplify mixed environments and discover hidden capacity. Data storage is the collective methods and technologies used to capture and retain digital information on a computer system.\nQuantum computing is one of the hot topics at the moment. However, we may not know much about some of these proper terms, such as QPU. In this paper, we will introduce the concept of QPU and its importance in quantum computing. QPU is short for Quantum Processing Unit, or Quantum Processor. It is the core component of quantum computing, similar to the CPU (central processing unit) in traditional computers. The QPU is a chip that performs quantum algorithms, operations, and logic gate operations. The quantum processing unit (QPU) is the \"brain\" of the quantum computer. It uses the behavior of particles such as electrons or photons to perform specific types of calculations, much faster than processors in today's computers. QPU depends on superimposed behavior, etc. QPUs work differently than traditional CPUs. The CPU in a conventional computer performs calculations based on the electronic motion of binary bits (0 and 1), while the QPU performs calculations based on the quantum states of qubits (qubits). Quantum state is a concept in quantum mechanics. Compared with binary bits, it has some special properties, such as superposition state and entangled state. These properties enable QPUs to perform tasks that traditional computers cannot do, such as solving large-scale optimization problems and simulating quantum systems in a short time. The CPU and GPU are measured in bits, using the on/off state of the current to indicate 0 or 1. QPUs are calculated as qubits, which represent many different quantum states and derive unique energy from them.\nA qubit is an abstract concept. Computer scientists use it to represent data based on the quantum states of particles in the QPU. Like a hand on a clock, a qubit points to a quantum state that resembles a point in the space of possibilities. The performance of a QPU is usually expressed in terms of the number of qubits it contains. Researchers are developing more methods to test and measure the overall performance of QPUs. Companies and academic researchers are using a variety of techniques to make qubits in QPUs. The most popular method is superconducting qubits. It is essentially made of one or more tiny metal sandwiches called \"Josephson junctions,\" where electrons pass through the insulating layer between two superconducting materials. The current state of the art is capable of creating more than 100 such knots in a single QPU. Quantum computers using this method isolate electrons by using powerful \"fridges\" that look like high-tech chandeliers, cooling them to temperatures close to absolute zero. QPU is the core component of quantum computing and the key to realizing quantum advantage. Quantum advantage refers to the significant advantages that quantum computers have over traditional computers in certain specific tasks, such as solving large-scale optimization problems and simulating quantum systems. However, achieving these benefits requires large-scale qubits and high-quality QPUs. Therefore, the research and development of high-performance QPU is one of the important tasks of quantum computing. Thanks to sophisticated science and technology, the researchers expect the QPUs inside quantum computers to bring amazing results.\nThey are particularly excited about four promising possibilities. First, they can take computer security to a whole new level. Quantum processors can quickly break down huge numbers, a core function of cryptography. This means they can break today's security protocols, but they can also create new, stronger protocols. In addition, QPUs are well suited to mimic the quantum mechanics of matter working at the atomic level. This could lead to radical advances in chemistry and materials science, from the design of light aircraft to more effective drugs, starting the domino effect. The researchers also hope that quantum processors will solve optimization problems that classical computers cannot handle in areas such as finance and logistics. Finally, they may even advance machine learning. QPUs GPUs There are several key challenges that need to be overcome before QPUs can become a practical and widely-used technology. Here are some of the major challenges of QPUs: Noise : QPUs are highly sensitive to noise and other forms of interference. Even small amounts of noise can cause errors in the computations performed by a QPU, which can make it difficult to obtain accurate results. To address this challenge, researchers are developing error-correction techniques and other methods to mitigate the effects of noise. Scalability : Building large-scale QPUs is a major technical challenge. As the number of qubits in a QPU increases, it becomes increasingly difficult to maintain the coherence of the quantum states, which can lead to errors and other issues.\nwhat is the management architecture,feature,Principles of X6000 HMM+iBMC? The X6000 HMM (Hot Swap Module) and iBMC (Intelligent Baseboard Management Controller) is a management architecture and platform developed by Huawei for their server products. It provides a comprehensive set of features and principles to enable efficient and effective management of the servers. Management Architecture: The X6000 HMM+iBMC management architecture consists of intelligent management modules that are integrated into the server hardware. These modules communicate with the iBMC to provide centralized management and control capabilities. Features: Remote Management: The iBMC allows administrators to remotely monitor and manage the server hardware, even when the server is powered off or experiencing issues. Power Management: It enables control and monitoring of the server's power consumption, such as power supply status, power usage monitoring, and power limit settings. Health Monitoring: The iBMC continuously monitors various health indicators of the server, such as temperature, fan speed, and voltage. It also provides alerts and warnings in case of any abnormalities. Fault Diagnosis: The management platform can detect and diagnose hardware faults, providing detailed information to aid in troubleshooting and reducing downtime. Firmware Management: iBMC facilitates firmware upgrades and management, ensuring the server's components have the latest software and security updates. Virtual Media Management: It allows remote mounting of virtual media, such as ISO files or USB drives, to the server, eliminating the need for physical media during installations or maintenance. Security Management: The iBMC implements various security measures to protect the server, such as user authentication, secure communication channels, and access control policies.\nPrinciples: Centralized Management: The X6000 HMM+iBMC architecture provides a centralized platform for managing multiple servers, simplifying administration and reducing management complexity. Proactive Monitoring: The system continuously monitors and reports various health indicators and alerts administrators in case of any abnormalities or potential issues, allowing proactive maintenance and minimizing downtime. Scalability: The management architecture is designed to support a scalable environment, allowing the addition or removal of servers without impacting the overall management capabilities. Efficiency and Ease of Use: The management platform provides a user-friendly interface and intuitive controls, enabling administrators to efficiently manage and control the servers. Reliability and Redundancy: The X6000 HMM+iBMC system is built with redundancy and fault tolerance mechanisms, ensuring high availability and minimizing the risk of system failure. Overall, the X6000 HMM+iBMC management architecture and platform offer comprehensive features, principles, and ease of use to enable efficient and reliable management of Huawei server products. The X6000 HMM (Hot Swap Module) and iBMC (Intelligent Baseboard Management Controller) is a management architecture and platform developed by Huawei for their server products. It provides a comprehensive set of features and principles to enable efficient and effective management of the servers. Management Architecture: The X6000 HMM+iBMC management architecture consists of intelligent management modules that are integrated into the server hardware. These modules communicate with the iBMC to provide centralized management and control capabilities. Features: Remote Management: The iBMC allows administrators to remotely monitor and manage the server hardware, even when the server is powered off or experiencing issues.\nPower Management: It enables control and monitoring of the server's power consumption, such as power supply status, power usage monitoring, and power limit settings. Health Monitoring: The iBMC continuously monitors various health indicators of the server, such as temperature, fan speed, and voltage. It also provides alerts and warnings in case of any abnormalities. Fault Diagnosis: The management platform can detect and diagnose hardware faults, providing detailed information to aid in troubleshooting and reducing downtime. Firmware Management: iBMC facilitates firmware upgrades and management, ensuring the server's components have the latest software and security updates. Virtual Media Management: It allows remote mounting of virtual media, such as ISO files or USB drives, to the server, eliminating the need for physical media during installations or maintenance. Security Management: The iBMC implements various security measures to protect the server, such as user authentication, secure communication channels, and access control policies. Principles: Centralized Management: The X6000 HMM+iBMC architecture provides a centralized platform for managing multiple servers, simplifying administration and reducing management complexity. Proactive Monitoring: The system continuously monitors and reports various health indicators and alerts administrators in case of any abnormalities or potential issues, allowing proactive maintenance and minimizing downtime. Scalability: The management architecture is designed to support a scalable environment, allowing the addition or removal of servers without impacting the overall management capabilities. Efficiency and Ease of Use: The management platform provides a user-friendly interface and intuitive controls, enabling administrators to efficiently manage and control the servers.\nData replication is the process ofwriting service data from onemedium to another to create redundant copies of the same data. This can be done for various reasons, includingdata migration from one storage to another,disaster recovery to setup 3DC architecture, data availabilitywith the storage using RAID levels or across the storages, load balancing of services, or even compliance requirements to store data at offshore sites. The goal of data replication is to ensure that the data is always available and accessible, even in the event of hardware or software failures or other disasters. Huawei is a leading provider of storage solutions that offers various features for data replication. Huawei Storage is a comprehensive storage solution that provides reliable, high-performance, and scalable data storage and management services. Huawei Storage offers various features for data replication, including synchronous and asynchronous replication, remote replication, and backup and recovery services. Synchronous replication Synchronous replication is a method of data replication in which data is written to both the primary and secondary storage systems simultaneously. This ensures that both systems are in sync and that any changes made to the data are reflected in both systems at the same time. Synchronous replication is often used for mission-critical applications that require high availability and zero data loss. HyperMetro HyperMetro is Huawei's active-active storage solution that enables two storage systems to process services simultaneously, establishing a mutual backup relationship between them. If one storage system malfunctions, the other one will automatically take over services without data loss or interruption.\nWith HyperMetro being deployed, you do not need to worry about your storage systems' inability to automatically switch over services between them and will enjoy rock-solid reliability, enhanced service continuity, and higher storage resource utilization. Asynchronous replication Asynchronous replication is a method of data replication in which data is written to the primary storage system first, and then the data is replicated to the secondary storage system at a later time. Asynchronous replication is often used for applications that do not require zero data loss but still require high availability. The delay between the writes is called the replication lag. Huawei Hyper Replication When a host sends a write I/O to the primary storage system, the primary storage system sends the write I/O to the primary file system. If the write is successful, a response message will be sent from the primary file system to the primary storage system and then to the host. At preset synchronization intervals, new data is copied from the primary file system to the secondary file system. Remote replication is a method of data replication in which data is copied from one storage system to another over a network. Remote replication is often used for disaster recovery purposes, where the secondary storage system is in a different geographic location, providing protection against natural disasters, power outages, or other catastrophic events. Huawei Support geo-redundant 3DC DR solution which can be deployed in both cascading and parallel architecture. Below diagram highlights the architecture supported by Huawei Storage Systems.\nHow does the New-Gen OceanStor 18810 ensure high availability and redundancy? The New-Gen OceanStor 18810 ensures high availability and redundancy through a number of features, including: Dual controllers: The New-Gen OceanStor 18810 has two controllers, which are responsible for managing the storage system. If one controller fails, the other controller can take over and ensure that the storage system remains available. RAID: The New-Gen OceanStor 18810 supports a variety of RAID levels, which can be used to protect data from disk failures. For example, RAID 1 provides mirroring, which means that each data block is written to two separate disks. This ensures that if one disk fails, the data is still available on the other disk. Hot spare: The New-Gen OceanStor 18810 has a hot spare disk, which is a disk that is not currently in use but is ready to be used if a disk fails. This ensures that if a disk fails, the storage system can quickly recover without any downtime. Data replication: The New-Gen OceanStor 18810 can be configured to replicate data to another storage system. This ensures that if the primary storage system fails, the data is still available on the secondary storage system. These features help to ensure that the New-Gen OceanStor 18810 is highly available and that data is protected from failures. Here are some additional details about each of these features: Dual controllers: The two controllers in the New-Gen OceanStor 18810 are connected to each other using a high-speed interconnect.\nWhat are the supported RAID levels on the New-Gen OceanStor 5310/5510/5610 storage systems? The New-Gen OceanStor 5310/5510/5610 storage systems support the following RAID levels: RAID 0: This is a non-redundant RAID level that stripes data across all disks in the array. This provides good performance but no fault tolerance. RAID 1: This is a mirrored RAID level that creates an exact copy of data on one disk on another disk in the array. This provides good fault tolerance but only half of the available capacity can be used. RAID 5: This is a striped RAID level that distributes data across all disks in the array and includes a parity disk. This provides good performance and fault tolerance, with only a small amount of capacity being used for parity. RAID 6: This is a striped RAID level that distributes data across all disks in the array and includes two parity disks. This provides even better fault tolerance than RAID 5, but it also uses more capacity for parity. RAID-TP: This is a Huawei proprietary RAID level that is designed for high performance and availability. It stripes data across all disks in the array and includes a parity disk. It also uses a technique called erasure coding to improve fault tolerance. The RAID level that you choose will depend on your specific needs. If you need the best possible performance, you should choose RAID 0. If you need the best possible fault tolerance, you should choose RAID 6.\nCan the New-Gen OceanStor 18510/18810 be used for data deduplication and compression? If yes, what are the performance implications? Yes, the New-Gen OceanStor 18510/18810 storage systems support data deduplication and compression as part of their advanced data reduction capabilities. These features help optimize storage capacity utilization and improve overall efficiency. However, it's important to consider the performance implications when enabling data deduplication and compression on these systems. Data deduplication and compression are resource-intensive processes that involve analyzing and processing data before it is stored on the storage system. Here are some key points to consider regarding the performance implications: 1. CPU Utilization: Enabling data deduplication and compression on the OceanStor 18510/18810 requires computational resources, particularly CPU cycles. The storage system's CPU utilization may increase as the data is processed for deduplication and compression. Higher CPU utilization can affect the overall system performance, particularly for data-intensive workloads. 2. Throughput and Latency: Data deduplication and compression may impact the storage system's throughput and latency. The processing overhead involved in deduplicating and compressing data can slightly increase the time taken to read and write data to the storage system. However, the specific impact on throughput and latency will depend on factors such as data patterns, compression ratios, and workload characteristics. 3. Storage Capacity Savings: Data deduplication and compression can significantly reduce the amount of storage capacity required for storing data. This reduction in storage capacity can have positive implications for cost savings, as less physical storage is needed.\nAre there any specific software requirements for integrating the Huawei OceanStor storage systems with existing IT infrastructure? When integrating Huawei OceanStor storage systems with existing IT infrastructure, there are several software requirements and considerations to ensure compatibility and smooth integration. Here are some key points to consider: 1. Operating System Compatibility: Verify the compatibility of the OceanStor storage systems with the operating systems (OS) running on your servers or hosts. Ensure that the storage systems support the OS versions you are using and that the necessary drivers or software components are available. 2. File System Support: Check if the OceanStor storage systems support the file systems used in your environment. Common file systems include NTFS (Windows), ext4 (Linux), and APFS (Apple). Ensure that the storage systems can handle the file system type and any associated features, such as snapshots or thin provisioning. 3. Storage Protocols: Determine the storage protocols used in your IT infrastructure, such as Fibre Channel (FC), iSCSI, or Network File System (NFS). Verify that the OceanStor storage systems support the required protocols and versions. Additionally, ensure that the host servers have the necessary drivers and software to communicate with the storage systems over the chosen protocol. 4. Multipathing Software: If you plan to use multipathing for enhanced performance and redundancy, check if your environment requires specific multipathing software or tools. Huawei OceanStor storage systems often support popular multipathing solutions, such as MPIO (Microsoft Multipath I/O) or PowerPath from Dell EMC. 5.\nBackup and Data Protection Software: If you have existing backup and data protection solutions, ensure that they are compatible with the OceanStor storage systems. Verify if the backup software supports features like snapshot integration, deduplication, or replication offered by the storage systems. This compatibility ensures efficient and consistent data protection operations. 6. Management and Monitoring Tools: Determine if your existing IT infrastructure management and monitoring tools can integrate with the OceanStor storage systems. Check if there are plugins, APIs, or software development kits (SDKs) available for integrating with the storage systems' management interfaces. This integration allows centralized management and monitoring of the storage systems alongside other infrastructure components. 7. Virtualization Platforms: If you are using virtualization platforms like VMware vSphere or Microsoft Hyper-V, ensure that the OceanStor storage systems are compatible with the respective platforms. Verify if the storage systems have certified integration with the virtualization platforms and support features like VMware vSphere Virtual Volumes (VVols) or Microsoft Hyper-V virtual machine storage integration. 8. Firmware and Software Updates: Regularly check for firmware and software updates provided by Huawei for the OceanStor storage systems. These updates often include bug fixes, performance enhancements, and new features. Keeping the storage systems up to date ensures compatibility with the latest software and security patches. It is recommended to consult the official documentation, compatibility matrices, and support resources provided by Huawei for detailed information on software requirements and compatibility.\nCan the New-Gen OceanStor 6810 be configured in a high-performance cluster? If so, what are the recommended configurations? Yes, the New-Gen OceanStor 6810 can be configured in a high-performance cluster to provide increased performance, scalability, and availability. When configuring the OceanStor 6810 in a high-performance cluster, the following recommendations can be considered: 1. Cluster Size and Configuration: - Deploy at least two OceanStor 6810 storage systems in a cluster configuration. This provides redundancy and fault tolerance. - Ensure that each storage system in the cluster has sufficient resources, including CPU, memory, and storage capacity, to handle the expected workload. 2. Network Configuration: - Utilize high-speed and low-latency network connections, such as 10 Gigabit Ethernet (GbE) or higher, between the storage systems in the cluster. - Implement redundant network paths to ensure network availability and minimize single points of failure. 3. Load Balancing: - Distribute the workload evenly across the storage systems in the cluster to maximize performance. - Use load balancing algorithms or techniques to optimize the distribution of I/O requests and prevent any single storage system from becoming a bottleneck. 4. Storage Pool Configuration: - Configure storage pools on each storage system in the cluster to aggregate the available storage capacity. - Distribute the data across the storage pools to leverage the combined performance and capacity of the cluster. 5. Multipathing: - Implement multipathing techniques to establish multiple paths between the host servers and the storage systems in the cluster. - Multipathing helps improve performance, load balancing, and resiliency by utilizing multiple network connections. 6.\nWhat are the options for data migration from legacy storage systems to the New-Gen OceanStor 5310/5510/5610? When migrating data from legacy storage systems to the New-Gen OceanStor 5310/5510/5610, there are several options available to facilitate a smooth and efficient migration process. Here are some common methods: 1. Host-based Migration: - This method involves using tools or software installed on the hosts to migrate the data. Host-based migration tools can perform block-level or file-level data transfers. - One popular host-based migration tool is \"rsync,\" which can efficiently transfer data from the source storage system to the OceanStor 5310/5510/5610. - Host-based migration allows for granular control over the migration process and can be suitable for smaller-scale migrations. 2. Storage-based Migration: - Storage-based migration involves utilizing migration features and tools provided by the storage systems themselves. This method is often recommended for larger-scale or more complex migrations. - The OceanStor 5310/5510/5610 offers migration tools like SmartMigration and SmartVirtualization to facilitate data migration from legacy storage systems. - These migration tools enable seamless data movement from the source storage systems to the OceanStor storage, ensuring minimal disruption to applications and services. 3. SAN-based Migration: - SAN (Storage Area Network) migration involves leveraging the capabilities of the SAN infrastructure to transfer data between storage systems. - SAN migration tools, such as SAN fabric-based data migration or SAN zoning, can be utilized to facilitate the data migration process. - This method allows for efficient and high-speed data transfers between storage systems connected to the same SAN fabric. 4.\nA storage container service provides storage resources and infrastructure for containerized applications. It is specifically designed to support the storage needs of containers and offers features such as persistent storage, scalability, and integration with container orchestration platforms. The storage container service allows developers and IT teams to easily manage and provision storage for their containerized applications without having to worry about underlying infrastructure complexities. It abstracts away the underlying storage infrastructure, providing a simplified and scalable storage solution for containers. Key features of a storage container service include. The service offers persistent storage options for containers, allowing data to be preserved even when containers are stopped or restarted. This ensures data durability and consistency across container lifecycles. Storage resources can be dynamically allocated and provisioned based on the requirements of containerized applications. This allows for efficient utilization of storage capacity and eliminates the need for manual provisioning. 30% faster deployment of mass containerized applications using Huawei Developed Container Storage Interface (CSI) plug-in to efficiently distribute storage resources on demand. The service seamlessly integrates with container orchestration platforms like Kubernetes, OpenShift, Tanzu, Rancher, CCE, Fusion Compute and Other Platforms enabling storage management through familiar container orchestration tools and APIs. To ensure high availability and data protection, the service employ replication and redundancy mechanisms. This helps to guard against data loss and provides failover capabilities in case of storage failures.\nHuawei Dorado A-A Hyper Metro ensures services availability in Production datacenter whereas Async Replication Synchronize the data to DR. Huawei Container Disaster Recover (CDR) plugin provides application-level protection within a Kubernetes Cluster. Huawei Container Disaster Recovery service leverages the snapshot and clone functions of the storage for backups and recovery resulting in reduced TCO and no need for additional backup software. Recovery provides immediate access to data. The storage container service is designed to scale horizontally to accommodate growing storage demands. It can automatically scale storage resources based on application requirements, ensuring optimal performance and capacity utilization. The service provides data encryption capabilities to protect sensitive data stored within containers. SAN/NAS ransomware protection technology in Dorado Storages provides protection against data ransomware attacks. It also offers security features such as access controls and authentication mechanisms to ensure data privacy and compliance. Storage container services include monitoring and management tools to track storage usage, performance metrics, and health status. These tools enable administrators to optimize storage resources and troubleshoot any issues that arise. SmartMatrix full-mesh architecture and gateway-free active-active deployment for SAN and NAS ensure up to seven-nines 99.99999% storage reliability, simplifying O&M and supporting more service innovation. The container service leverages the hardware resources present in Huawei storage systems to establish a nimble storage container platform. This platform is constructed upon the foundations of Kubernetes and iSulad container engines, resulting in a lightweight and effective environment. Its primary objective is to enable the secure and on-demand deployment of containerized applications to cater to external systems.\nRAID, or Redundant Array of Independent Disks, is a storage technique that brings together multiple physical hard drives to create a unified storage system with enhanced capabilities. It leverages the power of multiple drives to provide improved data performance, increased storage capacity, data redundancy for protection against drive failures, and streamlined management. By pooling the resources of multiple disks, RAID optimizes storage efficiency, data availability, and system reliability, resulting in a more resilient and efficient storage infrastructure. There are two Flavors of RAID. It is a way to install software application on OS to pull out behavior of RAID. Application is tied to OS, so if there is major OS revision, it may impact application. RAID software upgrades may add complexities as it will be upgraded with OS. Performance can be impacted since we are using processing of host. Hardware RAID refers to a RAID implementation that utilizes a dedicated hardware controller to manage the RAID functionality. In hardware RAID configurations, a separate RAID controller card is installed in the server or storage system, which handles all RAID operations independently of the host computer's CPU and operating system. RAID encompasses various techniques that are used to combine multiple physical disk drives into a logical unit with improved data reliability, availability, and performance. Here are some of the commonly used RAID techniques: Striping is a technique where data is divided into small segments and distributed across multiple drives in the RAID array.\nEach segment, also known as a stripe, is written to a different drive-in a round-robin fashion. Striping improves performance by allowing parallel read and write operations across the drives. However, striping alone does not provide redundancy or fault tolerance. Mirroring, also referred to as RAID 1, involves creating an exact copy of data on two or more drives. Each drive in the mirror holds the same set of data, providing redundancy. If one drive fails, the data can be retrieved from the remaining drives. Mirroring provides high data availability and resilience but at the cost of reduced usable capacity, as each drive is a duplicate of the other. Parity-based RAID techniques, such as RAID 3, RAID 4, RAID 5, and RAID 6, use parity information to provide redundancy and fault tolerance. Parity is a calculated value that can be used to reconstruct data in case of drive failures. In these techniques, data and parity information are distributed across multiple drives. RAID 5 uses distributed parity, while RAID 4 uses a dedicated parity drive. RAID 6 employs double parity for additional fault tolerance. Distributed parity, used in RAID 5, involves distributing both data and parity information across multiple drives. The parity information allows the recovery of data in case of a drive failure. However, write performance can be impacted due to the need to recalculate parity information during write operations. Double parity, employed in RAID 6, adds an extra layer of redundancy by using two sets of parity information.\nGreetings, everyone! In this article, Ill explain about Configure CIFS in Huawei storage. How to work CIFS? A network protocol called CIFS (Common Internet File System) is used to share files, folders, and resources across computers over a network. It is commonly used in Windows-based environments and allows for file sharing and remote access to shared resources. To work with CIFS, you typically need the following: Server: A computer or network-attached storage (NAS) device that hosts shared resources, such as files or folders. Client: Another computer or device that wants to access the shared resources on the server. The following for general overview of how CIFS works: Configuration: On the server-side, you need to enable CIFS sharing and configure the shared resources, such as specifying which files or folders are accessible and setting permissions for different users or groups. This can typically be done through the server's operating system settings or management interface. Connect: On the client-side, you need to establish a connection to the server hosting the shared resources. This usually involves providing the server's IP address or hostname and authenticating with valid credentials (username and password). Mounting: Once connected, you can mount the shared resources on the client's operating system. This creates a logical link between the client's file system and the shared resources on the server. The process of mounting can vary depending on the operating system being used. File Operations: After mounting, the shared resources appear as if they are part of the client's local file system.\nHow does the OceanStor Dorado 5000/6000 series differ from the OceanStor Dorado 8000/18000 series? Hello, dear. The OceanStor Dorado series is a family of all-flash storage systems developed by Huawei. The main differences between the OceanStor Dorado 5000/6000 series and the OceanStor Dorado 8000/18000 series are as follows: 1. Performance: The OceanStor Dorado 8000/18000 series is designed for high-performance computing and can deliver up to 20 million IOPS (Input/Output Operations Per Second), while the OceanStor Dorado 5000/6000 series can deliver up to 1.8 million IOPS. 2. Scalability: The OceanStor Dorado 8000/18000 series is more scalable than the OceanStor Dorado 5000/6000 series. The former can scale up to 16 controllers and 8 PB (Petabytes) of storage capacity, while the latter can scale up to 8 controllers and 2 PB of storage capacity. 3. Availability: The OceanStor Dorado 8000/18000 series has a higher level of availability than the OceanStor Dorado 5000/6000 series. The former has a 99.9999% availability, while the latter has a 99.999% availability. 4. Data protection: The OceanStor Dorado 8000/18000 series has more advanced data protection features than the OceanStor Dorado 5000/6000 series. The former supports RAID 2.0+ (Reed-Solomon Erasure Coding), which provides better protection against data loss due to disk failures. 5. Management: The OceanStor Dorado 8000/18000 series has a more advanced management system than the OceanStor Dorado 5000/6000 series. The former uses the SmartMatrix architecture, which provides a unified management platform for multiple storage systems.\nWhat are the key features of the Huawei OceanStor Dorado All-Flash Storage? Hello, dear. The key features of Huawei OceanStor Dorado storage products include: 1. All-Flash Storage: uses the all-flash storage technology to provide large-capacity storage capacity and support fast read/write and low-latency data transfer. 2. Virtual Data Interface (VDI): The VDI technology is used to bridge storage devices and computing devices to separate storage and computing devices, improving the performance and flexibility of computing devices. 3. High-speed transfer: Supports data transfer rates up to 10 Gbit/s for fast file transfer and data sharing. 4. Multi-mode storage: Multiple storage modes, including primary storage mode, secondary storage mode, and hybrid storage mode, are supported to meet the requirements of different application scenarios. 5. Intelligent management: Supports intelligent storage management and automatically plans and optimizes storage capacity based on storage device usage and data requirements. 6. Security protection: Provides multiple security protections, including data encryption, access control, and backup protection, to ensure data security and integrity. 7. Support for multiple operating systems: Support for multiple operating systems, including Windows, Linux, and macOS, to meet the requirements of different users. 8. Support for multiple storage devices: Connects to multiple storage devices, including hard disks, solid state disks, and hybrid disks, to implement diversified data storage and storage capacity expansion. Hello, dear. The key features of Huawei OceanStor Dorado storage products include: 1. All-Flash Storage: uses the all-flash storage technology to provide large-capacity storage capacity and support fast read/write and low-latency data transfer. 2.\nWhich huawei storage is recommend for Backups purpose.. ? Hello, dear. Huawei offers a range of storage solutions that can be used for backup purposes. The recommended storage solution would depend on the specific backup requirements of your organization, such as the amount of data to be backed up, the frequency of backups, and the recovery time objective (RTO) and recovery point objective (RPO) of your backup strategy. That being said, Huawei's OceanStor Dorado and OceanStor V3 series are popular choices for backup and recovery. The OceanStor Dorado series is designed for high-performance, low-latency applications and can support up to 16 controllers and 32 PB of storage capacity. The OceanStor V3 series, on the other hand, is a mid-range storage solution that offers a balance of performance, capacity, and cost-effectiveness. Both of these storage solutions come with advanced features such as data deduplication, compression, and encryption, which can help optimize backup storage utilization and enhance data security. Additionally, Huawei's backup and recovery software, such as the OceanStor Backup Solution, can be used in conjunction with these storage solutions to provide a comprehensive backup and recovery solution. Hello, dear. Huawei offers a range of storage solutions that can be used for backup purposes. The recommended storage solution would depend on the specific backup requirements of your organization, such as the amount of data to be backed up, the frequency of backups, and the recovery time objective (RTO) and recovery point objective (RPO) of your backup strategy.\nHello Everyone! This article explain you about PCIe 3.0 vs PCIe 4.0 vs PCIe 5.0. On March 6, 2012, server hardware manufacturers announced a new generation of PCIe 3.0-enabled servers that, among other things, had twice the I/O bandwidth of the previous generation. These servers support up to 40 PCIe 3.0 lanes per processor socket, which is at least twice as many lanes as previously available. Workstation and home computer motherboards supporting PCIe 3.0 first appeared at the end of 2011, at the same time the first PCI Express 3.0 graphics cards appeared. In November 2011, the PCI-SIG confirmed that the next generation of PCIe called PCIe 4.0 will have data transfer rates of 16GT/s. Technical analysis has established that known technologies allow the creation of a new interface with a specified data rate. The new standard will be backward compatible with PCIe 1.x, 2.x and 3.x. Version 0.5 of the PCIe 4.0 specification is expected in mid-2015, and version 0.9 will be available in the second half of 2016. After the specification is finalized, it will take about a year or more for PCIe-based products to appear. The single most important feature of PCIe 5.0 and the one everyone cares about is speed.PCIe 5.0 is twice as fast as PCIe 4.0.\nHigh-speed M.2 NVMe drives use PCIe x4 connections.This means NVMe drives supporting PCIe 5.0 speeds can read and write at close to 16 GB/s.For context, a regular SATA SSD tops out at about 550 megabytes per second (MB/s).PCIe 5.0 NVMe drives - when they arrive - promise about 30 times faster speeds.Loading times will be a thing of the past. Almost everyone has gained something from the latest iteration of the PCIe standard, but individuals and organizations dealing with \"big data\" may be the happiest beneficiaries.Data centers running services like Facebook, Google, and other massive services that process unimaginable amounts of data will be able to squeeze every last bit of performance out of the PCIe 5.0 interface.Scientific and engineering applications will of course also benefit from the increased bandwidth. PCIE interface rate: The transfer rate is GT/s, not Gbps, because the transfer includes overhead bits that do not provide additionalthroughput ; for example, PCIe 1.x and PCIe 2.x use an 8b/10b encoding scheme, resulting in Occupies 20% (= 2/10) of the original channel bandwidth. GT/sGiga transition per second (gigatransmission/second), that is, the number of transmissions per second. The key point is to describephysical layer communication protocol , which may not be associated with the link width.the rate attribute of the Gbps - Giga Bits Per Second (gigabits per second). There is no proportional conversion between GT/s and Gbps.\nPCIe throughput (available bandwidth) calculation method is described below: Throughput = transfer rate * encoding scheme For example: the PCI-e2.0 protocol supports 5.0 GT/s, that is, each Lane supports the transmission of 5G bits per second ; but this does not mean that each Lane of the PCIe 2.0 protocol supports a rate of 5Gbps. Why do you say that? Because the physical layer protocol of PCIe 2.0 uses the 8b/10b encoding scheme. That is, for every 8 bits transmitted, 10 bits need to be sent; the extra 2 bits are not meaningful information to the upper layer. Then, each Lane of the PCIe 2.0 protocol supports a rate of 5 * 8 / 10 = 4 Gbps = 500 MB/s. Taking a PCIe 2.0 x8 channel as an example, the available bandwidth of x8 is 4 * 8 = 32 Gbps = 4 GB/s. In the same way, The PCI-e3.0 protocol supports 8.0 GT/s, that is, each Lane supports the transmission of 8G Bits per second. The PCIe 3.0 physical layer protocol uses the 128b/130b encoding scheme. That is, for every 128 bits transmitted, 130 bits need to be sent. Then, each Lane of the PCIe 3.0 protocol supports a rate of 8 * 128 / 130 = 7.877 Gbps = 984.6 MB/s. A PCIe 3.0 x16 channel, the available bandwidth of x16 is 7.877 * 16 = 126.031 Gbps = 15.754 GB/s.\nPCIe5.0 CEM specification that for a lane, the Basic bandwidth of PCIe5.0 is 32.0 GT/s, which is consistent with what we say in daily life, and T/s is the number of transmissions per second ( Transfer per second). The concept of GT/s comes from PCI- SIG , which simply describes the speed of propagation and the rate of change of signal level from the perspective of signals. So, for X4's U.2 that supports Gen5 rate, the theoretical speed is 32*4=128GT/s. We know that the encoding of PCIe is 128B/130B, so although the bandwidth is so large, the actual effective transmission rate cannot reach so much, the actual rate is 128GT/s*(128/130)=126Gbps, pay attention to the change of this unit, Gb/s is bit per second. The maximum transmission rate of U.2 at Gen5 rate is 15.75GB/s. The difference between PCIe 4.0 and 3.0 slots & compatibility: Regarding the advantages of PCIe4.0, AMD officially popularized science and summarized the three technical advantages of PCI-E4.0, as follows: 1. The speed is faster, the bidirectional bandwidth of X16 has reached 32GB/s, twice that of PCIe 3.0. 2. Backward compatibility, PCIe 4.0 can be compatible with PCIe 3.0 devices. 3. More connections, PCIe 4.0 has high bandwidth, 1 is worth 2, you can connect more devices without worrying about performance degradation.\nWe found that the improvement of PCIe 4.0 compared with PCIe 3.0 is basically negligible, and the comprehensive comparison of various games has a maximum gap of only 1%, including the gap between the old generation of PCIe 2.0 and PCIe 4.0, which is only 2%. Doubling bandwidth does not mean doubling performance. Let me give you an example, we can think of PCIe broadband as a road, if PCIe3.0 has 8 lanes in both directions, and PCIe4.0 has 16 lanes in both directions, which means that PCIe4.0 (16 lanes) can drive more vehicles at the same time , but there are only ten vehicles on the road. Even if more two-way lanes are added, those ten vehicles will still be driving, so the traffic efficiency will not be significantly improved. As far as the current graphics card performance level is concerned, even if it is a high-end flagship graphics card, the bandwidth provided by PCI-E3. Only when the graphics card has run higher than the upper limit of the bandwidth and the performance of the graphics card cannot be fully utilized, can the advantage of PCI-E4.0 be brought into play. Someone once tested that when the RTX2080Ti dual graphics card SLI is fully fired, it can just fill up the bandwidth of PCI-E3.0 X16, so for ordinary consumers, there is almost no big difference between PCeI4.0 and PCIe3.0. No need to tangle. It is possible to fight the future.\nQ# Anti-tampering and anti-implantation of Huawei products include: A. Do not tamper with products. B. Do not implant malicious code, software, or backdoors in customer devices or systems. C. Do not reserve private or undisclosed interfaces or accounts. D. After the service, delete all daily maintenance accounts of the system. Thanks. The correct answer is A, B, C, and D . Huawei's anti-tampering and anti-implantation policy includes the following: Do not tamper with products. This means that Huawei products should not be modified or altered in any way that could compromise their security or functionality. Do not implant malicious code, software, or backdoors in customer devices or systems. This means that Huawei products should not be used to introduce malware or other malicious code into customer devices or systems. Do not reserve private or undisclosed interfaces or accounts. This means that Huawei products should not have any hidden or undocumented interfaces or accounts that could be used to gain unauthorized access to the product. After the service, delete all daily maintenance accounts of the system. This means that after Huawei technicians have finished servicing a customer's device, they should delete any accounts that they created for daily maintenance purposes. These policies are designed to protect the security and integrity of Huawei products and to ensure that they cannot be used to compromise the security of customer devices or systems.\nHere are some additional details about each of the policies: Do not tamper with products: This policy is designed to prevent unauthorized individuals from modifying or altering Huawei products in a way that could compromise their security or functionality. This could include things like adding or removing hardware or software, or changing the firmware or configuration of the product. Do not implant malicious code, software, or backdoors in customer devices or systems: This policy is designed to prevent Huawei products from being used to introduce malware or other malicious code into customer devices or systems. This could include things like installing malware on a customer's device, or creating a backdoor that could be used to gain unauthorized access to the device. Do not reserve private or undisclosed interfaces or accounts: This policy is designed to prevent unauthorized individuals from gaining access to Huawei products through hidden or undocumented interfaces or accounts. This could include things like using a secret command to access a product's configuration, or using a hidden account to log into a product. After the service, delete all daily maintenance accounts of the system: This policy is designed to prevent unauthorized individuals from using daily maintenance accounts to gain access to Huawei products after the service has been completed. This could include things like using a daily maintenance account to access a product's configuration, or using a daily maintenance account to log into a product. These policies are an important part of Huawei's commitment to security and privacy.\nWhat are Huawei Registered State Change Notifications (RSCNs)? Huawei Registered State Change Notifications (RSCNs) are notifications sent by Huawei devices to inform other devices in the network of changes in the state of a device or link. RSCNs are used to keep all devices in the network aware of the current state of the network, which helps to improve performance and availability. For example, if a link goes down, RSCNs will be sent to all devices in the network so that they can reroute traffic around the failed link. RSCNs are sent using the Fabric Control Protocol (FCP). FCP is a protocol that is used to control the operation of the fabric. RSCNs are sent as FCP packets. There are two types of RSCNs: Link-down RSCNs: These are sent when a link goes down. Fabric-change RSCNs: These are sent when a change occurs in the fabric that affects all devices, such as a change in the fabric topology. RSCNs are configured on each device in the fabric. The configuration specifies which types of RSCNs the device will receive and how the device will handle RSCNs. RSCNs are a critical part of Huawei Fibre Channel networks. They help to ensure that all devices in the fabric are aware of changes in the fabric, which can help to improve performance and availability. Here are some additional details about RSCNs: How RSCNs work: When a change occurs in the fabric, the device that detects the change sends an RSCN to all other devices in the fabric.\nHow do RSCNs work in a Fibre Channel (FC) network? Hello , RSCN stands for Registered State Change Notification, and it is a mechanism used in Fibre Channel (FC) networks to notify devices about changes in the fabric's topology or state. RSCNs are essential for maintaining the proper functioning and integrity of the FC network. Here's a general overview of how RSCNs work in a Fibre Channel network: Fabric Login: When a device (such as a host or a storage array) logs into the FC fabric, it registers its presence with the Fabric Name Server (FNS) and receives an assigned FC address. Name Server Database: The FNS maintains a database called the Name Server Database, which stores information about all the devices connected to the fabric. This information includes the device's FC address, port information, and the fabric's topology. State Changes: Any change in the fabric's topology or state, such as device logins, logouts, port failures, or fabric reconfigurations, triggers a state change event. State Change Notification: When a state change event occurs, the FNS generates an RSCN frame. The RSCN frame contains information about the event, including the device's FC address and the specific port or ports affected by the state change. RSCN Distribution: The RSCN frame is then distributed throughout the fabric, propagating to all devices connected to the fabric. The fabric switches use a special multicast mechanism to ensure that the RSCN reaches all relevant devices efficiently.\nDevice Handling: Upon receiving an RSCN, each device processes the notification to determine whether it is affected by the state change. If the device's address or port matches the information in the RSCN frame, it takes appropriate action based on the event. For example, if a device receives an RSCN indicating a port failure, it may re-route traffic or initiate error recovery procedures. Acknowledgment: Once a device has processed the RSCN and taken the necessary actions, it acknowledges the receipt of the notification by sending an acknowledgment frame back to the fabric. This acknowledgment helps the fabric ensure the successful delivery of the RSCN. By using RSCNs, Fibre Channel networks enable devices to remain aware of the changing fabric topology and respond accordingly. This mechanism helps maintain the integrity of the network, enables proper error handling, and allows for efficient reconfiguration in response to state changes. Thanks, i hope it will help you. Dear friend, When a device or a port changes its state, such as going offline or online, it sends an RSCN to the Fabric Login Server (FLS) in the fabric. The FLS then broadcasts the RSCN to all devices in the fabric, informing them of the state change. The RSCN contains information about the device or port that has changed its state, including its World Wide Name (WWN) and the type of state change that has occurred. This information allows the devices in the fabric to update their routing tables and ensure that traffic is directed to the correct destination.\nWhat are the benefits of using RSCNs in a FC environment? Hello, dear. RSCNs (Registered State Change Notifications) are used in Fibre Channel (FC) environments to notify all devices in fabric about changes in the state of a device or a link. Here are some benefits of using RSCNs in a FC environment: 1. Faster detection of changes: RSCNs enable faster detection of changes in the fabric, such as device logins, logouts, link failures, and topology changes. This helps in maintaining the stability and availability of the fabric. 2. Reduced traffic: RSCNs reduce the amount of traffic in the fabric by sending notifications only to the affected devices, instead of broadcasting them to all devices. This helps in improving the performance of the fabric. 3. Improved scalability: RSCNs enable the fabric to scale to a larger number of devices by reducing the amount of traffic and improving the efficiency of the fabric. 4. Better management: RSCNs provide better management of the fabric by enabling administrators to monitor and troubleshoot the fabric more effectively. They can quickly identify and resolve issues, reducing downtime and improving the overall reliability of the fabric. Overall, RSCNs are an important feature of FC environments that provide numerous benefits in terms of performance, scalability, and management. Hello, dear. RSCNs (Registered State Change Notifications) are used in Fibre Channel (FC) environments to notify all devices in fabric about changes in the state of a device or a link. Here are some benefits of using RSCNs in a FC environment: 1.\nDear All, Today we will learn about M.2, SATA, PCIe, and NVMe SSDs M.2 M.2 is a small, rectangular connector that is commonly found in laptops, ultrabooks, and small form factor desktop computers. It is designed to provide high-speed data transfer rates and take up minimal space within the device. M.2 SSDs are commonly used as storage devices in modern computers. They offer faster read and write speeds compared to traditional hard disk drives (HDDs) and are available in different lengths and widths, denoted by different key notches. The M.2 form factor supports various interfaces such as SATA (Serial ATA) and PCI Express (PCIe), including NVMe (Non-Volatile Memory Express) for even faster performance. M.2 SSDs come in various types, which are categorized based on their length, width, and key notches. The key notches determine the compatibility and supported interfaces of the M.2 slot on the motherboard. Type 2242: This M.2 type has a length of 42mm and is typically used for smaller storage capacities or wireless network cards. Type 2260: With a length of 60mm, this M.2 type offers a bit more space and is often used for SSDs with higher storage capacities. Type 2280: This is one of the most common M.2 types, measuring 80mm in length. It provides more space for larger capacity SSDs and is widely supported on motherboards. Type 22110: This M.2 type is relatively longer, measuring 110mm.\nIt is used for SSDs that require more space or offer specialized features such as additional NAND chips for higher performance or power loss protection. In addition to the length, M.2 SSDs can also have different key notches, which determine the supported interfaces. The key notches are labeled as B, M, or B+M. B-key: Supports SATA and PCIe 2 interfaces. M-key: Supports PCIe 4 and NVMe interfaces. B+M-key: Supports both B-key and M-key interfaces, offering compatibility for both SATA and PCIe. It's important to check the specifications of your motherboard or device to ensure compatibility with the specific M.2 type and key notch you plan to use. SATA SATA stands for Serial ATA, which is a computer bus interface used for connecting storage devices to a computer's motherboard. It is commonly used for connecting hard disk drives (HDDs), solid-state drives (SSDs), and optical drives. SATA replaced the older parallel ATA (PATA) interface, also known as IDE (Integrated Drive Electronics). The transition to SATA brought several improvements, including faster data transfer rates, thinner cables, and better airflow in computer systems. Here are some key features and characteristics of SATA: Speed: SATA interfaces support different generations, including SATA I (1.5 Gbps), SATA II (3 Gbps), and SATA III (6 Gbps). Each generation offers faster data transfer rates than its predecessor. Cable and Connector: SATA cables are thinner and more flexible compared to the wider ribbon cables used in PATA. SATA connectors are smaller and feature a 7-pin design for data transfer and power connections.\nHot-Swapping: SATA supports hot-swapping, which means you can connect or disconnect SATA devices (such as HDDs or SSDs) while the computer is powered on, without the need for a system restart. Compatibility: SATA is backwards compatible, meaning that newer SATA devices can be connected to older SATA interfaces, but they will operate at the maximum speed supported by the older interface. Power Efficiency: SATA devices tend to be more power-efficient compared to older PATA devices. most common SATA types: SATA I (SATA 1.5 Gbps): This is the first generation of SATA. It has a maximum data transfer rate of 1.5 gigabits per second (Gbps). SATA I is less common in modern systems and offers the slowest data transfer speed among the SATA types. SATA II (SATA 3 Gbps): SATA II is the second generation of SATA and provides a maximum data transfer rate of 3 Gbps. It offers faster speeds compared to SATA I and is still found in some older systems or low-end devices. SATA III (SATA 6 Gbps): SATA III is the most common and widely used SATA interface today. It offers a maximum data transfer rate of 6 Gbps, providing faster performance compared to the previous generations. SATA III is backward compatible with SATA II and SATA I, allowing you to connect older SATA devices to a SATA III interface. PCIe SSD A PCIe SSD (Peripheral Component Interconnect Express solid-state drive) is a type of storage device that uses the PCIe (Peripheral Component Interconnect Express) interface to connect to a computer's motherboard.\nUnlike traditional SATA-based SSDs that connect via SATA ports, PCIe SSDs utilize the faster and more direct PCIe bus for data transfer. Here are some key features and advantages of PCIe SSDs: Speed: PCIe SSDs offer significantly faster data transfer rates compared to SATA-based SSDs. By using the high-bandwidth PCIe interface, they can deliver faster read and write speeds, resulting in improved overall system performance and reduced file access times. PCIe Lanes: PCIe SSDs connect to the PCIe slots on the motherboard and utilize one or more PCIe lanes. The number of lanes determines the potential bandwidth and performance of the SSD. Higher-end PCIe SSDs often use four or more PCIe lanes for maximum performance. NVMe Protocol: Most PCIe SSDs utilize the NVMe (Non-Volatile Memory Express) protocol, which is designed specifically for solid-state storage. NVMe takes advantage of the low-latency and high-speed capabilities of PCIe, enabling efficient communication between the SSD and the system. M.2 Form Factor: Many PCIe SSDs come in the M.2 form factor, which is a small, compact form factor that plugs directly into the motherboard. This allows for easy installation and helps save space in smaller form factor systems. Scalability: PCIe SSDs can offer higher scalability compared to SATA SSDs. With multiple PCIe slots available on modern motherboards, it is possible to install multiple PCIe SSDs in a system, enabling higher storage capacities and increased performance through RAID configurations.\nGaming and High-Performance Applications: PCIe SSDs are particularly beneficial for demanding applications, such as gaming and content creation, where fast data access and transfer speeds are crucial. They can significantly reduce game load times, improve level streaming, and provide faster rendering and editing capabilities. NVMe (Non-Volatile Memory Express) SSDs NVMe (Non-Volatile Memory Express) SSDs, also known as NVMe drives, are a type of solid-state drive (SSD) that use the NVMe protocol to connect to a computer's motherboard and provide high-speed storage performance. NVMe is specifically designed to take advantage of the low-latency and high-bandwidth capabilities of PCIe (Peripheral Component Interconnect Express), resulting in faster data transfer rates compared to traditional SATA-based SSDs. NVMe SSD Here are some key features and advantages of NVMe SSDs: Speed: NVMe SSDs offer significantly faster data transfer speeds compared to SATA-based SSDs. By leveraging the PCIe interface, NVMe drives can achieve extremely high read and write speeds, reducing file access times and improving overall system performance. Low Latency: NVMe SSDs have lower latency than SATA-based SSDs, thanks to the direct communication between the SSD and the CPU via PCIe. This reduced latency enables faster data retrieval and improves system responsiveness. Parallelism: NVMe drives support parallel data paths and can handle multiple I/O (Input/Output) operations simultaneously. This parallelism allows for efficient utilization of the PCIe lanes and maximizes performance, particularly in multi-threaded workloads. Queueing: NVMe SSDs feature an advanced queueing mechanism called NVMe Queues, which allows for more efficient management and processing of I/O requests.\nThis improves the SSD's ability to handle multiple I/O operations concurrently, leading to increased performance and responsiveness. M.2 Form Factor: Many NVMe SSDs come in the M.2 form factor, which is a small, compact form factor that directly plugs into the motherboard. This form factor is especially popular in laptops and small form factor desktops, where space is limited. High-Performance Applications: NVMe SSDs excel in applications that demand high-speed storage, such as gaming, content creation, data analysis, and database management. They provide faster boot times, quicker game loading, accelerated data processing, and reduced render and compile times. It's worth noting that to take full advantage of an NVMe SSD, your system needs to support NVMe and have an available PCIe slot with the required number of lanes. Additionally, NVMe SSDs are available in different capacities and models, with varying performance characteristics, so it's essential to consider your specific requirements and choose the appropriate NVMe SSD for your needs. Comparison table highlighting the key differences between M.2, SATA, PCIe, and NVMe SSDs: SSD Type Interface Form Factor Speed Max Bandwidth Compatibility Common Use Cases M.2 SSDs SATA SSDs PCIe SSDs NVMe SSDs Note: The speeds and bandwidth mentioned above are general ranges and can vary depending on the specific SSD model and generation. It's important to consider your system's compatibility and requirements when choosing an SSD type. M.2 SSDs offer a compact form factor and support both PCIe and SATA interfaces.\nDear All, Today we are going to learn about CIFS vs SMB vs NFS In the world of networked computing, file sharing protocols play a crucial role in enabling seamless data access and collaboration among users and systems. Three widely used protocols in this domain are Common Internet File System (CIFS), Server Message Block (SMB), and Network File System (NFS). This article aims to provide a comparative analysis of these protocols, highlighting their features, advantages, and use cases. CIFS (Common Internet File System): CIFS, also known as SMB1, is a file sharing protocol that originated from Microsoft. It allows users to access files and printers over a network and is widely supported across Windows operating systems. Key features of CIFS include: CIFS supports authentication mechanisms such as NTLM (NT LAN Manager) and Kerberos. It also provides security features like file encryption and integrity checks. CIFS can be used with various operating systems, including Windows, Linux, macOS, and others. However, its performance on non-Windows systems may be limited. CIFS relies on NetBIOS (Network Basic Input/Output System) naming conventions for name resolution and network browsing. CIFS has some limitations, including slower performance compared to newer protocols, lack of support for modern security features, and vulnerability to certain types of attacks. SMB is an enhanced version of CIFS and is commonly referred to as SMB2 or SMB3, depending on the version. It is the default file sharing protocol used in modern Windows operating systems.\nKey features of SMB include: SMB introduces various performance enhancements, such as improved caching, pipelining, and support for larger file transfers. It also supports scalable features like SMB Multichannel and SMB Direct for high-speed data transfers. SMB incorporates stronger security mechanisms, including message signing, encryption, and the ability to negotiate secure communication protocols. SMB is now supported by non-Windows platforms, including Linux, macOS, and various network-attached storage (NAS) devices. SMB allows seamless sharing of files, directories, and printers across a network, enabling collaborative work environments. NFS is a file sharing protocol commonly used in UNIX and Linux environments. It enables file access and sharing among systems connected to a network. Key features of NFS include: NFS follows a client-server architecture, where the server exports directories and the clients mount them. This makes it relatively easy to set up and configure. NFS is known for its efficiency and performance in UNIX/Linux environments, with support for features like caching and read-ahead. NFS supports various security mechanisms, including the use of secure RPC (Remote Procedure Call) and Kerberos authentication. NFS is designed to be platform-independent and is supported on multiple operating systems, including Linux, UNIX, macOS, and some Windows versions. Feature CIFS SMB NFS Origin Authentication Security Name Resolution Compatibility Performance Scalability Cross-Platform Support File and Printer Sharing Configuration Interoperability CIFS, SMB, and NFS are all prominent file sharing protocols, each with its own set of features, strengths, and limitations.\nDear All, Today we are going to learn about AI-powered storage Artificial Intelligence (AI) has revolutionized numerous industries by enabling advanced automation, pattern recognition, and decision-making capabilities. One area where AI is making significant strides is in the field of storage operations and management (O&M). The convergence of storage and AI technologies has paved the way for intelligent storage solutions that optimize performance, enhance reliability, and streamline maintenance. This article explores the synergy between storage and AI and delves into the concept of Intelligent Storage O&M. 1. Synergy Between Storage and AI: 1.1 Data Management and Optimization: AI enables storage systems to intelligently manage and optimize data placement, migration, and replication. By analyzing usage patterns, AI algorithms can predict the most frequently accessed data and proactively move it to faster storage tiers, reducing latency and improving performance. Similarly, AI can identify and offload less frequently used data to lower-cost storage, optimizing capacity utilization. This synergy ensures that data is dynamically positioned to meet performance requirements and cost-efficiency goals. 1.2 Predictive Maintenance and Fault Prevention: AI-powered storage solutions employ machine learning algorithms to predict and prevent storage failures. By continuously monitoring various parameters such as temperature, vibration, and I/O patterns, AI can detect anomalies indicative of potential hardware failures. It can trigger proactive maintenance or replacement actions, minimizing downtime and improving overall system reliability. Furthermore, AI algorithms can analyze historical data to identify patterns that lead to system failures, helping in the design of more robust storage architectures.\n1.3 Intelligent Data Protection and Security: Storage systems integrated with AI can enhance data protection and security. AI algorithms can analyze data access patterns and user behavior to detect anomalous activities, potentially indicating security breaches or unauthorized access attempts. By leveraging AI, storage solutions can implement intelligent access controls and automatically respond to security threats, safeguarding critical data from unauthorized disclosure or loss. AI can also support advanced encryption techniques to protect data both at rest and in transit. 2. Intelligent Storage O&M: Intelligent Storage O&M leverages AI to automate performance optimization tasks. AI algorithms analyze real-time performance metrics, workload patterns, and historical data to identify performance bottlenecks and automatically adjust storage configurations for optimal performance. This automation reduces the need for manual intervention, speeds up troubleshooting, and ensures consistent performance levels even in dynamic environments. AI-powered analytics provide valuable insights into storage usage, enabling accurate capacity planning. By analyzing historical data growth patterns, AI algorithms can predict future storage requirements, helping organizations make informed decisions regarding storage expansion or consolidation. These predictive capabilities prevent costly over-provisioning or underutilization, optimizing storage investments. 2.3 Intelligent Diagnostics and Troubleshooting: Intelligent Storage O&M employs AI-based diagnostics and troubleshooting mechanisms. AI algorithms analyze system logs, error messages, and performance data to identify the root causes of issues quickly. With intelligent diagnostics, storage administrators can efficiently diagnose and resolve problems, reducing mean time to repair (MTTR) and minimizing the impact on business operations. 2.4 Proactive Resource Allocation: AI-powered storage systems continuously monitor workload patterns and performance demands.\nDear All, Today we will learn about Backup and Recovery. Backup and Recovery. Backup and recovery are two important processes in information technology that involve safeguarding data from potential loss or damage and restoring it in case of such occurrences. Backup refers to the process of creating copies of important data, files, or software programs and storing them in a safe location, typically in a different physical location from the original data . The purpose of backup is to ensure that data can be restored if it becomes lost, corrupted, or inaccessible due to hardware failure, software malfunction, human error, or natural disasters. Recovery , on the other hand, refers to the process of restoring lost, damaged, or corrupted data to its original state or to a previously saved state. Recovery can be done through various methods, such as restoring data from a backup, repairing damaged files, or using specialized software tools to recover data. Together, backup and recovery are crucial for ensuring data availability , maintaining business continuity, and preventing data loss, which can result in financial loss, legal liability, or damage to an organization's reputation.\nBackup Concept and Components Backup: In information technology and data management, a backup refers to a copy of data in a file system or database that can be used to swiftly and promptly recover the valid data and resume normal system operations if a disaster or mis operation occurs Backup Solution Architecture A backup management server is a software application or hardware device that facilitates the management and monitoring of backup and recovery processes. It acts as a centralized control point for managing backup policies, schedules, and storage resources. A backup media server is a dedicated server or software application that manages the storage and retrieval of backup data. Its primary function is to provide a centralized location for storing backup data from multiple sources, such as servers, desktops, and laptops. Backup System Components Backup Policy Common Backup Types Backup Topology - LAN-Based LAN-Base backup consumes network resources as both data and control flows are transmitted over a LAN. Consequently, when a large amount of data needs to be backed up within a short duration of time, network congestion is likely to occur. LAN-based backup solutions that are designed to meet the needs of different types of environments, such as small and medium-sized businesses, large enterprises, and service providers. These solutions include hardware devices such as network-attached storage (NAS) systems and tape libraries, as well as software applications such as the Huawei Backup Software. Lan-Based backup will add load on the network and recommend to have dedicated network ports.\nStrengths: The backup system and the application system are independent of each other, conserving hardware resources of application servers during backup. Weaknesses: Additional backup servers increase hardware costs Backup agents adversely affect the performance of application servers. Backup data is transmitted over a LAN, which adversely affects network performance. Backup services must be separately maintained, complicating management and maintenance operations. Users must be highly proficient at processing backup services Topology - LAN-Based Backup Topology - LAN-Free A backup topology refers to the way backup data flows through a network and the components that are involved in the backup process. LAN-free backup is a type of backup topology that involves transferring backup data directly from the source system to the backup storage device without passing through the local area network (LAN). In a LAN-free backup topology , a backup server is connected directly to the backup storage device using a high-speed storage area network (SAN) or other specialized backup network. The backup server communicates with the source system over the SAN to initiate and manage the backup process. The backup data is transferred from the source system directly to the backup storage device over the SAN, bypassing the LAN. One of the primary benefits of LAN-free backup is that it can significantly improve backup and recovery performance by reducing the impact of network congestion and latency. By bypassing the LAN, backup data can be transferred at high speeds, which can help to reduce the backup window and improve overall backup and recovery performance.\nM.2 is a specification for the physical form factor of storage devices, including SSDs, Wi-Fi and Bluetooth modules, and other expansion cards. It provides a flexible solution for integrating components into modern computing devices. M.2 uses a small rectangular PCB with edge connectors on one side. It provides a standardized interface and mounting mechanism for easy installation and compatibility across devices and manufacturers. It supports PCIe and SATA interfaces. NVMe SSDs offer high-speed storage performance, while SATA-based M.2 SSDs are a cost-effective storage solution.Its modules vary in size and shape, indicated by different \"keys\" on the connector. These keys prevent incorrect insertion of M.2 devices and ensure compatibility. Key types support different interfaces and protocols: B key for SATA and PCIe x2, M key for PCIe x4, and B+M key for a combination of SATA and PCIe x2 or x4.M.2 slots provide the connection and interface for M.2 devices on compatible devices. The slots can fit various M.2 module lengths for storage and component flexibility. M.2 SSDs are slim and compact. M.2 SSDs are ideal for space-limited devices like laptops and ultrabooks due to their smaller and lighter design compared to traditional 2.5-inch SSDs. M.2 SSDs vary in size and shape, indicated by different \"keys\" on the connector. M.2 SSDs connect to the motherboard via an M.2 slot. The M.2 slot on the motherboard is for M.2 devices. M.2 slots support PCIe (NVMe), SATA, and USB interfaces. Ensure M.2 SSD and slot compatibility for interface and key type. M.2 supports PCIe interface for high-speed storage.\nDear friend, There are a few possible causes why a Huawei storage system might not be providing the expected throughput or bandwidth. Here are a few of the most common causes: Incorrect settings: Make sure that the settings on the Huawei storage system are correct. You can check the settings by logging into the Huawei storage system management console. Hardware failure: If you have tried all of the above and the problem is still present, it is possible that there is a hardware failure with the Huawei storage system. In this case, you will need to contact Huawei support for further assistance. Too many concurrent I/O requests: If there are too many concurrent I/O requests, it can cause a decrease in throughput or bandwidth. You can try to reduce the number of concurrent I/O requests by staggering the I/O requests or by using a load balancer. Overheating: If the Huawei storage system is overheating, it can cause a decrease in throughput or bandwidth. You can try to cool down the Huawei storage system by increasing airflow or by using a cooling device. RAID configuration: The RAID configuration on the Huawei storage system can also affect throughput or bandwidth. You can try to change the RAID configuration to improve throughput or bandwidth. Firmware: If the Huawei storage system is running outdated firmware, it can cause a decrease in throughput or bandwidth. You can try to update the firmware to the latest version.\nHello friend, There are a few possible causes why a Huawei storage system might be experiencing excessive I/O latency. Here are a few of the most common causes: Incorrect settings: Make sure that the settings on the Huawei storage system are correct. You can check the settings by logging into the Huawei storage system management console. Hardware failure: If you have tried all of the above and the problem is still present, it is possible that there is a hardware failure with the Huawei storage system. In this case, you will need to contact Huawei support for further assistance. Too many concurrent I/O requests: If there are too many concurrent I/O requests, it can cause excessive I/O latency. You can try to reduce the number of concurrent I/O requests by staggering the I/O requests or by using a load balancer. Overheating: If the Huawei storage system is overheating, it can cause excessive I/O latency. You can try to cool down the Huawei storage system by increasing airflow or by using a cooling device. RAID configuration: The RAID configuration on the Huawei storage system can also affect I/O latency. You can try to change the RAID configuration to improve I/O latency. Firmware: If the Huawei storage system is running outdated firmware, it can cause excessive I/O latency. You can try to update the firmware to the latest version. Network bandwidth: If the network bandwidth between the Huawei storage system and the client is too low, it can cause excessive I/O latency.\nWhat steps are involved in configuring snapshots and replication features for data protection and disaster recovery purposes on a Dorado storage system? Configuring snapshots and replication features on a Dorado storage system for data protection and disaster recovery purposes typically involves the following steps: Snapshot Configuration: Snapshots provide point-in-time copies of data for quick recovery. Here's how you can configure snapshots on a Dorado storage system: a. Snapshot Pool Setup: Create a dedicated pool or designate a portion of the storage capacity for storing snapshots. Configure the snapshot pool settings, such as size limits and retention policies, based on your requirements. Define snapshot policies that specify the frequency and retention period of snapshots. Determine the number of snapshots to retain and the interval between snapshot creations. Configure other options like scheduling, snapshot consistency, and space-saving techniques (e.g., deduplication and compression). Associate the snapshot policies with the relevant volumes or file systems that you want to protect. Specify the snapshot policy to be applied to each volume or file system. Ensure that the snapshot policies align with your desired RPO (Recovery Point Objective). Replication Configuration: Replication allows you to create remote copies of data for disaster recovery purposes. Here's an overview of the steps involved in configuring replication on a Dorado storage system: a. Remote Replication Setup: Identify the remote storage system or target site where the replicated data will be stored. Establish the necessary connectivity between the source and target storage systems (e.g., IP network, Fibre Channel).\nToday we are discussing one of the Huawei storage covers a wide range of topics, including but not limited to: Cloud storage Huawei provides . These solutions for enterprises of all sizes. Huawei offers a range of cloud storage solutions that can be deployed in These solutions are designed to provide highly available, scalable, and cost-effective storage for modern cloud-based applications and workloads. Some of the key offerings from Huawei for cloud storage include: Distributed Cloud Storage solution is a highly scalable and distributed storage system that can be deployed in public, private, or hybrid cloud environments. DCS is designed to provide highly available and reliable storage for cloud-based applications and workloads. OceanStor Object Storage Huawei's OceanStor Object Storage is a that can be used for storing and managing large amounts of unstructured data. This system is optimized for FusionStorage: Huawei's FusionStorage is a This system is designed to provide highly available and scalable storage for cloud-based applications and workloads. Cloud Backup Huawei's cloud backup solution provides an This solution can be used to backup data to a cloud storage environment and can be . Cloud Archive: Huawei's cloud archive solution provides a cost-effective way to store and manage large amounts of data over a long period of time. This solution is designed to meet the needs of regulatory compliance and data retention requirements. Overall, Huawei's cloud storage solutions provide enterprises with the .\nQ# What are the energy conservation technologies of hard disks in the storage system? Thanks. There are several energy conservation technologies used in hard disks within a storage system. These technologies aim to reduce power consumption and improve energy efficiency. Some of the common energy conservation technologies for hard disks include: Power Management: Hard disks incorporate power management features that allow them to enter low-power modes when not in use. These modes can include idle spin-down, where the disk stops spinning after a period of inactivity, and sleep mode, where the disk consumes minimal power while maintaining data accessibility. Advanced Power Management (APM): APM is a feature that allows the hard disk to manage its power consumption based on configurable settings. It enables the disk to adjust its power usage depending on the workload, optimizing energy efficiency. IntelliSeek Technology: IntelliSeek is a technology employed by some hard disks that calculates optimal seek speeds, minimizing power consumption during read/write operations. By intelligently seeking data across the disk, power usage can be reduced. Fluid Dynamic Bearings (FDB): FDB is a bearing technology used in some hard disks that reduces friction and noise while improving energy efficiency. By reducing friction, FDB lowers power consumption and improves overall system performance. Thermal Management: Hard disks may incorporate thermal management mechanisms such as temperature sensors and adaptive fan control. These features help regulate the disk's operating temperature, ensuring optimal performance while minimizing power consumption. Low-Power Idle Modes: Hard disks can enter low-power idle modes when not actively accessed.\nThese modes reduce the disk's rotational speed and minimize power consumption while maintaining quick response times. Hybrid Drives and Solid-State Drives (SSDs): Hybrid drives combine traditional hard disks with solid-state storage, utilizing the high-speed and energy-efficient nature of SSDs for frequently accessed data. SSDs themselves consume less power than traditional spinning hard disks due to their lack of mechanical components. Intelligent caching: RAID (Redundant Array of Independent Disks): These energy conservation technologies aim to reduce power consumption, enhance energy efficiency, and prolong the lifespan of hard disks within a storage system. By optimizing power usage, storage systems can reduce energy costs and contribute to environmental sustainability. There are several energy conservation technologies used in hard disks in a storage system: Power management: Hard disks have built-in power management features that help conserve energy by reducing the disk's power consumption when it's not being used. Solid State Drives (SSDs): SSDs are more energy-efficient than traditional hard disk drives since they don't have moving parts. SSDs also offer faster access times, resulting in lower energy consumption. IntelliPower technology: IntelliPower technology is used in some hard drives by Western Digital. This technology balances the speed, power consumption, and performance of the drive to conserve energy. Spindle speed control: Some hard drives have a feature that adjusts the spindle speed according to the workload. This helps reduce power consumption while maintaining optimal performance. Advanced Format Technology: Advanced Format Technology improves the efficiency of data storage by reducing the number of disk platters required.\nIn the digital era, businesses are increasingly adopting hybrid cloud environments to leverage the benefits of both on-premises infrastructure and public cloud services. Hybrid cloud storage, a key component of this paradigm, offers a flexible and scalable approach to managing and storing data. Huawei, a renowned global technology leader, has recognized the growing significance of hybrid cloud storage and has been at the forefront of developing innovative solutions in this domain. In this article, we explore the growing importance of hybrid cloud storage and gain insights from Huawei, a company committed to empowering enterprises with advanced storage solutions. Hybrid cloud storage provides organizations with the flexibility to leverage the scalability and cost-effectiveness of public cloud services while retaining control over critical data in on-premises infrastructure. Huawei understands the need for this balance and offers storage solutions that seamlessly integrate with popular public cloud platforms, enabling businesses to move data between private and public clouds effortlessly. This flexibility allows organizations to optimize their storage resources, adapt to changing demands, and achieve greater agility in their operations. Data security and compliance remain paramount concerns for businesses across industries. Huawei acknowledges this and ensures that their hybrid cloud storage solutions incorporate robust security features. By leveraging encryption technologies, access controls, and data protection mechanisms, Huawei enables organizations to maintain data confidentiality and integrity both in transit and at rest. Additionally, compliance with regulatory standards becomes easier as organizations can retain sensitive data within their own infrastructure while benefiting from the cloud's agility and scalability.\nHybrid cloud storage offers a cost-effective approach to managing data. By leveraging the cloud for non-sensitive and less critical data, organizations can reduce the capital expenditure associated with on-premises storage infrastructure. Huawei's hybrid cloud storage solutions enable enterprises to seamlessly tier data based on its importance and access frequency. By automatically moving data between different storage tiers, organizations can optimize costs while ensuring that data remains readily accessible when needed. Business continuity and disaster recovery are critical aspects of any organization's storage strategy. Huawei recognizes the importance of robust disaster recovery solutions and provides hybrid cloud storage options that facilitate efficient backup, replication, and recovery of data. By leveraging the cloud as a secondary site for data replication and disaster recovery, organizations can achieve faster recovery times and improved resiliency, minimizing the impact of potential disruptions. Managing data across disparate storage platforms can be complex and time-consuming. Huawei's hybrid cloud storage solutions address this challenge by providing centralized management and control. Through unified management interfaces and intelligent data management features, organizations can streamline their data management processes, gain insights into storage utilization, and simplify the overall storage infrastructure. As organizations navigate the complexities of the digital landscape, hybrid cloud storage has emerged as a crucial enabler of business transformation. Huawei's commitment to developing cutting-edge storage solutions reflects the growing importance of hybrid cloud storage. By offering flexibility, data security, cost optimization, enhanced disaster recovery, and simplified data management, Huawei empowers businesses to leverage the benefits of both on-premises and cloud storage environments seamlessly.\nGreetings, everyone! In this article, Ill explain about Intelligent Storage O&M. What is Intelligent Storage? Intelligent storage refers to storage systems that incorporate advanced technologies, such as artificial intelligence (AI), machine learning (ML), and automation, to optimize and streamline storage operations. These systems utilize intelligent algorithms and analytics to enhance various aspects of storage, including performance, efficiency, reliability, and management. Here are some key characteristics and benefits of intelligent storage: Data-Driven Insights Intelligent storage systems leverage AI and ML algorithms to analyze and extract meaningful insights from storage-related data. By processing and interpreting large volumes of data, these systems can identify patterns, trends, and anomalies, enabling better decision-making and optimization of storage resources. Predictive and Proactive Operations Intelligent storage employs predictive analytics to anticipate storage needs, performance bottlenecks, and potential failures. By leveraging historical data and real-time monitoring, the system can proactively take actions to optimize performance, prevent downtime, and ensure high availability. Automation and Self-Management: Intelligent storage systems automate routine storage tasks, such as data tiering, data migration, and capacity management. By reducing manual intervention and human errors, these systems can simplify storage management, improve efficiency, and free up IT resources for more strategic tasks. Dynamic Resource Allocation: Intelligent storage optimizes resource allocation based on workload demands. It can dynamically adjust storage capacity, performance, and other resources to match application requirements, ensuring that resources are allocated efficiently and cost-effectively. Smart Data Placement: Intelligent storage systems intelligently place data across different storage tiers or media types based on data access patterns, frequency, and priority.\nThis enables faster access to frequently used data and optimal utilization of different storage technologies, such as solid-state drives (SSDs) and hard disk drives (HDDs). Enhanced Data Protection and Security: Intelligent storage incorporates advanced security features and algorithms to protect data from threats and unauthorized access. It can detect anomalies, identify security breaches, and initiate appropriate actions to ensure data integrity and confidentiality. Analytics-Driven Performance Optimization: Intelligent storage systems continuously monitor performance metrics and leverage analytics to identify opportunities for performance improvement. By analyzing workload patterns and storage configurations, these systems can optimize storage settings to deliver better performance and responsiveness. By integrating AI, ML, and automation, intelligent storage systems offer advanced capabilities that go beyond traditional storage solutions. They enable organizations to optimize their storage infrastructure, improve data management, enhance performance, and simplify operations, ultimately leading to more efficient and cost-effective storage environments. Introduction to Huawei Intelligent Storage: Huawei Intelligent Storage refers to the storage solutions provided by Huawei Technologies, a global technology company. Huawei offers a range of intelligent storage products and technologies that incorporate advanced features and capabilities to optimize storage operations. Key Aspects of Huawei Intelligent Storage: OceanStor Storage Systems: Huawei's OceanStor storage systems form the core of their intelligent storage portfolio. These systems are designed to deliver high-performance, scalable, and reliable storage solutions for various use cases and workloads. Artificial Intelligence Technologies: Huawei integrates artificial intelligence technologies into their storage solutions.\nBy leveraging AI algorithms, these systems can analyze data access patterns, optimize data placement, and predict storage needs to enhance performance, efficiency, and resource utilization. Distributed Storage Architecture: Huawei Intelligent Storage employs a distributed storage architecture that allows for scalability, high availability, and data protection. It ensures that data is distributed across multiple storage nodes or locations for redundancy and fault tolerance. Flash and Hybrid Storage: Huawei offers flash-based storage solutions, such as all-flash arrays (AFA), to deliver high-speed, low-latency storage performance for demanding workloads. They also provide hybrid storage solutions that combine flash and traditional hard disk drives (HDDs) to achieve a balance between performance and cost efficiency. Data Management and Protection: Huawei's intelligent storage solutions include features for data management, protection, and backup. These capabilities enable efficient data deduplication, compression, snapshotting, and remote replication to ensure data integrity, availability, and disaster recovery. Intelligent Tiering and Caching: Huawei's storage systems incorporate intelligent data tiering and caching mechanisms. By analyzing data access patterns and hotspots, these systems automatically move frequently accessed data to high-performance storage tiers or cache it for faster retrieval, optimizing overall system performance. Cloud Integration and Data Services: Huawei Intelligent Storage integrates with cloud environments, providing seamless data movement and management between on-premises and cloud storage. It supports various data services, including backup, archive, and data migration, enabling hybrid cloud deployments. Huawei's focus on intelligent storage aims to deliver high-performance, scalable, and efficient storage solutions for organizations across different industries.\nBy leveraging advanced technologies and features, Huawei Intelligent Storage offers enhanced data management, protection, and performance optimization capabilities to meet the evolving storage needs of businesses. Intelligent Storage O&M: Intelligent Storage O&M (Operations and Maintenance) refers to the management and monitoring of intelligent storage systems using advanced technologies and tools. It involves leveraging automation, analytics, and artificial intelligence (AI) to optimize and streamline the operations, maintenance, and troubleshooting processes for storage infrastructure. Proactive Monitoring: Intelligent Storage O&M incorporates real-time monitoring of storage systems to detect performance issues, potential failures, or anomalies. It uses automated monitoring tools and AI algorithms to collect and analyze system metrics, enabling proactive identification and resolution of problems before they impact the storage environment. Predictive Analytics: By leveraging historical data and AI-driven predictive analytics, Intelligent Storage O&M can anticipate potential issues or bottlenecks in the storage infrastructure. It can predict storage capacity requirements, performance trends, and maintenance needs, enabling proactive planning and resource allocation. Fault Diagnosis and Self-Healing: Intelligent Storage O&M uses AI-powered algorithms to diagnose storage faults or errors and initiate appropriate corrective actions. It can automatically identify the root cause of issues, provide recommendations for resolution, and even trigger self-healing mechanisms to restore system health and performance. Performance Optimization: Intelligent Storage O&M continuously analyzes storage performance metrics and workload patterns to identify optimization opportunities. It can recommend adjustments to storage configurations, data placement strategies, or caching mechanisms to improve overall performance and responsiveness. Resource Optimization: Intelligent Storage O&M optimizes resource utilization by analyzing storage capacity, throughput, and other metrics.\nIt helps in identifying underutilized or overutilized resources, making recommendations for resource allocation, and optimizing storage efficiency and cost-effectiveness. Automation and Policy-Based Management: Intelligent Storage O&M automates routine storage management tasks and adheres to predefined policies. It can automate data tiering, backup and restore processes, data protection mechanisms, and other operational workflows, reducing manual effort and minimizing human errors. Data Security and Compliance: Intelligent Storage O&M includes features and tools for ensuring data security and compliance with regulatory requirements. It incorporates encryption, access controls, audit trails, and other security measures to protect sensitive data stored within the storage infrastructure. Intelligent Storage O&M enhances the efficiency, reliability, and availability of storage systems by leveraging advanced technologies and automation. It enables proactive monitoring, predictive maintenance, performance optimization, and resource allocation, resulting in improved storage operations, reduced downtime, and better overall storage management. Huawei DME's Intelligent Storage O&M: DME (Data Management Engine) is Huawei's proprietary technology that encompasses intelligent storage management and operation capabilities. DME's Intelligent Storage O&M refers to the advanced features and functionalities provided by DME to optimize the operations and maintenance of Huawei's storage systems. Here are some key aspects of DME's Intelligent Storage O&M: Smart Diagnosis and Analytics: DME utilizes AI algorithms and advanced analytics to enable smart diagnosis of storage issues. It continuously collects and analyzes performance data, system metrics, and patterns to detect anomalies, identify potential problems, and provide insights for efficient troubleshooting and resolution.\nPredictive Maintenance: DME's Intelligent Storage O&M employs predictive analytics to forecast storage system health, predict failures, and schedule proactive maintenance activities. By leveraging historical data and real-time monitoring, DME can identify potential issues before they cause disruptions and initiate maintenance actions to minimize downtime. Automated Operations and Policy-Based Management: DME automates routine storage operations and management tasks, reducing manual effort and improving operational efficiency. It enables policy-based management, where administrators can define rules and policies for automated data migration, tiering, data protection, and other storage operations, ensuring consistent and efficient management practices. Performance Optimization: DME's Intelligent Storage O&M focuses on optimizing storage performance. It analyzes workload patterns, performance metrics, and system configurations to provide recommendations for performance optimization. These recommendations may include adjusting data placement, caching strategies, or optimizing storage resource allocation for better performance and responsiveness. Resource Utilization and Efficiency: DME helps optimize resource utilization and efficiency in storage systems. It analyzes capacity utilization, throughput, and other metrics to identify underutilized or overutilized resources and provides recommendations for resource allocation and optimization. This helps organizations make better use of their storage infrastructure and reduce unnecessary costs. Security and Compliance: DME's Intelligent Storage O&M incorporates security features and compliance capabilities to protect data stored in Huawei's storage systems. It includes encryption, access controls, data integrity checks, and auditing mechanisms to ensure data security and compliance with regulatory requirements. Unified Management Platform: DME provides a unified management platform that allows administrators to centrally manage and monitor Huawei's storage systems.\nGreetings, everyone! In this article, Ill explain about . Introduction to Network technology of NAS: Network-Attached Storage (NAS) relies on network technologies to facilitate data access and sharing over a network. Here are some key network technologies used in NAS: Ethernet: NAS systems commonly utilize Ethernet as the network technology for data transfer. Ethernet provides a reliable and widely supported network infrastructure, allowing NAS devices to connect to the local network or the internet. It supports various Ethernet standards, such as Fast Ethernet (10/100 Mbps), Gigabit Ethernet (10/100/1000 Mbps), and 10 Gigabit Ethernet (10 Gbps), providing different levels of network bandwidth. TCP/IP: Transmission Control Protocol/Internet Protocol (TCP/IP) is the standard networking protocol suite used for data communication in NAS. TCP/IP enables data packets to be transmitted and received over the network, ensuring reliable and orderly delivery of data between NAS devices and connected clients. It also handles IP addressing and routing, allowing NAS systems to be identified and accessed on the network. Network File Protocols: NAS devices typically support various network file protocols for data access. Common protocols include: NFS (Network File System): A protocol commonly used in UNIX/Linux environments for sharing files over a network. SMB/CIFS (Server Message Block/Common Internet File System ): A protocol used in Windows environments for file sharing and remote access. AFP (Apple Filing Protocol): A protocol used for sharing files between macOS devices. FTP (File Transfer Protocol): A protocol for transferring files over a network, commonly used for remote file access and management.\nHTTP/HTTPS: HyperText Transfer Protocol/Secure (HTTPS) protocols are used for web-based file access and NAS management interfaces. Wi-Fi: Some NAS devices also support Wi-Fi connectivity, allowing wireless access to the NAS system. Wi-Fi connectivity can be useful in scenarios where wired Ethernet connections are not available or when accessing the NAS from mobile devices or laptops. VLAN and Networking Features: Advanced NAS systems may support Virtual LANs (VLANs) and other networking features. VLANs allow the segmentation of a network into multiple logical networks, providing improved network security and isolation. Additionally, NAS devices may offer features like link aggregation (combining multiple network links for higher bandwidth), jumbo frame support, and Quality of Service (QoS) settings to optimize network performance and prioritize certain types of traffic. The specific network technologies and protocols supported by a NAS system may vary depending on the NAS device manufacturer, model, and configuration. It's important to consider network compatibility and performance requirements when selecting and setting up a NAS system in a given network environment. Network technology of NAS Active-Active : In an Active-Active configuration for Network-Attached Storage (NAS), multiple NAS systems are interconnected and work together simultaneously to provide data access and redundancy. The network technology used in an Active-Active NAS configuration is crucial for ensuring efficient data synchronization, load balancing, and failover capabilities. Here are some key network technologies involved in an Active-Active NAS setup: Ethernet: Active-Active NAS systems commonly utilize Ethernet as the underlying network technology.\nEthernet provides a reliable and scalable network infrastructure for interconnecting the NAS devices and facilitating data transfer between them. Link Aggregation: Link aggregation, also known as network bonding or NIC teaming, is a technology that combines multiple network interfaces into a single logical interface. In an Active-Active NAS configuration, link aggregation can be employed to aggregate the network bandwidth of the NAS systems. This enables efficient load balancing and improved network performance by distributing data traffic across multiple network links. Network Switches: Active-Active NAS setups typically require the use of network switches. Network switches provide the necessary connectivity and routing capabilities for interconnecting the NAS systems and clients accessing the data. High-performance switches with features like VLAN support, Quality of Service (QoS), and link aggregation support are commonly used to enhance network efficiency and manage traffic effectively. Network Protocols: Active-Active NAS configurations typically support standard network protocols for data synchronization, such as: Network File Protocols: NAS systems often support protocols like NFS (Network File System) and SMB (Server Message Block) for file sharing and access over the network. These protocols ensure that data changes made on one NAS system are synchronized with the other systems in real-time. Network Replication Protocols: To maintain data consistency and redundancy, replication protocols such as Rsync or proprietary replication mechanisms may be used. These protocols ensure that data modifications are propagated between the active NAS systems, keeping them in sync.\nFailover Mechanisms: In an Active-Active NAS configuration, failover mechanisms are employed to ensure continuous data availability in the event of a NAS system failure. Network technologies like Virtual IP (VIP) or Virtual Router Redundancy Protocol (VRRP) can be utilized to enable seamless failover by redirecting traffic from the failed system to the active system. Network Monitoring and Management: Active-Active NAS setups often include network monitoring and management tools to track the performance and health of the interconnected NAS systems. These tools help in detecting network congestion, identifying potential bottlenecks, and ensuring the smooth operation of the Active-Active configuration. It's important to note that the specific network technologies and configurations used in an Active-Active NAS setup may vary depending on the NAS vendor, software, and hardware capabilities. It is recommended to consult the documentation or guidelines provided by the NAS manufacturer for the specific network requirements and best practices for implementing an Active-Active NAS configuration. Conclusion: In conclusion, network technology plays a crucial role in the implementation of an Active-Active configuration for Network-Attached Storage (NAS). Ethernet serves as the foundational network technology, providing the connectivity and bandwidth required for interconnecting the NAS systems. Link aggregation can be utilized to combine multiple network interfaces and optimize network performance through load balancing. Network switches with advanced features like VLAN support and Quality of Service (QoS) help manage network traffic efficiently in an Active-Active NAS setup.\nWhat isHuawei Ransomware Protection Storage Solution? Ransomware protection solutions are designed to mitigate the risks associated with ransomware attacks. Ransomware is a type of malicious software that encrypts a victim's files and demands a ransom payment in exchange for restoring access to the encrypted data. To protect against ransomware, storage solutions often incorporate features such as: 1. Backup and Recovery: Regularly backing up data to offline or isolated systems can help ensure that a clean copy of the data is available for recovery in the event of a ransomware attack. Reliable backup solutions can aid in the restoration of data and minimize the impact of ransomware. 2. Security Measures: Storage solutions may include security mechanisms to safeguard against unauthorized access and malware. This can involve encryption of data both at rest and in transit, access controls, and authentication mechanisms to prevent unauthorized changes or tampering. 3. Threat Detection and Prevention: Advanced threat detection technologies can help identify and prevent ransomware attacks. These may include behavior-based analysis, anomaly detection, signature-based detection, and machine learning algorithms to identify patterns associated with ransomware activities. 4. Data Isolation: Isolating critical data from other systems and networks can help contain the spread of ransomware within an organization. Segmentation and network zoning techniques can limit the impact of an attack and prevent lateral movement of ransomware. It's important to note that the specific features and capabilities of any Huawei solution should be obtained directly from Huawei or their official documentation.\nWhat is HyperClone? Huawei OceanStor V6 is a storage system that includes a feature called HyperClone. HyperClone is a data protection and data management feature that allows you to create virtual copies of data volumes for backup, testing, or other purposes without impacting the original data. The HyperClone feature creates a virtual clone of a data volume by first creating a snapshot of the data volume and then creating a virtual clone of the snapshot. This approach enables the creation of point-in-time copies of data volumes that can be used for backup, recovery, and testing purposes. Following is the graphical explanation of what is hyper clone. The OceanStor V6 HyperClone feature provides various benefits, including reduced backup times, improved recovery times, and improved data management. HyperClone also enables you to create multiple virtual clones of a single data volume, which can be used for different purposes. How to enable HyperClone in OceanStor V6 Huawei OceanStor V6 series is a storage system that includes a feature called HyperClone, which allows you to create a virtual clone of a data volume for backup or testing purposes without impacting the original data volume. The working principle of HyperClone in the OceanStor V6 series involves creating a snapshot of a data volume and then creating a virtual clone of the snapshot. Here are the general steps involved in using Huawei OceanStor V6 Series HyperClone feature: Log in to the OceanStor Device Manager web interface and select the storage system you want to work with.\nCold data storage indicates the storage of slow data that is hardly approached or used. Commonly, cold data should be maintained continuously for conformity or business ambitions, if dont consider. Cold storage is commonly far more cost-effective than high administration main storage used to carry additional active data. Cold storage is a method of storing digital assets such as cryptocurrencies or other digital tokens securely offline. It involves keeping the private keys associated with the assets on a device that is not connected to the internet. Cold storage solutions provide an additional layer of security by ensuring that the private keys are never exposed to potential malicious actors online. This makes cold storage an ideal solution for those who want to protect their digital assets from theft or loss due to hacking attempts. By using cold storage, users can also ensure that their funds are safe from any potential inflation risks associated with holding digital currencies in hot wallets. Micro storage is a memory card. A memory card is flash memory data applied in a broad scope of digital devices like digital cameras, music players, mobile phones, music players, and PDAs. They are a little rough and provide high-record capacity. There is a broad range of memory card configurations. Micro-storage is a form of data storage technology that involves the use of miniaturized components to store and access data. It is a type of non-volatile memory technology that is used in many portable electronic devices, such as smartphones, tablets, and laptops.\nCold data storage permits corporations to: Encounter compliance and administration demands efficiently. Facilitate storage and data supervision Less all storage prices Economically appropriate storage capability for inactive data. Avert primary storage and conforming overburden along inactive data. Expand storage. The prime benefit you can take is that you can get expanded storage easily. Less consumption of phone memory Cost-effective Non-volatile memory Needs minimum power Break effortlessly Portable and removable Cold storage can be used for a variety of purposes, including storing cryptocurrencies, backing up important documents, and protecting sensitive information. It utilizes various security measures such as encryption, air-gapped networks, and physical security measures like biometrics and locks. By utilizing these measures, cold storage provides a secure way of storing valuable digital assets without the risk of being hacked or compromised. Why should I use cold storage? Cold storage is a type of secure, offline storage that offers peace of mind. It is ideal for storing cryptocurrencies and other valuable digital objects, such as personal documents or confidential information. When using cold storage, the only way to access a certain asset is by using the password that was originally given to it. This makes it easier for people to keep their money and sensitive information away from prying eyes, as even if someone manages to hack into the server's memory or steal its contents, they would not be able to access the data without knowing the password. Micro-storage is a type of storage technology that uses tiny devices to store data.\nThese devices are usually much smaller than traditional storage devices, such as hard drives and optical discs. In addition, they can store more data in a much smaller space, making them ideal for applications such as mobile computing and embedded systems. Micro-storage works by using a variety of technologies, including flash memory and other non-volatile memory technologies. The data is stored on the device in small blocks, allowing it to be accessed quickly and efficiently when needed. This makes micro-storage an attractive option for applications where speed and reliability are important factors. Additionally, because the data is stored in small chunks, it can be easily transferred between different types of devices without any loss of integrity or quality. SD cards and Micro cards commonly work the same. Every card uses a small processor and one or more small flash memory chips of NAND to handle the movement of instructions and data. Applying these electronic elements, the SD card can read and write data at rapid speeds. A micro-SD card is a small-scale difference from the SD card applied in mini portable appliances. Due to their size, micro-SD cards are restricted to 1 TB, during SD cards can attain further higher figures. A micro-SD card can be placed in any SD card niche and applied ordinally along an adapter. Why should I use micro-storage? Micro-storage is a cost-effective and efficient way to store data. It is becoming increasingly popular due to its ability to store large amounts of data in small physical spaces.\nWith micro-storage, businesses can save time and money by reducing the need for additional storage devices or larger servers. Additionally, micro-storage is much more secure than traditional storage solutions as it provides an extra layer of protection against cyber threats. Furthermore, micro-storage can be used to optimize performance in applications such as virtualization and cloud computing. Therefore, businesses should consider using micro-storage for their data storage needs in order to benefit from its cost savings, security advantages, and performance optimization capabilities. Cold storage is a way of operation and a computer system that is considered for the memory of inactive data. On the other side micro storage is a category of abolished flash memory cards planned particularly for mobile phones. Here is the difference between cold and micro storage: Cold data Cold storage is expensive compared to micro storage. Cold data storage delivers the storage of inactive data. It has less cost guaranteed dependability and availability. Micro storage The price of Micro SD storage is affordable. Microdata storage conduct storage assets and computing near consumers. It can lessen costs and formatting times during the growth of scalability and resilience. Microdata are miniature footprint centers of data. Boundary calculation is what while centers of microdata are how. How to store cold storage? Cold storage is capable you adequately attaining inactive data. General use reasons for cold storage consist of: Compliance data In this, needs information to balance consent. Replicated data Replicated data gathered as a defeat improvement, backup, and historical desires.\nMedia files Media files include videos and images. Afterward, cold data continue to be inactive many times, it is generally additionally cost-effective and performs efficiently in stores that accept it as the main depository. Here is mentioned some standards to apply when composing and deciding choices for cold storage: Data endurance and high ability Cost effective Media categories like LTO linear tape open tape and drives of the hard disk. Proportionally inactive data betterment and reaction time. Needs for cold data storage The acceptance of cold data storage has been growing in previous years. there is a reason behind it- aggressively growing data, exchange in the repository consumption, emerging and accepted conformity adjustment, and cost-effective cold storage. Data that is hardly obtainable, applied, or share doesnt need as many choices for consumers. Having an approach to basics, cost-effective storage capacity is necessary for such circumstances. Along with the storage of cold data, particulars can be cost-effective instruments that dont compulsory have their presentations requirement required for the access or approach. This perspective provides a better choice for businesses that desire to continue data for storage that they will not apply generally. Inactive data that consumers only required generally can be effortlessly kept in an individual area or above different locations inside several storage mediums. Consumers commonly demand high resiliency and high-capacity systems that can natural disasters, defensive fires, and floods. Along the system of cold storage, repetition is more essential than speed.\nHow does an RSCN affect the zoning configuration in a FC fabric? When an RSCN (Registered State Change Notification) is generated in a Fibre Channel (FC) fabric, it can affect the zoning configuration in the following ways: Zoning Update: The RSCN notifies the FC fabric about changes in the registered state of devices, such as device login/logout or port enable/disable. If the affected device is part of a defined zone, the RSCN triggers an update in the zoning information to reflect the changed device state. Zoning Enforcement: The FC fabric uses the zoning configuration to control access and communication between devices within the fabric. When an RSCN is generated, the fabric verifies the zoning information to ensure that the affected devices comply with the defined access policies. If necessary, the fabric adjusts the zoning enforcement to reflect the changed device states. Zone Membership Changes: If the RSCN indicates that a device has logged in or logged out of the fabric, the zoning configuration may be modified to reflect the new membership status of the device. This can involve adding or removing the device's WWN (World Wide Name) from relevant zones to manage its access privileges and control the flow of data. Zoning Consistency: The RSCN helps maintain the consistency of the zoning configuration across the fabric. By providing notifications about state changes, the RSCN allows the fabric to synchronize the zoning information among all switches within the fabric. This ensures that all switches have an up-to-date understanding of the device states and associated zoning configuration.\nWhat are the sequence of events that occur when an RSCN is generated? When an RSCN (Registered State Change Notification) is generated, the following sequence of events typically occurs: Event Trigger: A registered state change occurs on a Fibre Channel device within a Fibre Channel fabric. This change can include device login/logout, port enable/disable, or changes in device attributes. Generation of RSCN: The Fibre Channel switch detects the state change event and generates an RSCN frame. The RSCN frame contains information about the affected device, such as its WWN (World Wide Name) and the specific state change that occurred. RSCN Distribution: The Fibre Channel switch broadcasts the RSCN frame to all devices within the fabric. This allows all devices connected to the fabric to be notified of the state change event. Device Notification: Upon receiving the RSCN frame, the devices within the fabric process the notification. The affected devices, based on the information in the RSCN frame, take appropriate actions in response to the state change event. State Synchronization: Devices that receive the RSCN update their internal state information to reflect the state change event. This ensures that the device's local state matches the current state within the fabric. Application Response: Depending on the specific state change event, applications or management systems connected to the Fibre Channel fabric can initiate actions or trigger workflows based on the RSCN notification. This allows administrators to respond to the state change and manage the fabric accordingly.\nGreetings, everyone! In this article, Ill explain about Storage (AI-powered), AI (Artificial Intelligence) and Synergy Between Storage and AI. Storage (AI-powered): AI-powered storage refers to the integration of artificial intelligence (AI) techniques and capabilities into storage systems and infrastructure. It involves leveraging AI algorithms and technologies to enhance various aspects of storage, including performance, efficiency, reliability, and management. Here are some key aspects and benefits of AI-powered storage: Intelligent Data Placement: AI can analyze data access patterns, user behavior, and workload characteristics to intelligently place data in storage tiers or media types. By dynamically optimizing data placement, AI-powered storage systems can improve performance and cost-efficiency by ensuring frequently accessed data resides on high-performance storage media. Predictive Analytics and Optimization: AI algorithms can analyze historical data, system performance metrics, and other relevant factors to make predictions and optimize storage operations. This includes predicting storage capacity requirements, identifying potential bottlenecks or failures, and optimizing storage configurations for better overall performance. Automated Data Management: AI can automate routine storage management tasks, such as data migration, data deduplication, compression, and data tiering. This helps in reducing manual intervention and human errors, while improving storage efficiency and resource utilization. Intelligent Data Recovery and Protection: AI algorithms can help in detecting and mitigating data corruption, identifying anomalies or security breaches, and providing intelligent data recovery solutions. AI-powered storage systems can quickly detect and respond to potential threats, ensuring data integrity and security.\nEnhanced Resource Utilization: AI can optimize storage resource allocation and utilization, ensuring that storage capacity, performance, and other resources are efficiently utilized. This can result in cost savings by avoiding over-provisioning and maximizing the utilization of existing storage infrastructure. Proactive Maintenance and Monitoring: AI-powered storage systems can monitor the health and performance of storage components, predict potential failures, and proactively take actions to prevent downtime or data loss. This helps in maintaining high availability and reliability of the storage infrastructure. Overall, AI-powered storage brings intelligence and automation to storage systems, enabling more efficient, reliable, and optimized storage operations. By leveraging AI capabilities, organizations can improve performance, reduce costs, enhance data management, and ensure better data protection and security in their storage environments. AI (Artificial Intelligence): Artificial Intelligence (AI) refers to the simulation of human intelligence in machines, allowing them to perform tasks that typically require human intelligence. It is a multidisciplinary field that encompasses various subfields such as machine learning, natural language processing, computer vision, robotics, and more. Artificial Intelligence (AI) purposes to establish intelligent systems that can perceive, reason, study, and make decisions based on data. AI systems are designed to process and analyze large amounts of data, identify patterns, and make predictions or take actions without explicit human programming for each specific scenario. They learn from examples, experience, and feedback, continuously improving their performance over time. Machine Learning (ML) is a core component of AI, enabling systems to automatically learn and improve from data without being explicitly programmed.\nML algorithms can be trained on labeled data to recognize patterns and make predictions or to uncover hidden insights from unstructured data. NLP (Natural Language Processing) concentrateson allowing computers to identify, interpret, and generate human language. It involves tasks like language translation, sentiment analysis, speech recognition, and chatbot development. Computer Vision involves the development of algorithms and systems that enable machines to perceive and understand visual information from images or videos. Computer vision applications include object recognition, image classification, facial recognition, and autonomous driving. AI has numerous real-world applications across various industries, including: Healthcare: AI assists in medical diagnosis, drug discovery, personalized treatment plans, and patient monitoring. Finance: AI is used for fraud detection, algorithmic trading, credit scoring, and risk assessment. Transportation: AI powers autonomous vehicles, traffic management systems, and predictive maintenance for transportation infrastructure. E-commerce: AI is employed for personalized recommendations, customer service chatbots, and demand forecasting. Manufacturing: AI enhances process automation, quality control, predictive maintenance, and supply chain optimization. Education: AI supports adaptive learning, intelligent tutoring systems, and automated grading. Entertainment: AI contributes to content recommendation systems, virtual assistants, and computer-generated imagery. Ethical considerations, such as fairness, transparency, and accountability, are crucial in AI development and deployment to ensure that AI systems are unbiased, reliable, and respect privacy. As AI continues to advance, it holds the potential to revolutionize industries, improve decision-making processes, and drive innovation across various domains, making it one of the most transformative technologies of our time.\nSynergy Between Storage and AI: There is a strong synergy between storage and AI, as both fields complement and support each other in numerous ways. The followings are some main aspects of their relationship: Data Storage: AI algorithms require vast amounts of data to train and operate effectively. Storage systems play a critical role in collecting, storing, and managing the enormous volumes of data required for AI applications. Efficient and scalable storage solutions enable organizations to handle the data-intensive nature of AI workloads. Data Preprocessing: Before feeding data into AI models, it often needs to undergo preprocessing steps such as cleaning, transformation, and normalization. Storage systems with high-throughput capabilities and low latency enable rapid data retrieval and processing, facilitating the preprocessing phase and optimizing AI model training. Model Training: Training AI models involves iterative computations on large datasets, which can be time-consuming and resource-intensive. High-performance storage systems, including solid-state drives (SSDs) and distributed file systems, reduce data access latency and provide faster read/write speeds. This accelerates the training process by minimizing bottlenecks and ensuring data is readily available to the AI algorithms. Model Serving and Inference: After training, AI models are deployed for inference, where they make predictions or process new data in real-time. Efficient storage systems with low latency and high throughput are crucial for serving AI models and quickly retrieving the required data during inference. This enables real-time or near-real-time AI applications, such as recommendation systems, fraud detection, and autonomous vehicles.\nHello Community! After the advent of HDD (hard disk drive) technology, many new HDD-based storage devices were introduced to the world, but there were still some serious issues like high energy consumption, low read/write speed and many others that HDD technology was unable to cure. Henceforth, a new breed of storage technology named SSD (Solid-State Drive) was introduced to the world by Dataram in 1976. Bulk Core was the first solid-state drive. Later on, many new developments and improvements were made in SSD technology, allowing it to transform the storage landscape by offering faster speeds, improved reliability, and compact form factors compared to traditional hard disk drives. Today, SSDs come in different shapes, sizes, interfaces, and protocols that affect their performance, compatibility, and price. Henceforth, in this article, we will explore several types of SSDs, including M.2, SATA, PCIe, and NVMe SSDs, and find out how important they are. As mentioned earlier, SSD stands for solid-state drive. Solid-state drives (SSDs) are storage devices that make use of flash memory to store data. They are faster, quieter, and more reliable than traditional hard disk drives (HDDs), which use spinning magnetic platters and moving read/write heads. SSDs have become increasingly popular in recent years, especially for laptops, gaming PCs, and servers. There are several types of SSDs, but the most common are M.2, SATA, PCIe, and NVMe SSDs. All of them have their own unique characteristics, performance capabilities, and use cases due to the technology they use.\nM.2 is a form factor specification that defines the physical size and shape of an SSD card. It was introduced in 2012 as a replacement for the older mSATA standard. M.2 SSDs are small and rectangular, resembling a stick of gum. They can have 22mm width and length (16mm, 26mm, 30mm, 38mm, 42mm, 60mm, 80mm, or 110mm), but they are usually 22mm wide and 80mm long (M.2 2280). Thus, you can know the size of an M.2 SSD by just reading the four- or five-digit number in its name or on its printed circuit board (PCB). Samsung SSD 850 EVO M.2 To connect an M.2 SSD to your computer, you need a motherboard that has an M.2 slot. The M.2 slot can have different key types (B, M, and B+M) that determine which type of M.2 SSD can fit into it. The most common key type for storage devices is M. M.2 SSDs tend to use interfaces as physical connections to communicate with the computer. SATA and PCIe are the two most common interfaces used by M.2 SSDs to connect the storage to the computer, each offering its own respective advantages over others. Crucial BX500 2.5 SATA SSD SATA stands for Serial Advanced Technology Attachment. It is used to describe an interface that connects storage devices to the computer.\nIt was initially launched as an improvement over the Parallel ATA (PATA) interface and was originally designed for HDDs, but later on, it was also adopted for SSDs in order to allow users to update their storage more easily. SATA SSDs are similar to laptop hard disk drives but have no physically moving parts. They are abundant and cheap, but they offer nearly three to four times more read and write speed than traditional HDDs. SATA SSDs are currently available in both 2.5-inch and 3.5-inch sizes. There are three versions of SATA being used in SSDs: SATA I, SATA II, and SATA III. Each SATA version offers faster transmission speeds and lower latency than its predecessors. Intel 750 Series PCIe SSD PCIe stands for Peripheral Component Interconnect Express. PCIe SSDs are those SSDs that make use of the PCIe interface that connects various components to the computer. PCIe SSDs make use of a serial bus with multiple lanes to transfer data between storage component and computer. PCIe has gone through several generations over the years, increasing its maximum data transfer rate per lane from 250 MB/s (PCIe 1.0) to 1 GB/s (PCIe 4.0). PCIe also supports multiple lanes per component, resulting in higher bandwidth. According to Kingston, PCIe SSDs can come in three different forms: add-in cards (AIC), M.2, and M.3. To connect a PCIe SSD to your computer, you need a motherboard that has a PCIe slot or an M.2 slot with PCIe support.\nHowever, it is important to note that PCIe SSDs are more expensive than SATA SSDs. Samsung 970 EVO NVMe SSD NVMe stands for Non-Volatile Memory Express. It is a protocol that makes use of the capabilities of PCIe with some more advanced features like multiple queues, parallelism, low-latency commands, and optimized power management to enable faster and more efficient access to non-volatile memory devices such as SSDs. According to Kingston, NVMe tends to use mainly three form factors: add-in card (AIC), M.2, and U.2. However, for general users, mostly the M.2 2280 factor is used, while for enterprise-class NVMe SSDs, all three form factors can be used according to the requirements of servers. There are several difference and similarities between all these types of SSDs but in the table below, we will discuss few of them. Property M2 SATA PCIe NVMe Interface SATA interface or PCIe interface Serial ATA interface PCIe.\nHuawei, a leading provider of storage solutions, has integrated advanced deduplication technology into its latest storage product lines. Here are some of the key features of Huawei's storage deduplication: Inline deduplication: Huawei's storage systems perform deduplication inline, meaning data is deduplicated at the time of ingestion. This approach reduces the storage footprint from the outset, optimizing storage capacity utilization. Variable chunking: Huawei employs variable chunking algorithms to adapt to different data types and optimize deduplication efficiency. By using variable chunk sizes, Huawei can identify common patterns across various data types, resulting in higher deduplication ratios. Global deduplication: Huawei storage systems can perform deduplication across multiple systems or storage pools. This global deduplication capability enables organizations to achieve deduplication benefits across their entire storage infrastructure, eliminating duplicate data redundancies. Hybrid deduplication: Huawei employs a combination of inline and post-process deduplication techniques. Inline deduplication ensures efficient use of storage resources during data ingestion, while post-process deduplication scans existing data for further optimization. Intelligent deduplication: Huawei's storage systems employ intelligent algorithms that optimize deduplication processes. These algorithms analyze data patterns, prioritize deduplication operations, and adapt to changing workloads to achieve optimal performance and resource utilization. These features make Huawei's storage deduplication a powerful tool for organizations that want to optimize their storage capacity and reduce costs. By eliminating redundant data, Huawei's storage deduplication can help organizations save money on storage hardware, reduce storage management overhead, and improve data access performance.\nHuawei's deduplication technology can provide the following benefits: Increased storage efficiency: Huawei's deduplication technology can significantly reduce storage requirements, enabling organizations to store more data within the same capacity. This can save organizations money on storage hardware, reduce storage management overhead, and improve data access performance. Cost savings: By minimizing the storage footprint, organizations can save on hardware costs, power consumption, and data center space. This can lead to significant cost savings over time. Faster backup and recovery: Deduplication can reduce backup time and accelerate recovery by eliminating redundant data, enabling faster data transfers and shorter backup windows. This can be especially beneficial for organizations that need to recover data quickly in the event of a disaster. Enhanced data protection: With deduplication, organizations can retain more backup copies within limited storage space, improving data redundancy and resilience against data loss. This can help to protect organizations from the financial and reputational damage that can result from data loss. Data Deduplication on Huawei OceanStor Dorado Storage OceanStor Dorado storage systems offer a variety of data deduplication features, including inline deduplication, variable chunking, global deduplication, and intelligent deduplication. Inline deduplication is performed at the time of data ingestion, which means that duplicate data is eliminated before it is even stored on disk. This can significantly reduce the storage footprint of data, especially for workloads that contain a lot of duplicate data. Variable chunking allows Huawei OceanStor Dorado storage systems to adapt to different data types and optimize deduplication efficiency.\nHuawei's storage backup solution leverages OceanStor Dorado all-flash storage and OceanStor Pacific series of mass data storage as the primary and backup storage devices respectively. OceanStor Dorado delivers high performance, low latency, and high reliability for production data. OceanStor Pacific provides large capacity, high scalability, and cost efficiency for backup data. Huawei's anti-ransomware solution provides end-to-end data encryption (data leakage prevention), WORM and secure snapshot (data tampering prevention), and Air Gap (physical isolation) technologies to ensure high security and availability of data copies. - Data encryption: Huawei's storage devices support self-encrypting drives (SEDs) that encrypt data at rest using AES-256 algorithm. The encryption keys are stored in a secure key management server (KMS) that is isolated from the storage network. This ensures that even if the drives are stolen or lost, the data cannot be accessed by unauthorized parties. - WORM and secure snapshot: Huawei's storage devices support WORM (Write Once Read Many) feature that prevents data from being modified or deleted after being written. This feature complies with various compliance standards, such as SEC 17a-4 and FINRA. Moreover, Huawei's storage devices support secure snapshot feature that creates immutable point-in-time copies of data that can be used for fast recovery in case of ransomware attack. - Air Gap: Huawei's storage devices support Air Gap technology that automatically disconnects replication links between primary and backup storage devices based on predefined policies or manual triggers. This creates a physical isolation zone that prevents ransomware from spreading to backup data.\nThe replication links can be reconnected after the ransomware threat is eliminated. Huawei's storage backup and anti-ransomware solution provides businesses with comprehensive protection against all types of threats. It offers the following benefits: - Enhanced data security: The solution ensures that data is encrypted at rest and in transit, preventing data leakage. It also ensures that data is immutable and isolated from ransomware attacks, preventing data tampering. - Reduced recovery time: The solution enables fast and granular recovery of data from secure snapshots or WORM copies in case of ransomware attack. It also enables quick reconnection of replication links after the threat is eliminated. - Lowered total cost of ownership (TCO): The solution leverages all-flash storage and mass data storage to provide high performance and large capacity for production and backup data respectively. It also reduces the complexity and cost of backup management by integrating with mainstream backup software vendors. Ransomware is a serious threat to data security that can cause huge losses for businesses. Huawei's storage backup and anti-ransomware solution provides a highly reliable protection system that covers both primary and backup storage. It leverages key technologies such as encryption, WORM, secure snapshot, and Air Gap to implement data anti-tampering, security detection, and secure recovery. It also provides flexible, reliable, and efficient backup and recovery capabilities for various scenarios and platforms. With Huawei's storage backup and anti-ransomware solution, businesses can rest assured that their data is safe and available at all times. (1) Ransomware Protection Storage Solution - Huawei Enterprise. https://e.huawei.com/en/solutions/storage/oceanprotect/ransomware.\nHello, everyone! Today, I will share with you about What is data backup? Key Aspects of Data Backup: Redundancy: Disaster Recovery Versioning Backup Frequency: Methods of Data Backup: Local Backup Cloud Backup Hybrid Backup Importance of Data Backup: Data Loss Prevention Business Continuity Compliance and Legal Requirements Peace of Mind Types of Data Backups: There are several types of backup methods, each with its own approach to storing and managing data copies. The choice of backup type depends on factors such as data size, frequency of changes, recovery time objectives, and available storage resources. Here are some common types of backup: Full Backup: A full backup involves creating a complete copy of all data and files in a system or designated storage. It captures all the data at a specific point in time and is typically the initial step in implementing a backup strategy. While full backups provide comprehensive data recovery, they can be time-consuming and require significant storage space. Incremental Backup Incremental backups only store changes made since the last backup, whether it was a full backup or incremental backup. It captures the differences or new data since the last backup, resulting in smaller backup sizes and faster backup operations. However, restoring data from incremental backups requires sequential restoration of multiple backup sets. Differential Backup Similar to incremental backups, differential backups capture changes made since the last full backup. However, they do not consider subsequent incremental backups.\nEach differential backup captures all changes made since the last full backup, resulting in larger backup sizes compared to incremental backups. Restoring data from differential backups is faster than from incremental backups since only the latest full backup and the most recent differential backup need to be restored. Mirror Backup A mirror backup creates an exact copy of selected files or directories in real-time. It continuously synchronizes the source and backup locations, ensuring that both contain identical copies of data. Mirror backups do not store versions or historical changes, making them useful for immediate data availability and quick recovery. Snapshot Backup Snapshot backups capture the state of data at a specific point in time without duplicating the actual data. They create a read-only copy of the data, allowing for efficient backups with minimal storage requirements. Snapshots are commonly used in storage systems and virtualization environments, providing a quick method to restore data to a specific point in time. Cloud Backup Cloud backup involves storing data copies in remote servers or cloud-based storage services. It provides off-site backup and data redundancy, eliminating the need for local storage infrastructure. Cloud backups can utilize various backup methods, such as full, incremental, or differential backups, depending on the service provider and configuration. It's worth noting that these backup types can be combined to create comprehensive backup strategies. For example, organizations may employ a combination of full and incremental backups to balance data recovery time and storage space requirements.\nAdditionally, backup solutions often offer scheduling, compression, encryption, and retention options to enhance backup operations and data protection. Selecting the appropriate backup type depends on the specific requirements of your data, recovery objectives, and available resources. It's advisable to assess your data backup needs, consult with backup professionals, and consider factors such as data criticality, recovery time objectives, and storage capacity to determine the most suitable backup approach for your needs. What is Data Recovery? Data recovery refers to the process of retrieving or restoring lost, deleted, corrupted, or inaccessible data from storage devices. It involves the use of specialized techniques and software tools to recover data that has been accidentally or intentionally deleted, damaged due to hardware or software failures, or affected by logical or physical errors. Key Aspects of Data Recovery: Lost Data : Data recovery focuses on retrieving data that has been lost or made inaccessible. This can include various types of files, such as documents, images, videos, databases, emails, and more. Storage Devices : Data recovery can be performed on various storage devices, including hard disk drives (HDD), solid-state drives (SSD), USB flash drives, memory cards, RAID arrays, NAS (Network-Attached Storage), and even virtualized storage systems. Causes of Data Loss : Data loss can occur due to a variety of reasons, including accidental deletion, formatting, virus or malware attacks, software or hardware failures, power outages, natural disasters, file system corruption, and physical damage to the storage device.\nRecovery Techniques : Data recovery techniques vary depending on the cause of data loss and the type of storage device involved. Common recovery methods include scanning for deleted files, repairing or rebuilding file systems, extracting data from damaged sectors, and reconstructing RAID arrays. Data Recovery Software : Specialized data recovery software is commonly used to assist in the recovery process. These tools employ various algorithms and scanning methods to locate and recover lost data from storage devices. Professional Data Recovery Services : In cases where the data loss is complex or the storage device has suffered physical damage, professional data recovery services may be necessary. These services employ advanced equipment and techniques to recover data from severely damaged or failed storage devices. Importance of Data Recovery: Data Loss Mitigation Business Continuity Personal Data Preservation Legal and Compliance Requirements Peace of Mind It is important to note that data recovery is not always guaranteed, especially in cases of severe physical damage or overwritten data. To maximize the chances of successful data recovery, it is advisable to stop using the affected storage device immediately after data loss, avoid attempting DIY repairs or recovery methods that may cause further damage, and seek professional assistance when needed. Preventive measures such as regular data backups, implementing reliable storage systems, using up-to-date antivirus software, and practicing safe data handling practices can significantly reduce the risk of data loss and the need for extensive data recovery efforts. Conclusion: In conclusion, data backup and recovery are essential components of a comprehensive data management strategy.\nDear Members, More enterprises are introducing AI for IT Operations (AIOps) to handle huge data volumes, improve efficiency, and facilitate automated O&M. Today, AI technologies in the storage field are no longer limited to the monitoring and O&M of devices; instead, they are integrated into storage products. Trend analysis 1. Enterprises are using AI to improve O&M automation of storage systems The explosive growth of data volumes in data centers (DCs) has created new challenges for storage management such as fault location and risk identification. This means that existing O&M methods are no longer sufficient. According to Gartner, by 2023, 40% of I&O teams in large enterprises will use AI-augmented automation. Enterprises are expected to invest more in AI tech to automate storage O&M in DCs, improving resource management and O&M efficiency with less reliance on human labor. 2. Enterprises and storage vendors are jointly developing 3-layer AI architecture (Cloud-Center-Device AI) To produce high accuracy and reliability, AI training requires a large amount of data for accumulation and model optimization. To meet this demand, enterprises are using storage vendors AI management tools to build 3-layer AI architecture to centrally manage storage devices, simplify infrastructure O&M, and improve efficiency. Figure 1: 3-layer AI architecture Device AI: Software and hardware resources on devices are automated, with recommendations for device configuration items, auto-detection faults, and slow disks, and data acquisition from devices for cloud training and running AI model updates from the cloud via online updates or offline imports.\nThe storage network architecture has evolved significantly over the past few decades. Early storage networks were simple and consisted of a few direct-attached storage (DAS) devices connected to a single host. As the demand for storage capacity and performance grew, storage networks became more complex and began to use shared storage technologies such as Fibre Channel (FC) and iSCSI. In recent years, the adoption of cloud computing and virtualization has further complicated storage network architectures. Cloud-based storage solutions, such as Amazon S3 and Microsoft Azure, offer a pay-as-you-go model that can be very cost-effective for businesses that need to scale their storage capacity quickly. However, these solutions can be complex to manage and may not offer the same performance and availability as on-premises storage solutions. Virtualization technologies, such as VMware vSphere and Microsoft Hyper-V, allow multiple operating systems and applications to run on the same physical server. This can help businesses to save money on hardware costs, but it can also put a strain on storage networks. Virtualization can lead to increased storage traffic, which can impact performance and availability. The evolution of storage network architectures can be divided into four main phases: Phase 1: DAS The first generation of storage networks used DAS, which is a direct connection between a storage device and a host. DAS is a simple and cost-effective solution, but it can be limited in terms of scalability and performance.\nFigure 1: What is DAS Phase 2: SAN The second generation of storage networks used SAN, which is a shared storage architecture that uses Fibre Channel or iSCSI to connect storage devices to hosts. SANs offer greater scalability and performance than DAS, but they can be more complex and expensive to deploy and manage. Figure 2: SAN Phase 3: NAS The third generation of storage networks used NAS, which is a network-attached storage architecture that uses a file sharing protocol to connect storage devices to hosts. NAS is a simple and easy-to-use solution, but it can be limited in terms of performance and scalability. Figure 3: NAS Phase 4: Cloud-based storage The fourth generation of storage networks is cloud-based storage, which offers a pay-as-you-go model that can be very cost-effective for businesses that need to scale their storage capacity quickly. However, cloud-based storage can be complex to manage and may not offer the same performance and availability as on-premises storage solutions. The evolution of storage network architectures has created a number of challenges for businesses, including: Complexity . As storage networks have become more complex, it has become more difficult to manage and troubleshoot them. Cost . The cost of storage networks has increased as they have become more complex and feature-rich. Performance . The performance of storage networks can be impacted by factors such as the type of storage devices used, the network topology, and the amount of traffic on the network. Availability .\nDigital transformation challenges to overcome Next-gen IT systems in healthcare Understanding digital hospital systems Healthcare with Huawei NAS storage Consider the following case study: A top European hospital overhauled its existing setup with a unified platform for its clinical and management information systems running database, virtualization, file sharing, email, forensic video, and office applications. With a 20% YOY increase in data volumes, hospital IT systems are becoming unresponsive, while legacy storage devices have insufficient performance and cant easily be expanded or store mass data. Because medical professionals are busy with providing medical services and other tasks like teaching and conducting medical research, it is necessary to find time-saving solutions to review medical records and reports. This is why the hospital upgraded its information systems with OceanStor Dorado, Huaweis flagship storage product thats able to respond to different applications thanks to a tiered storage design. SAN storage equipped with high-performance SSDs is used for databases and virtualization platforms, while mailboxes, medical images, and forensic videos are deployed on NAS storage. This setup ensures huge volumes of both structured and unstructured data are effectively stored, reducing O&M costs by an average of 50%. This solution also streamlines normal operations. SmartQoS ensures optimal quality of different services and provides excellent response to critical services giving staff more time to spend on more important issues. Integrated SAN and NAS Active-Active Solution provides HA for both database and file services, and a premium framework to prevent interruptions to critical services.\nExcessive heat can cause SAS hard drives to malfunction or fail. If we have all the necessary environmental checks done and still SAS hard drives getting hotter than is this the reason of burden of various appliances in the datacenter or due to loose connections of SAS drives. I am not able to find the actual root cause that why it is happening? Excessive heat can indeed cause SAS (Serial Attached SCSI) hard drives to malfunction or fail. While environmental factors and loose connections could be potential causes, there are several other factors that could contribute to the increased heat and the root cause of the issue. Here are a few possibilities to consider: 1. Inadequate Cooling: Insufficient cooling within the data center can lead to higher ambient temperatures, affecting the temperature of the hard drives. If the cooling infrastructure, such as air conditioning or ventilation systems, is not properly designed, maintained, or unable to handle the heat load generated by the equipment, it can result in increased temperatures for the SAS drives. 2. High Workload or Burden: If the data center infrastructure is burdened with excessive workloads or the appliances within the data center are running at full capacity, it can generate additional heat. This increased heat load can impact the temperature of the SAS drives and potentially lead to higher temperatures than normal. 3. Poor Airflow or Cabling: Improper airflow management, such as blocked air vents, improper rack configurations, or cable congestion, can impede the circulation of cool air and lead to heat buildup.\nSimilarly, loose or improper connections of SAS drives can create resistance and generate additional heat. 4. Hardware or Firmware Issues: Certain hardware or firmware issues within the SAS drives themselves could contribute to increased heat. This could include firmware bugs, outdated firmware versions, or faulty components within the drives. To troubleshoot the root cause of the issue, consider the following steps: 1. Conduct Temperature Monitoring: Use temperature monitoring tools to measure and monitor the temperatures of the SAS drives and the surrounding environment. This will help identify any abnormal temperature patterns or hotspots. 2. Review Environmental Checks: Verify that the environmental checks performed were comprehensive and accurate. Ensure that cooling systems are functioning correctly, airflows are unobstructed, and temperature sensors are calibrated properly. 3. Assess Workload and Infrastructure: Evaluate the workload on the data center infrastructure and the capacity of the cooling systems. Determine if the infrastructure is properly sized to handle the heat load generated by the appliances and if any adjustments or upgrades are needed. 4. Check Airflow and Connections: Inspect the airflow management within the racks, including cable management and proper airflow paths. Ensure that all connections of the SAS drives are secure and properly seated. 5. Consult with Vendor or Expert: If the issue persists or you are unable to identify the root cause, it may be beneficial to consult with the SAS drive vendor or an expert in data center infrastructure. They can provide specialized guidance and support to diagnose the issue and provide appropriate solutions.\nFrom management to products, AI powers autonomous-driving storage throughout data lifecycle. More enterprises are introducing AI for IT Operations (AIOps) to handle huge data volumes, improve efficiency, and facilitate automated O&M. Today, AI technologies in the storage field are no longer limited to the monitoring and O&M of devices; instead, they are integrated into storage products. Trend analysis 1. Enterprises are using AI to improve O&M automation of storage systems The explosive growth of data volumes in data centers (DCs) has created new challenges for storage management such as fault location and risk identification. This means that existing O&M methods are no longer sufficient. According to Gartner, by 2023, 40% of I&O teams in large enterprises will use AI-augmented automation. Enterprises are expected to invest more in AI tech to automate storage O&M in DCs, improving resource management and O&M efficiency with less reliance on human labor. 2. Enterprises and storage vendors are jointly developing 3-layer AI architecture (Cloud-Center-Device AI) To produce high accuracy and reliability, AI training requires a large amount of data for accumulation and model optimization. To meet this demand, enterprises are using storage vendors AI management tools to build 3-layer AI architecture to centrally manage storage devices, simplify infrastructure O&M, and improve efficiency. Figure 1: 3-layer AI architecture Device AI: Software and hardware resources on devices are automated, with recommendations for device configuration items, auto-detection faults, and slow disks, and data acquisition from devices for cloud training and running AI model updates from the cloud via online updates or offline imports.\nCenter AI: Dedicated software can implement unified management on multiple devices in a DC, as well as resource pooling, standardization, and the automation of storage devices. The software is deployed in a private DC and therefore isolated from the extranet for stringent data security controls. Cloud AI: Powerful cloud-based computing resources are used to train AI models using the training data uploaded from storage devices. Optimized AI models are distributed on demand to DC management software and devices. And cloud management software can implement remote intelligent O&M on storage devices, despite weaker capabilities than DC management software. For security purposes, remote O&M prohibits device modifications. 3. Storage vendors are building intelligent storage products to optimize device efficiency and reliability To fit the diverse storage requirements of different applications, storage vendors are integrating AI into storage products to enhance device performance and reliability. Dell EMC storage systems use built-in intelligent tuning and data reduction algorithms for self-optimized storage provisioning and optimal data reduction ratios. NetApp systems can intelligently optimize hardware resources scheduling to accelerate data access. And Huawei storage intelligently allocates hardware resources to accelerate data read and write, while intelligently adjusting data reduction algorithms based on data types to increase data compression rates and reduce the storage cost per unit of data. In traditional storage, algorithms and data are coupled and multiple fixed algorithms are distributed at the cache, in the scheduling layers, and in the storage pools of storage devices.\nHowever, algorithm parameters need to be manually adjusted to ensure the access efficiency of different types of data. In contrast, intelligent storage incorporates architectural innovations by decoupling algorithms from data. A self-learning and adaptive algorithm library enables autonomous decisions on the layout, scheduling, and reduction of different data types, ensuring efficient and flexible access in diverse data applications. Figure 2: Algorithm-data decoupling with intelligent storage What we suggest 1. Develop new evaluation elements for storage AI management software To accelerate enterprise digital transformation, both storage vendors and enterprises must consider how to integrate AI management software into enterprise production and management services. It is recommended that enterprises establish clear evaluation factors and standards for the AI management software provided by storage vendors. This will drive storage vendors to upgrade AI management software based on the core values enterprises care most about. Evaluation elements should cover the following dimensions: Responsibility scope: AI is not developed to replace humans, but to assist and strengthen human abilities and contributions by learning and transcending how human beings perceive and respond to the world. It is recommended that enterprises develop the responsibility scope of AI within which storage vendors can upgrade and expand AI capabilities to guarantee that storage AI management is under enterprise control. Technical specifications: AI algorithms depend on learning and training. Model understanding and training data volumes determine the error rate of AI inference results.\nThe NVMe (Non-Volatile Memory Express) protocol and interface are tailored to enhance the performance of solid-state drives (SSDs). NVMe (Non-Volatile Memory Express) is a contemporary storage technology that offers faster data transfer rates than conventional storage interfaces such as SATA (Serial ATA). The NVMe storage technology employs the PCIe (Peripheral Component Interconnect Express) interface, which is a widely used interface for connecting high-speed components in a computer system. NVMe SSDs utilize the high bandwidth and low latency of PCIe to achieve faster data transfer speeds and lower latency than SATA-based SSDs. Pic credit:https://www.pcworld.com/article/432532/everything-you-need-to-know-about-nvme.html Advantages: The parallelization of I/O operations is a significant benefit of NVMe storage, as it enables the execution of multiple data transfers simultaneously. The utilization of parallelism, in conjunction with the high bandwidth of PCIe, allows NVMe SSDs to achieve remarkable performance, particularly in activities that require extensive data read and write operations, such as operating system boot-up, application loading, and large file transfers. NVMe storage devices are available in various form factors, such as M.2 and add-in PCIe cards. The M.2 form factor has become increasingly popular in recent years due to its versatility and compact size. M.2 slots capable of accommodating NVMe SSDs are now a common feature in modern laptops and desktops. NVMe storage is a storage technology that utilizes the PCIe interface to provide high performance and responsiveness, making it faster and more efficient than traditional storage options such as SATA.\nThe utilization of high-speed storage has gained significant popularity in both consumer and enterprise environments due to its criticality in supporting demanding applications and workloads. NVMe SSDs offer faster data transfer speeds than conventional storage options such as SATA. The aforementioned activities, namely booting up the operating system, loading applications, and transferring large files, can be executed at a significantly faster pace. NVMe storage technology reduces latency by optimizing the data transfer time between the storage device and the system, resulting in faster data access. The outcome of this is nearly immediate response times, which increases the overall system's responsiveness and enhances the user experience. The utilization of NVMe technology results in enhanced I/O performance due to the ability to execute parallel I/O operations. This allows for the processing of multiple queues and I/O requests concurrently. The implementation of parallelism results in an improvement in storage performance, which in turn leads to enhanced multitasking capabilities and faster data access. NVMe SSDs offer an improved gaming experience by considerably reducing game loading times. This results in faster level transitions and smoother gameplay. The implementation of optimized stuttering reduction and accelerated asset streaming can enhance the gaming experience for users. NVMe storage enables faster application launch, resulting in reduced load times. This feature facilitates immediate access to work or gaming applications without any delay. Resource-intensive applications, such as video editing software or 3D modeling programs, can particularly benefit from this.\nThe NVMe protocol is intended for use with solid-state drives (SSDs) that are connected to a computer system through the PCIe (Peripheral Component Interconnect Express) interface. The Peripheral Component Interconnect Express (PCIe) is a serial interface that operates at high speeds. It is frequently utilized for linking different components, including graphics cards, network adapters, and storage devices. The Non-Volatile Memory Express (NVMe) protocol utilizes the high bandwidth and low latency of the Peripheral Component Interconnect Express (PCIe) interface to optimize the performance of Solid State Drives (SSDs). The NVMe storage interface presents a number of notable performance benefits when compared to conventional storage interfaces such as SATA. The aforementioned advantages encompass: NVMe SSDs offer significantly faster data transfer speeds in comparison to SSDs based on SATA. SATA III has a maximum transfer rate of 6 Gbps, whereas NVMe can offer varying speeds ranging from several gigabits to multiple terabits per second, contingent upon the particular drive model. NVMe storage reduces latency by minimizing the time required for data to travel between the storage device and the system. The NVMe technology leverages the direct communication pathways of the PCIe interface to reduce data transfer delays. This results in faster application loading times and improved system responsiveness, as the technology offers near-instantaneous response times. The NVMe technology facilitates parallelism by permitting the processing of multiple queues and I/O operations concurrently. The implementation of parallelism in storage systems can improve performance by allowing for simultaneous handling of multiple read and write requests.\nThis results in faster data access and improved multitasking capabilities for the SSD. NVMe storage devices are available in various form factors, among which the most prevalent ones are M.2 and add-in PCIe cards. M.2 is a compact form factor that establishes a direct connection with the motherboard. M.2 slots are a common feature in contemporary laptops, ultrabooks, and desktop motherboards. The M.2 form factor NVMe SSDs are widely preferred owing to their compact design and effortless installation. Add-in PCIe cards are a type of NVMe SSD that can be inserted into a PCIe slot on the motherboard. The utilization of these cards is commonly observed in desktop computers or servers that possess accessible PCIe slots. NVMe storage is advantageous in situations where there are high-performance workloads and applications that heavily depend on storage performance. Typical scenarios where a product or service is utilized include: NVMe SSDs are known to significantly decrease game loading times, resulting in faster level transitions and a more seamless gaming experience. Professionals who handle large multimedia files, such as graphic designers and video editors, can take advantage of the high read/write speeds of NVMe storage. This allows for faster file transfers and rendering times. NVMe storage can enhance database operations, data caching, and virtual machine performance in server environments, leading to increased system efficiency. NVMe is a suitable solution for high-speed data transfer applications, particularly those that involve transferring large files, such as 4K/8K video editing, data analysis, and scientific research.\nThe Common Internet File System (CIFS) is a protocol for network file systems that enables clients to remotely access and share files and resources located on servers over a network. Microsoft developed it as a successor to the Server Message Block (SMB) protocol. The Common Internet File System (CIFS) is a protocol that has been primarily developed to cater to the needs of Windows operating systems. It provides a smooth and uninterrupted integration with Windows file systems, permissions, and security mechanisms. CIFS enables users to access shared files and folders in a manner that resembles local access, utilizing familiar concepts such as drive mappings. The system provides support for various authentication mechanisms, including NTLM (NT LAN Manager) and Kerberos, which are implemented to guarantee secure access to shared resources. In addition to file sharing, CIFS offers supplementary features such as shared printer support and print job management. The Common Internet File System (CIFS) functions based on the client-server model. In this model, clients transmit requests to servers, which in turn provide the requested files or services. The SMB protocol facilitates message exchange between clients and servers for communication purposes. The Common Internet File System (CIFS) facilitates file locking, which is a mechanism that prohibits multiple users from making changes to a file concurrently, thereby ensuring the consistency of data. Client-server CIFS allows networked file access and services. It communicates using the SMB1 protocol. The server replies to client requests with files or services. Clients and servers share files and resources via messages.\nThe Server Message Block (SMB) file system is a network file sharing protocol that allows machines on a network to exchange files, printers, and other resources. Microsoft created it, and it is commonly used in Windows-based environments. Here are some important facts concerning the SMB file system: - SMB is a client-server protocol that allows file and printer sharing across a network. It enables numerous clients to access shared server resources. Picture credit :https://www.techtarget.com/searchnetworking/definition/Server-Message-Block-Protocol some key features of SMB File system: - Networking: SMB communicates through port 445 and runs on top of the TCP/IP network protocol. - File Sharing: SMB allows users to access remote machines' files and directories as if they were local. It enables different operating systems to communicate files over heterogeneous networks in a standardized manner. - Authentication: To guarantee secure access to shared resources, SMB supports a variety of authentication procedures. This includes authentication via username/password, Kerberos, and guest access to public shares. - Fine-grained access control: SMB allows managers to establish permissions for individual users or groups. This prevents unauthorized users from reading, writing, or modifying shared files. - Shared Resources: SMB can be used to share printers, scanners, and other server-connected devices in addition to file sharing. This allows for centralized control as well as easy access to shared peripherals. - File and Print Services: SMB is used by Microsoft Windows to deliver file and print services, allowing users to exchange files and printers across a network.\nThe Network File System (NFS) is a distributed file system protocol that enables network-based file access. Sun Microsystems created it, and it has since become a widely accepted standard for file sharing in Unix-like environments. The following is a full explanation of the NFS file system, containing major points: Definition: NFS is a client-server protocol that allows file sharing and remote file access across a network. It enables clients to mount remote file systems as if they were local, allowing them to access files and directories on remote servers in a transparent manner. Picture credit :https://zhu45.org/posts/2018/May/01/suns-network-file-system-nfs/ - Distributed File System: NFS is a distributed file system that allows files and directories to be stored on remote servers rather than a single workstation. This facilitates cross-system communication and unified file management. - Network Protocol: NFS communicates through port 2049 and runs on top of the TCP/IP network protocol stack. It requests and provides file access services via remote procedure call (RPC) protocols. - File Sharing: NFS allows clients to access files and directories on remote servers as if they were local file systems. This allows for the seamless sharing of files and resources across workstations and operating systems. - Client-Server Architecture: NFS operates on a client-server basis, with the server hosting the file system and clients mounting the distant file system to access shared files. The server exports directories to clients, describing the access permissions and resources accessible.\n- File Access and Locking: NFS allows a wide range of file access operations, such as reading, writing, creating, and deleting files. It also has file locking techniques to protect data integrity and prevent several clients from making changes at the same time. - Authentication and Security: NFS provides authentication and security features to safeguard file access and prevent unauthorized users from accessing shared resources. This involves the usage of user and group permissions, as well as network-level security and encryption choices. - Versioning: NFS has changed over time, with many versions of the protocol deployed. NFSv3 and NFSv4 are the most regularly used versions, with each bringing improvements in performance, security, and features. - Performance Optimization: NFS uses a variety of strategies to optimize file access and reduce network latency, such as client-side caching and read-ahead and write-behind procedures. - Cross-Platform Support: NFS is extensively supported by third-party software on a variety of operating systems, including Unix-like platforms (such as Linux, macOS, and many flavors of Unix) and Windows. This enables file sharing across heterogeneous settings. - Broad Adoption: NFS is widely used in a variety of contexts, including server farms, clusters, cloud computing, and virtualized systems. It offers a scalable and effective alternative for file sharing and network collaboration. Overall, the NFS file system provides a standardized, distributed method to file sharing that enables smooth access to remote files and resources via a network.\nThe HyperMetro (active-active) solution mirrors data at 2 data centers. When one storage system fails, the service automatically switches to another storage system, and the service continues to run. The dual-active solution ensures that service is not interrupted, and data is not lost during the failure process, which resolves single points of failure issues encountered by traditional storage. One Device Gateway-free; one device supports HyperMetro for both files and databases. One Arbitration System SAN and NAS share a quorum device. Services are provided by the same site in the event of link failures. ensuring data consistency. One Network The heartbeat, configuration, and physical links between two sites are integrated into one. One network supports both SAN and NAS transmission. High availability for local dual arrays High availability for arrays in the same city A host delivers the write I/Os to the HyperMetro management module. The system records the log. The HyperMetro management module concurrently writes the write I/Os to both the local and remote caches. The local and remote caches return the write I/O result to the HyperMetro management module. The storage array returns the write |/O result to the application host after receiving feedback from the local and remote caches. The storage arrays determine whether the dual-write succeeds. If the write I/O request is processed successfully, the log is deleted. If the write I/Os fail to be written to the local or remote caches, the log is converted into a DCL. The DCL records the differential data between the local and remote LUNs.\nThe explosive growth of massive data poses many challenges to cloud and Internet data centers. In the multi-cloud era, the emerging Diskless architecture helps data centers achieve energy saving, high performance, and high reliability. The cloud and Internet industries have built the largest IT infrastructure platform in China, storing and processing the largest proportion of data. With the expansion of data center construction scale, how to cope with the sharp increase in construction requirements, meet the requirements of green and intensive, and achieve high-quality data center evolution is an important issue that needs to be solved urgently. The traditional data center architecture is a typical multi-layer architecture. Each layer consists of a complete computer system consisting of CPUs, memory, buses, and hard disks, including servers, networks, and storage devices. When new data applications emerge, enterprises usually adopt an integrated server architecture that couples applications and local disks to quickly deploy and try new services. The traditional integrated storage and computing architecture leads to unbalanced development of hardware resources, such as computing and storage resources. The computing power life cycle and data life cycle are increasingly different. In addition, the architecture has problems such as inflexible expansion, idle resources, and low utilization. In addition, the architecture cannot meet the requirements of service data sharing and access. The development of new hardware technologies, such as high-performance disk enclosures, dedicated data processors (such as DPUs), remote direct memory access (RDMA), and CXL protocol, provides a technical foundation for reconstructing data center infrastructure.\nHelps new data centers meet requirements on resource utilization, reliability, performance, and efficiency. Driven by multiple factors, such as new service challenges and rapid development of hardware technologies, the Diskless architecture emerges rapidly. The diskless architecture expands local disks of servers and completely decouples and integrates the original multi-level hierarchical resources, implementing independent expansion and flexible sharing of various hardware. Breaks through the traditional processing logic centered on general-purpose CPUs, offloads tasks that general-purpose CPUs are not good at to dedicated data processors, and achieves the optimal combination of hardware resources from a global perspective, helping enterprises achieve the optimal energy efficiency ratio. Huawei launches the OceanDisk smart disk enclosure for the diskless server architecture. With four advantages, the OceanDisk smart disk enclosure helps cloud and Internet data centers achieve green, intensive, high-performance, and high-reliability. OceanDisk smart disk enclosures are connected to servers through the NoF high-speed Ethernet network to offload complex data storage capabilities to smart disk enclosures, achieving high performance like local disks. Designed based on the diskless server architecture, the storage and computing separation advantages are fully utilized to implement independent elastic capacity expansion of computing and storage, and data migration is not required during CPU computing power upgrade. The large-scale EC coding algorithm and deduplication and compression capabilities reduce the cabinet space and power consumption by 40%. Multiple reliability technologies, such as disk sub-health management and intelligent optimization of slow disks, ensure system-level reliability. Data disk faults are predictable, visible, and manageable, greatly simplifying O&M.\nSmartQoS is an intelligent service quality control feature developed by Huawei. It dynamically allocates storage system resources to meet the performance requirement of certain applications. As storage technologies develop, storage systems provide larger capacities. An increasing number of users choose to deploy multiple applications on one storage system. Non-critical applications contend for bandwidth and Input/Output Operations Per Second (IOPS) resources with critical applications, compromising the performance of the latter. OceanStor Dorado storage systems provide SmartQoS to help users properly use storage system resources and ensure high performance of mission-critical services. SmartQoS enables you to set a different performance objective for each application according to the application's key performance characteristics (IOPS or bandwidth). Based on the set performance objectives, SmartQoS dynamically allocates storage resources to meet specific service level agreement (SLA) requirements, especially those of critical applications. SmartQoS extends the Information Lifecycle Management (ILM) strategy of a storage system. It is an essential add-on to a storage system, especially when certain applications have demanding SLA requirements. In a storage system shared by two or more applications, use of SmartQoS derives the maximum value from the storage system: SmartQoS controls the performance level for each application, avoiding conflicts between applications and ensuring the performance of critical applications. SmartQoS prioritizes critical applications in storage resource allocation by limiting the resources allocated to non-critical applications. Service Quality Control SmartQoS helps mission-critical services obtain storage resources in a timely manner to realize their performance goals. SLA-based Management of Storage Resources Monitors application performance and achieves performance goals.\nData storage is the process of storing digital information on a computer or other digital device. Data storage is an essential part of any computer system, as it allows users to store and access data quickly and easily. Data storage can be divided into two main categories: primary storage and secondary storage. Primary storage is the main memory of a computer system, and is used to store data that is currently being used or accessed. Secondary storage is used to store data that is not currently being used or accessed and is typically used for long-term storage. Data storage can be further divided into two types: volatile and non-volatile. Volatile storage is temporary storage that is lost when the power is turned off. Non-volatile storage is permanent storage that is not lost when the power is turned off. Volatile memory Non-volatile memory Data storage can also be divided into two types: local and remote. Local storage is data that is stored on the same computer or device as the user. Remote storage is data that is stored on a remote server or device, such as a cloud storage service. Data storage can also be divided into two types: structured and unstructured. Structured data is data that is organized in a specific way, such as a database. Unstructured data is data that is not organized in a specific way, such as a text file. Data storage can also be divided into two types: physical and virtual.\nPhysical storage is data that is stored on physical media, such as a hard drive or a CD-ROM. Virtual storage is data that is stored on a virtual machine, such as a cloud storage service. Data storage can also be divided into two types: active and inactive. Active storage is data that is actively being used or accessed. Inactive storage is data that is not actively being used or accessed. Data storage can also be divided into two types: permanent and temporary. Permanent storage is data that is stored permanently, such as on a hard drive. Temporary storage is data that is stored temporarily, such as in RAM. Data storage can also be divided into two types: sequential and random. Sequential storage is data that is stored in a specific order, such as a list. Random storage is data that is stored in a random order, such as a hash table. Data storage can also be divided into two types: compressed and uncompressed. Compressed storage is data that is stored in a compressed format, such as a ZIP file. Uncompressed storage is data that is stored in an uncompressed format, such as a text file. Data storage can also be divided into two types: encrypted and unencrypted. Encrypted storage is data that is stored in an encrypted format, such as an AES-encrypted file. Unencrypted storage is data that is stored in an unencrypted format, such as a plain text file. Encrypted Unencrypted Data storage can also be divided into two types: cloud and local.\nCloud storage is data that is stored on a remote server, such as a cloud storage service. Local storage is data that is stored on the same computer or device as the user. Data storage can also be divided into two types: distributed and centralized. Distributed storage is data that is stored on multiple computers or devices. Centralized storage is data that is stored on a single computer or device. Data storage can also be divided into two types: object and block. Object storage is data that is stored in an object-oriented format, such as an XML file. Block storage is data that is stored in a block-oriented format, such as a hard drive. Data storage can also be divided into two types: hot and cold. Hot storage is data that is actively being used or accessed. Cold storage is data that is not actively being used or accessed. Data storage can also be divided into two types: primary and secondary. Primary storage is data that is stored on the main memory of a computer system. Secondary storage is data that is stored on a secondary storage device, such as a hard drive or a CD-ROM. Data storage can also be divided into two types: online and offline. Online storage is data that is stored on a remote server, such as a cloud storage service. Offline storage is data that is stored on the same computer or device as the user. Data storage can also be divided into two types: public and private.\nThe Common Internet File System (CIFS), which is also referred to as Server Message Block version 1 (SMB1), is a protocol for network file systems that is predominantly utilized in Windows environments. Although newer versions of SMB have largely replaced it, it is worthwhile to examine its features. CIFS is a protocol that is tailored to Windows-based systems, providing a smooth integration with Windows file systems, security, and permissions. The CIFS protocol facilitates the mapping of remote shared folders as network drives, thereby offering users a recognizable interface to access files. CIFS provides support for file locking, which enables exclusive access to files and prevents conflicts when multiple users are concurrently modifying the same file. The Server Message Block (SMB) is a protocol for sharing files over a network, which was developed by Microsoft. The Server Message Block (SMB) protocol has undergone evolution over time, with subsequent versions such as SMB2 and SMB3 focusing on improving security and performance. The salient characteristics of SMB encompass: SMB is a file sharing protocol that is compatible with multiple operating systems, including Windows, Linux, and macOS. This cross-platform support enables users to share files seamlessly across different operating systems. SMB3 has incorporated advanced capabilities such as SMB Direct and SMB Multichannel to harness the potential of high-speed networks and multiple network connections, thereby enhancing data transfer speeds. SMB3 provides various security features, such as encryption in transit and end-to-end encryption, which guarantee the confidentiality and integrity of data.\nHi, all, Please allow me to introduce the information that the IT manager can view and the operations that the role can perform on the DME IQ home page. In the Service Requests area, you can view the number of SRs of each type in the current system. SRs can be classified into Processing, Processed, and Closed based on the SR processing progress, and can be classified into P1, P2, P3, and P4 based on the SR severity. Processed means that the alarm is resolved but now there are any other tasks in background on our side such as survey feedback etc. and they will be marked as closed afterwards. Closed means that the ticket is resolved and no action needed. You can consult this guide: 1. Log in to DME IQ using your Uniportal account and select the IT manager role. The DME IQ home page is displayed. 2. View information on the home page. Storage System Health On the left of the Storage System Health area, you can view the number of devices in different health status. The health status can be Good , Fair , or Poor . On the right, you can view the average scores of devices in five aspects: system, hardware, configuration, capacity, and performance. Capacity In the Capacity area, you can view the total capacity and current capacity usage of devices connected to DME IQ. You can also view the capacity usage trend, data reduction ratio, and saved capacity in the latest year.\nCI/CD was established to overcome the issues and inefficiencies in traditional software development and deployment procedures. In the past, development processes were often long, testing and deployment were done by hand, and there wasn't much communication between the development and operations teams. These practices led to a number of problems, such as a longer time to market, a higher chance of mistakes, and trouble keeping software apps running and making them bigger. CI/CD was made with these main goals in mind: 1. Speeding up the delivery of software: The goal of CI/CD is to automate the software creation and deployment processes, so that software can be released faster and more often. By automating things like building, testing, and deploying software, development teams can bring new features, bug fixes, and improvements to end users much faster. CI/CD encourages practices like continuous integration, automated tests, and code reviews, which help make sure that the code is good and reliable. These techniques help find problems early in the development process, improve the quality of the code, and reduce the chance that bugs and regressions will be introduced into the software. By finding problems early, writers can fix them quickly, making the software more reliable and stable. 3. Making teamwork and communication easier: CI/CD makes it easier for the development, testing, and operations teams to work together and talk to each other. CI/CD helps break down silos and make it easier for people to work together by giving them a shared platform and automating processes.\nIntroduction: Cybersecurity threats are a serious threat to businesses in various industries in today's quickly changing digital environment. Traditional security measures are no longer sufficient to keep up with the complex strategies used by cybercriminals. As a result, danger hunting has become an essential proactive defense tactic. Threat hunting is the process of actively looking for and spotting potential security risks or vulnerabilities within a network infrastructure of an organization before they can be exploited. The Importance of Threat Hunting: Traditional cybersecurity strategies, which mainly concentrate on reactive defense and incident response, go beyond threat hunting. Organizations may keep one step ahead of cyber attackers by actively looking for dangers, and proactively identifying and neutralizing possible attacks before they do serious harm. Threat hunting helps find hidden or latent hazards that may have eluded conventional security measures and provide insightful information about the methods and objectives of the attackers. The Process of Threat Hunting: Threat hunting is a methodical, proactive technique for identifying and reducing online dangers. Understanding the organization's infrastructure, network traffic patterns, and potential vulnerabilities is the first step. Tools for threat intelligence and advanced security analytics are used to spot unusual behavior, signs of compromise, or potential security holes. The data is then analyzed by highly qualified security specialists who look for indications of malicious behavior and carry out in-depth investigations to find potential dangers. Benefits of Threat Hunting Several advantages are available to organizations looking to strengthen their cybersecurity posture through threat hunting.\nIntroduction: Modern software development teams can now provide high-quality software fast and effectively thanks to Continuous Integration and Continuous Deployment (CI/CD) pipelines. It takes careful preparation and attention to best practices to implement a reliable CI/CD pipeline. In this post, we'll examine the crucial factors, such as code quality, testing approaches, automation tools, and delivery speed optimization, that must be taken into account while setting up a successful CI/CD pipeline. Code Quality: A dependable CI/CD pipeline depends on maintaining code quality. To spot possible problems early on, enforce coding standards, conduct code reviews, and use static code analysis tools. Automated code quality checks should be used to guarantee consistency and best practices compliance. Testing Techniques: A successful CI/CD pipeline must include thorough testing as a core component. Include end-to-end tests, unit tests, and integration tests in your pipeline. For all crucial functionalities to be validated, test coverage should be substantial. Give test automation top priority to hasten the testing process and facilitate quick response on code changes. Tools for automation: CI/CD pipelines rely heavily on automation. For effective code management, use version control tools like Git. To automate the creation and packaging of software artifacts, use systems like Jenkins, Travis CI, or GitLab CI/CD. Platforms for containerization like Docker can provide consistent deployment across many environments. Optimization of Delivery Speed: Improving delivery speed is essential for reducing time to market. Applications should be divided into more manageable, independently deployable components to enable parallel development and deployment.\nWhat are the specific requirements for the primary and secondary storage systems in Huawei Storage Dorado HyperReplication? Network Planning The replication network configuration of HyperReplication is the same as that of HyperMetro. For details, see section \"Configuring Fibre Channel Switches (Applicable to Fibre Channel Connections)\" in the HyperMetroFeature Guide for File of the corresponding device version to learn about the domain ID planning and planning. Capacity planning Plan the same available capacity that can be actually used for both primary and secondary file systems. If the available capacity of the secondary file system is smaller than that of the primary file system, remote replication will be interrupted due to insufficient capacity of the secondary file system during data synchronization. If the available capacity of the secondary file system is larger than that of the primary file system, the capacity utilization of the secondary file system is low. In the following two scenarios, however, the available capacity that can be actually used for the primary file system and the available capacity for the secondary file system are different: A file system is being created. The reserved snapshot space and the deduplication or compression feature are configured for primary and secondary file systems. Network Requirements for HyperReplication: For asynchronous remote replication, the write latency of applications is irrelevant to the distance between the primary and secondary sites. Therefore, asynchronous remote replication is applicable to DR scenarios where the primary and secondary sites are far away from each other, or the network bandwidth is limited.\nCan you explain the process of configuring host connectivity and zoning for Fibre Channel or iSCSI connections to a Dorado storage system? Configuring the Host Connectivity This section describes how to configure the connectivity between a host and a storage system to ensure that the host can properly use the storage resources allocated by the storage system. Context OceanStor Dorado V6 uses logical ports to establish iSCSI connections with hosts. The home ports of logical ports can be Ethernet ports, bond ports, or VLANs. For details about how to configure bond ports, VLANs, and logical ports, see . If the storage system communicates with the host using logical ports that reside on Ethernet ports, configure the storage service ports and other connectivity configurations by referring to \"Establishing iSCSI Connections\" in the . represents a specific operating system, for example, Windows. If the storage system communicates with the host using logical ports that reside on bond ports or VLANs, configure the storage service ports by following instructions in this section and then configure the connectivity by referring to \"Establishing iSCSI Connections\" in the . If the storage system is connected to a host over Fibre Channel connections, configure the host and storage system by referring to the . represents a specific operating system, for example, Windows. Configuration Method Verify that storage resources required by services have been successfully created on the storage system. The storage resources include storage pools, LUNs, hosts, and mappings between hosts/host groups and LUNs/LUN groups.\nConfigure the connectivity by following instructions in the \"Configuring Connectivity\" in . represents a specific operating system, for example, Windows. FC Connectivity A Fibre Channel (FC) SAN is a specialized high-speed network that connects host servers to storage systems. The FC SAN components include HBAs in the host servers, switches that help route storage traffic, cables, storage processors (SPs), and storage disk arrays. To transfer traffic from host servers to shared storage, the FC SAN uses the Fibre Channel protocol to package SCSI commands into Fibre Channel frames. Ports in FC SAN Each node in the SAN, such as a host, a storage device, or a fabric component has one or more ports that connect it to the SAN. Ports are identified in a number of ways, such as by: World Wide Port Name (WWPN) A globally unique identifier for a port that allows certain applications to access the port. The FC switches discover the WWPN of a device or host and assign a port address to the device. Port_ID (or port address) Within a SAN, each port has a unique port ID that serves as the FC address for the port. This unique ID enables routing of data through the SAN to that port. The FC switches assign the port ID when the device logs in to the fabric. The port ID is valid only when the device is logged on. Zoning Zoning provides access control in the SAN topology. Zoning defines which HBAs can connect to which targets.\nHow can I configure RAID levels on a Huawei Dorado storage system to optimize performance and data protection? Configuring RAID levels on a Huawei Dorado storage system involves selecting the appropriate RAID level that balances performance, data protection, and capacity utilization based on your specific requirements. Here are the steps to configure RAID levels: Understand RAID Levels: Familiarize yourself with different RAID levels and their characteristics. Common RAID levels include RAID 0, RAID 1, RAID 5, RAID 6, and RAID 10. Each RAID level offers a different combination of performance, data protection, and capacity utilization. Assess Performance Needs: Determine the performance requirements of your applications and workloads. Consider factors such as I/O operations per second (IOPS), throughput, and latency. Some RAID levels provide better performance than others, but they may come with reduced data protection or capacity utilization. Evaluate Data Protection Requirements: Consider the importance of data protection in your storage environment. RAID levels such as RAID 1, RAID 5, RAID 6, and RAID 10 offer varying degrees of data redundancy and protection against drive failures. Evaluate the tolerance for data loss and the impact of downtime on your business to determine the appropriate RAID level for data protection. Consider Capacity Utilization: RAID levels have different impacts on storage capacity utilization. Some RAID levels, like RAID 0, offer maximum capacity utilization but provide no data redundancy, while others, like RAID 1 or RAID 10, have lower capacity utilization due to mirroring or striping with redundancy.\n[Value range] The description can be left blank or contain up to 255 characters. Storage Pool Storage pool to which the LUN belongs. NOTE: You can click Create to create a storage pool. Capacity Capacity of the LUN. This is the maximum capacity that will be allocated to a thin LUN. The total storage resources dynamically allocated to a thin LUN must not exceed the value of this parameter. NOTE: The maximum capacity of the LUN must not exceed the system specifications. For details about the specifications, visit . You can set the capacity unit to Blocks to create LUNs by block. A block is equal to 512 bytes. The LUN capacity must not be smaller than 1024 blocks (that is, 512 KB). Storage system capacity equation: 1 PB = 1,024 TB, 1 TB = 1,024 GB, 1 GB = 1,024 MB, 1 MB = 1,024 KB, 1 KB = 1,024 bytes. Quantity Number of LUNs created in a batch. Set this parameter based on site requirements. [Value range] 1 to 500 NOTE: LUNs created in a batch have the same capacity. Start Number Start number from which the system incrementally adds a suffix number to the name of each LUN for distinction. [Value range] 0 to (10000 Number of LUNs created in a batch) NOTE: This parameter is displayed only when Quantity is greater than 1 and Advanced is selected. For example, if you want to create 300 LUNs, the value range of the start number is 0 to 9700.\nHow do I configure storage tiering and data migration policies to optimize data placement and performance in a Dorado storage environment? To configure storage tiering and data migration policies to optimize data placement and performance in a Dorado storage environment, you can follow these general steps: Understand the Storage Tiers: Dorado storage environments typically offer multiple tiers of storage, such as high-performance solid-state drives (SSDs) and lower-cost, higher-capacity hard disk drives (HDDs). Each tier has different performance characteristics and costs associated with it. Determine the available tiers and their capabilities in your Dorado storage system. Identify Data Access Patterns: Analyze the data access patterns in your environment to determine which data requires high performance and which can tolerate lower performance. Frequently accessed or critical data may benefit from being placed on high-performance tiers, while less frequently accessed or colder data can be moved to lower-cost tiers. Define Tiering and Migration Policies: Based on the analysis of your data access patterns, define tiering and migration policies that determine how data is placed on different storage tiers. These policies typically involve setting rules or criteria for moving data between tiers based on factors such as access frequency, data age, or other metadata. Enable Automated Tiering: Configure the Dorado storage system to enable automated tiering based on the policies you defined. This may involve setting up tiering policies at the volume or LUN (Logical Unit Number) level, or using storage management software provided by the vendor.\nWhat would happen if a remote replication enters the interrupted state if data replication from the primary File system to the secondary file system is interrupted by a fault such as a link failure? If a remote replication enters the interrupted state, it means that the data replication from the primary file system to the secondary file system has been interrupted by a fault, such as a link failure. This can happen for a number of reasons, including: The link between the primary and secondary file systems is down. The primary or secondary file system is not accessible. The HyperReplication service is not running on one or both of the file systems. When a remote replication enters the interrupted state, the following things happen: The replication of new data is stopped. The replication of existing data is paused. The status of the remote replication is changed to \"Interrupted\". If the fault that caused the interruption is resolved, the remote replication will automatically resume. However, if the fault is not resolved, the remote replication will remain in the interrupted state. There are a few things that you can do to troubleshoot a remote replication that is in the interrupted state: Check the status of the link between the primary and secondary file systems. Check the accessibility of the primary and secondary file systems. Check the status of the HyperReplication service on the primary and secondary file systems. If you are still unable to resolve the issue, you can contact Huawei support for assistance.\nHyperCDP is a continuous data protection feature developed by Huawei. A HyperCDP object is similar to a common writable snapshot, which is a point-in-time consistent copy of original data to which the user can roll back, if and when it is needed. It contains a static image of the source data at the data copy time point. HyperCDP provides data protection at an interval of seconds, with zero impact on performance and small space occupation. Support for scheduled tasks You can specify HyperCDP schedules by day, week, month, or specific interval, meeting different backup requirements. Intensive and persistent data protection HyperCDP provides higher specifications than common writable snapshots. It achieves continuous data protection by generating denser recovery points with a shorter protection interval and longer protection period. Benefit Description Efficient use of storage space, protecting user investments A HyperCDP object and its source data share the storage space. You do not need to plan separate storage space for HyperCDP objects. HyperCDP duplicates for various applications Duplicates of HyperCDP objects can be created for data analysis and testing. Continuous data protection HyperCDP protects data at an interval of several seconds. In the event of data corruption, any recovery point can be used to recover the data, minimizing data loss and ensuring the security of service systems. Snapshots provided by HyperCDP are read-only. HyperCDP is used in the same scenarios as HyperSnap. However, HyperCDP offers better specifications and granularity than HyperSnap and is used more often.\nDear All, Today we are going to learn about IOPS and Throughput IOPS stands for Input/Output Operations Per Second. It is a performance metric used to measure the speed and efficiency of data storage systems, such as hard disk drives (HDDs), solid-state drives (SSDs), and storage area networks (SANs), in terms of the number of read and write operations they can perform in a given time period. IOPS Performance Characteristics: Random vs Sequential IOPS: Random IOPS measure the performance of storage systems in handling random read and write operations, where data is accessed in a non-sequential manner. Sequential IOPS, on the other hand, measure the performance of storage systems in handling sequential read and write operations, where data is accessed in a sequential manner. Read IOPS vs Write IOPS: Read IOPS measure the performance of storage systems in handling read operations, where data is retrieved from storage, while write IOPS measure the performance of storage systems in handling write operations, where data is written to storage. Latency: Latency is the time delay between a storage system receiving a request for data and the data being accessed or written. Lower latency indicates faster storage performance, as it means data can be accessed or written more quickly. How to Measure IOPS: IOPS can be measured using various benchmarking tools and software, such as Iometer, FIO, and CrystalDiskMark. These tools simulate different types of workloads and measure the IOPS performance of storage systems under those workloads. IOPS vs Throughput: IOPS and throughput are related but distinct performance metrics.\nHello, everyone! This post will share with you the HyperReplication and the HyperReplicationworking principles. As a core technology for DR and backup, HyperReplication can realize remote data backup and disaster recovery. Function Purpose Benefit When the HyperReplication feature is used, two data centers work in active/standby mode. The primary site is in the service running status, and the DR center is in the non-service running status. For active/standby DR, when a device in data center A is faulty or even the entire data center A is faulty, services are automatically switched to data center B. For backup, data center B backs up only data in data center A and does not carry services when data center A is faulty. Question: What are HyperReplication pairs, consistency groups, synchronization, splitting, primary/secondary switchover, data status, and writable secondary LUNs? To implement remote backup and recovery of service data, HyperReplication involves the following phases: creating a HyperReplication relationship, data synchronization, service switchover, and data recovery. What do these phrases mean? For more details, see . By viewing the running status of a pair, you can perform synchronization, splitting, and primary/secondary switchover operations on HyperReplication in time. After performing an operation, you can view the running status of the pair to check whether the operation is successful. Running Status Description When the primary LUN is synchronizing data to the secondary LUN, the secondary LUN cannot be read or written. If a disaster occurs, data on the secondary LUN cannot be used for service recovery.\nWhen the secondary LUN is in a complete status, data on the secondary LUN can be used for service recovery. Based on the known asynchronous remote replication, try to draw a schematic diagram of synchronous remote replication HyperReplication Data Recovery Analysis Item Central DR and Backup Geo-redundancy Backup data is managed centrally so that data analysis and data mining can be performed without affecting services. When a disaster occurs at any service site, the central DR and backup site can quickly take over its services and recover data, achieving unified service data management. Hyper Replication mode can be selected for a service site flexibly based on the distance between the service site and the central DR and backup site Three data centers are deployed in two cities to perform real-time backup and remote backup concurrently. Service data is backed up to an intra-city DR center in real time through a high-speed link. After data in the primary site is invalid, services are quickly switched to the intra-city DR center. If a disaster damages the primary site and the DR center in the same city, an inter-city DR center takes over services and implements DR. HyperReplication mode Intra-city: asynchronous remote replication Inter-city: asynchronous remote replication Central DR and backup refer to backing up service data from different places to the same site for centralized management. Service data at multiple service sites is centrally backed up to and managed at the central DR and backup site.\nVMwares vision and strategy are to drive transformation through the hypervisor, bringing to storage the same operational efficiency that server virtualization brought to compute. As the abstraction between applications and available resources, the hypervisor can balance all IT resourcescompute, memory, storage, and networkingneeded by an application. With server virtualization as the de facto platform to run enterprise applications, VMware is uniquely positioned to deliver Software-Defined Storage (SDS) by leveraging the pervasiveness of this software tier. FIGURE 1: Software-Defined Storage vVols simplifies operations through policy-driven automation that enables more agile storage consumption for VMs and dynamic adjustments in real-time when needed. It simplifies the delivery of storage service levels to individual applications by providing granular control of hardware resources and native array-based data services that may be instantiated at the VM level. Simplifies Storage Operations Simplifies Delivery of Service Levels Improves Resource Utilization Integration and management framework that virtualizes SAN/NAS arrays VMware, vVols is a SAN/NAS management and integration framework that exposes virtual disks as native storage objects and enables array-based operations at the virtual disk level. vVols transform the data plane of SAN/NAS devices by aligning storage consumptions and operations with the VM. In other words, vVols make SAN/NAS devices VM-aware and unlocks the ability to leverage array-based data services with a VM-centric approach at the granularity of a single virtual disk.\nvVols allows customers to leverage the unique capabilities of their current storage investments and transition without disruption to a simpler and more efficient operational model optimized for virtual environments that work across all storage types. FIGURE 2: vVols partner ecosystem vVols partner ecosystem Key Elements FLEXIBLE CONSUMPTION AT THE LOGICAL LEVEL vVols virtualize SAN and NAS devices by abstracting physical hardware resources into logical pools of capacity (called Virtual Datastore) that can be flexibly consumed and configured to span a portion of or several storage arrays. The Virtual Datastore defines capacity boundaries, and access logic, and exposes a set of data services accessible to VMs provisioned in the pool. Virtual Datastores are purely logical constructs that may be configured on the fly, when needed, without disruption and do not require file system formatting. PRECISE CONTROL AT THE VM LEVEL vVols define a new virtual disk container (vVol) that is independent of the underlying physical storage representation (LUN, file system, object, etc.). In other words, with vVols, the virtual disk becomes the primary data management unit at the array level. This turns the Virtual Datastore into a VM-centric pool of capacity. It becomes possible to execute storage operations with VM granularity and to provision native array-based data services to individual VMs. This allows administrators to provide the right storage service levels to each VM. Integration and management framework that virtualizes SAN/NAS arrays VMware, SPBM automates VM placement by identifying available data stores that meet policy requirements and coupled with vVols; it dynamically instantiates necessary data services.\nThrough policy enforcement, SPBM also automates service-level monitoring and compliance throughout the lifecycle of the VM. FIGURE 3: vVols architecture SIMPLIFIED STORAGE OPERATIONS For both the VI Admin and Storage Admin, vVols greatly simplifies management over the existing operational model. vVols allow the separation of provisioning and consumption of storage for VMs. In the VMware HCI model with vVols, the Storage Admin sets up an entity called the Virtual Datastore. The capacity and data services published by the Storage Admin in the Virtual Datastore are similar to menu items that the VI Admin can consume on demand. The Storage Admin retains control of the storage resources, as the VI Admin can only consume published capabilities. However, the Storage Admin no longer needs to determine which data services should be assigned to an application. Thus, the Storage Admin is responsible for up-front setup, allowing the VI Admin to be self-sufficient afterwards. With vVols, the VI Admin gains control and becomes responsible for defining the various storage classes of service for applications. However, the classes of service are no longer physical pre-allocations, but an Integration and management framework that virtualizes SAN/NAS arrays VMware, logical entities controlled and automated entirely by software and interpreted through the mechanism of policies. By associating one or many VMs to the right policy, the provisioning and instantiation of storage service levels are automated for that VM or set of VMs.\nHello Guys, Today we will learn about Modernizing Enterprise Storage for Machine Learning Analytics Workloads INTRODUCTION As analytics workloads have taken hold in IT organizations as potential game-changing applications, storage requirements have changed dramatically. New kinds of solutions are necessary to meet the performance, agility and economics challenges associated with machine learning analytics workloads, and selecting the right storage infrastructure is critical. The explosion of data represents both a blessing and a curse for organizations. On the plus side, it offers up a world of opportunity to parse the data for hidden treasuresdata insights that enable businesses and industries to solve their greatest challenges like manufacturing yields, cancer diagnosis and treatments, financial trading trends, and pharma drug discovery. But with that dramatic data growth comes a big challenge: ensuring that data-rich workloads can be properly supported and accelerated with next-generation IT infrastructureespecially storage. In particular, the opportunities presented by artificial intelligence for powerful data insights are causing IT professionals to think long and hard about choosing the correct storage infrastructure. High-performance machine learning (ML) workloads require substantially greater bandwidth, performance, and storage capacity in order to process enormous amounts of data in real-time so it can be transformed into actionable insights and accelerate time to value. Choosing the wrong storage infrastructure would be costly, both in financial terms and in terms of not meeting critical business objectives.\nImagine how C-suite executives and other business stakeholders will react when they learn that the increased Capex funding they allocated to new computing and storage infrastructure for those workloads will not meet the needs of the organization. For years, network-attached storage (NAS) was an attractive and effective way to deal with storage demands for enterprise workloads. NAS has played a valuable role in helping organizations share and access data across multiple users and diverse client devices from a central disk repository. The proliferation of local-area networks made NAS appliances a cost-effective option for sharing storage across standard Ethernet connectivity, resulting in easy access for relatively low capital expense. However, in the modern, high-performance computing era, enterprise application performance requirements have evolved dramatically. In particular, the pivot toward new storage architectures has been accelerated by the dramatic impact of artificial intelligence (AI), inclusive of deep learning and machine learning, workloads that demand not only much higher storage capacity but also much higher performance to the compute nodes housing the applications. This class of performance can only be delivered with a highly parallel and scalable file system. Adding to these new workload demands are such issues as shortening backup windows, more demanding recovery time/ recovery point objectives (RTOs/RPOs), the need to share data across applications and workloads, the proliferation of cold data and the demand for instantaneous access for analytics and other business-critical decision-making. This paper examines a modernized approach for enterprise storage that delivers enhanced data centre agility, accelerates data transformation and optimizes data centre investments.\nWHATS CAUSING THE PROBLEM Todays modern, analytics-driven workloads offer exciting business potentialbut they represent an operational hurdle for IT organizations. These workloads are marked by massive data sets that require far more than higher-capacity storage or cost-efficient scaling; todays workloads are sharing critical data and need the kind of performance delivered by parallel scale-out storage systems. Adding to the problem, massively parallel GPU-based servers have concentrated the I/O demands into one single machine, placing even more demands on the storage infrastructure. Figure 1 contrasts CPU-based architectures with new GPU-based infrastructure. A new approach is necessary because of the data I/O bottleneck that has resulted in a performance tax that is inhibiting organizations ability to surface data insights in a timely manner without having to invest more Capex and deal with greater management complexity. Because AI-driven workloads are a must for all enterprises, investing in the right storage infrastructure is a big decision. But that decision must be made with the critical knowledge that 80% of the time that data scientists devote to analyzing data is spent transforming and preparing the data for use in analytics workloads. Traditional storage architectures, while still doing yeomans work for a variety of enterprise applications, were never envisioned for the kinds of algorithmic-based analytics workloads that no longer can be done by a group of data scientists manually tuning those storage systems. Instead, IT decision-makers need to look for a shared-file system that delivers all the I/O required to avoid waiting for data availability.\nTHE 3 PILLARS OF STORAGE SOLUTIONS FOR ML-DRIVEN ANALYTICS WORKLOADS There are three pillars that highlight the plight of storage administrators looking to ensure their storage infrastructure can keep up with their ML-driven workloads: Drive Data Center Agility While the term business critical has been widely used to describe the increasingly valuable array of data being created, stored and shared in enterprise computing environments, the term has taken on real significance with the introduction of analytics workloads, powered by such technologies as artificial intelligence, machine learning and deep learning. The resultant explosion of data has created a big strain on storage systems, many of which lack the bandwidth and management functionality necessary to keep business-critical data available and flowing among key workloads. Traditional data centres with legacy storage systems have been hamstrung by isolated storage silos that have sprung up over the years in response to mounting demands. In these data centres, data can no longer be ingested or shared efficiently, nor can it be adequately protected in the event of security threats, user error or configuration problems. These data centres no longer have the requisite agility to manage all this stored data, such as knowing the location or ownership of the data, adhering to governance and compliance requirements, and controlling access to the data according to clear, yet flexible, policies and rights management. All organizations need to juggle both on-premises and cloud-based environments to promote a nimbler approach to delivering IT services.\nClearly, a new approach is requiredone that eliminates data silos and reduces complexity, supports multiple deployment models, and performs as a multi-protocol infrastructure for mixed workloads increasingly prevalent in the data centre. Accelerate Data Transformation for ML Analytics Artificial intelligencespecifically, machine learning and deep learninghas transformed the way enterprises use data for sophisticated analytics. But whichever AI avenue you go down, enterprise storage must evolve to keep up. Specifically, new analytics workloads require: Massive amounts of data Faster, parallel, and more efficient access to that data Algorithms for training and facilitating the learning process Traditional NAS systems are unable to take advantage of higher-speed networking and lack the ability to handle the I/O demands of AI-driven data for analytics. Instead, these and other analytics workloads require a parallel file system with substantially higher performance than legacy storage systems, support for multiple types of data (structured, unstructured and semi-structured), and support for hyper-scale performance that streamlines the requisite data preparation phase in AI. Figure 2 outlines the architectural difference between NAS and Parallel file systems. Accelerating data transformation for ML workloads not only means substantially improved performance, but also far better cost efficiency, simplified management, massive scalability, and a more agile, sustainable approach to future data transformation. A new class of parallel file storage systems can speed up local file system performance, reduce data copies and support cloud bursting for load-levelling under demand peaks.\nOptimize Infrastructure Investment Finally, since cost efficiency is a critical mantra for IT and storage administrators in a data-rich environment, organizations have put significantly greater emphasis on increasing bang for the buck when it comes to storage infrastructure investments. New storage systems must optimize CPU and GPU utilization without requiring organizations to make massive new investments in Capex, while also leveraging object storage to improve the economics of storing, protecting and backing up masses of data. Not to be overlooked is the need for a new storage management approach that reduces management costs through integrated tiering, remote backup and recovery, and encryption both at rest and in flight. Making the wrong choice for optimized storage in ML-driven workloads can be a costly mistake, not only resulting in higher Capex and Opex costs but also in delaying the time of critical data and insights getting to the right person in the right place at the right time. And poor storage management controls can result in costly misconfigurations that further limit data availability and result in performance bottlenecks. Avoid the temptation to turn an array of legacy systems into a solution for next-generation, ML workloads. Build-it-yourself scale-out file systems built around open-source software is a time-consuming, people-dependent exercise that can easily turn into a science project. SOLUTION has made its mark with a new approach to enterprise storage solutions that works equally well in on-premises environments, in cloud deployments or in hybrid scenarios. The core productWMatrixis a software-based, scale-out storage solution optimized for ML-driven analytics and performance-intensive workloads.\nMatrix offers the simplicity of NAS, but it delivers all-flash-level performance, cloud scalability and simplified management never envisioned by NAS product vendors. In production environments, Matrix has been shown to deliver 10 times the performance of a traditional NAS system with linear scaling as the infrastructure grows. The Matrixes file system is a distributed, scalable, high-performance software platform that connects multiple servers with locally attached solid-state drives into a POSIX-compliant global namespace for web-scale performance and simplified management. The software is deployed on standard commercially available servers providing true hardware independence and the best cost. The software supports internal tiering to any commercially available S3 object storage solution delivering massive scalability and great economics for an ever-growing data catalogue. Figure 3 provides an architectural overview of a typical deployment for deep learning environments. This Matrix offers enterprise storage and IT decision-makers important deployment flexibility. For instance, Matrix can be implemented in converged infrastructure, as a dedicated storage server or as a public cloud storage resource. It is designed as an integrated, easily configured and quickly deployed storage resource that flexibly adapts to changes in enterprise IT requirements, including support for workload migration to and from multiple cloud environments. CONCLUSION Todays ML workloads demand a new class of storage that delivers the performance, manageability, scalability, and cost efficiency demanded in the era of digital transformation. The parallel file storage system was designed and optimized for modern workloads and is ideally engineered to take storage performance and data availability to the next level as performance demands further intensify.\nGreetings, everyone! In this article, Ill explain about CIFS vs. SMB vs. NFS. Network file Sharing Protocols: Network file sharing protocols are used to enable the sharing and access of files and resources over a network. Here are some commonly used network file sharing protocols: Server Message Block (SMB): SMB is a network file sharing protocol used primarily in Windows environments. Common Internet File System (CIFS): CIFS is an older version of SMB that provides compatibility with older Windows systems. Network File System (NFS): NFS is a network file sharing protocol commonly used in UNIX/Linux environments. It allows files to be shared and accessed across different operating systems. Apple Filing Protocol (AFP): AFP is a network file sharing protocol used for sharing files between macOS devices. It provides features specific to macOS, such as support for metadata and resource forks. File Transfer Protocol (FTP): File Transfer Protocol (FTP) is a protocol used for transferring files over a network. It allows users to upload and download files to/from a remote server. Secure Shell File Transfer Protocol (SFTP): Secure File Transfer Protocol (SFTP) is a that uses SSH for encryption. It provides a secure way to transfer files between systems over a network. Web-based protocols: HTTP (HyperText Transfer Protocol) and HTTPS (HTTP Secure) are protocols used for web-based file access. They allow files to be accessed and downloaded using a web browser. Distributed File System (DFS): DFS is a file sharing protocol that allows multiple file servers to be logically grouped into a single directory structure.\nIt provides a unified view of files and simplifies file access across different servers. These protocols provide different features, security levels, and compatibility with various operating systems. The choice of protocol depends on the network environment, client devices, and specific requirements of the file sharing scenario. CIFS, SMB and NFS) Introduction to CIFS: Common Internet File System (CIFS) is a network file sharing protocol that enables remote clients to access and share files and resources over a network. It was progressed by Microsoft and is normally used in Windows environments. CIFS is an enhanced version of the Server Message Block (SMB) protocol, which is the predecessor of CIFS. CIFS extends the functionality of SMB and provides additional features and improvements for file sharing and remote access. How Does CIFS Work? Common Internet File System (CIFS) is a network file sharing protocol that allows clients to access and interact with shared files and resources on remote servers. It is an enhanced version of the Server Message Block (SMB) protocol and is primarily used in Windows environments. Here is a simplified overview of how CIFS works: Client Request: A client (such as a computer or device) sends a request to access a shared file or resource on a remote server. The request includes the necessary information, such as the name or path of the file, the requested operation (read, write, delete, etc. ), and the client's authentication credentials. Connection Establishment: The client initiates a connection with the server hosting the shared resource.\nThe connection can be established over a local network or the internet using TCP/IP as the underlying transport protocol. Authentication: The client provides its authentication credentials to the server to validate its identity and authorize access to the shared resource. Authentication can be based on various protocols, such as NTLM (NT LAN Manager) or Kerberos. File Operation: Once the client is authenticated, it can send specific commands or requests to perform file operations. These operations can include reading or writing data, creating or deleting files, modifying permissions, or executing other actions on the shared resource. Server Response: The server receives the client's request, processes it, and generates a response. The response includes the requested data or information about the status of the operation (success, failure, error codes, etc.). The server may also perform additional checks, such as access control, file locking, or conflict resolution. Data Transfer: If the client requested to read or write data, the server and client engage in data transfer over the network. The data is divided into packets and transmitted between the client and server using the CIFS protocol. The server ensures data integrity, sequencing, and error checking during the transfer. Session Management: CIFS supports the concept of sessions, which allow clients to maintain an ongoing connection with the server for multiple file operations. Sessions help to optimize performance by reducing the overhead of repeated authentication and connection establishment.\nConnection Termination: When the client no longer needs to access the shared resource or wants to terminate the connection, it sends a request to the server to close the connection. The server acknowledges the request and releases any resources associated with the client's session. CIFS provides a robust and standardized way for clients to access shared files and resources on remote servers. It supports features like file locking, access control, and authentication to ensure secure and efficient file sharing across a network. CIFS Functionality: CIFS (Common Internet File System) provides a range of functionality to facilitate file sharing and resource access in a networked environment. Here are some key functionalities of CIFS: File Sharing: CIFS allows multiple clients to access shared files and directories stored on remote servers. It enables users to collaborate and share documents, media files, and other data across the network. Authentication and Authorization: CIFS supports authentication mechanisms to verify the identity of clients and authorize their access to shared resources. It integrates with various authentication protocols like NTLM (NT LAN Manager) and Kerberos to ensure secure access control. File and Directory Operations: CIFS enables clients to perform a variety of file and directory operations, including reading, writing, modifying, deleting, and renaming files. Clients can navigate directory structures, retrieve file attributes, and perform file-related tasks using CIFS commands and protocols. Access Control: CIFS provides mechanisms for controlling access to shared resources. It supports permissions and access rights management, allowing administrators to define who can read, write, or execute files and directories.\nAccess control lists (ACLs) can be used to grant or restrict access at a more granular level. File Locking: CIFS supports file locking mechanisms to prevent multiple clients from simultaneously modifying the same file. It ensures data consistency and avoids conflicts by allowing exclusive or shared locks on files, ensuring that only one client has write access at a time. Name Resolution: CIFS allows clients to discover and access shared resources using human-readable names, such as server names and share names. It provides name resolution services, including NetBIOS name resolution and DNS (Domain Name System) integration, to translate names into network addresses. Printing Services: CIFS includes support for printing services, allowing clients to send print jobs to network printers shared by CIFS-enabled servers. Clients can submit print jobs, monitor their status, and manage printer configurations using CIFS commands. Error Reporting and Recovery: CIFS incorporates error reporting mechanisms to communicate status and error conditions between clients and servers. It provides error codes and messages to help diagnose and resolve issues during file sharing operations. Additionally, CIFS supports recovery mechanisms to handle network interruptions and resume interrupted file transfers. Interoperability: CIFS is designed to work in heterogeneous network environments, enabling file sharing and resource access between Windows-based systems and other platforms. It promotes interoperability by adhering to industry standards and protocols. CIFS is widely used in Windows-based networks for its robust functionality and compatibility with various operating systems and network architectures. It plays a vital role in enabling efficient and secure file sharing and collaboration across organizations.\nSMB (Server Message Block): Server Message Block (SMB) is a network file sharing protocol that allows remote clients to access shared files, printers, and other resources over a network. Originally developed by IBM in the 1980s, SMB has become a widely adopted protocol and is the foundation of file sharing in Windows environments. SMB operates at the application layer of the TCP/IP networking model and provides a set of commands and procedures for clients and servers to communicate with each other. It enables seamless sharing and collaboration between devices on a network, allowing users to access files and resources as if they were local. How Does SMB Work? SMB is a network protocol used for file sharing, printer sharing, and communication between computers in a network. It allows devices, such as computers, servers, and storage devices, to share files, resources, and services with each other. Here's a general overview of how SMB works: Connection establishment: When a client (a computer or device) wants to access a shared resource from a server, it initiates an SMB connection request. The user/client sends a request to the host/server to establish a connection. Negotiation: During the negotiation phase, the client and server exchange information about the supported versions of SMB, security settings, and other parameters. This step ensures that both the client and server agree on the protocol version, authentication methods, and other relevant details. Authentication: Once the negotiation is complete, the client needs to authenticate itself to the server to access shared resources.\nThis authentication can involve username and password credentials or other authentication methods like Kerberos. File and resource access: After successful authentication, the client can send commands to the server to access files, directories, printers, and other resources. The client can request to read, write, delete, or modify files, and the server will respond accordingly. File transfer and sharing: SMB provides mechanisms for transferring files and sharing resources. It supports features like file locking, which ensures that multiple clients can access a file simultaneously without conflicts. It also enables directory browsing, file attribute retrieval, and other operations related to file management. Session termination: Once the client has finished accessing the shared resources, it can terminate the SMB session. This involves sending a termination request to the server, which closes the connection and releases any associated resources. It's important to note that there have been multiple versions of SMB over the years, with SMBv1 being the earliest and SMBv3 being the most recent major release. Newer versions have introduced improvements in terms of performance, security, and additional features. SMB is widely used in Windows environments, but it is also supported by other operating systems, including macOS and Linux, through implementations like Samba. SMB Functionality: SMB (Server Message Block) provides a range of functionality for file sharing, printer sharing, and interprocess communication between computers in a network. Some of the key functionalities of SMB include: File and Directory Sharing: SMB allows users to share files and directories between computers in a network.\nIt enables users to access shared files and folders on remote servers as if they were located on their own computer, facilitating seamless file sharing and collaboration. Printer Sharing: SMB enables printer sharing, allowing users to access and utilize shared printers connected to a server or another computer in the network. This functionality allows multiple users to send print jobs to a shared printer, eliminating the need for individual printers for each user. Access Control and Permissions: SMB incorporates access control mechanisms that provide security and control over shared resources. It allows administrators to define user permissions and access rights for shared files, directories, and printers, ensuring that only authorized users can access specific resources. File and Print Operations: SMB supports a wide range of file and print operations. Users can perform actions such as reading, writing, renaming, and deleting files on remote servers. Additionally, they can send print jobs to shared printers, control print settings, and monitor the status of print jobs. Directory Browsing and Enumeration: SMB facilitates browsing and enumeration of directories on remote servers. Users can navigate through shared directories, view folder contents, and retrieve information about files and directories, such as file attributes and timestamps. File Locking: SMB includes file locking mechanisms to manage concurrent access to shared files. It allows multiple users to work on the same file simultaneously by preventing conflicting modifications. File locking ensures data integrity and prevents data corruption or loss due to simultaneous write operations.\nInterprocess Communication: SMB enables interprocess communication between applications running on different computers in a network. It provides a mechanism for applications to send messages, commands, and requests to each other, facilitating collaboration and interaction between processes on different machines. Security Features: SMB incorporates security features to protect data and ensure secure communication. It supports various authentication mechanisms, such as username and password authentication, NTLM (NT LAN Manager), and Kerberos. Additionally, newer versions of SMB (like SMBv3) include enhancements like encryption, integrity checking, and improved security protocols. These functionalities make SMB a powerful protocol for network file sharing and resource access, particularly in Windows-based environments. NFS (Network File System): (Network File System) NFS is a network file sharing protocol developed by Sun Microsystems (now Oracle) and commonly used in UNIX and Linux environments. It allows clients to access files and directories on remote servers as if they were local. NFS provides a transparent mechanism for sharing files across a network and supports features such as file locking, file permissions, and access control. It uses a client-server architecture, where the NFS server exports directories that can be mounted by NFS clients. NFS is widely supported in UNIX, Linux, and other operating systems, and it provides efficient file sharing capabilities for networked environments. How Does NFS Work? NFS, which stands for Network File System, is a distributed file system protocol that allows a computer to access files over a network as if they were stored locally.\nIt enables file sharing between computers running different operating systems, facilitating seamless file access and collaboration. Here's a general overview of how NFS works: Server Configuration: The computer hosting the files and directories to be shared is set up as an NFS server. The server configures directories or file systems to be shared with other computers in the network. Client Configuration: The computers or devices that want to access the shared files are configured as NFS clients. The clients need appropriate NFS client software or modules installed to communicate with the NFS server. Mounting: On the NFS client, the shared directories or file systems from the NFS server are mounted, which means they are made accessible as part of the client's file system. The client sends a mount request to the server, specifying the directory or file system to be mounted and its location on the client. Authentication: Once the NFS server receives the mount request, it verifies the client's authentication credentials to ensure that the client is authorized to access the requested files. Authentication methods can include user-based authentication, host-based authentication, or a combination of both. File Access: Once the client is successfully authenticated, it can access the shared files and directories as if they were located on the local file system. The client can perform read, write, create, delete, and other file operations on the mounted NFS share. File Locking: NFS supports file locking mechanisms to handle concurrent access to shared files.\nIt allows multiple clients to access and modify the same file simultaneously while preventing conflicts. File locking ensures data integrity and prevents data corruption or loss due to simultaneous write operations. File Attribute Caching: NFS employs file attribute caching to improve performance. The client caches file attributes, such as permissions and timestamps, to reduce network overhead. However, this caching can result in potential inconsistencies if file attributes are modified by other clients or the NFS server. Network Communication: NFS utilizes RPC (Remote Procedure Call) for communication between the client and server. RPC allows the client to send requests for file operations to the server, which processes the requests and returns the results. Unmounting: When the client no longer needs access to the shared files, it can unmount the NFS share. This involves sending an unmount request to the server, which terminates the NFS connection and releases any associated resources. It's important to note that there are multiple versions of NFS, with NFSv3 and NFSv4 being the most commonly used. Newer versions have introduced improvements in terms of performance, security, and features like file locking and access control. NFS is widely used in Unix-like operating systems and is also supported on other platforms through NFS client software or modules. NFS Functionality: NFS (Network File System) provides a range of functionality for remote file access and sharing across a network. Here are some key functionalities of NFS: Remote File Access: NFS allows a client computer to access files and directories stored on a remote server over a network.\nThe client can read, write, create, delete, and modify files on the remote server as if they were located on the local file system. Transparent File Access: NFS provides transparent file access, meaning that the client does not need to be aware of the physical location of the files on the server. The remote files are seamlessly integrated into the client's file system hierarchy, making it easy for users to work with shared files as if they were local. File Sharing and Collaboration: Multiple clients can simultaneously access and share the same files on the NFS server. This enables collaborative work scenarios where users can access and modify shared files, facilitating teamwork and efficient file sharing. Directory Access and Navigation: NFS allows clients to access and navigate through remote directories on the server. Clients can list the contents of directories, retrieve file attributes (such as permissions, ownership, and timestamps), and perform operations like file searches and directory traversals. File Locking: NFS provides file locking mechanisms to handle concurrent access to shared files. It allows multiple clients to access and modify the same file simultaneously while preventing conflicts. File locking ensures data integrity and prevents data corruption or loss due to simultaneous write operations. Caching: NFS incorporates caching mechanisms to improve performance. The client caches file data and metadata locally, reducing the need for frequent network communication. Caching helps minimize latency and bandwidth usage, enhancing overall file access speed. Security and Authentication: NFS supports authentication and access control mechanisms to ensure secure file access.\nIt allows administrators to define access permissions for shared files and directories, limiting access to authorized users or groups. Additionally, newer versions of NFS (like NFSv4) include stronger security features such as Kerberos-based authentication and support for secure transport protocols. Fault Tolerance and High Availability : NFS implementations often include features for fault tolerance and high availability. These mechanisms ensure that the NFS server remains accessible and reliable even in the event of hardware failures, network issues, or server downtime. Redundancy and failover mechanisms can be implemented to maintain continuous file access. Cross-Platform Compatibility: NFS is designed to be cross-platform, allowing clients and servers to run different operating systems. This enables file sharing between various systems, including Unix-like operating systems (e.g., Linux, macOS) and Windows systems with the appropriate NFS client software or modules installed. Overall, NFS provides a robust and flexible solution for remote file access and sharing, making it widely used in networked environments where centralized file storage and collaboration are required. CIFS vs. SMB vs. NFS: CIFS, SMB, and NFS are network file sharing protocols commonly used in computer networks to enable file access and sharing between devices. While CIFS and SMB are often used interchangeably to refer to the same protocol, it's important to note that CIFS is an enhanced version of SMB and includes additional features and improvements. NFS, on the other hand, is a distinct protocol primarily used in UNIX and Linux environments.\nQ# How do I mount storage volumes to a Linux host and use them for databases? To mount storage volumes to a Linux host and use them for databases, you can follow these steps: Create a storage volume. This can be done using a variety of methods, such as creating a partition on a physical hard drive, creating a logical volume on a storage array, or creating a file-based volume. Format the storage volume. This can be done using a filesystem that is compatible with the database software that you will be using. Mount the storage volume. This can be done using the command. Create a database on the storage volume. This can be done using the database software that you will be using. Here are some additional details about each step: Creating a storage volume: There are a variety of ways to create a storage volume. One common method is to create a partition on a physical hard drive. To do this, you will need to use the command. Once you have created a partition, you can format it using the command. Formatting the storage volume: The filesystem that you use to format the storage volume will depend on the database software that you will be using. For example, if you will be using MySQL, you will need to format the storage volume using the filesystem. Mounting the storage volume: To mount the storage volume, you will need to use the command.\nDear All Today we will Learn aboutHyperReplication. HyperReplication When a storage system runs block services, HyperReplication supports the following two modes: Synchronous remote replication In this mode, data is synchronized in real time to achieve full protection for data consistency, minimizing data loss in the event of a disaster. Asynchronous remote replication In this mode, data is synchronized periodically to minimize service performance deterioration caused by the latency of long-distance data transmission. lists the benefits of the HyperReplication feature shown in Table 1 Table 1 Benefits of the HyperReplication feature HyperReplication is the remote replication feature developed by Huawei. The feature provides flexible and powerful data replication functions to achieve remote data backup and recovery, continuous support for service data, and disaster recovery. This feature requires at least two OceanStor storage systems that can be placed in the same equipment room, same city, or two cities up to 1000 km apart. The storage system that provides data access for production services is the primary storage system, and the storage system that stores backup data is the secondary storage system. HyperReplication supports the following replication modes: HyperReplication/S for LUN: In this mode, data is synchronized between two storage systems in real time to achieve full protection for data consistency, minimizing data loss in the event of a disaster. However, production service performance is affected by the data transfer latency. HyperReplication/A for LUN: In this mode, data is synchronized between two storage systems periodically to minimize service performance deterioration caused by the latency of long-distance data transmission.\nProduction service performance is not affected by the data transfer latency. However, some data may be lost if a disaster occurs. How Asynchronous Remote Replication (HyperReplication/A) Works How Asynchronous Remote Replication (HyperReplication/A) Works HyperReplication/A for file system: In this mode, data is synchronized between two file systems periodically to minimize service performance deterioration caused by the latency of long-distance data transmission. Production service performance is not affected by the data transfer latency. However, some data may be lost if a disaster occurs. HyperReplication provides the storage array-based consistency group function for synchronous or asynchronous remote replication between LUNs to ensure the consistency of cross-LUN applications in disaster recovery replication. A consistency group is a collection of pairs that have a service relationship with each other. For example, the primary storage system has three primary LUNs that respectively store service data, logs, and change tracking information of a database. If data on any of the three LUNs becomes invalid, all data on the three LUNs becomes unusable. For the pairs in which these LUNs exist, you can create a consistency group. Upon actual configuration, you need to create a consistency group and then manually add pairs to the consistency group. The consistency group function protects the dependency of host write I/Os across multiple LUNs, ensuring data consistency on secondary LUNs. In addition, HyperReplication allows data to be replicated through both Fibre Channel and IP networks.\nDistributed Clustered NAS system In today's data-driven world, the required for productive and adaptable storage arrangements is paramount. Traditional network-attached storage (NAS) systems have served us well, but as data volumes continue to explode and bigger, new approaches are required to overcome and manage their limitations [1]. Enter the distributed clustered NAS system, a revolutionary solution that harnesses the power of distributed computing and clustered architecture to provide enhanced performance, scalability, and fault tolerance. Distributed clustered NAS systems are built upon the principles of distributed computing, where multiple interconnected nodes collaborate to provide a unified storage infrastructure. Unlike traditional NAS setups, which rely on a single storage device, distributed clustered NAS systems leverage a cluster of nodes, each with its processing power and storage capacity [2]. This distributed approach not only improves performance but also enables seamless scalability. Scalability: By distributing data across multiple nodes, distributed clustered NAS systems can effortlessly scale their storage capacity to accommodate ever-growing data volumes. Performance: With parallel processing and load balancing capabilities, these systems deliver impressive performance, ensuring quick and efficient data retrieval. Fault Tolerance: The distributed nature of clustered NAS systems enhances fault tolerance, as data redundancy and replication mechanisms are built into the architecture. In the event of a node failure, data can be seamlessly accessed from other available nodes. [3] Distributed clustered NAS systems employ a robust architecture that combines storage and compute resources in a scalable manner.\nThe architecture typically consists of three key components: Storage Nodes: These nodes hold the actual data and provide storage capacity. They can be physical servers or virtual machines running on a cluster. Metadata Servers: Metadata servers keep track of the file system hierarchy and the location of data across the storage nodes. They play a crucial role in facilitating efficient data access and retrieval. Access Nodes: Access nodes act as the interface between the end-users and the underlying storage infrastructure. They handle user requests and ensure seamless data access. [4] Distributed clustered NAS systems utilise intelligent data distribution and redundancy mechanisms to ensure data availability and fault tolerance. These mechanisms include: Data Striping: Data is divided into small chunks and distributed across multiple storage nodes, allowing for parallel access and improved performance. Data Replication: Critical data is replicated across multiple nodes to prevent data loss in case of node failures. Replication ensures data availability and enhances fault tolerance [5]. Consistency and metadata management are crucial aspects of distributed clustered NAS systems. Techniques such as distributed locking and version control ensure data consistency, while metadata servers play a vital role in maintaining an accurate and up-to-date file system hierarchy [6]. Ensuring data security is of fundamental importance in any storage system. Distributed clustered NAS systems implement various security measures, including access control, encryption, and authentication protocols, to safeguard sensitive data from unauthorised access and tampering [7].\nDistributed clustered NAS systems find applications in various industries, including: Media and Entertainment: These systems provide high-performance storage for handling large media files, supporting video editing, rendering, and content distribution. Healthcare: Distributed clustered NAS systems enable secure storage and retrieval of vast amounts of medical data, ensuring quick access to patient records and diagnostic images. Research and Scientific Computing: These systems are ideal for handling data-intensive research workloads, allowing for efficient data sharing and collaboration among researchers [8]. Implementing distributed clustered NAS systems comes with its own set of challenges, including: Network Bandwidth: The performance of distributed clustered NAS systems heavily relies on network bandwidth. Ensuring sufficient bandwidth and network infrastructure is crucial for optimal performance [9]. Data Consistency: Maintaining consistency over the distributed frameworks can be challenging and troublesome. Careful consideration must be given to techniques such as distributed locking and conflict resolution to ensure data integrity. As technology continues to evolve, the future of distributed clustered NAS systems looks promising. Advancements in network infrastructure, capacity technologies, and distributed computing paradigms will further improve the performance, adaptability, and fault tolerance of these systems. [10] Caching Strategies: Explore the utilisation of caching mechanisms to improve data access latency and reduce network overhead. Load Balancing: Discuss load balancing techniques to evenly distribute data and workload across storage nodes, maximising performance. Parallel Processing: Highlight the benefits of parallel processing in distributed clustered NAS systems and how it enhances performance for data-intensive tasks.\nData Tiering: Examine the concept of data tiering, where frequently accessed data is stored in high-performance storage tiers, optimising access times. Data Compression and Deduplication: Explore the utilisation of compression and deduplication procedures to reduce capacity space prerequisites and improve overall system performance [11]. Heterogeneity Management: Discuss the challenges of managing heterogeneous storage nodes with varying capacities, performance levels, and hardware configurations. Consistency Models: Examine different consistency models and their trade-offs in distributed clustered NAS systems, addressing the challenges of maintaining data consistency. Interoperability and Standards: Explore the need for interoperability and standardised protocols in distributed clustered NAS systems to ensure seamless integration and data portability. Security and Privacy: Discuss the evolving security threats in distributed storage environments and the need for robust security measures to protect sensitive data. Hybrid Approaches: Investigate the potential of hybrid approaches, combining distributed clustered NAS systems with cloud storage or object storage, to further enhance scalability and flexibility [12]. In conclusion, the emergence of distributed clustered NAS systems has revolutionised the storage landscape. By adopting a distributed approach and leveraging clustered architecture, these systems address the limitations of traditional NAS setups and pave the way for a new era of scalable and high-performance storage. With ongoing research and development, we can expect these systems and frameworks to continue evolving and shaping the future of data storages and capacities. [1] Tanenbaum, A. S., & van Steen, M. (2007). Distributed systems: principles and paradigms. Pearson Education. [2] Patterson, D. A., Gibson, G., & Katz, R. H. (1988).\nA case for redundant arrays of inexpensive disks (RAID). In Proceedings of the SIGMOD conference (Vol. 17, No. 3, pp. 109-116). [3] Ghemawat, S., Gobioff, H., & Leung, S. T. (2003). The Google file system. ACM SIGOPS Operating Systems Review, 37(5), 29-43. [4] Hadoop Distributed File System. (n.d.). Retrieved from https://hadoop.apache.org/hdfs/ [5] Weil, S. A., Brandt, S. A., Miller, E. L., & Long, D. D. E. (2006). Ceph: A scalable, high-performance distributed file system. In Proceedings of the 7th Symposium on Operating Systems Design and Implementation (OSDI) (pp. 307-320). [6] Kaur, A., & Garcha, J. S. (2018). A survey on distributed storage systems. International Journal of Computer Science and Mobile Computing, 7(8), 192-199. [7] Chordas, S. W., Du, D. H., & Qiu, L. (2011). Survey on distributed file systems. International Journal of Computer Science and Information Security, 9(7), 67-73. [8] Shvachko, K., Kuang, H., Radia, S., & Chansler, R. (2010). The Hadoop distributed file system. In Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST) (pp. 1-10). [9] Sathiamoorthy, M., Sun, X., Sripanidkulchai, K., & Ganger, G. R. (2010). FAST: fast architecture sensitive tree search on modern CPUs and GPUs. In Proceedings of the 9th USENIX conference on File and Storage Technologies (FAST) (pp. 65-78). [10] Muthitacharoen, A., Morris, R. T., Gil, T., & Chen, B. (2002). Ivy: A read/write peer-to-peer file system. In Proceedings of the 5th Symposium on Operating Systems Design and Implementation (OSDI) (pp. 31-46). [11] Li, C., Yang, Y., Li, Y., & Wu, C. (2016).\nGreetings, everyone! In this article, Ill explain about Introduction to the NAS Storage Solution. Introduction to the NAS Storage : Network-Attached Storage (NAS) is a type of storage system that provides centralized data storage and file sharing capabilities over a network. It is designed to be easily accessible by multiple users and devices connected to the network, making it a popular choice for homes, small businesses, and enterprise environments. NAS storage offers several advantages, including simplified data management, seamless collaboration, and scalable storage capacity. Types of NAS Storage: There are several types of NAS storage available, each offering unique features and capabilities. Consumer NAS: Consumer NAS devices are designed for home users and small-scale personal use. They typically offer basic storage capabilities, easy setup, and user-friendly interfaces. Consumer NAS devices are often used for file sharing, media streaming, and backup purposes in home environments. Small Business NAS Small business NAS solutions are designed for small to medium-sized businesses. These NAS systems offer more advanced features compared to consumer-grade devices, such as support for multiple users, RAID configurations for data redundancy, and integration with business applications and services. They provide centralized storage and backup options for small office environments. Enterprise NAS Enterprise NAS solutions are designed for large-scale deployments in enterprise environments. They offer high-performance storage capabilities, scalability, and advanced features for data management, security, and disaster recovery. Enterprise NAS systems are typically used in data centers and enterprise-level applications that require robust and reliable storage solutions.\nRackmount NAS Rackmount NAS devices are designed to be mounted in standard server racks. They offer high-density storage options and are suitable for data centers or environments where space is limited. Rackmount NAS devices are often used in enterprise settings and provide scalability, high-performance networking, and redundant power supplies. Desktop NAS Desktop NAS devices are compact, standalone units designed for desktop or small office use. They offer a smaller storage capacity compared to rackmount models but provide similar functionality. Desktop NAS devices are suitable for home offices, small businesses, and personal use where space is not a constraint. Virtualized NAS Virtualized NAS refers to a NAS solution that runs as a virtual machine on a virtualization platform. It allows organizations to leverage their existing virtualization infrastructure and consolidate storage resources. Virtualized NAS provides flexibility, scalability, and simplified management in virtualized environments. Cloud NAS: Cloud NAS, also known as cloud storage gateway or cloud-integrated storage, combines NAS functionality with cloud storage services. It allows organizations to extend their storage capacity by leveraging cloud storage providers while maintaining the familiar NAS interface and protocols. Cloud NAS offers scalability, data redundancy, and remote access to data stored in the cloud. These are some of the common types of NAS storage available in the market. The choice of NAS storage depends on the specific requirements, scale of operations, and budget of the organization or individual. Key Features of NAS Storage: NAS provides a centralized storage repository for storing files, documents, media, and other types of data.\nUsers can access and manage their files from any connected device within the network, eliminating the need for individual storage devices or file servers. Security is a critical aspect of NAS storage to ensure the protection and integrity of stored data. To enhance security, access controls and permissions should be implemented, limiting access to authorized individuals. Secure protocols like SSH or SFTP should be used for encrypted data transfer during file sharing and collaboration. Implementing RAID technology provides data redundancy and protection against drive failures, while regular backups to separate storage media or offsite locations safeguard against data loss. Network security measures such as firewalls, IDS, and VPNs should be implemented, along with strong authentication mechanisms and data encryption at rest. Regular updates and patch management help mitigate security vulnerabilities. By addressing these security considerations, organizations can enhance the overall security of their NAS storage infrastructure and protect sensitive data from unauthorized access and data loss. Reliability is a crucial aspect of NAS storage to ensure uninterrupted access to data and minimize the risk of data loss. By addressing these reliability considerations, organizations can ensure that their NAS storage system operates consistently, provides continuous access to data, and protects against potential failures or data loss. It is advisable to follow best practices recommended by the NAS manufacturer and regularly maintain and monitor the NAS infrastructure for optimal reliability. RAID (Redundant Array of Independent Disks) is a key feature in NAS storage that provides data redundancy, performance improvement, and increased reliability.\nBy implementing RAID in NAS storage, organizations can benefit from data redundancy, improved performance, fault tolerance, and scalability. It is important to select the appropriate RAID level based on the organization's needs and regularly monitor the RAID array's health and performance to ensure optimal operation. Overall, NAS storage provides a reliable, flexible, and cost-effective solution for storing and managing data in home, small business, and enterprise environments. Its centralized storage, file sharing, data protection, scalability, and remote access capabilities make it a valuable tool for efficient data management and collaboration. Thanks for reading. Introduction to the NAS Storage Solution: NAS (Network Attached Storage) is a storage solution that provides centralized data storage and file sharing capabilities within a network environment. It is designed to offer easy and efficient access to data for multiple users and devices connected to the network. Here is an introduction to the key aspects of NAS storage: Centralized Storage: NAS serves as a centralized repository for storing and organizing data. It typically consists of one or more hard drives or SSDs (Solid State Drives) housed in a dedicated NAS enclosure or server. This centralized approach allows users to access their data from any connected device within the network, making it convenient for individuals, workgroups, or small businesses. File Sharing and Collaboration: NAS enables seamless file sharing and collaboration among users within the network. It supports common file sharing protocols such as SMB (Server Message Block) for Windows environments and NFS (Network File System) for Unix/Linux environments.\nUsers can easily share files, documents, multimedia content, and other data, fostering collaboration and productivity. Data Protection and Redundancy: NAS systems often incorporate data protection mechanisms such as RAID (Redundant Array of Independent Disks). RAID configurations, such as RAID 1 (mirroring) or RAID 5 (striping with parity), provide data redundancy and protection against drive failures. This helps ensure data integrity and minimizes the risk of data loss due to hardware failures. Data Backup and Recovery: NAS storage solutions offer built-in backup features or can be integrated with backup software to create regular backups of critical data. These backups can be stored on the NAS itself or on external storage devices. In the event of data loss or accidental deletion, the backups can be used for data recovery, providing an added layer of data protection. Remote Access and Cloud Integration: Many NAS solutions offer remote access capabilities, allowing users to access their data from outside the local network. This is especially beneficial for remote or mobile workers who need to access files and data on the go. Additionally, NAS systems often support cloud integration, enabling seamless synchronization and backup of data to popular cloud storage services. Scalability and Expandability: NAS storage solutions are designed to scale and accommodate growing storage needs. They typically support adding additional hard drives or expanding storage capacity as required. This scalability allows organizations to easily adapt their storage infrastructure to changing data requirements without major disruptions. Security and Access Controls: NAS systems provide various security features to protect data from unauthorized access.\nDear Members, As data increasingly becomes a core asset for enterprises, digital resilience is a major metric for building enterprise resilience frameworks. Enterprises must prioritize digital resilience, to build sustainable long-term supply chains, legal compliance, financial transparency and health, and operational continuity. Trends analysis 1. Digital resilience is key to building leadership Digital transformation empowers enterprises to carry information in their data. The acquisition, storage, transmission, exchange, and processing of digital information depends on highly reliable and stable infrastructure. Data infrastructure is the foundation of digital information, which in turn is the upper-layer application of data infrastructure. The integrity, confidentiality, and availability of digital infrastructure play a key role in economic and social informatization and digitalization. If critical IT infrastructure is damaged or data is breached, it can significantly affect society and destabilize digital economic systems. predicts that 65% of global GDP will be digitized by 2022. This makes digital resilience more important than ever, with 80%+ of enterprises at medium- or high-risk of a data breach. 2. Natural disasters and human error make digital resilience essential Today, data is a new means of production and the lifeblood of organizations. However, IT systems tend to be vulnerable under unprotected conditions. System faults, natural disasters, and human error can cause system breakdowns, the losses from which cannot be understated. In March 2021, a fire destroyed four major data centers operated by OVH, Europes largest cloud infrastructure operator, paralyzing a large number of customer websites, and rendering some customer data lost and unrecoverable.\nProtection object: Both structured and unstructured data need protection. For example, big data is helping in production decision-making for enterprises and mass unstructured data on which it depends also needs DR protection. Protection performance: Cold data in the DR center is converted to warm data to maximize value. For example, the number of CDRs generated by a million 5G users increases by 7-fold, requiring higher protection performance. Protection scale: The amount of data to be protected has increased from PBs to EBs. For example, an L4 autonomous car can generate 60 TB of data per day, 50 times that of an L2 car. Given this background, there is huge emphasis on zero service interruptions or data loss. For service continuity, data protection solution vendors offer integrated SAN/NAS active-active solutions to protect both structured and unstructured data. For data retention in new application ecosystems, the retrieval and backup of mass unstructured data are used to protect mass small files. What we suggest Achieve the goal of Three Zeros, Two Always for secure, trustworthy data Different workloads face different data security challenges and threats. To improve digital resilience, the goal of Three Zeros, Two Always for data security and trustworthiness must be achieved: Zero data loss : Customer data must not be damaged or lost due to software and hardware problems. Even in the event of an abnormality, damaged data, or mis-operations, the system can be recovered and continue without major disruption.\nZero data leakage : Data must not be accessed or obtained without authorization during storage, transmission, and processing. Zero data tampering : Data will be protected from unauthorized modification or damage during storage, transmission, and processing, and even if an unlikely infiltration occurs, tampered data can be recovered upon detection. Always-on services : The fulfillment of customer service specifications ensures storage services, but if storage services are interrupted due to customer environment problems or a malicious attack, the services will be recovered within the customers tolerance period after the environment recovers or the malicious attack is resolved. Always compliant access : Data storage, transmission, and processing must comply with laws and regulations such as General Data Protection Regulation (GDPR). Enhance DR security for core data to ensure service continuity Enterprises should enhance protection grades to fully upgrade service continuity. For enterprises that lack sufficient DR and data backup measures, a comprehensive protection framework is essential for full DR of mission-critical services and full data backup. To improve service continuity and data reliability, the following preparations should be met: examine enterprise services, check for the lack of DR for key services, and help enterprises build redundant data centers. If DR systems are already set up, upgrading the local active-passive DR solution to an active-active solution to ensure zero loss of key data, and develop intra-city, active-active DR architecture to a multi-site, multi-center protection solution to facilitate multi-copy, cross-region recovery.\nDistributed layout and complex management in traditional storage hinders Internet architecture from evolving to all industries. The Rise of HCI From Hyper Convergence to Full Convergence Transform Enterprise Data Centers with Streamlined FusionCube HCI Improve Resource Utilization with Converged Distributed Storage and Virtualization, along with Cloud-based Infrastructure Build Diverse Ecosystems through ARM and x86 Support The Innovative Dynamic Algorithm Shortens Data Reconstruction Time FusionCube Achieves Highest Storage Usage (90%) By Using Core EC Technologies Exclusive FCA Acceleration Card, Improving VM Density Per Node by 20% More Reliable Asynchronous Replication Enables Instant Service Recovery Mainstream Backup Solutions Available for Flexible Reliability Options System Health Prediction Keeps You Informed on System Status Integrated Delivery Enables Services Rollout in 30 Minutes Scale-Out Architecture: On-Demand Capacity Expansion and Linear Performance Improvement Compute or storage nodes can be expanded separately based on service requirements, allowing linear performance improvement FusionCube logical architecture FusionCube cluster includes : FusionCube Center Unified management platform: FusionCube management software. It implements management of virtualization and hardware resources, system monitoring, and O&M. FusionSphere/VMware virtualization platform : Provides the ability to compute resource pools, including virtual machine lifecycle management and advanced features. FusionCube distributed block storage Distributed block storage software. It uses distributed technologies to group the drives (SAS/SATA/NL-SAS HDDs, SSDs, or NVMe SSDs) of x86 servers into a large storage resource pool and provides block-based storage services with high performance and reliability. Huawei Server Platform: FusionCube supports E9000, X6800, X6000, and rack servers, which provides the following features: On-demand configuration of computing and storage nodes.\nAn optical disk is any computer disk that uses optical storage techniques and technology to read and write data. It is a computer storage disk that stores data digitally and uses laser beams (transmitted from a laser head mounted on an optical disk drive) to read and write data. An optical disk is an electronic data storage medium that can be written to and read from using a low-powered laser beam. Most of today's optical disks are available in three formats: compact disks (CDs), digital versatile disks (DVDs) -- also referred to as digital video disks -- and Blu-ray disks, which provide the highest capacities and data transfer rates of the three. An optical disk is primarily used as a portable and secondary storage device. It can store more data than the previous generation of magnetic storage media, and has a relatively longer lifespan. Compact disks (CD), digital versatile/video disks (DVD), and Blu-ray disks are currently the most commonly used forms of optical disks. These disks are generally used to: distribute software to customers; store large amounts of data such as music, images, and videos; transfer data to different computers or devices; back up data from a local machine. Optical disks rely on a red or blue laser to record and read data. Most of today's optical disks are flat, circular and 12 centimeters in diameter. Data is stored on the disk in the form of microscopic data pits and lands. The pits are etched into a reflective layer of recording material.\nContinued Part 2... Stepper Motor: Stepper motorsare DCmotorsthat move in discrete steps. They have multiple coils that are organized in groups called \"phases\". By energizing each phase in sequence, themotorwill rotate, onestepat a time. With a computer controlled stepping you can achieve very precise positioning and/or speed control. Logic Board: Amotherboard(sometimes alternatively known as the mainboard, systemboard, baseboard, planarboardor logicboard, or colloquially, a mobo) is the main printed circuitboard(PCB) found in general purpose microcomputers and other expandable systems. Cables & Connectors: Connection for interfacing to the computer receiving power. Data Cables. SATA Cable. SCSI Cable IDE Cable SAS Cable SATA Cable: SCSI Cable: IDE Cable: SAS Cable: Disk Structures: 1) Track 2) Track Sector 3) Cluster SSD VS HHD: Hard Disk Drives (HDDs) have been a popular storage solution for many years, but they also have their advantages and disadvantages. Here are the pros and cons of using hard disk drives: Pros of Hard Disk Drives (HDDs): Cost-effective: HDDs are generally more affordable compared to other storage options such as solid-state drives (SSDs). This makes them a cost-effective choice, especially when you require large amounts of storage capacity. High storage capacity: HDDs are available in larger capacities compared to SSDs. You can find HDDs with several terabytes of storage, making them suitable for storing vast amounts of data, including large media files, games, and backups. Compatibility: HDDs are compatible with most computers and operating systems. They use standard interfaces like SATA and are easily interchangeable between systems.\nIn today's digital age, data has become a valuable commodity for both individuals and businesses.The loss of critical data can result in significant financial costs, legal liability, and damage to a company's reputation.This is where cloud backup comes in.Cloud backup is a way to protect important data by storing it securely in remote data centers accessible via the Internet.In this article, we'll take a look at what cloud backup is, how it works, its benefits, how it compares to traditional backup, security issues, and choosing the right cloud provider. What is cloud backup? Cloud backup is the process of copying data to the cloud segment (NAS) located in data centers.It involves sending copies of the data over the Internet to a secure server where it is stored in a secure location.This data can then be easily accessed from anywhere with an internet connection.There are two types of cloud-based backup solutions: File-level backup, Image-level backup. File-level backup backs up individual files, while image-level backup backs up the entire system or server.Cloud backup has a number of advantages over traditional backup methods, including lower costs,scalability and ease of access.With cloud backup, businesses can save money by avoiding the need to invest in expensive hardware and software.In addition, cloud-based backup solutions provide scalability, allowing businesses to increase or decrease their storage needs as needed. How does cloud backup work?\nThe cloud backup process includes several steps.The data is first encrypted and compressed before being transmitted over the Internet to a remote server.The remote server then securely stores the data, making it easily accessible to authorized users.When the data needs to be extracted, it is decrypted and restored on the user's device. When choosing a cloud backup solution, it is important to consider the following factors: backup frequency, data retention policy, recovery time. In addition, companies should look for a provider that offers strong security measures to protect data. What are the benefits of cloud backup? 1- Disaster Recovery: One of the most significant benefits of cloud backup is the ability to recover from a disaster.In the event of a natural disaster or cyber-attack, companies can easily recover their data from a remote location.In addition, cloud backup provides cost savings by eliminating the need for expensive hardware and software.It also provides scalability, allowing companies to easily expand their storage needs as needed.Finally, cloud backup provides accessibility and flexibility, allowing users to access their data from anywhere with an internet connection.\n2-Fast start: 3- Affordable price of services: 4- Lack of binding to equipment, technologies, and geographic location: 5- Cloud backup versus traditional backup: Cloud backup has a number of advantages over traditional backup methods.Traditional backup involves backing up data to physical media such as tapes or hard drives, which can be costly and time-consuming.Also, physical backups can be lost or corrupted, resulting in data loss.Cloud backup, on the other hand, is more cost-effective and offers greater scalability and availability. 6- Versatility and flexibility: 7- Security issues with cloud backup: While cloud backup offers a number of benefits, it also raises security concerns.Cyber threats such as hackers and malware can compromise sensitive data stored in the cloud.It's important to choose a provider that offers strong security measures, such as encryption and two-factor authentication, to protect data from unauthorized access. 8- Choosing the right cloud backup service provider: When choosing a cloud backup service provider, it is important to consider factors such as backup frequency, data retention policy, recovery time, and security measures.In addition, companies should look for a provider with a user-friendly interface and good customer support.\nReplication and HyperMetro each have a set of storage on both sides to realize the disaster recovery function of the business. What is the difference? Thank you! Hi friend, Replication and HyperMetro are both DR solutions that involve data replication between two storage systems. Replication is an asynchronous solution where data is periodically replicated from the primary storage system to the secondary storage system. If a disaster occurs before the next replication cycle, this may result in some data loss. Active-active, on the other hand, is a synchronous solution, which means that data is replicated between two storage systems in real time. This ensures that data is not lost in the event of a disaster. Replication requires a manual failover process to switch from the primary storage system to the secondary storage system in the event of a disaster. HyperMetro, on the other hand, provides automatic failover, which means that in the event of a disaster, the secondary storage system immediately takes over. Replication requires additional hardware and software to manage the replication process, and HyperMetro is a built-in function of some storage systems. Refer to . Thanks. Hi friend, Replication and HyperMetro are both DR solutions that involve data replication between two storage systems. Replication is an asynchronous solution where data is periodically replicated from the primary storage system to the secondary storage system. If a disaster occurs before the next replication cycle, this may result in some data loss.\nWhat is Storage? Storage hold data, instruction and information for future use. Storage Medium is physical material used for storing data. Storage Media Types: Hard Disk Floppy Disk Zip Disk CD DVD SD Card Flash Memory SSD Storage Medium Capacity: Drive Types: Hard Disk Solid State Drives What is Hard Drive: It was invented in 1954 by IBM High capacity Storage Consists of several Inflexible Circular platter that store items electronics, The Operating System, Software and most other files are stored in the HDD. Hard Drive Interfaces: Parallel Advance Technology Attachment (PATA). Small Computer System Interface (SCSI). Serial AT Attachment (SATA). Serial Attached SCSI (SAS). Parallel Advance Technology Attachment (PATA): It has 40 point data connector. It has 4 point power connector. Data transfer rate 133.3/MB/S. If you have two devices connected to one IDE controller , one must be set master and the other must be set to slave using jumpers and IDE cable. Small Computer System Interface (SCSI): SCSI is a higher performance storage device. It has 40 Pins data connector. It has 4 Point power connector. Data transfer rate 320./MB/S. SATA Drive Interface: It is the latest high speed type of hard drive connectors.. It has 7 Pins data connector. It has 15 pin Power connector. Data transfer rate 6/GB/S. The Serial Attached SCSI (SAS): It has a 7 Pins data connector. It has 15 pin Power connector. Data transfer rate 12/GB/S. Basic Components of Hard Drive: Disk Platters. Read/Write Heads. Spindle Motor. Stepper motor. Logic Board. Cable & Connectors.\nDear Community Members, Today we are going to read about the power of Intelligent Data Management. In the digital era, data has become the lifeblood of organizations across industries. Managing vast amounts of data efficiently and extracting value from it has become a critical priority. This is where intelligent data management emerges as a game-changer. Among the key players in this field is Huawei, a technology giant renowned for its storage solutions that seamlessly integrate intelligence and innovation. In this article, we will explore the concept of intelligent data management and the significant benefits it offers. Intelligent data management refers to the application of advanced technologies, such as artificial intelligence (AI) and machine learning (ML), to optimize data storage, retrieval, analysis, and protection processes. It goes beyond traditional data management methods by leveraging automation, analytics, and predictive capabilities to make data-driven decisions and optimize storage resources. Intelligent data management solutions, such as those offered by Huawei, incorporate automated data tiering capabilities. This technology intelligently moves data across different storage tiers based on usage patterns, performance requirements, and cost considerations. Frequently accessed or critical data is stored on high-performance tiers like flash storage, while less frequently accessed data is migrated to cost-effective storage tiers. This approach ensures that data is stored in the most appropriate and cost-efficient manner, optimizing both performance and resource utilization. Redundant data can consume valuable storage space and impact overall efficiency. Intelligent data management solutions employ data deduplication and compression techniques to identify and eliminate duplicate or unnecessary data.\nBy reducing the storage footprint, organizations can achieve significant cost savings while maintaining data integrity and accessibility. AI and ML algorithms enable intelligent data management systems to analyze historical data access patterns, workload trends, and performance metrics. This insight empowers organizations to predict future storage requirements accurately. By proactively adjusting storage resources and performance parameters, organizations can optimize their infrastructure to meet current and future demands, ensuring consistent and efficient performance. Intelligent data management systems also prioritize data protection and security. Advanced encryption algorithms, access controls, and continuous monitoring mechanisms help safeguard sensitive data from unauthorized access, breaches, and data loss. Additionally, features like data replication, snapshotting, and backup ensure business continuity and disaster recovery capabilities. By automating storage optimization processes, intelligent data management solutions reduce manual effort, minimize administrative overhead, and improve overall efficiency. This enables IT teams to focus on strategic initiatives, innovation, and higher-value tasks, driving productivity and agility within the organization. The automated tiering and data reduction techniques employed in intelligent data management solutions significantly reduce storage costs. By aligning data placement with usage patterns and eliminating redundant data, organizations can make efficient use of storage resources, thereby lowering hardware, maintenance, and operational expenses. Intelligent data management leverages predictive analytics and performance optimization to ensure that critical data resides on high-performance storage tiers, reducing latency and improving application responsiveness. This enhances user experience, accelerates business processes, and supports data-intensive workloads. By leveraging the insights gained from data analytics, intelligent data management solutions enable organizations to make data-driven decisions.\nWhat is OceanStor BCManager? OceanStor BCManager is a data protection and disaster recovery (DR) management software solution developed by Huawei. It is designed to help businesses ensure the availability and continuity of their critical data and applications in the event of disruptions or disasters. Data Replication: BCManager enables data replication between primary and secondary sites, ensuring that data is consistently and synchronously replicated to a remote location. This replication helps to protect against data loss and allows for quick recovery in the event of a disaster. Disaster Recovery Management: BCManager provides centralized management and monitoring of the entire disaster recovery process. It helps organizations define recovery objectives, set recovery plans, and manage the failover and failback processes to minimize downtime and data loss. Replication Technologies BCManager supports multiple replication technologies, including synchronous replication, asynchronous replication, and semi-synchronous replication. Companies can choose the suitable replication mode built on their RPO-recovery point objectives and RTO-recovery time objectives. Application Consistency: BCManager ensures application-consistent replication, ensuring that data and applications are replicated in a consistent state. This ensures data integrity and helps maintain the reliability and usability of the replicated data. Data Compression and Deduplication: BCManager incorporates data compression and deduplication technologies to optimize bandwidth utilization and reduce storage costs. These techniques help in efficient data transfer over the network and minimize storage requirements at the secondary site. Automated Failover and Failback: BCManager automates the failover and failback processes, allowing for quick and efficient recovery.\nIn the event of a disaster, it automatically switches the operations to the secondary site and provides seamless access to critical applications and data. Once the primary site is restored, it facilitates the smooth transition back to the primary environment. Monitoring and Reporting: BCManager offers comprehensive monitoring and reporting capabilities to track the status of replication, perform health checks, and generate reports on key metrics. This helps administrators ensure the effectiveness of the DR solution and make informed decisions. OceanStor BCManager is widely used by organizations across industries to establish robust disaster recovery strategies and protect their critical data and applications. By providing efficient replication, management, and automation capabilities, BCManager helps businesses minimize downtime, maintain data integrity, and achieve business continuity in the face of disruptive events. Functions: Security Features: The eBackup software provides the following security features: HTTPS security access Encryption of sensitive data Verification code authentication for login and login error lock settings to prevent brute force password cracking. User password complexity and session timeout policies Integrity check before installation. User operation logs Database, web, and operating system hardening in VM template-based software installation. Web application security, such as protection against cross-site scripting, structured query language (SQL) injection attacks, and cross-site request forgery (CSRF) attacks. Internal Software Architecture: Logical Architecture: Summary: OceanStor BCManager is a data protection and disaster recovery (DR) management software solution developed by Huawei. It enables businesses to ensure the availability and continuity of critical data and applications in the event of disruptions or disasters.\nHello, everyone! Today, I will share & describe the Data Backup and Recovery. What is Backup and Recovery? Backup and recovery refer to the processes and techniques used to protect and recover data in the event of data loss or corruption. Backup involves making a copy of data to protect against accidental deletion, hardware failure, or other forms of data loss, while recovery involves restoring the lost or damaged data from the backup. What are the Three Types of Backups? Full backups: A full backup involves copying all data in a system or device to a backup storage location. This type of backup is typically time-consuming and requires significant storage space. Incremental backups: An incremental backup copies only the data that has changed since the last backup, reducing the amount of time and storage required for backups. Differential backups: A differential backup copies all data that has changed since the last full backup. This type of backup requires more storage than incremental backups but less than full backups. What is the Difference Between Backup and Recovery? Backup refers to the process of making a copy of data, while recovery involves restoring the lost or damaged data from the backup. Backup is a proactive measure to protect against data loss, while recovery is a reactive measure to recover data after it has been lost. What Are the Types of Data Recovery?\nGranular recovery of files, folders, and objects: Granular recovery of files, folders, and objects: This type of recovery involves restoring specific files or folders from a backup. Instant mass restore: This type of recovery involves restoring a large number of files or an entire system from a backup. Volume recovery: This type of recovery involves restoring an entire storage volume or disk from a backup. Virtual Machine Disk (VMDK) recovery: This type of recovery involves restoring virtual machines from backups. Bare machine recovery: Bare machine recovery is a type of data recovery method where an entire system is restored after a major hardware failure or system crash. This method involves recovering an entire system image, including the operating system, applications, settings, and data. The goal of bare machine recovery is to quickly restore a system to its original state, minimizing downtime and reducing the risk of data loss. This method is typically used for critical systems or servers that cannot afford to be offline for extended periods of time. Instant volume mounts: Instant volume mounts are a type of data recovery technique that allows you to quickly mount a backup volume or snapshot to a running system, without having to restore the data first. This can be useful in situations where you need to access a specific file or application from a previous point in time, without disrupting the current system. Instant volume mounts are commonly used in virtualized environments, where multiple virtual machines share the same storage volumes.\nBy mounting a backup volume or snapshot to a new virtual machine instance, you can quickly restore service to a failed or corrupted system. Instant restores of VMs: Instant restores of VMs (virtual machines) is a type of data recovery method that enables users to quickly restore a virtual machine to a previous state. This type of recovery is possible because VMs run on a host system and are stored as virtual hard disks or virtual machine disks (VMDKs). When an instant restore of a VM is needed, the VMDK can be mounted directly from the backup storage, allowing users to access the VM as if it were running on the production system. This method is often used for rapid recovery of virtualized workloads, reducing downtime, and improving business continuity. What is Disaster Recovery Backup? Disaster Recovery Backup is a process of creating and maintaining a duplicate set of data and infrastructure that can be used to quickly recover from a disaster or a disruptive event. The disaster recovery backup is a crucial part of a disaster recovery plan and is designed to ensure that an organization can quickly recover its data and IT infrastructure in the event of a disaster. The backup data is typically stored offsite in a secure location and can be used to restore critical systems and data after a disaster, such as a natural disaster, cyber-attack, or equipment failure.\nThe goal of disaster recovery backup is to minimize downtime and data loss, and to ensure that business operations can resume as quickly as possible. What types of Data Sources to be Recovered? VMs (VMware, Microsoft, Nutanix): These virtual machines can store critical business applications and data, making them an essential component of a disaster recovery plan. Physical servers (Windows, Linux): on the other hand, refer to the traditional servers that run on physical hardware and typically host critical applications and data. Data loss or corruption on physical servers can cause significant disruption to business operations, making their recovery essential. Databases (RDBMs) and Distributed Databases (NoSQL, Hadoop, Mongo, Apache, etc. ): Databases are critical components of many business applications, and data loss or corruption can be catastrophic. Recovering databases is a common use case for backup and recovery. Files stored on network-attached storage (NAS): devices may also need to be recovered, as they can contain important documents, user data, and other files critical to business operations. Containers (e.g., Kubernetes): Containerization technologies like Kubernetes are becoming increasingly popular for deploying and scaling applications. Data loss or corruption on containerized applications can cause severe disruptions, necessitating their recovery. Applications (Microsoft Exchange, SAP HANA ): Applications such as Microsoft Exchange and SAP HANA are also critical to business operations and may require recovery in the event of data loss or corruption.\nSaaS applications (Microsoft 365, Salesforce): Finally, SaaS applications such as Microsoft 365 and Salesforce are cloud-based services that can store critical business data and require backup and recovery measures to prevent data loss or corruption. Primary storage: refers to the main, direct-access storage devices in a computing system, such as hard disk drives or solid-state drives, where data is actively stored and accessed by the system's processor. This is in contrast to secondary storage, such as tape or cloud storage, which is used for backup or long-term archiving. Mainframes: powerful computers that are primarily used by large organizations for critical business applications, such as financial transactions or inventory management. They have been in use since the 1950s and are still commonly used today. Mainframes typically use specialized hardware and operating systems designed for high reliability, security, and scalability, and they often require specialized backup and recovery processes to ensure that data can be quickly and reliably recovered in the event of a disaster or system failure. Why need a Data Backup and Disaster Recovery Plan Important? Prevent data loss: Data backups and disaster recovery plans help prevent data loss in the event of unexpected events like system failures, natural disasters, cyber attacks, and human errors. Sustain operations: A backup and disaster recovery plan ensures that business operations can continue uninterrupted or with minimal disruption during an emergency. Maintain a good customer experience: Customers expect reliable service from businesses.\nWith a disaster recovery plan in place, businesses can minimize the impact of disruptions on their customers, thereby maintaining a positive customer experience. Keep employees productive: When systems fail, employees may be unable to access the data they need to perform their jobs. A backup and disaster recovery plan can keep employees productive by ensuring that critical data and systems are available when needed. Retain historical records: Data backups ensure that historical records are preserved and can be accessed when needed. This is particularly important for businesses in regulated industries that are required to keep records for a certain period. Satisfy auditors: Compliance regulations require businesses to have backup and disaster recovery plans in place. These plans are subject to periodic audits to ensure that they are effective and up to date. Achieve peace of mind: Having a backup and disaster recovery plan in place gives business owners peace of mind knowing that their data and systems are protected, and they can recover from any disruption quickly and efficiently. Conclusion: In conclusion, backup and recovery are critical processes for ensuring the continuity of operations and preventing data loss in case of unexpected events such as hardware failures, natural disasters, cyber-attacks, and human errors. Backup is the process of creating copies of data and storing them in a separate location to restore them in case of data loss. Recovery, on the other hand, is the process of restoring data from backups. There are three methods of backups, including full backups, incremental backups, and differential backups.\nGreetings, everyone! In this article, Ill explain about . Introduction to Network technology of NAS: Network-Attached Storage (NAS) relies on network technologies to facilitate data access and sharing over a network. Here are some key network technologies used in NAS: Ethernet: NAS systems commonly utilize Ethernet as the network technology for data transfer. Ethernet provides a reliable and widely supported network infrastructure, allowing NAS devices to connect to the local network or the internet. It supports various Ethernet standards, such as Fast Ethernet (10/100 Mbps), Gigabit Ethernet (10/100/1000 Mbps), and 10 Gigabit Ethernet (10 Gbps), providing different levels of network bandwidth. TCP/IP: Transmission Control Protocol/Internet Protocol (TCP/IP) is the standard networking protocol suite used for data communication in NAS. TCP/IP enables data packets to be transmitted and received over the network, ensuring reliable and orderly delivery of data between NAS devices and connected clients. It also handles IP addressing and routing, allowing NAS systems to be identified and accessed on the network. Network File Protocols: NAS devices typically support various network file protocols for data access. Common protocols include: NFS (Network File System): A protocol commonly used in UNIX/Linux environments for sharing files over a network. SMB/CIFS (Server Message Block/Common Internet File System ): A protocol used in Windows environments for file sharing and remote access. AFP (Apple Filing Protocol): A protocol used for sharing files between macOS devices. FTP (File Transfer Protocol): A protocol for transferring files over a network, commonly used for remote file access and management.\nHTTP/HTTPS: HyperText Transfer Protocol/Secure (HTTPS) protocols are used for web-based file access and NAS management interfaces. Wi-Fi: Some NAS devices also support Wi-Fi connectivity, allowing wireless access to the NAS system. Wi-Fi connectivity can be useful in scenarios where wired Ethernet connections are not available or when accessing the NAS from mobile devices or laptops. VLAN and Networking Features: Advanced NAS systems may support Virtual LANs (VLANs) and other networking features. VLANs allow the segmentation of a network into multiple logical networks, providing improved network security and isolation. Additionally, NAS devices may offer features like link aggregation (combining multiple network links for higher bandwidth), jumbo frame support, and Quality of Service (QoS) settings to optimize network performance and prioritize certain types of traffic. The specific network technologies and protocols supported by a NAS system may vary depending on the NAS device manufacturer, model, and configuration. It's important to consider network compatibility and performance requirements when selecting and setting up a NAS system in a given network environment. Network technology of NAS Active-Active : In an Active-Active configuration for Network-Attached Storage (NAS), multiple NAS systems are interconnected and work together simultaneously to provide data access and redundancy. The network technology used in an Active-Active NAS configuration is crucial for ensuring efficient data synchronization, load balancing, and failover capabilities. Here are some key network technologies involved in an Active-Active NAS setup: Ethernet: Active-Active NAS systems commonly utilize Ethernet as the underlying network technology.\nEthernet provides a reliable and scalable network infrastructure for interconnecting the NAS devices and facilitating data transfer between them. Link Aggregation: Link aggregation, also known as network bonding or NIC teaming, is a technology that combines multiple network interfaces into a single logical interface. In an Active-Active NAS configuration, link aggregation can be employed to aggregate the network bandwidth of the NAS systems. This enables efficient load balancing and improved network performance by distributing data traffic across multiple network links. Network Switches: Active-Active NAS setups typically require the use of network switches. Network switches provide the necessary connectivity and routing capabilities for interconnecting the NAS systems and clients accessing the data. High-performance switches with features like VLAN support, Quality of Service (QoS), and link aggregation support are commonly used to enhance network efficiency and manage traffic effectively. Network Protocols: Active-Active NAS configurations typically support standard network protocols for data synchronization, such as: Network File Protocols: NAS systems often support protocols like NFS (Network File System) and SMB (Server Message Block) for file sharing and access over the network. These protocols ensure that data changes made on one NAS system are synchronized with the other systems in real-time. Network Replication Protocols: To maintain data consistency and redundancy, replication protocols such as Rsync or proprietary replication mechanisms may be used. These protocols ensure that data modifications are propagated between the active NAS systems, keeping them in sync.\nFailover Mechanisms: In an Active-Active NAS configuration, failover mechanisms are employed to ensure continuous data availability in the event of a NAS system failure. Network technologies like Virtual IP (VIP) or Virtual Router Redundancy Protocol (VRRP) can be utilized to enable seamless failover by redirecting traffic from the failed system to the active system. Network Monitoring and Management: Active-Active NAS setups often include network monitoring and management tools to track the performance and health of the interconnected NAS systems. These tools help in detecting network congestion, identifying potential bottlenecks, and ensuring the smooth operation of the Active-Active configuration. It's important to note that the specific network technologies and configurations used in an Active-Active NAS setup may vary depending on the NAS vendor, software, and hardware capabilities. It is recommended to consult the documentation or guidelines provided by the NAS manufacturer for the specific network requirements and best practices for implementing an Active-Active NAS configuration. Conclusion: In conclusion, network technology plays a crucial role in the implementation of an Active-Active configuration for Network-Attached Storage (NAS). Ethernet serves as the foundational network technology, providing the connectivity and bandwidth required for interconnecting the NAS systems. Link aggregation can be utilized to combine multiple network interfaces and optimize network performance through load balancing. Network switches with advanced features like VLAN support and Quality of Service (QoS) help manage network traffic efficiently in an Active-Active NAS setup.\nDatacenter architecture refers to the design and organization of components within a datacenter facility to support the storage, processing, and management of data and applications. It involves the arrangement of various hardware, software, networking, and infrastructure elements to ensure the efficient and reliable operation of the datacenter. Here are some key components typically found in datacenter architecture: Servers: Datacenters house a large number of servers, which are powerful computers designed to handle processing and storage tasks. These servers can be physical machines or virtualized environments running on shared hardware. Storage Systems: Datacenters require robust storage systems to store and retrieve data. This can include various technologies such as direct-attached storage (DAS), network-attached storage (NAS), or storage area network (SAN) solutions. Networking: Datacenters rely on complex networking infrastructure to facilitate communication between servers, storage devices, and external networks. This includes routers, switches, firewalls, load balancers, and other network equipment. Virtualization: Virtualization technologies enable the creation of virtual instances of servers, storage, and networks. This allows for better utilization of hardware resources, easier management, and flexibility in deploying applications. Power and Cooling: Datacenters consume significant amounts of power and require effective cooling systems to maintain optimal operating conditions for the equipment. Redundant power supplies, uninterruptible power supplies (UPS), and backup generators are often used to ensure uninterrupted operation. Security: Datacenter architecture incorporates various security measures to protect the data and infrastructure. This may include physical security measures like access controls, surveillance systems, and biometric authentication, as well as cybersecurity measures like firewalls, intrusion detection systems (IDS), and encryption.\nManagement and Monitoring: Datacenters employ management and monitoring tools to oversee and control the entire infrastructure. These tools provide administrators with visibility into resource usage, performance monitoring, capacity planning, and issue resolution. Scalability and Redundancy: Datacenter architecture is designed to provide scalability and redundancy to handle increasing data loads and ensure high availability. This may involve clustering servers, implementing redundant networking components, and employing data replication and backup strategies. Single sign-on (SSO) Single sign-on (SSO) is an authentication process that allows a user to access multiple applications with one set of login credentials. SSO is a common procedure in enterprises, where a client accesses multiple resources connected to a local area network (LAN). Eliminates credential reauthentication and help desk requests; thus, improving productivity. Streamlines local and remote application and desktop workflow. Minimizes phishing. Improves compliance through a centralized database. Provides detailed user access reporting.\nFortinet Single Sign-On in Polling Mode for a Windows AD network Adding the LDAP Server to the FortiGate Configuring the Firewall unit to poll the Active Directory Adding an FSSO user group Adding a firewall address for the internal network Adding a security policy that includes an authentication rule Single Sign-On (FSSO) Collector Agent Installing the FSSO Collector Agent Configuring the Single Sign-on Agent Configuring the Firewall unit to connect to the FSSO agent Adding a FSSO user group Adding a firewall address for the internal network Adding a security profile that includes an authentication rule Policy Based Routing In computer networking, policy-based routing (PBR) is a technique used to make routing decisions based on policies it is a way to have the policy override routing protocol decisions Policy-based routing includes a mechanism for selectively applying policies based on access list, packet size or other criteria Demilitarized zone (DMZ) In computer networks, a DMZ (demilitarized zone) is a physical or logical subnetwork that separates an internal local area network (LAN) from other untrusted networks, usually the Internet External-facing servers, resources and services are located in the DMZ so they are accessible from the Internet but the rest of the internal LAN remains unreachable This provides an additional layer of security to the LAN as it restricts the ability of hackers to directly access internal servers and data via the Internet. HIGH AVAILABILITY NETWORKING High availability refers to systems that are durable and likely to operate continuously without failure for a long time.\nThe term implies that parts of a system have been fully tested and, in many cases, that there are accommodations for failure in the form of redundant components. LAYER 2 HIGH AVAILABILITY Access Switch Up-link fail-over redundancy with Distribution or Core Switches Fast convergence across multiple up-links Bandwidth scalability(Through Ether channel) Per-VLAN STP allows for load sharing LAYER 3 HIGH AVAILABILITY The core layer of the network uses Layer 3 switching (routing) to provide the necessary scalability, load sharing, fast convergence, and high speed capacity Layer 3 switching to provide for the appropriate balance of policy and access controls, availability, and flexibility in subnet allocation and VLAN usage For routed layer 3 for WAN using EIGRP or OSPF end-to-end within the organization provides faster convergence, better fault tolerance, improved manageability, and better scalability. Hot Standby Routing Protocol (HSRP) Hot Standby Router Protocol (HSRP) is a routing protocol that allows host computers on the Internet to use multiple routers that act as a single virtual router, maintaining connectivity even if the first hop router fails, because other routers are on standby. Virtual private network (VPN) A virtual private network (VPN) is a technology that creates an encrypted connection over a less secure network. A VPN or Virtual Private Network is a method used to add security and privacy to private and public networks. Site-to-site VPN A site-to-site VPN allows offices in multiple fixed locations to establish secure connections with each other over a public network such as the Internet.\nWhat is Disk Cloning? Disk cloning is the process of copying the contents of one computer hard disk to another disk or to an \"image\" file. How to Do Disk Clone? Disk clone can be used to copy all the data on a disk to another one. Its uses can be embodied in the following aspects: Hard disk upgrade: Disk clone can help to upgrade hard disk from old small disk to a new larger one without reinstallations of operating system and the applications. Data migration: it can help to quickly transfer all the data on a disk to others, such as, Migrate HDD to SSD, etc. Making a disk duplicate: If all the data on the current system installed disk are important, it is necessary to do a complete backup of the source disk by cloning it to a target disk. Data recovery: In order to avoid secondary damage to the hard disk during data recovery, it is necessary to clone the disk and then restore what you want from the cloned disk. Steps of Disk Cloning by Software: When cloning, you can clone a small disk to a large one, vice versa. But there is a precondition. That is the space on the destination disk is able to accommodate all the valid data on the source disk.\nDear All, Today we are going to learn about Cloud Storage Trends Introduction: Cloud storage has revolutionized the way businesses and individuals store, access, and manage their data. With the continuous advancements in technology, cloud storage has witnessed significant developments in areas such as security, scalability, and hybrid cloud solutions. In this article, we will delve into the latest trends in cloud storage, highlighting the innovative approaches and solutions that are shaping the future of data storage in the cloud. Security has always been a top concern when it comes to storing sensitive data in the cloud. To address this, the latest developments in cloud storage focus on enhancing security measures to protect against data breaches, unauthorized access, and data loss. Here are some notable advancements: a. Encryption: Cloud storage providers are implementing robust encryption techniques, both in transit and at rest, to ensure the confidentiality and integrity of data. This includes end-to-end encryption, encryption key management, and the use of advanced encryption algorithms. b. Multi-Factor Authentication (MFA): MFA has become a standard security practice in cloud storage. It adds an extra layer of protection by requiring multiple forms of authentication, such as passwords, biometrics, or tokens, to access data. c. Identity and Access Management (IAM): IAM solutions help organizations control and manage user access to cloud storage resources. Advanced IAM features, such as role-based access control (RBAC) and fine-grained access controls, provide granular control over data access permissions. The scalability of cloud storage refers to its ability to accommodate growing data volumes seamlessly.\nRecent developments in this area have focused on enhancing scalability and elasticity to meet the ever-increasing storage demands. Key trends include: a. Object Storage: Object storage has gained popularity due to its scalability and flexibility. It allows users to store vast amounts of unstructured data and scale up or down based on their requirements. Advanced object storage solutions offer features like auto-scaling and dynamic data tiering to optimize storage utilization. b. Distributed Storage Systems: Distributed storage systems, such as distributed file systems and NoSQL databases, provide horizontal scalability by distributing data across multiple servers or nodes. This architecture enables organizations to handle massive datasets and achieve high performance in cloud storage environments. c. Serverless Storage: Serverless storage services eliminate the need for provisioning and managing infrastructure. They offer automatic scalability and pay-per-use pricing models, making them highly efficient for handling unpredictable workloads and reducing storage costs. Hybrid cloud solutions have gained significant traction as organizations seek a balance between on-premises and cloud storage environments. The latest trends in hybrid cloud storage include: a. Cloud Data Migration: Organizations are adopting efficient data migration techniques to seamlessly transfer data between on-premises infrastructure and the cloud. This involves technologies like cloud data gateways, which facilitate secure and efficient data transfer with minimal disruption. b. Cloud Bursting: Cloud bursting enables organizations to dynamically scale their storage infrastructure by utilizing cloud resources during peak demand periods. This approach optimizes costs by avoiding over-provisioning of on-premises storage and leveraging the scalability of the cloud.\nDear All, Today we are going to learn about BCManager eBackup. BCManager eBackup Product Introduction BCManager eBackup is a backup software product designed for FusionSphere and VMware vSphere as well as cloud platforms. Employing VM snapshot, storage snapshots, and change block tracing (CBT) technologies, eBackup provides comprehensive protection for the VM data. With highlights such as ease-of-use and cost effectiveness, it meets backup requirements of a large number of VMs. Scenarios and use cases Private Cloud Backup Scenario 1: Converged resource pool, cloud data center, governmental cloud scenarios, oriented to large enterprises and user-built cloud data centers, providing backup as a service. Scenario 2: Oriented to traditional carriers such as NFV-I and BSS, providing backup as a service to migrate data to cloud systems. Product model: Serves as the backup engine in the FusionCloud private cloud DR solution, providing tenants with VM and volume data backup and restoration capabilities. Public Cloud Backup Scenario: Public cloud systems constructed based on Huawei's public cloud solution. The typical customers include Huawei enterprise cloud, DT, TLF, and CTC, providing tenants with self-service backup service capability (public cloud VBS). Product model: Serves as a public cloud VBS service backup engine, providing tenants with data backup and restoration capabilities of EVS volumes Highlights Backup of massive VMs Backup servers and backup proxies perform their own functions, forming a scalable distributed architecture. Intelligent VM selection and automatic VM protection simplify maintenance.\nThe user needs a way to schedule Block Snapshot for HyperMetro LUNs. Unfortunately, the storage doesnt have the capability to schedule LUN snapshot creation with HyperSnap feature. The alternative is to use HyperCDP for block to schedule automatic snapshot creation for desired HyperMetro LUNs, however checking thelicense of the device, this feature is not available. Therefore, the option to schedule Snapshots for theHyperMetro LUNs is to use BCManager eReplication which need to be installed on a Linux OS (SUSE or Euler). Working with BCManager eReplication, HyperSnapallows you to configure snapshot policies. Once installed, can create 2 sites in which can add the primary storage in 1st and the secondary in 2nd site, then create a Protection Policy for desired LUNs (HyperMetro and Backup: Disaster Recovery Data CenterSolution (HyperMetro Mode)) which will invoke HyperSnap feature to create snapshots on the storage. When the protection policy is created can specify to create snapshots on both sides and also to configure the retention policy (how many snapshots to keep for each LUN), then the schedule task will automatically create and delete snapshots. Note that after the installation of BCmanager eReplication, without importing a license, you can use all DR services for the first 90 days. After that, you can only use functions provided by the basic edition, however if using only to configure snapshot policies, no need to import any license and this is covered by the basic license, however, if you plan to use other DR functions then after 90 days need to import a valid license.\nObject storage provides superb small object processing performance and comprehensive disaster recovery capabilities. It is fully compatible with Amazon S3. Huawei uses OceanStor 100D to map internal departments and employees to accounts, users, and buckets and limit the capacity of each account and bucket. For example, employee B can upload a maximum of 10 TB data to a bucket. Otherwise, uploading the object fails. The user does not have the resource control attribute. All users under the account share the capacity quota of the account. A bucket quota is the capacity used by a user. Bucket Quota: specifies the upper limit of the bucket capacity. When the bucket capacity reaches the quota, users cannot write data to the bucket. The bucket quota can control object upload only after the quota is set. If the bucket quota is less than the capacity of the uploaded object, the existing object will not be deleted, but new objects cannot be uploaded. If the current quota is less than the capacity of uploaded objects, new objects cannot be uploaded unless the existing objects are deleted. Account Quota: upper limit of the account capacity. When the total size of all buckets in an account reaches the specified upper limit, the account and its users cannot write new data. You can view account and bucket resource statistics on DeviceManager. Bucket resource statistics: bucket capacity, bucket capacity usage, and number of objects Account resource statistics: account capacity, account capacity usage, number of buckets owned by an account, and number of objects.\nGreetings, everyone! In this article, Ill explain about NAS and Object Storage. NAS-An Overview: Network-Attached Storage (NAS) is a specialized storage device or system that provides centralized file storage and sharing over a computer network. It is designed to serve files to multiple clients or users, making it a popular choice for both home and enterprise environments. NAS is a versatile and efficient storage solution that simplifies file sharing, enhances data protection, and centralizes storage management. It provides scalable storage capacity, facilitates collaboration, and offers various features and integration options to meet the diverse needs of home users, small businesses, and large enterprises. Note: For further details of NAS in below my article link. Introduction to NAS, Components, Architecture Object Storage -An Overview: Key Features: Object-Based Approach Scalability: Metadata and Custom Metadata Simplified Data Access: Data Durability and Resiliency: Scalable Metadata Management: Compatibility with Cloud and Web Applications: Data Accessibility and Sharing: NAS and object storage : NAS (Network-Attached Storage) and object storage are two distinct storage technologies with different architectures and use cases. Here's a comparison between NAS and object storage: NAS (Network-Attached Storage): Architecture: NAS uses a file-based architecture, organizing data into a hierarchical file system with folders and files. Access Method: NAS provides file-level access using protocols like NFS (Network File System) or SMB (Server Message Block). Clients can access and manipulate files directly. Use Cases: NAS is commonly used for shared file storage and file server applications. It is suitable for traditional file-based workloads, such as document storage, media sharing, and collaboration.\n. Introduction to NFS protocol The NFS (Network File System) protocol is a widely used network file-sharing protocol that allows remote file access and sharing over a network. It enables clients to access files and directories located on a remote server as if they were local files. NFS is commonly used in Unix and Linux environments, providing a convenient and transparent way to share files and collaborate across multiple systems. Key Features: Here are the key aspects and features of the NFS protocol: Remote File Access: NFS enables clients to access files and directories on a remote server over a network. It provides a mechanism for clients to mount remote directories as part of their local file system hierarchy, allowing them to read, write, and manage files just as if they were stored locally. Transparent File Sharing: NFS provides a transparent and unified view of the shared files. Clients can access remote files and directories using standard file system operations, such as opening, reading, writing, and deleting files. This transparency allows users and applications to work with remote files as if they were local, without needing to be aware of the underlying network access. Client-Server Model: NFS operates based on a client-server model, where the client is the system that accesses the remote files, and the server is the system that shares the files. The server exports directories that are accessible to authorized clients, and clients can mount these exported directories to access the shared files.\nNetwork Transparency: NFS provides network transparency, meaning that clients are shielded from the underlying details of the network communication. They can access remote files regardless of the specific network protocols or configurations used by the server. This allows NFS to work over various network protocols, including TCP/IP and UDP/IP. File Locking: NFS supports file locking mechanisms, allowing clients to coordinate access to shared files and prevent conflicts when multiple clients attempt to access the same file simultaneously. File locking ensures data consistency and prevents data corruption in multi-user environments. Security and Access Control: NFS provides security features to control access to shared files. It supports authentication and authorization mechanisms to verify client identities and enforce access restrictions. Additionally, NFS can be configured to use secure network protocols, such as NFS over Secure Sockets Layer (SSL) or NFS over IPsec, to protect data during transit. NFS has evolved over time, and different versions of the protocol have been released, including NFSv2, NFSv3, and NFSv4, each with enhancements and additional features. NFSv4, in particular, introduced improved security, better performance, and enhanced support for access control. The NFS protocol is a network file-sharing protocol that allows remote file access and sharing in Unix and Linux environments. It provides transparent access to remote files, operates on a client-server model, and supports features like file locking, security, and access control. NFS simplifies collaboration and file sharing across multiple systems, enabling users and applications to work with remote files as if they were local.\nIntroduction to CIFS protocol: The CIFS (Common Internet File System) protocol, also known as SMB (Server Message Block), is a network file-sharing protocol used for sharing files, printers, and other resources between computers on a network. CIFS/SMB is primarily used in Windows operating systems, allowing seamless file access and collaboration across Windows-based systems. Key Features: Here are the key aspects and features of the CIFS protocol: File Sharing and Access: CIFS enables users to share files and access shared files and directories located on remote systems. It provides a mechanism for clients to mount shared resources, such as network drives or folders, as part of their local file system. This allows users to perform standard file operations, such as reading, writing, and managing files, on shared resources as if they were stored locally. Client-Server Model: CIFS operates on a client-server model, where the client is the system that accesses shared resources, and the server is the system that hosts and shares the resources. The server makes its resources available to authorized clients, who can then access and manipulate the shared files. Transparent File Access: CIFS provides a transparent file access mechanism, allowing clients to access shared files and directories using familiar file system operations. Users can browse, open, modify, and save files on remote systems as if they were working with local files. This transparency enables seamless collaboration and file sharing across multiple Windows-based systems.\nCross-Platform Compatibility: While CIFS/SMB is predominantly used in Windows environments, it is also supported by other operating systems, including macOS and Linux. This cross-platform compatibility allows different systems to interoperate and share files using the CIFS protocol. Authentication and Security: CIFS provides authentication and security features to protect shared resources. It supports various authentication mechanisms, including username and password authentication, Kerberos authentication, and NTLM (NT LAN Manager) authentication. Additionally, CIFS can be configured to use secure network protocols, such as SMB over SSL/TLS, to encrypt data during transit and enhance security. Print Services: In addition to file sharing, CIFS also supports print services. It allows clients to connect to shared printers on remote systems and send print jobs to these printers over the network. This enables users to access and utilize printers located on different systems without physically connecting to them. Distributed File System: CIFS includes support for Distributed File System (DFS), which allows the organization of shared resources across multiple servers into a single, logical namespace. DFS enables clients to access shared files and directories using a unified and consistent path, even if the underlying storage is distributed across multiple servers. CIFS has evolved over time, and different versions of the protocol have been released, including CIFSv1, CIFSv2, and CIFSv3. The latest version, CIFSv3, includes enhancements such as improved performance, better security, and support for advanced features like remote file access, file locking, and opportunistic locks. The CIFS protocol is a network file-sharing protocol used primarily in Windows-based environments.\nIt enables seamless file access and sharing across systems, supports a client-server model, and provides features like transparent file access, authentication, security, and print services. CIFS simplifies collaboration and resource sharing in Windows networks, allowing users to access and work with shared files and printers as if they were local resources. Comparison between CIFS and NFS : CIFS (Common Internet File System) and NFS (Network File System) are two widely used network file-sharing protocols, each with its own characteristics and strengths. Here's a comparison between CIFS and NFS: Platform Support: CIFS : CIFS is primarily used in Windows operating systems and offers native support for Windows-based file sharing and collaboration. It is the default file-sharing protocol in Windows networks. NFS : NFS is commonly used in Unix and Linux environments, but it also has support for Windows and other operating systems. It is well-integrated into Unix/Linux distributions and widely supported across different platforms. Transparency and Integration: CIFS: CIFS provides seamless integration with the Windows operating system. It offers transparent file access, allowing users to access shared resources as if they were local files or folders. NFS: NFS also offers transparent file access, providing users with a unified view of remote files and directories. It integrates well with Unix/Linux systems, allowing users to access and manage files seamlessly. Performance: CIFS : CIFS typically offers good performance within Windows networks. However, it can be slower compared to NFS when used in Unix/Linux environments due to differences in implementation and optimizations.\nNFS: NFS is known for its excellent performance, especially in Unix/Linux environments. It leverages efficient caching mechanisms, asynchronous operations, and optimized data transfer protocols, resulting in faster file access and transfer speeds. Network Protocol: CIFS: CIFS operates over TCP/IP, using the Server Message Block (SMB) protocol. It relies on the NetBIOS (Network Basic Input/Output System) protocol for name resolution and network discovery. NFS : NFS operates over UDP (User Datagram Protocol) or TCP/IP, using the NFS protocol. It leverages Remote Procedure Call (RPC) for client-server communication and file operations. File Locking and Concurrency: CIFS : CIFS supports file locking mechanisms to manage concurrent access to shared files. It provides byte-range locks and oplocks (opportunistic locks) to coordinate access and prevent conflicts. NFS: NFS also supports file locking for concurrent access. It provides a range of file locking mechanisms, including advisory locks and mandatory locks, to ensure data consistency and prevent conflicts. Security: CIFS: CIFS offers robust security features, including authentication mechanisms like username/password, Kerberos, and NTLM. It supports encryption through SMB over SSL/TLS for secure data transmission. NFS: NFS has security features but is typically considered less secure compared to CIFS. It relies on the underlying network security measures, such as IPsec, for encryption and authentication. Cross-Platform Compatibility: CIFS: While CIFS is primarily used in Windows environments, it has support for other operating systems. However, interoperability with non-Windows systems can sometimes require additional configuration and setup. NFS: NFS is designed for cross-platform compatibility and is well-supported across different operating systems, including Unix, Linux, macOS, and Windows.\nHello Community! This post encompasses an overview of HyperLock. Please see more information displayed below A file protected by WORM enters the read-only state immediately after data is written to it. In read-only state, the file can be read, but cannot be deleted, modified, or renamed. The WORM feature can prevent data from being tampered with, meeting data security requirements of enterprises and organizations. File systems with the WORM feature configured are called WORM file systems. WORM can only be configured by administrators. There are two WORM modes: Regulatory Compliance WORM (WORM-C for short) and Enterprise WORM (WORM-E). For details about the two modes: Enterprise WORM and Compliance WORM , see Table 1. Table 1 Mode Major Application Scenario WORM-C This mode applies to archive scenarios where data protection mechanisms are implemented as required by laws and regulations. WORM-E This mode is mainly used by enterprises to implement internal control. Financial industry transactions Legal documents Medical Records The Write Once Read Many (WORM) technology enables files to enter the protection state immediately after data is written. A protected file can be read, but cannot be deleted, modified, or renamed. WORM mainly applies to file archive scenarios. WORM has two modes: Enterprise WORM: This mode enables flexible file management, and applies to enterprise internal control. Compliance WORM: This mode protects data in compliance with the law, eliminating legal risks when archiving confidential files. This mode restricts access to data from all users, including administrators. Currently, OceanStor V5 converged storage systems support both Enterprise WORM and Compliance WORM.\nDear All, Today we are going to learn aboutStorage-Layer Performance Tuning Storage-Layer Performance Tuning Storage-Layer Performance Tuning Process Basic Information Collection Basic information collection is the basis for locating problems and optimizing performance on a storage device. It helps specify performance tuning objectives and lays a foundation for planning monitoring parameters. Information that needs to be collected includes the types of services provided by the storage system, I/O characteristics, and a storage resource plan. Performance Monitoring Process on DeviceManager Introduction to SystemReporter SystemReporter can also monitor and analyze the performance of Huawei storage devices. For more information, log in to the Huawei technical support website: https://support.huawei.com/enterprise/ Locating Storage Problems After collecting information, you will need to locate the source of any performance problems. Below is the basic process for accurately determining where problems originated. Internal Transactions and Value-Added Features As mentioned earlier, before optimizing performance, you must check any alarms being reported by the storage system that might be affecting performance, such as those for disks, BBUs, power supplies, port modules, and features. All alarms related to internal transactions or features must be checked and handled first. If there are performance issues, check for any internal transactions or value-added features. CPU Performance Analysis High CPU usage by a storage system increases the scheduling and I/O latencies. You can use SystemReporter or the CLI to query the CPU usage of the current controller. A storage system's CPU usage varies with different I/O models and networking modes. Write I/Os consume more CPU resources than read I/Os.\nRandom I/Os consume more CPU resources than sequential I/Os. IOPS-intensive services consume more CPU resources than bandwidth-intensive services. iSCSI networks consume more CPU resources than Fibre Channel networks Performance Analysis of Front-End Ports Before analyzing the performance of front-end ports, confirm the locations of interface modules, and the number, working status, and speed of connected ports. Key performance indicators of front-end ports include the average read I/O response time, average write I/O response time, average I/O size, IOPS, and bandwidth. Cache Performance Analysis With a cache, a host that attempts to read or write data does not need to access disks each time, which improves the storage system's read/write performance . A cache is a key module in improving performance and user experience. Most cache-related problems are reported in storage fault alarms, so preferentially handling these alarms can resolve most storage performance problems. Cache Prefetch Policy Upon receiving a read request from a host, the storage system reads data from disks to the cache and starts prefetching data. In this way, data that may be subsequently read is prefetched to the cache based on the data sequence, improving the hit ratio of subsequent read requests. Cache Write Policy The default cache write policy is write-back. Write-through may be triggered when a BBU fails; only one controller is working; the temperature is too high; or the number of LUN fault pages is higher than the threshold.\nHigh and Low Watermarks LUN Performance Analysis Analyzing LUN performance will allow you to know LUN types, see how local access to LUNs is impacting performance, and discover potential bottlenecks in a storage system. When SmartThin is used, the read and write performance of a thin LUN is different from that of a thick LUN. LUN Local Access The host and controller A are physically connected. I/Os from the host can only be sent to controller A. Controller A is the working controller of LUN 1. Controller B is the working controller of LUN 2. LUN 1 is accessed locally. LUN 2 is accesse remotely. The peer access scenario involves the mirror channel between controllers. The channel limitations affect LUN read/write performance. Tuning suggestions: The host to which a LUN is mapped must be physically connected to the owning controller of the LUN to ensure local access. RAID Performance Analysis For different I/O models, different RAID levels vary greatly in read and write performance. The impact of RAID on system performance is mainly reflected by the RAID level and stripe depth. For traditional RAID, the number of member disks in a RAID group also affects system performance Stripe Depth The stripe depth determines the sizes of I/Os written to disks after host I/Os are processed based on the RAID algorithm. For I/Os with different characteristics, the stripe depth affects performance to some extent.\nWhy Do You Need NAS Backup? HI HI, Greetings! HI April, Today, I would like to explain the overall NAS Backup and requirements. Let's get into the article. Data backup from a Network-Attached Storage (NAS) device to a different place, such as an external hard drive, cloud storage, or another NAS device, is referred to as NAS backup. To guard against data loss brought on by hardware failure, unintentional deletion, cyberattacks, or natural catastrophes, NAS backup is crucial. Organizations and individuals who need to store large amounts of data in a central place and share it across multiple devices frequently choose NAS devices. They are frequently used to store crucial information like financial records, client data, and essential documents. Backup is essential because NAS systems are susceptible to data loss. NAS backup can be done automatically with backup software or manually by transferring data to an external device. Many NAS devices also include backup features that let users select a backup location and plan recurring backups. Due to the following factors, NAS backup is highly required for Organizations and individuals. Protect Your Data from Loss: Your data must be safeguarded with NAS backup options in case of hardware failure, natural disasters, cyberattacks, or accidental deletion. You can quickly restore your data if it is lost or damaged by creating a backup of your data. Make sure data is accessible: Even if the original data is lost or damaged, NAS backup options help make sure that your data is always accessible.\nThis is particularly crucial for companies whose operations depend on data. Time and effort savings: You can save time and effort by automating the backup process with NAS backup options. As a result, you won't need to manually copy data to an external device or stress about routinely remembering to back it up. Boost Security: Your data's security can also be improved by NAS backup options. You can lessen the chance that your data will be lost as a result of hacker assaults, theft, or other security flaws by backing it up to a different location. Scalability: You can simply scale NAS backup solutions to suit your changing needs. You can simply expand the backup storage capacity of your NAS device as your data storage requirements increase, or you can decide to back up your data to a cloud-based backup service. NAS drives can be backed up in a variety of ways depending on the destination and data transmission protocol being used: Backup from NAS to USB Backup from NAS to NAS Backup from NAS to Cloud Backup from NAS to NDMP Backup from NAS to USB NAS systems typically have one or more USB ports where USB storage devices can be connected. A USB hard drive (HDD) can be connected to such a connection so that you can back up the necessary data from the NAS to it. The NAS vendor's web interface is used to manually manage the NAS device during this process.\nPros Cons Backup from NAS to NAS Configuring backup storage on a NAS device or another server is required for NAS to NAS backup, which also includes saving up data from NAS devices to other kinds of servers. Using a file share on one of these with the supported file-sharing protocol, data can be copied from the main NAS to the destination backup server. To conduct a NAS storage backup, you must link one NAS to a file share set up on another NAS and copy the required files. The native NAS web interface allows for the execution of these tasks. You obtain a precise duplicate of the data as a result. You can make a backup repository in a specific format on the destination NAS or server to be used as backup storage by employing a specialized backup solution. Advanced features like automation, change monitoring for incremental backups, compression, and encryption are applicable in this situation. Pros Cons Backup from NAS to Cloud When backing up NAS data, which is often increasing exponentially, cloud vendors offer easily scalable storage. When NAS data are backed up to a USB HDD, there is no such restriction. Pros Cons Backup from NAS to NDMP A network protocol called NDMP (Network Data Management Protocol) was created to regulate contact between storage systems installed on various machines, including NAS devices. This protocol has been modified to allow network backups of NAS and file systems.\nFor NDMP, there are two kinds of traffic connections: Manage the traffic generated by backup apps that transfers data. The flow of data through a network from a source device to a destination device. Another NDMP feature that makes it easier to create and configure backup software is standardized backup agents. Another benefit of the open NDMP standard is that standard ports can be used for NAS data backup. NDMP is designed with file backup in mind. Data from NAS can be immediately backed up using NDMP to a backup server or tape. Pros Cons The way of securing NAS Backup: A second NAS computer, installed as a NAS offsite backup or on-premises, is frequently used as your NAS backup storage. Since your backup NAS houses the same sensitive data as your main NAS and could be compromised if it is, it is essential to follow security best practices there as well. Updates should be made with care: Maintaining an updated server is one of the simplest methods to enhance the security of your NAS backups. Updates can help NAS speed up and close security holes before problems are exploited. Regularly check for changes or create alerts for when new updates are made available. The updates will be applied as quickly as possible thanks to this. Similarly, make sure that any accompanying software you employ is routinely updated. Make two-factor verification available: Username and password are combined with a second form of identity in two-factor authentication. For instance, transmitting a pin to a user's smartphone.\nYou can make sure that only authorized users are able to access your backup memory, data, and systems by using two-factor verification. It can assist in reducing the danger of shared credentials and the possibility that stolen credentials will allow unauthorized access. Access using HTTPS: Make sure you are using an HTTPS connection if you are accessing your NAS offsite. This encrypts your connection to guarantee that requests and data cannot be intercepted or changed while being transferred. If you haven't deliberately activated HTTPS, your connection is probably not encrypted. Installing an SSL certificate and connecting it to a domain name connected to your NAS's IP address is required before you can activate HTTPS. You can obtain this license using a number of free services or you can pay for one that requires only a yearly renewal. Establish a firewall: Any security plan must include a firewall. You can use it to keep an eye on and filter data going to your NAS so that only authorized connections are accepted. Make sure your firewalls are turned on and that your security procedures are adhered to by your filtering policies on a regular basis. If at all possible, attempt to use whitelisting rather than blacklisting when configuring your policies. Blacklisting only blocks known malicious traffic while whitelisting only lets in approved traffic. This indicates that while whitelisting prevents intruders from unknown addresses, blacklisting may. Difference between NAS vs DAS There are two distinct kinds of storage solutions: NAS (Network Attached Storage) and DAS (Direct Attached Storage).\nHi Community Members, Today I will explain to you about Connecting the Storage to the Server: Step by Step Instructions and Procedures. General information and definitions Scheme of connecting the storage system to the server via SCSI, SAS, FC Helpful Hints First, let's go over the terminology.Storage systems(or storage networks) in the circle of specialists are called SAN (Storage Area Network).The main components of a SAN are drives and controllers.In order to evenly distribute the load between disks and servers, without losing speed and reliability, they use several I / O paths, at least two connection interfaces and HBA (Host Bus Adapter) - a dedicated controller board that connects disks. Communication between blocks is provided by switches.They are also responsible for belonging to certain resources.For these purposes, each SAN has its own WWN address (World Wide Name), a kind of network analogue of the MAC address. The connection is made via the LUN interface (Logical Unit).This is somewhat analogous to a partition on a hard drive but used for controllers.With this approach, the server sees the LUNs as physical copies.A useful approach, because in the future, through the LUN, you can connect an unlimited number of physical and cloud capacities into a single system, forming one full-fledged disk.From it information on servers will be distributed. Distribution occurs through Disk Manager or through LAN / SAN Mapping level utilities.The host itself does not see the LUN interface.\nExperts note that it is not worth attaching all available LUNs to a single controller - quickly reboot the system.Always share power. When creating a storage system, you can use both SATA and SAS disks.Depends on budget and needs. The SAN connection to the host itself is performed according to the following principle: physical layout; software setting. And if you are at least three times a well-deserved admin with the most beautiful tambourine and chronically red eyes (the beard, of course, should be long and full), do not forget to follow the instructions.The manufacturer always puts it in the kit to minimize risks and reduce the number of classic calls to the service center. Let's skip the differences between manufacturers and focus on the essence.The basic process of creating a system is as follows: Install the HBA controller in a free PCI slot on the server. Connect the SAN to the appropriate interfaces that are implied by the disk storage (SCSI, SAS, FC). Set up the network connection, define the ports used for the optical network. Download the official software if it is not provided in the kit (often this is necessary, because most servers have no CD-drive for a long time). Install the software with further registration, if necessary. Connect the host HBA and the disk shelf (rack with drives) to the same subnet on the physical switch. Temporarily disable the firewall to avoid unnecessary errors and system disturbances.\nRun the storage scan program, which will determine the network topology, and at the same time show the number and numbers of devices. Do a host registration on the shelf.The task can be performed automatically (recommended), or manually through the web-face. Enter the IP of the host, associating it with the disk shelf.Also don't forget to set the administrator password on the first start. Log into the storage system and verify that all drives are working properly as standard SCSI devices. Navigate using a browser to the storage system by IP address, and at the same time make sure that there are servers that have been registered in the array. Open the tab with initiators that link storage units to hosts.Additionally, check the IP addresses and, if necessary, enter them manually. Link together hosts and LUNs, only sequentially. If everything is done correctly, then the corresponding icon will appear next to the HBA section. As soon as we figured out the shelf, it's time to go to the software part and do the settings.This is done differently than on classic home PCs.As they say, there are nuances. First, you need to cut the disks into LUNs and leave 1-2 GB of free space just in case.Next, transfer all drives to the \"online\" status, but do not assign letters to anyone - the features of the work.But it's worth formatting (NTFS. ExFAT or another algorithm at the discretion of the administrator). We complete the configuration by installing HBA drivers for FC, SAS, or SCSI.\nI have hands on experienced with integrated & implemented as POC on customer site. Overview of OceanProtect X8000: OceanProtect X8000 can be deployed in two forms: all-flash and hybrid-flash. Product Advantages: 1- Fast backup and restore: 2- Low resource consumption: 3- High reliability: Veeam Backup & Replication Version 11: Veeam Backup & Replication provides data protection and disaster recovery Solution Architecture: This solution backs up VMs to OceanProtect X8000 by using the Veeam Backup & Replication11 backup software. The functions of each component shown in above Figure are as follows: VMware vSphere: VMware virtualization environment for VM provisioning. Backup server: Veeam Backup & Replication is installed on the backup server. As a core component of the backup infrastructure, the backup server is responsible for performing all types of administrative activities, such as coordinating backup, replication, recovery verification, and restore tasks, controlling job scheduling and resource allocation, and setting and managing backup infrastructure components. In addition, the backup server also performs the roles of the default backup proxy and the backup repository. Backup proxy: It operates as a data mover that transfers data between the source file and the backup repository. It also processes backup jobs and delivers backup and restore traffic. For VMware backup, if the Hot-Add transport mode is used, VMs must be used as backup proxies. Gateway server: It connects the backup storage and the backup server. Generally, the backup proxy also serves as the gateway server. In Hot-Add transport mode, the backup proxy is a VM.\nA storage Architecture in which data is separated into fixed-sized blocks and accessed directly by the storage system is known as block storage architecture. Each block has its own address and can be read or written to independently. In storage area network (SAN) contexts, block storage systems are extensively employed. Among the applications of block storage architecture are: 1. Database Systems: Block storage is commonly utilized in database applications that require high performance and low latency. Databases frequently demand random access to specific data blocks, and block storage enables fast read and write operations at the block level. 2. Virtualization: Block storage is required for virtualization technologies such as VMware and Hyper-V. Block storage is commonly used as virtual disks by virtual machines (VMs), giving direct access to data blocks and enabling capabilities such as live migration and high availability. 3. High-performance Computing (HPC): Block storage is employed in HPC contexts where storage access must be quick and low-latency. Block storage is frequently used in scientific research, simulations, and data analysis to provide the performance required for complicated computations. 4. Enterprise Applications: Block storage is useful for many enterprise applications, including as enterprise resource planning (ERP) systems, customer relationship management (CRM) platforms, and file servers. The design enables high-speed data access and the scalability required by enterprise-level workloads. 5. Storage Area Networks (SANs): A key component of SANs is block storage. Storage area networks (SANs) are specialized networks that connect storage devices to servers, allowing numerous servers to share storage resources.\nDear All, Today we are going to learn about What is pem file? A PEM (Privacy-Enhanced Mail) file is a standard file format that is widely used for storing and transmitting cryptographic keys, certificates, and other secure data. It is commonly associated with the X.509 certificate standard and is used in various cryptographic applications, particularly in the context of SSL/TLS certificates and public-key infrastructure (PKI). Here are the features, benefits, and use cases of PEM files: Features: Encoding Format: PEM files use Base64 encoding, allowing binary data to be represented in ASCII text format. This makes them easily readable and transferable across different systems and protocols. Privacy and Security: PEM files can store sensitive cryptographic information securely. They can contain private keys, public keys, X.509 certificates, Certificate Signing Requests (CSRs), and other cryptographic objects. Flexible and Extensible: PEM is a flexible format that can accommodate different types of data. It supports multiple types of cryptographic data, including keys, certificates, and miscellaneous information such as Certificate Revocation Lists (CRLs) and Certificate Authorities (CAs). Benefits: Compatibility: PEM files are widely supported by various cryptographic libraries, tools, and applications. They can be used with different programming languages and operating systems, making them highly interoperable. Human-Readable: The ASCII text format of PEM files makes them easily readable by humans, simplifying the process of viewing, editing, and sharing cryptographic data. Transportability: PEM files can be easily transferred and shared via email, file transfer protocols, and other means. The text-based format ensures that they can be transmitted without issues of binary corruption.\nUse Cases: SSL/TLS Certificates: PEM files are commonly used to store SSL/TLS certificates and private keys for securing web servers, applications, and network communication. They are used in configuring web servers like Apache or Nginx, and in the setup of HTTPS-enabled websites. Certificate Authorities: PEM files play a crucial role in the PKI infrastructure by storing and distributing CA certificates, intermediate certificates, and certificate chain information. Code Signing: PEM files can be used to store private keys and certificates for code signing purposes, providing authenticity and integrity to software applications and ensuring they have not been tampered with. Secure Email Communication: PEM files can be used in email encryption and digital signatures, enabling secure communication between users and ensuring the authenticity and privacy of email content. It's important to note that while PEM files are widely used and have several benefits, they should be handled with care and protected appropriately since they contain sensitive cryptographic information. The functioning of PEM files revolves around the encoding and decoding of cryptographic information using Base64 encoding. Here's a high-level overview of how PEM files work: Data Encoding: The data, such as cryptographic keys or certificates, is first encoded using Base64 encoding. Base64 encoding converts binary data into ASCII text format, making it readable and transportable. Header and Footer: PEM files include header and footer sections that indicate the type of data stored within the file.\nHi, everyone, Today we will configure a HOST in Huawei Dorado Storage V6 using an iSCSI initiator. The initiator for the host will be added to establish a mapping storage and host. First, access Huawei Dorado Storage Management portal and login using the admin credentials. Storage Health is Good. We can navigate to common operations tabs to configure the LUN, HOST and perform the mapping. We will also install the Huawei recommended multipathing Software ULTRAPATH on HOST and scan/manage the configured LUN. You can also use Services tab to configure the block and host services. Install Ultrapath software on Linux Host OS. Media can be downloaded for desired OS from Huawei support. Configure the Front-Ent port to scan the ultra-path hosts. Service Ports configuration is required for iSCSI traffic. Ideally create a bond network using physical ethernet and then create logical ports on top of it. Currently LUN is not presented in HOST OS. We have verified the status by using ultrapath upadm utility. Login to HOST and get the IQN number. By default, iSCSI service is stopped and in disabled state. Make sure to start and enable the service. In Huawei Dorado Storage, Create an Initiator for the HOST add the IQN and Alias Name. Now Create a HOST, provide Name, OS, and IP address. Select the initiator created in above step. Host was created successfully. Once LUN is created it would be in an unmapped state. Map the LUN to desired HOST. LUN has now been successfully mapped.\nThe OceanStor Dorado Series is another line of storage systems offered by Huawei. It is specifically designed to provide high-speed, low-latency storage solutions using all-flash technology. The Dorado Series offers advanced features and capabilities to meet the performance demands of mission-critical applications. Here are some key aspects of the OceanStor Dorado Series: All-Flash Storage: The Dorado Series is built on all-flash technology, utilizing solid-state drives (SSDs) to deliver superior performance compared to traditional spinning disk storage systems. This results in significantly reduced data access latency and higher IOPS (Input/Output Operations Per Second), making it ideal for workloads that require fast and responsive storage. High Performance and Efficiency: The Dorado Series leverages Huawei's proprietary technologies, such as the intelligent multi-protocol optimization engine and the smart I/O acceleration engine. These technologies optimize data access and processing, improving overall system performance and efficiency. Data Deduplication and Compression: The Dorado Series incorporates advanced data reduction technologies, including deduplication and compression, to optimize storage capacity utilization. By eliminating redundant data and compressing data at a higher ratio, organizations can maximize their storage efficiency and reduce costs. High Availability and Reliability: The Dorado Series features built-in redundancy and data protection mechanisms to ensure high availability and data integrity. It supports features such as RAID protection, multi-controller architecture, and remote replication to minimize the risk of data loss and ensure business continuity. Intelligent Management and Monitoring: The Dorado Series comes with a comprehensive suite of management tools and software that enable efficient storage administration.\nThe OceanStor V3 Series is a line of storage systems offered by Huawei. It is designed to meet the storage needs of enterprise-level applications and provides a range of features to ensure high performance, scalability, and reliability. Here are some key aspects of the OceanStor V3 Series: High Performance: The OceanStor V3 Series utilizes advanced hardware and software technologies to deliver high-speed data access and processing. It supports various storage protocols, including Fibre Channel (FC), Internet Small Computer System Interface (iSCSI), and Network-Attached Storage (NAS), catering to different application requirements. Scalability: The OceanStor V3 Series offers scalability to accommodate growing storage demands. It allows for the expansion of storage capacity by adding additional drives or enclosures, enabling organizations to easily scale up their storage infrastructure as their data needs increase. Data Protection and Reliability: The series incorporates multiple data protection mechanisms to ensure the safety and integrity of stored data. It supports features such as RAID (Redundant Array of Independent Disks), snapshot, remote replication, and data deduplication to safeguard against data loss and provide data redundancy. Unified Storage: The OceanStor V3 Series provides unified storage capabilities, allowing organizations to consolidate both block-level (SAN) and file-level (NAS) storage onto a single platform. This simplifies storage management and reduces infrastructure complexity. Advanced Storage Management: The OceanStor V3 Series includes a comprehensive suite of management tools and software to facilitate efficient storage administration. It offers features such as storage provisioning, performance monitoring, data migration, and system monitoring, enabling administrators to effectively manage and optimize their storage resources.\n, the differences between RoCE VS iWARP. Introduction to RoCE: RoCE, which stands for RDMA over Converged Ethernet, is a network protocol that enables the efficient and low-latency transfer of data between servers and storage devices over Ethernet networks. It combines the benefits of Remote Direct Memory Access (RDMA) technology with the widespread availability and familiarity of Ethernet infrastructure. RDMA allows data to be transferred directly between the memory of one computer system and another, bypassing the involvement of the operating system and reducing CPU overhead. This results in faster data transfers, lower latency, and reduced network congestion. RDMA has been widely adopted in high-performance computing (HPC) and data center environments for its ability to enhance performance and scalability. RoCE extends RDMA capabilities over Ethernet networks, which are commonly used in enterprise environments. It enables applications and servers to leverage the advantages of RDMA without the need for dedicated InfiniBand or specialized networking hardware. With RoCE, organizations can take advantage of their existing Ethernet infrastructure while still benefiting from the performance and efficiency gains of RDMA technology. Key Advantage: One of the key advantages of RoCE is its ability to provide a lossless and low-latency transport mechanism for data-intensive workloads. It allows for high-speed data transfers with minimal CPU involvement, enabling applications to fully utilize available network bandwidth and reduce overall latency. This is particularly beneficial for applications that require fast and efficient data movement, such as database replication, real-time analytics, and storage systems.\nRoCE also supports network convergence by enabling the coexistence of traditional Ethernet traffic and RDMA traffic on the same Ethernet fabric. This allows organizations to consolidate their networking infrastructure and simplify management, while still maintaining the performance benefits of RDMA for specific workloads. It's important to note that RoCE requires both network adapters and Ethernet switches that support the protocol. The network adapters must have RDMA capabilities, and the Ethernet switches should be able to handle lossless Ethernet traffic and provide the necessary support for RoCE. In summary, RoCE is a network protocol that enables RDMA over Ethernet networks, offering the advantages of RDMA technology while leveraging existing Ethernet infrastructure. It provides high-performance, low-latency data transfers and supports network convergence. RoCE is widely adopted in data center and HPC environments, where fast and efficient data movement is critical for demanding workloads. Introduction to iWARP: iWARP (Internet Wide Area RDMA Protocol) is a network protocol that extends the benefits of Remote Direct Memory Access (RDMA) technology over standard TCP/IP networks. It enables efficient and low-latency data transfers between servers and storage devices, leveraging the ubiquity and compatibility of TCP/IP infrastructure. RDMA technology allows direct memory-to-memory data transfers between systems, bypassing the involvement of the operating system and reducing CPU overhead. This results in faster data transfers, lower latency, and reduced network congestion. Traditionally, RDMA has been implemented using specialized networking technologies like InfiniBand. However, iWARP enables RDMA over standard Ethernet networks using TCP/IP.\nBy utilizing TCP/IP, iWARP provides compatibility with existing Ethernet infrastructure and requires no special network switches or adapters. It leverages the widespread use of Ethernet and the familiarity of TCP/IP protocols, making it easier for organizations to adopt RDMA technology without significant infrastructure changes. Key Advantage: One of the key advantages of iWARP is its ability to provide high-performance and low-latency data transfers over long distances. It is particularly suitable for wide-area networks (WANs) and geographically distributed systems. iWARP can deliver RDMA benefits across remote sites, enabling efficient data movement and access between data centers or cloud environments. iWARP achieves this by encapsulating RDMA operations within TCP/IP packets, effectively leveraging TCP/IP's reliable delivery and congestion control mechanisms. This ensures reliable and lossless data transfers even over long-haul networks. Additionally, iWARP includes features such as congestion control and flow control mechanisms to optimize performance and prevent network congestion. By enabling RDMA over TCP/IP, iWARP allows organizations to take advantage of the benefits of RDMA technology without the need for specialized networking hardware or protocols. It enables applications to achieve high-performance data transfers, reduce latency, and improve overall system efficiency. This is particularly beneficial for data-intensive workloads, such as storage systems, distributed databases, and real-time analytics. It's important to note that both the network adapters and the operating system software need to support iWARP for it to be effectively utilized. The network adapters must have RDMA capabilities, and the operating system needs appropriate drivers and software support for iWARP.\nIn summary, iWARP is a network protocol that extends RDMA technology over standard TCP/IP networks. It enables efficient and low-latency data transfers, leveraging the compatibility of Ethernet and TCP/IP infrastructure. iWARP is suitable for wide-area networks and enables high-performance data movement over long distances. It allows organizations to adopt RDMA technology without specialized hardware or protocols, benefiting data-intensive applications and distributed systems. The differences between RoCE VS iWARP : RoCE (RDMA over Converged Ethernet) and iWARP (Internet Wide Area RDMA Protocol) are two different protocols that enable the use of Remote Direct Memory Access (RDMA) technology over standard Ethernet networks. While both protocols aim to provide efficient and low-latency data transfers. There are some key differences between RoCE and iWARP, Underlying Transport Protocol: RoCE: RoCE operates directly over the Ethernet protocol, allowing RDMA operations to bypass the TCP/IP stack. It utilizes the InfiniBand verbs interface for communication between the application and the RDMA hardware. iWARP: iWARP is built on top of TCP/IP, using TCP as the transport protocol for RDMA operations. It encapsulates RDMA messages within TCP/IP packets and relies on TCP's reliable delivery and congestion control mechanisms. Network Stack Compatibility: RoCE: RoCE requires network adapters and Ethernet switches that explicitly support the RoCE protocol. These components must have support for lossless Ethernet traffic and be able to handle RoCE-specific requirements. iWARP : iWARP operates over standard Ethernet infrastructure and utilizes standard TCP/IP protocol stack. It doesn't require specialized network adapters or switches, making it compatible with existing Ethernet networks.\n1.What is cloud storage? Answer :Cloud storage refers to a service that allows you to store and access data over the internet on remote servers. It provides an alternative to storing data locally on physical devices such as hard drives or servers. 2.How does cloud storage work? Answer:Cloud storage providers maintain large data centers with multiple servers. When you upload your files to the cloud, they are distributed and stored across these servers. The data is usually replicated to ensure redundancy and protect against data loss. You can then access your files from any device with an internet connection. 3.What are the benefits of using cloud storage? Some benefits of cloud storage include: Accessibility: You can access your files from anywhere using any internet-connected device. Data backup and recovery: Cloud storage provides a reliable backup solution, ensuring your data is protected from hardware failures or other disasters. Scalability: Cloud storage allows you to easily expand your storage capacity as your needs grow. Collaboration: Cloud storage often includes collaboration features, enabling multiple users to work on and share files in real-time. Cost savings: Cloud storage eliminates the need for maintaining expensive on-premises storage infrastructure, potentially reducing costs. 4.Is cloud storage secure? Cloud storage providers implement security measures to protect your data, including encryption, access controls, and data redundancy. However, security breaches can still occur, so it's important to choose a reputable provider and take additional precautions such as using strong passwords and enabling two-factor authentication. 5.How much does cloud storage cost?\nThe cost of cloud storage varies depending on the provider, the amount of storage you require, and additional features you may need. Some providers offer free plans with limited storage, while others offer tiered pricing based on storage capacity and additional services. 6.Which are the popular cloud storage providers? There are several popular cloud storage providers, including: Google Drive Dropbox Microsoft OneDrive Amazon S3 (Simple Storage Service) Apple iCloud Box pCloud Mega 7.Can I sync my files across multiple devices with cloud storage? Yes, most cloud storage providers offer synchronization capabilities. By installing the provider's desktop or mobile app, you can automatically sync files between your devices and the cloud storage, ensuring that you have the latest version of your files across all devices. 8.How much storage space do I need in the cloud? The amount of storage space you need depends on your specific requirements. Consider factors such as the type of files you want to store, their size, and whether you plan to use cloud storage for personal or business purposes. Most providers offer various storage plans, allowing you to choose the one that suits your needs. 9.Can I share files and folders stored in the cloud with others? Yes, cloud storage providers often offer file and folder sharing functionality. You can generate shareable links or invite specific individuals to access and collaborate on files and folders stored in your cloud storage. 10.What happens to my files if I cancel my cloud storage subscription?\nLinux is one of the most powerful and efficient operating systems that provides a fast and reliable option for directory and permission management. It is a difficult step to learn how to use the Chmod command and modify the file directory, management, and permission of the file effectively and encrypted. File permission helps to decide who can read, write, and execute the file directory and who can manage and modify them accurately providing security and controlling features to chmod commands in Linux. The basic chmod command in Linux includes the symbol that is numeric and alphanumeric and the syntax for permission and how to securely change permission. We will see how to use the chmod command in Linux in this article. The term Chmod refers to the change mode in Linux that is to modify the permission to access the directory and files. The basic syntax for Linux is as follows: Syntax: chmod (option)(mod0e )(file) The Linux based system helps to define the permission to access the users or groups to access the particular file in different modes, syntax and option that are as follows: Mode: it is the permission mode that is set to file and directory to access. It is basically the combination of numbers and letters. File: it helps to change the directory of files and allows to change the permission Options: option flags help to modify the behavior of chmod command.\nMode: it consists of the three different types numeric, symbolic and octal mode The first is the numeric modes that comprises of 3 digits to describe the permission for the owner and others each digit has a value from 0 to 7 0: No permission 1: execution permission 2: write permission 3: write and execute permission 4: Read permission 5: read and execute permission 6: read and write permission 7: read, write, and execute permission For instance, if the owner want to give an instruction to read, write and execute he will give the following chmod command: Syntax: chmod 742 file.txt 7 represent the read, write and execute permission, 4 represent the read permission and 2 represents the write permission. It also provides the specific permission using letters u for user g for group o for other a for all The second is the symbolic mode that specifies using the symbols such as: + for addition - for subtraction = for setting rfor read w for write x for execution u=rw read and write permission to the user /owner g=r read permission to the group o=r read permission to others. The third is the octal mode that allows the three digits to represent a single option for instance. Syntax: chmod 644 File.txt chmod u=rx, g=rx, o= file.txt chmod -R u=rw, g=r, o=r directory chmod a=x file.txt It gives the read and write only permission to users and read only to others.\nOptions: There are different options for Linux chmod command that are as follows: -c or change: it displays when permission is changed -f or silent, quiet: it suppresses the error message -R or recursive: it allows to change the permission recursively for file directories -v or verbose: it displays the permission when changed and additional information regarding the change. -H or dereferences it prevents chmod to generate symbolic and permission links. -h or help: it displays the options for help on chmod and details -version: it describes the details of version - -: this option declares the end of options It is important to use the \"man chmod\" to get the option list on Linux that helps to provide the control for permission change and access to the directory files in Linux. There are several applications of chmod in real life that includes: It helps to get permission to access files and directories using the chmod we can restrict access tk sensitive data, files private keys and personal data to only allow the user to read , write and execute removing all others not to access the file. It manages the file directory to allow users to read, write and other to only read and hence it is an established way to manage the files. It allows the scripts and programs to run and perform the intended actions while preventing the unauthorized execution by any other or group.\nToday we will configure and learn a HOST in Huawei Dorado Storage V6 using iSCSI initiator. Initiator for the host will be added to establish a mapping storage and host. Host Configuration of OceanStor Dorado V6 Configuration Process Logging In to DeviceManager Check Before Configuration Operation Modify Host Access Mode Asymmetric OK OK Host Configuration Multipath I/O Server Manager Manage Add Roles and Features Multipath I/O VID indicates to vendor ID, example, HUAWEI . PID indicates the product ID, examples, S5500T , S5600T , or XSG1 . mpclaim HUAWEI XSG1 MPIO-ed NO Note: PID and VID in this section are only examples . mpclaim -r -i -dHUAWEI XSG1 ( Note, The VID must contain 8 characters and the PID must contain 16 characters. If the characters are insufficient, add spaces. Copy the VID-PID from the output of the mpclaim -e command. . View MPIO policies. Host Access-Mode Asymmetric Round Robin With Subset the services Properties MPIO Enable path verification. MPIO Details Path Verify Enabled OK mpclaim -s -d mpclaim -s -d) iSCSI pre-configurations at HOST Currently LUN is not presented in HOST OS. We have verified the status by using ultrapath upadm utility. Login to HOST and get the IQN number. By default, iscsid service is stopped and in disabled state. Please make sure to enable the start services . Initiator Creation in Dorado In Huawei Dorado Storage Create an Initiator for the HOST, add the IQN and Alias Name. HOST Creation Now Create a HOST, provide Name, OS and IP address.\nDNA storage is one of the most futuristic technologies of data storage. This concept allows the system to store data in DNS molecules. The most tempting feature of this technology is that it has overcome the limitations of using silicon, achieving a high density and storage capacity of up to 1 Exabyte. Also, DNA is more durable with its life extension for more than 500 years. On the flip side, it is tedious, time-consuming to read, write on DNA, and involves great expenses. To be more realistic, the crystal technology trend promises data storage in a minute and long-lasting glass structures. These crystals are made with lasers that encode data in microscopic structures of glass surfaces. As all of the five dimensions of quartz are utilized, a huge amount of data can be stored in these crystals. For its capacity, this type of emerging technology is also known as a 5D tech. Since crystal technology is at its initial stage, the exact structures and commercial patterns cannot be commented upon. When the scalability of cloud storage is in question, blockchain storage structures are the perfect answer. They are cost-efficient and are less prone to damages by disasters. The biggest advantage of blockchains is that the data is spread over nodes and in case one of the nodes gets disrupted, a replica of data is automatically recovered from other nodes. Smart storage options involve cloud storage.\nThese data storage trends for 2023 cover ways vendors are working on improvements, as well as customer attitudes toward how they store data. 1. Storage as a service The basic idea behind storage as a service is that major vendors offer couldlike pricing for storage resources located in an organization's own data center. Instead of paying a hefty upfront price for an enterprise storage array, organizations can opt for consumption-based pricing instead. Storage as a service is ideal for organizations that like the public cloud pricing model but have certain data that needs to reside on-premises. At the same time, organizations that adopt storage as a service will have to watch out for minimum spending requirements and the potential for price increases when it is time to renew a contract. 2. CXL adoption Micron pioneered the Compute Express Link (CXL) standard, as a way of getting around the problems caused by proprietary memory interconnects. Consider, for example, how Intel Optane persistent memory only works on computers that are equipped with Intel CPUs because of Intel's use of a proprietary interconnect. CXL will essentially act as a universal memory interconnect. Although CXL is first and foremost a memory interconnect standard, it will undoubtedly prove useful for high performance storage. Earlier this year, for example, Samsung announced a CXL-based SSD, which the vendor said will improve random read performance by about 1,900% over existing SSDs, particularly those that are not equipped with DRAM. 3.\nStorage sustainability Organizations increasingly view their IT operations through the lens of sustainability -- storage is part of that movement. In addition to the environmental benefits, this data storage trend also helps organizations to decrease their power consumption. As a result, organizations save money, especially given that electricity is usually the single biggest expense for a data center. Tape is one example of storage media that does not use a lot of energy, especially when cartridges are simply sitting on a shelf. While the sustainability trend has been going on for a while, it is unlikely to slow down in 2023. 4. Storage becomes less expensive Early on, it seemed that storage was going to become more expensive in 2022. More recently, however, storage prices have been steadily declining. These price declines apply to both SSDs and HDDs. The onset of the pandemic in 2020 caused a huge spike in the demand for solid state storage. Manufacturers ramped up production to keep pace with the demand. However, the demand eventually waned, leaving manufacturers with unsold inventory. This excess inventory has directly contributed to falling SSD prices. A surplus in NAND memory also fuels price drops. At the same time, HDD shipments fell by about 13.9% in the third quarter of 2022, according to Forbes. Much of this decrease stems from organizations -- and even cloud providers -- choosing SSDs over HDDs. 5.\nThis collateral damage can be huge to a company in some cases, these collateral losses are as high as 23 times that of the initial ransom. A Cybereason report shows 49% of enterprises that have paid the ransom from an attack only retrieve part or none of the lost data, while 80% of enterprises that paid the ransom are targeted by ransomware a second time. Storage: Part of the ransomware protection process and the last line of defense for data security Traditional causes of data security risks include natural disasters and system hardware faults, such as fire, flood, and disk damage. These threats can be easily handled with DR solution and disk wear detection technology. However, currently, the number of human-caused damages represented by ransomware attacks keeps increasing and causes enormous economic losses. This necessitates the construction of comprehensive data security protection covering both network and storage. Figure 1: Comprehensive data security protection Ransomware exploits zero-day vulnerabilities (a system or device vulnerability that has been disclosed but not yet patched), phishing emails, and physical attacks to embed your system with ransomware. The network functions, while designed to prevent, block, scan, and eliminate ransomware, are rendered useless if your system is infected with a virus. In our modern digital age, data storage needs to do more than just storing data it needs to protect data.\nSpecifically, storage uses technologies such as pattern recognition and machine learning to identify ransomware, and uses data security features such as ransomware detection, secure snapshot, data isolation, and data recovery to provide logical and physical protection for data. As the final stop of data, storage is the last line of defense and is key to building protection capabilities. Figure 2: Data storage ransomware protection overview Ransomware protection with primary storage: After data enters the production storage, a safe zone is created inside the storage to prevent data from being tampered with or deleted through the secure snapshot and Write Once Read Many (WORM) features of the storage. An independent physically-isolated zone is also created, combining with air gap technology to automatically disconnect replication links and replicate data to the isolation zone for enhanced protection. Ransomware protection with backup storage: Similar to primary storage, the encryption, secure snapshot, and WORM features of backup storage ensure the data in the storage system is clean. An isolation zone is also established to ensure data security, allowing operators to quickly restore secure data and services in the event of an attack. When countering the ransomware attack, the ransomware protection appliance not only detects ransomware, it also helps simplify system deployment. It is critical that storage accurately detects ransomware.\nThe leading detection strategy in the industry is as follows: A baseline model is established based on historical data to check for abnormalities in the changed feature values of the metadata of copies; abnormal copies are further compared to determine the file size change, entropy value, and similarity; the machine learning model is used to determine whether file changes are caused by ransomware encryption and mark them accordingly. What we suggest 1. Combine resources of the storage and data security teams, for a comprehensive data security protection system Typically, enterprises data security teams comprise network experts who are responsible for imposing strict security policies on network security devices, such as firewalls, to protect high-risk ports and reduce exposure to threats. But even mainstream solutions alone are insufficient for handling ransomware attacks. One option is to include storage experts in the data security team to establish a comprehensive data security protection system. The importance of storage experts and a storage protection layer in a system cannot be understated. Data protection measures such as secure snapshots and data isolation technologies prevent data from being tampered with, while detection and analysis technology accurately and quickly identifies ransomware, helping recover data from ransomware attacks as soon as possible. 2.\nWhat IOPS Means? IOPS Performance Characteristics. How to Measure IOPS?IOPS vs. Throughput What Means? IOPS stands for Input/Output Operations Per Second. It is a performance metric that measures the number of input and output operations a storage system or device, such as a hard disk drive (HDD), solid-state drive (SSD), or storage area network (SAN), can perform in one second. IOPS is commonly used to assess the speed and responsiveness of storage systems, as it provides an indication of how quickly a storage system can process data requests. Higher IOPS generally indicate better performance, as it means the storage system can handle more data requests per second, resulting in faster data access and retrieval. IOPS can be affected by various factors such as storage technology, storage capacity, storage configuration, workload characteristics, and system settings. It is an important consideration in storage performance benchmarking, capacity planning, and storage system design, as it directly impacts the overall performance and responsiveness of storage systems in various applications, such as databases, virtualization, and high-performance computing. IOPS Performance Characteristics: IOPS performance characteristics refer to the key attributes or factors that affect the Input/Output Operations Per Second (IOPS) performance of a storage system or device. Some of the common IOPS performance characteristics include: Storage technology : The underlying technology used in the storage system or device can significantly impact IOPS performance. Traditional hard disk drives (HDDs) generally have lower IOPS performance compared to solid-state drives (SSDs) due to their mechanical nature and slower access times.\nSSDs, on the other hand, offer significantly higher IOPS performance due to their lack of moving parts and faster data access times. Storage capacity: The capacity of the storage system or device can also impact IOPS performance. As the storage capacity increases, the IOPS performance may decrease, as the system may take longer to locate and retrieve data from a larger storage space. Storage configuration: The configuration of the storage system, including RAID levels, caching, and other settings, can affect IOPS performance. For example, RAID configurations that provide data redundancy and striping, such as RAID 0+1 or RAID 5, may offer higher IOPS performance compared to RAID 1 (mirroring) due to improved data access and distribution across multiple disks. Workload characteristics: The characteristics of the workload, such as read-heavy or write-heavy workloads, random or sequential I/O patterns, and small or large I/O sizes, can impact IOPS performance. Different storage systems or devices may have different performance characteristics optimized for specific types of workloads. System settings : The system settings, including caching settings, I/O scheduler settings, and other system-level configurations, can also affect IOPS performance. Properly configuring these settings based on the workload requirements and storage system characteristics can optimize IOPS performance. Storage system design: The overall design and architecture of the storage system, including the number of drives, storage tiers, storage controllers, and other components, can impact IOPS performance. Well-designed storage systems with efficient data pathways and optimized components can deliver better IOPS performance compared to poorly designed systems.\nUnderstanding these IOPS performance characteristics is crucial when evaluating, benchmarking, and designing storage systems or devices for specific applications or workloads. It allows for informed decision-making to achieve optimal storage performance based on the requirements of the intended use case. How to Measure IOPS? Measuring IOPS typically involves using benchmarking tools or performance monitoring software that can generate and collect data on the input/output operations performed by a storage system or device. Here are some common methods for measuring IOPS: Benchmarking tools: There are several benchmarking tools available that are specifically designed to measure storage system performance, including IOPS. These tools typically simulate real-world workloads by generating various types of I/O requests, such as reads, writes, random or sequential I/O patterns, and different I/O sizes, and then measure the performance metrics, including IOPS, based on the system's response. Examples of popular benchmarking tools include FIO, Iometer, DiskSpd, and CrystalDiskMark. Operating system utilities: Some operating systems, such as Windows, Linux, and macOS, have built-in utilities that can be used to measure storage performance, including IOPS. For example, Windows Performance Monitor, Linux iostat, and macOS Activity Monitor provide performance monitoring features that can measure IOPS and other storage performance metrics. Storage system vendor tools: Many storage system vendors provide their own performance monitoring and benchmarking tools that are specifically designed for their storage systems. These tools may offer more advanced features and insights into the storage system's performance, including IOPS, and may be useful for measuring IOPS in a vendor-specific storage environment.\nThird-party performance monitoring software: There are third-party performance monitoring software available in the market that can measure storage performance, including IOPS, across different storage systems and devices. These tools may provide comprehensive monitoring and reporting capabilities for storage performance, including historical data, real-time monitoring, and customizable metrics. When measuring IOPS, it's important to consider the workload characteristics and configurations of the storage system or device being tested, as well as the benchmarking or monitoring tool being used, to ensure accurate and meaningful results. It's also important to perform multiple measurements and analyze the data from different perspectives to obtain a comprehensive understanding of the storage system's IOPS performance. IOPS vs. Throughput: IOPS (Input/Output Operations Per Second) and Throughput are two important performance metrics used in computer systems, particularly in storage systems, to measure their efficiency and effectiveness. While both IOPS and Throughput are related to the performance of storage systems, they represent different aspects of performance and are used to measure different characteristics. IOPS: IOPS refers to the number of Input/Output operations that a storage system can perform in one second. It measures the speed or responsiveness of a storage system in terms of the number of read or write operations it can handle in a given time frame. IOPS is typically used to measure the random access performance of storage systems, where data is accessed in a non-sequential manner, such as in random reads or writes.\nDear All, Today we are going to learn about Data replication. Data replication refers to the process of creating and maintaining multiple copies of the same data across multiple locations or systems. It involves duplicating data from a source system or database to one or more target systems or databases. The purpose of data replication is to ensure data consistency, availability, and reliability, and it is commonly used in various scenarios, such as backup and disaster recovery, data distribution for geographically distributed applications, load balancing, and improving system performance. Data replication can be implemented using various methods Full Replication: In this method, an exact copy of the entire dataset is created in multiple locations. Any changes made to the source data are replicated to all the target locations, ensuring that all copies remain consistent. Full replication provides high data consistency and availability but may require significant storage and bandwidth resources. Partial Replication: In this method, only a subset of data is replicated to the target locations, typically based on specific criteria or rules. Partial replication allows for more selective data replication and can help reduce storage and bandwidth requirements compared to full replication. Incremental Replication: In this method, only the changes made to the source data since the last replication are replicated to the target locations. This can help reduce the amount of data that needs to be transferred and stored, making it more efficient in terms of storage and bandwidth usage.\nData replication can be implemented using various technologies, such as database replication, file system replication, block-level replication, and object-level replication. It can also be synchronous, where changes are replicated immediately after they occur, or asynchronous, where changes are replicated with a delay. The choice of data replication method and technology depends on the specific requirements of the system or application, such as the need for data consistency, performance, and scalability, as well as the available resources and budget. Asynchronous and synchronous data replication are two different approaches to replicating data across multiple locations or systems Asynchronous Replication: In asynchronous replication, changes made to the source data are replicated to the target locations with a delay. When data changes occur in the source system, they are logged or queued for replication, and the changes are replicated to the target system at a later time, typically based on predefined intervals or thresholds. Asynchronous replication introduces a delay in the replication process, which allows for some flexibility in terms of timing and can potentially reduce the impact on system performance. However, it also means that there may be a time lag between changes made in the source system and their availability in the replicated copies, which could result in temporary inconsistency between the source and target data. Synchronous Replication: In synchronous replication, changes made to the source data are replicated to the target locations in real-time, and the source system waits for acknowledgment from the target system before proceeding with further operations.\nThis means that changes made in the source system are immediately reflected in the replicated copies, ensuring high data consistency between the source and target data. Synchronous replication provides the most up-to-date and consistent data across all locations, but it can introduce additional latency in the source system as it waits for acknowledgments from the target systems, which could impact system performance. Both asynchronous and synchronous replication have their advantages and disadvantages, and the choice between them depends on the specific requirements of the system or application. Asynchronous replication is often used when data consistency can tolerate some delay and system performance is a priority, while synchronous replication is preferred when data consistency is critical and the potential for temporary inconsistency is not acceptable. Factors such as the distance between the source and target systems, available network bandwidth, and the importance of data consistency and availability play a role in determining the appropriate replication approach to use in a given scenario. HuaweiHyperReplication HyperReplication is the remote replication feature developed by Huawei. The feature provides flexible and powerful data replication functions to achieve remote data backup and recovery, continuous support for service data, and disaster recovery. This feature requires at least two OceanStor storage systems that can be placed in the same equipment room, same city, or two cities up to 1000 km apart.\nThe storage system that provides data access for production services is the primary storage system, and the storage system that stores backup data is the secondary storage system HyperMetro HyperMetor is a data replication feature offered by Huawei Technologies Co., Ltd. in their storage systems, specifically in their OceanStor series of storage systems. HyperMetro provides synchronous data replication for high availability and disaster recovery purposes between two geographically dispersed storage systems. HyperMetro allows for creating a virtualized storage pool spanning across two storage systems, typically located in two different sites, and provides real-time synchronous replication of data between them. This means that any changes made to the data in the primary storage system are replicated to the secondary storage system in real-time, ensuring that the two systems have consistent and up-to-date copies of the data. In case of a failure in the primary storage system, the secondary storage system can automatically take over and provide uninterrupted access to the data, thus ensuring high availability and business continuity. Some key features of Huawei HyperMetro Synchronous Replication: HyperMetro provides synchronous data replication, where changes made to the data in the primary storage system are immediately replicated to the secondary storage system. This ensures high data consistency and eliminates the risk of data loss in case of a primary system failure. Active-Active Access: HyperMetro enables active-active access to the data, meaning that both the primary and secondary storage systems can serve read and write requests from applications simultaneously. This allows for load balancing and better utilization of storage resources.\nPEM (Privacy Enhanced Mail) file is a complete chain of certificates which are concatenated in a single file. It includes primary key, server certificate, followed by intermediate and root certificates. It is also referred as a container of digital certificates and keys which are encoded by Base64, each certificate has the pain-text header and footer marking at the beginning and end which separates and help to identify the certificates. Pem file includes. Private Key Primary Certificate Intermediate Certificate Root Certificate PEM files can be saved with different extensions which includes .pem, .crt, .cert, .key. SSL certificates are used to secure communications between client and server. These certificates are installed in applications, front-end load balancers or webservers. Today we will explore the certificate management option in Huawei Dorado Storage. Huawei storage supports SSL communication with an external entity which could be a service like SMTP, NTP, LDAP domain authentication, HyperMetro configuration with quorum server etc. Certificate configured in Dorado Storage is stored in active and passive controller's database. Below Diagram list the scenario where SSL configuration is required When LDAP domain-based authentication is configured for centralized user management. When HyperMetro feature is used, HyperMetro clients connect to the quorum server over secure network. This is used to setup Syslog notification feature; the storage system securely communicates with the Syslog server. This is used to setup NTP synchronization feature is used. The storage system serves as a client for the NTP server. When Call Home feature is configured, storage system communicates with the technical support center.\nDefault certificate when you access the Device Manager. Replace the default certificate with your certificate to manage the storage system. CAS server configuration is required. eSight users can directly access the storage system through SSO. Storage configured with SMTP server over secure network. The disk functions as a client and the storage system functions as a server. The storage system communicates with the SMTP server. The storage system functions as the client and the SMTP server functions as the server. Now we will login into the Device Manager, explore the existing certificate, generate self-signed CA and server certificate, and then sign the certificate using local CA. Ideally, we should generate a server CSR and sent it to public CA authorities to sign it. For demo purpose we will create local CA, it can be created on Linux or Windows. View the existing certificate. Go to Setting and click on certificate management to view the certificate scenarios. Click on Device Manager certificate and import the certificates. We require three files. I am using openssl and ketool utilities on Linux to generated required root certificate, private key and self-signed certificate. Create self signed certificate and CSR, Sign the certificate with self generated CA in previous step. Create a storage certificate in CSR format. Execute the following command to generate the SSL certificate that is signed by the Self-created Certification Authority. Import the generated certificate. To convert certificates into a pem format, combine all the certificates in a single file to form a complete chain of certificates.\nData migration is the process of transferring data from one storage system to another. This process is often necessary when organizations switch to new technology platforms, merge with other companies, or simply need to upgrade their existing systems. Data migration can involve moving data from one database to another, from one cloud storage platform to another, or from an on-premises system to a cloud environment. The success of a data migration project depends on several factors, including the size and complexity of the data, the availability of resources, and the accuracy and completeness of the data being transferred. To ensure a successful migration, organizations must plan the migration process carefully, including preparing a comprehensive inventory of their data, mapping data fields between systems, and testing the new system to ensure data is accurate and complete. There are different type of workload: likevirtual machines (VMs), backups, or databasescan usually be moved with software vendor-provided tools that are specific to the type of data being migrated. Data Size: While migration Data Size Is important faction whether is moved to Cloud or on prem. Data Transfer Speed: this is also important faction while transferring the data and will determine the time and duration depends on the speed and Size. Data migration can also be a time-consuming process, particularly when large amounts of data are involved. For example, moving petabytes of data from one database to another can take several weeks or even months to complete.\nGreetings Everyone! What is PEM (Privacy-Enhanced Mail) File? PEM (Privacy-Enhanced Mail) is a file format that is commonly used for storing and transmitting cryptographic keys, digital certificates, and other secure data. It was originally defined in the early 1990s as part of the Privacy Enhanced Mail standard (RFC 1421) and has since been widely adopted in various applications and protocols. A PEM file is encoded using Base64 and contains textual data that represents the encrypted or digitally signed content. It typically has a .pem file extension, although other extensions like .crt, .cer, or .key are also commonly used. PEM files are human-readable and can be opened and viewed with a text editor. What is PEM File Extension? The PEM file extension is commonly associated with files that are encoded in the Privacy-Enhanced Mail (PEM) format. PEM files are used for storing and transmitting cryptographic keys, digital certificates, and other secure data. PEM files are encoded using Base64 and typically have a .pem file extension, although other extensions like .crt, .cer, or .key may also be used. These files are text-based and can be opened and viewed with a text editor. PEM files are widely used in various applications and protocols related to secure communications, such as SSL/TLS (Secure Sockets Layer/Transport Layer Security) for securing web connections, email encryption, secure file transfer (SFTP), and secure shell (SSH).\nIn the context of PEM files, the extension .pem serves as an indication that the file follows the PEM format and contains encrypted or digitally signed content, such as X.509 certificates or private keys. It's important to note that the file extension alone does not guarantee the file's content or its compatibility with a specific application or system. The actual content and usage of a PEM file should be determined based on the context in which it is being used and the application or protocol that supports the PEM format. X.509 Certificates PEM files often contain X.509 digital certificates, which are used for authentication, encryption, and secure communication. X.509 certificates are widely used in protocols like HTTPS (SSL/TLS) for securing web connections. Private Keys PEM files can also store private keys that are associated with the corresponding X.509 certificates. Private keys are used for decrypting encrypted data or creating digital signatures. It's important to keep private keys secure and protect them with strong passwords. Certificate Signing Requests (CSRs) When requesting a digital certificate from a certificate authority (CA), a PEM file can be used to store a Certificate Signing Request. The CSR includes information about the entity requesting the certificate and the desired attributes of the certificate. PEM files are often used in various security-related applications and protocols, such as SSL/TLS, email encryption, secure file transfer (SFTP), and secure shell (SSH). They provide a standardized and widely supported format for storing and exchanging cryptographic information securely. What does a CSR request code look like?\nHi, everyone! In this article, Ill explain everything about replacing a BBU in OceanStor Dorado 6000. Introduction to BBU in Huawei OceanStor Dorado 6000: The Huawei OceanStor Dorado 6000 is a storage system designed to provide high-performance and reliable data storage solutions. Within the Dorado 6000, the term BBU refers to the Backup Battery Unit. The BBU in the Huawei OceanStor Dorado 6000 is an essential component that provides power backup for the system. It is responsible for supplying power to the cache in case of a sudden power outage or failure. By using a backup battery, the BBU ensures that data residing in the cache can be preserved and written to the persistent storage media during power disruptions, preventing data loss or corruption. The BBU plays a crucial role in maintaining data integrity and protecting against potential risks in case of power-related incidents. It helps to safeguard the data stored in the OceanStor Dorado 6000 by ensuring that any ongoing read and write operations can be completed and properly committed to the storage system. The BBU in the Huawei OceanStor Dorado 6000 is designed to provide a reliable and efficient power backup solution, offering enhanced data protection and system availability. It is an important component for organizations that require continuous access to their data and cannot afford disruptions or loss of information during power outages or failures. Procedure of Replacing a BBU in OceanStor Dorado 6000: Replacing the Backup Battery Unit (BBU) in the Huawei OceanStor Dorado 6000 is a relatively straightforward process.\nThe HyPerDDP12D is a modular and scalable high performance Ethernet SAN shared storage system that combines the best of both worlds: SSD cache performance with HD capacity for recording and editing audio, video & film. It functions with Avid Media Composer, Adobe Premiere, DaVinci Resolve, FCPX and all other applications and MAM systems. The HyPerDDP12D has a bandwidth of 1GByte/s and is suitable for companies in the media and entertainment, broadcast and post production sectors that need an ultra fast shared storage solution with SSD cache performance at an affordable price. The HyPerDDP12D shared storage server comes in a 2U chassis with dual 10GbE/RJ45 ports and 2 extra PCIe slots for cards. In the standard configuration, the HyPerDDP12D is equipped with a 4TB SSD cache, 32TB HD packs, and a total capacity of 36 TB. The SSD and HD packs are RAID5 protected. The 4 spare slots of the HyPerDDP12D can be equipped with an additional SSD or HD pack. https://ddpsan.com/wp-content/uploads/2021/05/HyPerDDP12D-BLUE-2021.png HyPerDDP12D comes with dual 10GbE/RJ45 ports and 2 extra PCIe slots for cards Includes V5 scale out DDP Ethernet SAN software Modular and capacity and bandwidth expandable within one file system SSD and HD packs are RAID 5 protected The HyPerDDP12D has 4 spare slots for additional SSD and HD packs The HyPerDDP12D comes with 1 year warranty, see Delivery Conditions on www.ddpsan.com Support can be purchased for 1200,- Euro per year, see Support on www.ddpsan.com Available desktop drivers are always free.\nDDP updates are part of a support contract The DDP components are of the shelf and replacements can be purchased locally Archiware P5 is installed. Ask your dealer to purchase a license Scale out shared storage solution for post production Sliding arms can be supplied for 165,- Euro Functions with Avid Media Composer, Premiere Pro, FCPX, DaVinci Resolve and MAM Watch this video with a short explanation of the HyperDDP12D and SSD caching Video, audio and film formats example on reading 1GB/s Ethernet SAN HyPerDDP12D Having an SSD cache and HD Data Locations has advantages: 1. Files transparently can be moved/copied within the HyperDDP12D between cache and spindles without changes to the directory tree. 2. Audio, DPX and low bandwidth video formats which give bad performance (seek time issues) with spindles, can be kept or consolidated on SSDs. That way they do not influence the spindle performance of regular video formats. 3. If needed you can split copying / rendering from streaming with streaming materials on SSDs to prevent drop frames. 4. Because bandwidth of SSDs and spindles add up the DDP has a bandwidth of 1 GB/s plus the spindle bandwidth. 5. Files ingested to SSD are duplicated to spindles. The newest files are then on SSD and also on spindles. 6. Folder volume files can be pinned to cache. 7. Folder volume files can be copied on demand to cache. Note: bandwidth during sustained writing or combining R/W may be lower.\nHi everyone, In this article we are going to explain everything about distributed storage architecture and what are its pros and cons. Distributed storage design has a lot of benefits that make it a good choice for storing data in the modern world. Scalability is one of the best things about it. With a distributed storage system, adding more nodes to the network is an easy way for a group to add more storage space. This gives companies the ability to handle growing amounts of data without stopping their operations. High access is another important benefit. Distributed storage architectures use methods like redundancy and replication to make sure that the same data is stored on multiple nodes. This redundancy reduces the chance of losing data and makes sure that even if a storage device breaks, data can still be accessed from other nodes. This makes it possible for important data to be more reliable and always available. Distributed storage architectures really shine when it comes to speed because they take advantage of parallelism. Data is stored on many different media, so it can be accessed and retrieved at the same time. This shared processing feature makes it much faster to read and write, so you can get to your data faster and the system as a whole works better. Load balancing is another benefit of distributed storage systems that is worth mentioning. Storage systems can do a good job of balancing the load by spreading data across various nodes.\nThis makes sure that no single node gets too busy with too many data requests. This lets the whole storage system make the best use of its resources and run better. Also, distributed storage architectures offer data locality , which is the idea of storing data close to the computer resources that need it. This cuts down on network latency and speeds up the time it takes to get info, making the whole system more efficient and responsive. But it's also important to think about the bad things about distributed storage design too. A distributed storage system can be hard to set up and manage if you don't know much about distributed systems. Keeping data consistent and in sync across multiple nodes can be hard and managers need to deal with issues like data partitioning, replication strategies and data integrity . Also, distributed storage architectures may cost more than standard storage systems because they need more storage nodes and mechanisms for copying data. Setting up a distributed storage system and keeping it running can cost a lot of money. Conclusion In conclusion, distributed storage design has many great benefits, such as scalability, high availability, better performance, load balancing and keeping data close to where it is used. But it also has problems in terms of how hard it is to use, how much it costs and how hard it is to keep up.\nHello, everyone, I will start a small series of articles about Data Storage and Python. This is the first one, so please read below to find out more about this subject. Data storage is the act of saving data so that it may be retrieved and utilized at a later time. Python isa highlyversatile programming language, thatis widely used for various data-related tasks, such as data storage. Data storage is an important part of many Python apps because it makes sure that data stays around and can be accessed. Python has a lot of different ways to store data, from simple file types like CSV and JSON to powerful databases like SQLite and MongoDB. There are numerous data storage options available in Python, each with their own advantages and disadvantages. A file is one of the most common data storage options in Python. Text, integers and images are among the data types that can be stored in files. A variety of devices, such as computers, hard drives and USB drives are capable of storing files. However, files can be difficult to manage and are susceptible to corruption and deletion. Using a database is another prevalent data storage option in Python. The purpose of databases is to store vast amounts of structured data. Databases are more complicated than files, but they offer several benefits, including: Databases can be protected using passwords and other security measures. It is possible to design databases to ensure that data is accurate and consistent.\nObject-based storage (OBS) is a data storage strategy that handles data as distinct, independent objects, each with its own unique identifier. This architecture has various benefits and drawbacks, which I will discuss below: 1.Scalability: The OBS architecture allows for smooth scaling, which makes it easy for groups to handle large amounts of data. It overcomes the restrictions of standard file systems and block storage by allowing things to be added or removed separately without affecting the entire system. 2. Metadata and Extended Attributes: Metadata and extended attributes are associated with objects in OBS and provide more information about the object. Rich indexing and searching capabilities are enabled, making it easier to discover and retrieve specific objects within a vast storage pool. 3. Durability and Resilience: To achieve high durability and resilience, OBS platforms frequently feature data redundancy and fault-tolerant algorithms. Typically, objects are scattered across numerous physical locations or storage nodes to reduce the risk of data loss in the event of hardware failures or disasters. 4. Cost-effectiveness: For large-scale storage deployments, OBS design can be cost-effective. It improves storage utilization by reducing the requirement for typical file system overhead and simplifies storage by employing a simpler storage paradigm. 5. Object Accessibility and Sharing: Object-based storage allows for easy data access and sharing across remote systems or numerous applications. Objects can be accessed by common network protocols such as HTTP, making them available from any location with network access. 1.\nWhat is Huawei Storage Architecture ? Huawei is a leading global provider of information and communication technology (ICT) solutions, including storage systems that are designed to provide high performance, scalability, and reliability. Huawei's storage architecture is based on a distributed storage system that incorporates Solid State Drives (SSDs) and software-defined storage (SDS) technologies. One of the key features of Huawei's storage architecture is its use of SSDs. SSDs are faster and more reliable than traditional hard disk drives (HDDs), which allows Huawei's storage systems to deliver high performance and low latency. Additionally, Huawei's storage systems use advanced caching algorithms to optimize the use of SSDs and further enhance performance. Another important component of Huawei's storage architecture is its distributed storage system. Data is stored across multiple nodes, which helps to increase scalability and fault tolerance. If one node fails, data can be easily retrieved from other nodes in the system. This distributed storage system also allows for easy scaling of storage capacity as business needs grow. Huawei's storage architecture also includes SDS technologies, which allow for the separation of storage hardware from software. This enables greater flexibility and agility in managing storage resources. With SDS, storage resources can be easily provisioned, managed, and allocated as needed, without the need for manual intervention. In addition, Huawei's storage architecture includes advanced data protection and disaster recovery capabilities. Huawei's storage systems can replicate data across multiple sites, which provides protection against data loss due to hardware failure, natural disasters, or other disruptions.\nHello, everyone! Today, I will share with you about What is the M.2, SATA, PCIe, and NVMe SSDs? M.2, SATA, PCIe, and NVMe are different types of SSD (Solid State Drive) interfaces used in modern computer systems. Each has its own characteristics and performance capabilities. Let's look at each of them: M.2: M.2 (pronounced \"M-dot-two\") is a form factor for storage devices, primarily used for solid-state drives (SSDs). It is a small, rectangular-shaped card that connects directly to the motherboard of a computer or other compatible device. M.2 is designed to provide high-speed data transfer rates and compact size, making it suitable for use in modern, space-constrained devices such as laptops, ultrabooks, and small form factor desktop computers. M.2 storage devices use the Serial ATA (SATA) or the faster PCIe (Peripheral Component Interconnect Express) interface to connect to the system. The PCIe interface allows for faster data transfer speeds compared to traditional SATA connections, enabling M.2 SSDs to offer significantly improved performance. M.2 SSDs come in different lengths and widths, denoted by various keying notches, which determine the compatibility with specific slots on the motherboard. Common lengths include 42mm, 60mm, 80mm, and 110mm, while the most common key types are B, M, and B+M. In addition to SSDs, M.2 slots can also accommodate other expansion cards, such as Wi-Fi and Bluetooth modules, providing a versatile and space-saving solution for integrating different functionalities into a single device.\nOverall, M.2 storage has gained popularity due to its compact size, high performance, and versatility, making it a preferred choice for storage expansion and system upgrades in many modern computing devices. SATA: SATA stands for Serial Advanced Technology Attachment, and it refers to a computer bus interface used for connecting storage devices to a computer's motherboard. SATA is commonly used for connecting hard disk drives (HDDs), solid-state drives (SSDs), and optical drives (such as DVD and Blu-ray drives) to the computer system. SATA replaced the older parallel ATA (PATA) interface, also known as IDE or ATA, which was used for many years. Unlike PATA, which used a wide ribbon cable to connect devices, SATA uses a smaller, more streamlined cable with thinner connectors. This allows for better airflow and easier cable management within the computer case. SATA interfaces support different versions, including SATA I (also known as SATA 1.5 Gbps), SATA II (3 Gbps), and SATA III (6 Gbps). Each new version increases the data transfer rate, providing faster and more efficient communication between the storage device and the motherboard. SATA drives are widely used due to their compatibility, ease of use, and cost-effectiveness. However, it's worth noting that SATA interfaces have a limited maximum transfer speed compared to newer interfaces like PCIe. As a result, SATA SSDs can be outperformed by M.2 SSDs using the faster PCIe interface.\nNonetheless, SATA remains a viable and widely used option for connecting storage devices in many desktop and laptop computers, offering a reliable and straightforward solution for data storage and retrieval. PCIe: PCIe (Peripheral Component Interconnect Express) is a high-speed interface commonly used for connecting expansion cards, such as graphics cards, network cards, and SSDs. PCIe SSDs utilize the PCIe interface to deliver significantly faster data transfer rates compared to SATA-based SSDs. PCIe SSDs can provide higher bandwidth and lower latency, resulting in improved overall system performance. They are available in different form factors, including add-in cards and M.2 modules. NVMe SSDs: NVMe (Non-Volatile Memory Express) is a protocol specifically designed for solid-state storage devices. It is optimized to take full advantage of the high-speed capabilities of PCIe SSDs. NVMe SSDs use the PCIe interface and provide even faster data transfer rates and lower latency than traditional SATA or PCIe AHCI-based SSDs. NVMe SSDs are designed to fully utilize the parallelism and high throughput of PCIe, resulting in superior performance and responsiveness. Classification by Interface Type: The interface type of Disk modules is categorized into SAS disks, SATA disks, PCIe (NVMe) disks, and M.2 disks. Disk classification diagram by interface type, Disk modules are categorized into HDDs and SSDs by working principle. Disk classification diagram by working principle, Difference (M.2, SATA, PCIe, and NVMe): M.2, SATA, PCIe, and NVMe SSDs are different types of solid-state drives that differ in their interfaces, performance, and other characteristics.\nHello, everyone, This is the fourth article of the series Data Storage and Python. In this article, we are going to discuss about Excel in Python with Pandas. Pandas is another popular library for interacting with Excel. While Pandas' primary strength is in its ability to manipulate data, it also has helpful functionality for reading and writing Excel files. You may easily execute data analysis and transformations by importing Excel data into a Pandas DataFrame using the read_excel() method. To export DataFrame data back to Excel, use the to_excel() function. You may easily incorporate Excel files into your data storage processes using Python's Excel libraries . Python is an accessible and effective tool for any data-related task, be it extraction, manipulation, or report generation. Excel may be used to store and analyze data in a more automated and flexible way with the help of Python. Pandas is a Python package for manipulating and analyzing structured data. It provides data structures and functions to simplify data cleaning, transformation, analysis, and display. Pandas has many Excel data reading and writing functions. It lets you import Excel data into pandas data structures like DataFrames and use pandas' powerful functions to manipulate it. 1. Reading Excel files 2. Data manipulation 3. Pandas has several data analysis tools 4. Data visualization 5. Writing Excel files Pandas is a sophisticated tool for manipulating and analyzing Excel spreadsheets. Its straightforward syntax and thorough documentation make it easy to learn and use in projects with small or large datasets.\nTo work with Excel files in Python, you can use the `pandas` library. Pandas is a powerful data manipulation and analysis tool that provides easy-to-use data structures and data analysis tools for Python. To get started, you'll need to install the `pandas` library. You can do this by running the following command: Once you have `pandas` installed, you can use it to read and write Excel files. To read data from an Excel file, you can use the `read_excel()` function in `pandas`. Here's an example: ``` In this example, `filename.xlsx` should be replaced with the actual path and filename of your Excel file. The `read_excel()` function reads the contents of the Excel file and returns a DataFrame, which is a tabular data structure provided by `pandas`. You can then perform various operations on the DataFrame, such as filtering, transforming and analyzing the data. To write data to an Excel file, you can use the `to_excel()` function. Here's an example: ``` In this example, we create a DataFrame `df` with two columns, and then use the `to_excel()` function to write the DataFrame to an Excel file named `output.xlsx`. The `index=False` argument specifies that the index column should not be included in the output. These are just a few basic examples to get you started with using `pandas` for working with Excel files in Python. `Pandas` provides a wide range of functionality for data manipulation and analysis, so you can explore its documentation for more advanced usage and features. Thank you for reading!\nToday we are going to discuss the main difference between Data Scientist and Data Engineer you can take part in this topic too. Data scientists analyze data using scientific methods, algorithms, and systems. It covers data collection, cleaning, processing, analysis, visualization, and interpretation. The scientific methoddesigning experiments, statistical analysis, verifying models, establishing a hypothesis, collecting data, evaluating data, and making conclusions. AlgorithmsAutomate data preparation, feature engineering, and model selection. SystemsDatabases, cloud platforms, and big data frameworks store, handle, and process huge amounts of data. Scienceuses hypothesis testing and experimentation to learn from evidence. Research Paradigm provides an overarching framework to understand and approach data problems, such as what data scientists ask, how they gather and analyze data, and how they interpret their findings using statistical analysis and machine learning algorithms. Discover patterns, relationships, and insights to inform decision-making and solve real-world challenges. Research Method Formulating a research issue or problem, collecting and preparing pertinent data, evaluating it using statistical and machine learning methods, and drawing findings or generating predictions. WorkflowCollecting, cleaning, processing, analyzing, visualizing, and interpreting data. Data collectionUsing databases, APIs, web scraping, or surveys. Cleaning and preprocessingensure quality, completeness, and analytical readiness. AnalyzingUsing statistical and machine learning algorithms to find patterns in data. Predictions, suggestions, and data understanding can be based on insights and patterns. VisualizingUsing graphs, charts, and dashboards. Data visualization helps stakeholders identify insights and patterns. InterpretingPresenting results to stakeholders and using them to guide future activities and decisions.\nProfessionManage and analyze vast, complex datasets, construct predictive models and algorithms, and use data visualization to communicate findings to stakeholders. One can study data science or start with data analytics. This career requires strong mathematical and statistical skills, proficiency in programming languages like Python, R, SQL, and Java, knowledge of data structures and algorithms, software development processes, domain-specific concepts and data sources, and expertise in collecting and preprocessing data, applying statistical modeling and machine learning techniques, creating effective visualizations are the responsibilities of Data Scientist Data engineers build and manage data storage, processing, and transformation infrastructure. They develop and implement scalable data infrastructures to manage organized, semi-structured, and unstructured data from multiple sources for analysis. They also create and improve data pipelines, which extract, convert, and load (ETL) data from many sources into a target system for processing. They work with stakeholders to understand business needs and build data models. Data engineers plan, create, and maintain data architectures and create reports and dashboards that clearly convey data findings. Data engineers simplify data models to construct data-intensive software systems. Developers, business analysts, and stakeholders need a simple data conceptualization. They make sure the data model meets business needs and can handle system data volume, velocity, and variety. Data engineers use R, Python, Java, SQL, and Scala. They also know Agile, DevOps, architectural design, service-oriented architecture, and repetitive work automation and scripting.\nLet's talk about optimization strategies for Huawei OceanStor 6000. In the period of data-driven enterprises, optimizingstorehouse performance is pivotal for icing effective operations and meeting demanding workloads. Huawei OceanStor 6000, a robust storehouseresult, offers a suite of performance optimization strategies that empower associations to maximize theirstorehouse structure's capabilities. This composition delves intocrucial strategies that can be employed to enhance the performance of the Huawei OceanStor 6000 and achieve remarkable effectiveness earnings. Effective workloadoperation is the foundation of storehouse performance optimization. The OceanStor 6000 enables intelligent workload distribution by stoutly balancing I/ O demands across storehousecoffers. directors can work features like dynamic cargo balancing and quality of service( QoS) settings to prioritize critical operations and allocate applicable coffers to meet their performance conditions. This ensures that coffers are allocated efficiently, precluding backups and maximizing the system's responsiveness and overall performance. Hiding ways play a vitalpart in accelerating storehouse performance. The OceanStor 6000 supportscolorful caching mechanisms, including read and write hiding. Byexercising high- speed solid- state drives( SSDs) as cache bias,constantly penetrated data can be stored near to the ciphercoffers, reducing quiescence and accelerating data reclamation. The intelligent hiding algorithms intelligently anticipate data access patterns,icing that the most applicable data remains readily available in the cache. As a result, the systemgests briskly response times and bettered overall performance, particularly for operations with high read or write demands. Enforcing tieredstorehouse configurations is animportant strategy to optimize performance and cost-effectiveness.\nData backup and recovery What is the Backup and Recovery of Data? disaster-recovery Offsite and Onsite Servers vs. Independent Drives. Offsite servers Onsite-storage Offsite-storage access to data from any location, via Internet or FTP. data will be preserved in the event of an event taking place within the business. backup data can be shared with a number of different remote locations. Data-recovery is usually painless. in the rare case where offsite servers crash. Offsite servers have backed up the data further onto their own drives. Offsite-storage provides more storage but at a recurring cost . External drive-storage is often a one-time charge unless the drive crashes. Cloud Backup and Recovery Cloud-backup , or online-backup, refers to a data backup-strategy that involves sending a copy of your primary data over a public or proprietary network to an off-site server. The server is typically hosted by a third-party service provider (CSP) that charges you a fee based on bandwidth, capacity, or number of users. cloud data-backup (AWS) , Google Cloud , IBM-Cloud and Microsoft Azure . SaaS-applications . This method copies your data to another cloud. cloud-backup services cloud-seeding SMBs small & medium-sized businesses Types of Data Backup The most common backup types used are as follows: FullBackup Incremental Backup Differential Backup: Importance of the Backup and Recovery of Data. ransomware attacks , malware What is Disaster Recovery Backup?\nThis article will discuss about the definition of archiving how important it is and the typical data archiving tools. Archiving is the process by which inactive information, in any format, is securely stored for long periods of time. Such information may or may not be used again in the future, but nonetheless should be stored until the end of its retention schedule. It should be emphasized that inactive data that has been archived can be made active again because the thought of losing access to information deters some organizations from keeping their records on file. If necessary, information should still be easily accessible. Archived data can be kept on a variety of devices and in a number of different formats. Businesses typically archive entire collections of files when they archive data. The very name \"archive\" implies the preservation of numerous records. Data may need to be archived due to court orders or because it is important information that will probably be utilized again in the future. The reason why archives are important is that archiving benefits are enormous and play an important role in an organizations success. Organizations can reduce data loss, cut operating costs, enhance document security, comply with more rules and regulations, and give audit and legal proof in the event of a legal or audit problem by archiving their data. As a business grows, more data is produced, which needs to be closely maintained and managed in order to be properly utilized.\nIn an era where cyber-attacks and breaches are becoming increasingly common, preserving corporate documents may help businesses maintain track of all their information throughout their lifespan. Using a system to archive documents allows you to define precise permissions throughout your organization by defining who may see what. Furthermore, paper-based documents are more likely to be misplaced or fall into the wrong hands. In reality, internal data breaches are more prevalent than external data breaches. Archiving documents removes papers from circulation, reducing the likelihood of a hack or malware infection. In a competitive environment, archiving documents is critical for company continuity and guaranteeing the greatest level of performance. 4. Enhanced Compliance Documents preservation is also necessary for legal reasons. Many organizations inadvertently destroy records that they are legally required to retain. Due to regulatory compliance, certain organizations are obligated to keep archived data for specific periods of time. Thats why archiving benefits are considered important to organizations. To avoid penalties and fines, organizations should constantly follow laws and industry rules and regulations. Organizations can be in compliance with various standards and regulations if they have a comprehensive archiving and retention plan in place. Health Insurance Portability and Accountability Act (HIPAA) and General Data Protection Regulation (GDPR) are two examples of rules with which companies may be required to comply. In reality, all businesses and organizations are controlled in some way for record-keeping purposes. While financial services, energy, and healthcare-related companies are more highly regulated than most other types of organizations, all must comply with record retention standards.\n5. Legal claims In the event that your business is sued by a third party, whether a client, employee, or another firm, you may be asked to produce particular papers to support your case. This emphasizes the significance of keeping all documents conveniently available and safe in order to protect your company from legal action. If a company is unable to effectively place a hold on data when it is needed, it may face a number of significant repercussions, ranging from embarrassment to substantial legal charges or fines. Email and other business documents can be lost in the absence of a comprehensive archiving solution, most commonly due to the unintentional destruction of material that should have been kept. This might have significant repercussions for a company that is involved in legal action. 6. Audit-proof An audit entails checking your companys records to ensure they are correct and is considered the main benefit of archiving. An archive system is audit-proof when it can assure that a document cannot be changed or lost from the time it enters the archive, through transportation, through final storage, and beyond. Todays document management or enterprise content management solutions actively support organizations in establishing audit-proof archiving requirements. The last thing you want is to need material for a court lawsuit, a tax audit, or after a natural disaster only to learn that your archive storage was compromised. The advantages and life expectancy of various data archiving techniques and programs vary.\nHello, everyone, This is the second article from the Data Storage and Python Series and we are going to talk about CSV files in Python. CSV files, which stand for \"Comma Separated Values,\" are a common way to store tabular data as plain text. They have rows and columns, where each row is a record and each column is a field or other characteristic. The values of the data in the file are divided by commas, which is why the file is called a CSV. One of the best things about CSV files is that they are easy to use. They have a simple, human-readable framework that makes them easy to make, change, and work with. Also, many types of software, such as spreadsheet programs like Microsoft Excel and data analysis tools, can open and work with CSV files. CSV files are a good way to store data in Python for a number of reasons. First, they can be used on any platform, so they are easy to move and share between different running systems. Also, CSV files can handle many different kinds of data, such as numbers, text and dates. They also take up less room than binary formats because the data is stored in a plain text format, which makes files smaller. But CSV files also have some things they can't do. They don't have built-in support for data structures other than simple tables, like data that is stacked or organized in a hierarchy.\nAlso, they don't have standard ways to check data or make sure limits are followed. Also, when working with big datasets, CSV files may have trouble because reading or writing large files can slow down performance. Different tools in Python make it easier to work with CSV files. The built-in 'CSV' module lets you read data from and write data to CSV files. It also has ways for parsing and formatting data. Also, the famous Pandas library makes it possible to do more advanced things with CSV files, such as filtering, aggregating and manipulating data. CSV files are a simple way to store data in Python that is generally supported. Even though they are easy to use, portable and compatible, they may not be able to handle complex data structures and big datasets well. Still, using Python libraries like 'CSV' and 'Pandas' makes it easy and flexible for Python applications to handle CSV data. Reading and writing data to/from CSV files is a common task in data processing with Python. Thankfully, Python provides powerful libraries such as `CSV` and Pandas to simplify this process. Let's explore some examples of how to accomplish this. Using the `CSV` library, reading data from a CSV file is straightforward. So, let's see how to do this: First we have to open the file using the `open()` function Then, we have to pass it to the `csv.reader()` function This allows us to iterate over the rows and access the data within.\nHello, everyone, This is the third article in the series about Data Storage and Python. This time, we are going to discuss about Excel in Python. Data can be efficiently organized and analyzed with Microsoft Excel, a popular spreadsheet tool. The Excel file format is well-supported by Python's robust library, which allows for easy data storage, retrieval and modification. Excel is a widely used tool for data management and analysis, but when combined with the power of Python, it becomes an invincibleforce. Python's openpyxl interface with Microsoft Excel opens up new options. Python automates tedious activities, manipulates data and maximizes Excel's features. Imagine easily extracting, manipulating and analyzing large datasets, making complicated charts and graphs, and automating complex calculations with just a few Python lines. Python and Excel give unmatched flexibility, efficiency and data control. Python with Excel will replace human data manipulation with speed and accuracy. Openpyxl is a package that allows Python users to read and write Excel files. You can use its many features to read, edit and modify Excel files in your code. You may use openpyxl to get information out of Excel sheets, make new ones, edit existing ones and add styles like fonts, colors and borders. Python's openpyxl module makes it possible to programmatically perform recurring operations on Excel spreadsheets. Some common uses of Excel scripting include data extraction, calculation and report generation. The time savings and reliability of the processed data are two additional benefits of this automated system. Large datasets are routinely stored and organized using Microsoft Excel.\nYou can use openpyxl to extract data from Excel files and perform data analysis in Python. As an example: Filtering, sorting and transforming data are common tasks in Excel files. You can programmatically change Excel data with openpyxl. Here's an example of new data being added and existing data being modified: Other than these examples, it can do various task with openpyxl such as: Creating and Modifying Worksheets: within an Excel file, you can add new worksheets and set their names, placements and properties. Openpyxl lets you change the names of current worksheets, delete them and move them around in the workbook. Sheets can also be copied or moved from one Excel file to another. Formatting cells: openpyxl lets you organize cells in different ways, such as by setting font size, bold/italic/underline attributes, text alignment and cell borders. You can change the fill color and background color of cells to make the Excel data look better. Formula Manipulation: working with formulas in Excel is possible with Openpyxl. Formulas, such as math calculations, references to other cells and built-in Excel features, can be added to cells through programming. Using openpyxl's range feature, you can copy formulas and paste them into more than one cell. Merging and splitting cells: you can join cells in both the horizontal and vertical directions in OpenPyXL. You can use this feature to make headers or combine cells for a certain style. On the other hand, you can split united cells back into their original states.\nHello, everyone! Today, I will share with you about Host Configuration with Fiber Channel. Introduction to Host Configuration: Storage host configuration refers to the process of setting up and configuring a computer or server to act as a storage host, providing storage services to other devices or clients on a network. The storage host is responsible for managing and providing access to storage resources, such as disks, drives, or network-attached storage (NAS) devices. The host configuration for the Huawei OceanStor Dorado V6 storage system depends on the specific requirements of your environment and the connectivity options available. However, I can provide you with a general overview of the host configuration process. Here are the key steps involved: Host Compatibility: Ensure that the host operating system and version are compatible with the OceanStor Dorado V6 storage system. Huawei provides specific compatibility lists and guidelines for different operating systems. Host Connection: Determine the type of connectivity required between the host and the OceanStor Dorado V6 storage system. Common options include Fibre Channel (FC), iSCSI (Internet Small Computer System Interface), and NVMe over Fabrics (NVMe-oF). Physical Connectivity Fiber Channel (FC) with host, Storage and SAN Switches: Initiator Configuration: Configure the host initiator settings to enable communication between the host and the OceanStor Dorado V6 storage system. This includes configuring initiator parameters such as the WWN (World Wide Name) for FC or the IQN (iSCSI Qualified Name) for iSCSI.\nHuawei OceanStor Dorado V6 Storage Services Block Service Hosts Groups Zoning (for FC): If you are using Fibre Channel, you may need to configure zoning on your Fibre Channel switches. Zoning ensures that only authorized hosts have access to specific storage resources. Initiators: FC initiators, also known as Fibre Channel initiators, are devices or components that enable the connection of servers or hosts to a Fibre Channel (FC) storage area network (SAN). They function as the initiators or sources of data requests and commands that are sent over the Fibre Channel network to access storage resources. FC initiators are typically host bus adapters (HBAs) or network interface cards (NICs) specifically designed for Fibre Channel connectivity. These devices contain ports or interfaces that connect to the FC fabric or switches within the SAN infrastructure. The FC initiators establish communication links between the servers and the storage devices or arrays. Hosts Groups Initiators, Initiators and chose to modify, Hosts: Hosts Groups Create Create Host host Host Groups: host groups provide a way to organize, manage, and control multiple hosts within a network or server environment. They enable streamlined administration, efficient configuration management, access control, monitoring, and troubleshooting, leading to improved efficiency and productivity in managing complex IT infrastructures. Host Groups LUN Mapping: Create and configure LUN (Logical Unit Number) mappings between the OceanStor Dorado V6 storage system and the host. LUN mapping allows the host to access specific logical volumes on the storage system. Multipathing: If you require high availability and improved performance, configure multipathing on the host.\nThe process of copying data to guarantee that all information remains similar in real-time between all data resources is known as data replication, also known as database replication. Consider database replication as a net that prevents your information from slipping through the cracks and disappearing. Data rarely remains constant. It changes constantly. Data from a primary database is continuously replicated in a replica, even if it's on the opposite side of the world, thanks to an ongoing process. Writing or replicating the same data to various locations is known as replication. Data can be duplicated, for instance, to or from a cloud-based server, to numerous storage devices on the same host, or to hosts in different regions. Data can be replicated in real time as it is written, altered, or deleted in the master source, or it can be copied on demand, sent in bulk or in batches in accordance with a schedule, or all three at once. By making data available on multiple hosts or data centers, data replication facilitates the large-scale sharing of data among systems and distributes the network load among multisite systems. Organizations can expect to see benefits including: Improved reliability and availability: If one system goes down due to faulty hardware, malware attack, or another problem, the data can be accessed from a different site. Improved network performance: Having the same data in multiple locations can lower data access latency, since required data can be retrieved closer to where the transaction is executing.\nIncreased data analytics support: Replicating data to a data warehouse empowers distributed analytics teams to work on common projects for business intelligence. Improved test system performance: Data replication facilitates the distribution and synchronization of data for test systems that demand fast data accessibility. Though replication provides many benefits, organizations should weigh the benefits against the disadvantages. The challenges to maintaining consistent data across an organization boil down to limited resources: Money: Keeping copies of the same data in multiple locations leads to higher storage and processor costs. Time: Implementing and managing a data replication system requires dedicated time from an internal team. Bandwidth: Maintaining consistency across data copies requires new procedures and adds traffic to the network. Consistent access to data can be provided by using data replication. Additionally, it expands the number of concurrent users who have access to data. By combining databases and updating slave databases with partial data, data redundancies are eliminated. Additionally, databases are accessible faster with data replication. Large amounts of storage space and equipment are needed to maintain data replication. Replication is expensive, and infrastructure upkeep is complicated in order to preserve data consistency. Additionally, it exposes additional software components to security and privacy flaws. The benefits of data replication are useful only if there's a consistent copy of the data across all systems. Following a process for replication helps ensure consistency. 1. Identify the data source and destination. 2. Select tables and columns from the source to be copied. 3. Determine the frequency of updates. 4.\nA database management system (DBMS) is system software for creating and managing databases. A DBMS makes it possible for end users to create, protect, read, update and delete data in a database. The most prevalent type of data management platform, the DBMS essentially serves as an interface between databases and users or application programs, ensuring that data is consistently organized and remains easily accessible. A database management system, or DBMS, is a complex piece of system software made up of numerous interconnected components that provide a standardized, controlled environment for creating, accessing, and editing data in databases. These components include the following: Storage engine. This basic element of a DBMS is used to store data. The DBMS must interface with a file system at the operating system (OS) level to store data. It can use additional components to store data or interface with the actual data at the file system level. Metadata catalog . Sometimes called a system catalog or database dictionary, a metadata catalog functions as a repository for all the database objects that have been created. When databases and other objects are created, the DBMS automatically registers information about them in the metadata catalog. The DBMS uses this catalog to verify user requests for data, and users can query the catalog for information about the database structures that exist in the DBMS. The metadata catalog can include information about database objects, schemas, programs, security, performance, communication and other environmental details about the databases it manages. Database access language .\nThe DBMS also must provide an API to access the data, typically in the form of a database access language to access and modify data but may also be used to create database objects and secure and authorize access to the data. SQL is an example of a database access language and encompasses several sets of commands, including Data Control Language for authorizing data access, Data Definition Language for defining database structures and Data Manipulation Language for reading and modifying data. Optimization engine . A DBMS may also provide an optimization engine, which is used to parse database access language requests and turn them into actionable commands for accessing and modifying data. Query processor . After a query is optimized, the DBMS must provide a means for running the query and returning the results. Lock manager . This crucial component of the DBMS manages concurrent access to the same data. Locks are required to ensure multiple users aren't trying to modify the same data simultaneously. Log manager . The DBMS records all changes made to data managed by the DBMS. The record of changes is known as the log, and the log manager component of the DBMS is used to ensure that log records are made efficiently and accurately. The DBMS uses the log manager during shutdown and startup to ensure data integrity, and it interfaces with database utilities to create backups and run recoveries. Data utilities . A DBMS also provides a set of utilities for managing and controlling database activities.\nExamples of database utilities include reorganization, runstats, backup and copy, recover, integrity check, load data, unload data and repair database. One of the primary benefits of utilizing a DBMS is that it enables concurrent access and usage of the same data by users and application programmers while maintaining data integrity. Instead of creating fresh iterations of the same data stored in new files for every new application, data is better safeguarded and maintained when it can be shared using a database management system (DBMS). Multiple users can access the central data store that the DBMS offers in a regulated manner. Central storage and management of data within the DBMS provide the following: data abstraction and independence; data security; a locking mechanism for concurrent access; an efficient handler to balance the needs of multiple applications using the same data; the ability to swiftly recover from crashes and errors; strong data integrity capabilities; logging and auditing of activity; simple access using a standard API; and uniform administration procedures for data. The ability of database administrators (DBAs) to enforce a logical, structured arrangement on the data using a DBMS is another benefit. Because DBMSs are designed for such tasks, they provide economies of scale for processing massive amounts of data. Access a lightweight relational database management system (RDMS) included in Microsoft Office and Office 365. Amazon RDS a native cloud DBMS that offers engines for managing MySQL, Oracle, SQL Server, PostgreSQL and Amazon Aurora databases.\nDear Members, I would like to share with you some news about data storage evolution in the Yottabyte Era. The Innovative Data Infrastructure Forum (IDI Forum) 2023 , revolving around the theme of \"New Apps New Data New Resilience,\" took place today in Munich, Germany. The Forum brings together global industry experts and partners to explore the future of digital infrastructure towards the yottabyte era ( ). At the forum, Huawei delved into a host of different topics, such as embracing the emerging application ecosystem, efficiently handling masses of unstructured data, and comprehensively improving data resilience, in order to unlock the value of data towards the evolution of data storage industry. Dr. Peter Zhou, Vice President of Huawei, President of Huawei IT Product Line, delivering a speech Huawei believes data storage faces major changes and great opportunities as digital transformation deepens: Firstly, 56% of enterprises deploy AI applications while 96% of enterprises plan to build cloud-native applications to deal with ever-changing data applications. Then, data is growing exponentially. 80% of new data is unstructured data with a compound annual growth rate (CAGR) of 38%. Last but not the least, data resilience faces severe challenges. Ransomware is constantly evolving and the number of ransomware attacks is growing at an annual rate of 98%. Worryingly, over 14% of enterprises are unable to restore their data after a ransomware attack. Emerging big data and AI applications pose higher requirements on the parallel processing of diversified data.\nThe collaboration model between data storage and data applications is being reconstructed to embrace new data paradigms. Cloud-native applications are becoming more prevalent in enterprise data centers. To tackle these challenges, reliable and high-performance container storage will be a necessity. As unstructured data used for production and decision-making systems becomes increasingly hot due to real-time access, the read/write bandwidth and I/O access efficiency of scale-out storage need to be significantly improved. To meet the need for cost-effective storage of mass unstructured data, innovations in software, hardware and algorithms are crucial for scale-out storage. In addition, with the explosion in data volume, severe data gravity challenges arise. Therefore, intelligent data fabric is required to implement global data view and unified data scheduling across systems, regions and clouds. Data resilience threats are evolving from natural disasters and physical damages to human-factor disasters such as ransomware. Hence, a shift from reactive response to proactive defense is critical to ensure higher data resilience. The surge in zero-day vulnerabilities and huge losses caused by unrecoverable data reveal the inadequacy of the current enterprise data resilience system. Such a system consists of networks, applications, and hosts, and is no longer adequate for meeting the latest data resilience requirements of enterprises. Data storage is becoming the last line of defense for data resilience, with more resilience features being integrated into data storage products, including ransomware detection, data encryption, secure snapshots, and data recovery in Air Gap.\nQ: An engineer maps the storage system to the file server to store various enterprise files. One day, the file system is attacked by viruses and files cannot be opened. To access the files, the engineer performs a snapshot rollback. Which statement is FALSE? A. A snapshot rollback can quickly restore data B. Some data may be lost during a snapshot rollback C. If there are writes to the source LUN during a snapshot rollback, the rollback is performed after the writes are complete D. When no host reads or writes data, the snapshot data is rolled back to the source volume in sequence The answer to your question is the option: C C. If there are writes to the source LUN during a snapshot rollback, the rollback is performed after the writes are complete Justification: Snapshot rollback is a mechanism that uses the snapshot of the source disk at a certain point in time to quickly restore data on the source disk. If data on the source disk is damaged or deleted by mistake, or the source disk is infected with viruses, you can use the snapshot rollback function to quickly restore data on the source disk to the point in time when the snapshot was created, reducing the amount of lost disk data. According to the snapshot rollback principle, option A is correct. The source disk data is restored to the point in time when the snapshot was created.\nWhat is Huawei Flash Link Technology? Innovative FlashLink Technology After continuously accumulating technical experience over the past 20 years, Huawei proudly launched the lightning-fast and rock-solid OceanStor Dorado V3 all-flash storage in 2016. Still a player in the field today, it delivers the industry-leading performance powered by the innovative FlashLink technology, and the high performance is maintained from three aspects: chip, architecture, and operating system. OceanStor Dorado V3 adopts three intelligent chips to achieve end-to-end service acceleration and provides performance 45% higher than SAS all-flash storage. Huawei is a groundbreaking telecommunication provider in that it continuously keeps up with the latest architectural technology trends and even develops its own technologies. For example, OceanStor Dorado V3 is one of the first all-flash storage systems to use NVMe in commercial use. Further, OceanStor Dorado V3 adopts a brand-new SSD-optimized design and disk-controller collaboration technology to enable storage controllers to detect data layouts in SSDs in real time and synchronize data in controllers and SSDs. This helps reduce performance losses caused by garbage collection and ensures rapid response to data read and write I/Os. While these are just the highlights of OceanStor Dorado V3s abundant back catalog, together they help maintain a predictable latency of 0.5 ms even under heavy workloads. The secret for such advancements is FlashLink, helping OceanStor Dorado to improve service performance by three times in comparison with traditional storage. Innovative Disk-Controller Collaboration Ensures Predictable High Performance The flash storage cells in an SSD can be re-written only after being erased.\nGenerally, the basic writing unit of an SSD is a 16 KB page, and the basic erasing unit is an 8 MB block. To avoid erasing valid pages, valid pages in a block need to be migrated to another space for storage. The block space of valid pages is converted into invalid page space, after which the block can be erased at a time. The process of migrating valid pages is known as garbage collection. Garbage collection improves the space re-utilization of an SSD, but each migration undermines the performance of the storage system. Large amounts of migrated valid data and shorter periods lasting from when each page is written to the SSD to when the page becomes invalid imposes greater impacts on the system performance. To ensure that they are maximizing the performance of SSDs and flash storage systems, enterprises must effectively control garbage collection. Powered by proprietary SSDs and the flash operating system, OceanStor OS, Huaweis OceanStor Dorado adopts an innovative disk-controller collaboration technology to prevent a drop in performance caused by garbage collection. By optimizing internal software algorithms, OceanStor Dorado enables storage controllers to detect the data layouts in SSDs in real time and make adjustments accordingly. This helps prevent data migration after being written to SSDs and garbage collection, ensuring predictable high performance for flash storage systems. Large Block Sequential Writes Reduce the Frequency of Garbage Collection Take real-time ridesharing as an example.\nThe Overview of Distributed Database HI HI, Greetings! HI April, Today, I would like to explain the overall Distributed Database. Let's get into the article. Databases are the basis of all current applications. Databases were first hosted on a single physical machine. In essence, it was a computer that was simply running the database program. In order to share resources among several operating systems and applications, we then switched to running databases on virtual machines. We switched to running databases in the cloud in recent years. Additionally, we no longer store the data in a single database instance. In today's databases, data is stored, managed, and accessed across a network of nodes, or many computers. As was already said, a \"distributed database\" is a database design that consists of multiple nodes cooperating. A node is essentially a computational instance that runs the database (it might potentially be a virtual machine or a container). In a distributed database, each node has a separate copy of the database, and these nodes converse with one another to ensure that they all have the same data. A centralized database is one that is kept in one place on the network and is located there. Due to the database's single location, multiple users can access it, making it easier for them to have a comprehensive view of the data. A centralized database makes it easier to manage, update, and backup data. Centralized databases, however, can reduce productivity and time efficiency due to increased utilization.\nDistributed Database Centralized Database Why do you need to switch from a centralized database to a distributed database? A distributed database may have completed the task in the past when data was measured in gigabytes and database users were counted in the dozens. An on-site mainframe machine holding the database is a typical situation for this type of architecture. Developers accessed the database, ran queries, received the results, and then disconnected. A single database administrator or system administrator was in charge of the system's performance, availability, and updates. Example Scenario : Use \"YouTube\" as an illustration. Its database architecture is current. The app's hundreds of millions of users use numerous devices worldwide. The system is being used by millions of people at once. It must be accessible constantly. In this case, it was impossible for \"YouTube\" to rely on a single computer running a centralized database program. Millions of people will experience service interruptions if it crashes. Furthermore, storing all the data in one location is neither possible nor advantageous from an economic standpoint. Consider storing all user data on a single server in a centralized database instance. As more members sign up for the service, the database backend ought to expand automatically. Thus, from the standpoint of availability, scalability, and fault tolerance, a single on-premises database is just impractical. Different distributed database architectures come in different varieties. The choice of architecture depends on the particular requirements of the application because each has advantages and disadvantages of its own.\nMaster-Slave Replication A single primary database controls all write operations in a master-slave architecture, while one or more slave databases duplicate the data from the master for read operations. Thus, all insert operations are directed to a single node, whereas read operations are split among multiple nodes. For applications that require a lot of reading, this configuration is perfect. Multi-Master Replication All nodes in a multi-master replication setup have master and slave read and write capabilities. Sharing-Nothing Architecture Data is shared in a shared-nothing architecture, and each node is only in charge of a portion of the data. In essence, data is divided across nodes, and each node is in charge of both reading and writing. Federated Database Architecture A meta-database that contains multiple independent databases (and even different database types) is called a federated database architecture. In essence, what you have here is a searchable virtual database that is united. The virtual database manager distributes the queries internally. Benefits of a Distributed Database Architecture Scalability Distributed databases may scale vertically in contrast to a single database, which can only scale horizontally. In other words, the only method to scale a single database to manage increased load is to increase memory and RAM. A distributed database allows for the insertion of new nodes. Fault Tolerance and Availability If you only have one database and it fails, the application will also fail if you have just one database.\nHowever, with a distributed database, losing a node won't stop the service and won't have an impact on the entire application. Data Protection Data can be divided among several nodes. As a result, the majority of the application's data will be secure even if a node is compromised. The same is true with corrupt data. Other nodes won't be impacted if a server or software fault causes node data to become corrupt. Network traffic is slow. By storing data close to where it will be utilized, distributed databases can save network traffic by eliminating the need to send data over the network. Pros Cons Modular Development In centralized database systems, expanding the system to more locations or units necessitates significant work and causes interruptions to how things are currently operating. Expensive software It is frequently important to provide data openness and cooperation across numerous sites using expensive software in a distributed database system. More Trustworthy The entire network of centralized databases comes to a halt in the case of a database failure. However, distributed systems may continue to operate even if a component fails,, albeit at a less efficient level. DDBMS is hence more dependable. Overhead When database replication is employed, several activities over numerous sites necessitate numerous calculations and ongoing synchronization, adding a significant amount of processing overhead. Improved Reaction If data is disseminated effectively, user requests can be satisfied from local data, resulting in a quicker response.\nIn contrast, centralized solutions necessitate that each query be handled by a separate computer, which increases the response time. Data reliability Data integrity, which is jeopardized by changing data at various sites, is a potential issue when employing database replication. Reduced Cost of Communication In distributed database systems, communication costs for data manipulation can be reduced if data is kept close to where it is most frequently needed. This is not feasible in centralized systems. An improper distribution of data For user request responses, effective data transmission is a crucial component, which implies that if data is not evenly spread across many sites, responsiveness may suffer. Enhanced Efficiency Due to the proximity of the data to the \"greatest demand\" point and the inherent parallelism of distributed DBMSs, database access times may be faster than those possible from centralized remote databases. Absence of standards Increased capacity to share and local autonomy Users in one location can access data kept at different locations, which might reflect the geographical dispersal of an organization. Insufficient professional assistance Complex data design Distributed database examples There are numerous vendors and examples of effective database solutions that you can use in a distributed architecture. The most preferred examples are as follows: The well-liked NoSQL document database MongoDB, which you can deploy across numerous servers. Instead of using tables or rows, it stores data in collections and documents. Apache Cassandra, a distributed database system with excellent scalability that is intended for handling massive amounts of organized and unstructured data across various data centers.\nA fully managed NoSQL database service called Amazon DynamoDB. How to choose Distributed Database architecture: There are various factors to take into account when deciding which database architecture to use for your business or application. Here, there is no right or wrong response. Every architecture has its own use cases; therefore, you should pick one that most closely matches yours. Data replication, consistency, and partitioning should be taken into account, among other things. Here are some of the actions you should take in more detail: Identify the information that needs to be entered into the distributed database and made accessible. This will determine the amount of storage, the schema design, and other factors. Choose an effective data division approach. Choose a method for partitioning data among different nodes. Select your replication plan. You have a choice of master-slave, multi-master, or another system. Select a model for consistency. Select whether strongly consistent, eventually consistent, or both are required for your data to be consistent between nodes. A distributed database is a database system where the information is split across several nodes or computers linked by a network. In comparison to a centralized database system, it offers a number of benefits, including scalability, fault tolerance, and superior performance. However, creating and maintaining a distributed database may be difficult, and there are a number of issues that must be resolved, including network latency, data security, and consistency issues.\nAre there any security features or access controls available in the Device Manager for OceanStor Pacific to ensure secure storage management? Can the Device Manager be used for firmware upgrades and software patch management on OceanStor Pacific systems? If yes, what is the process involved? Does the Device Manager support integration with other management tools or platforms for a unified storage management experience? How does the Device Manager assist in troubleshooting and diagnosing issues in OceanStor Pacific storage environments? Are there any automation capabilities or APIs available in the Device Manager to streamline storage management tasks? Can the Device Manager generate reports and provide insights on storage capacity utilization, performance trends, and overall system health for OceanStor Pacific? OceanStor DeviceManager for OceanStor Pacific provides several security features and access controls to ensure secure storage management. Some of these features include: 1-Role-Based Access Control (RBAC): DeviceManager supports RBAC, allowing administrators to define user roles and assign appropriate permissions. This helps enforce access controls and restrict unauthorized access to storage resources. 2-Authentication and Authorization: DeviceManager supports authentication mechanisms such as LDAP (Lightweight Directory Access Protocol) and Active Directory integration. Administrators can leverage existing user directories for authentication, ensuring secure access to the management interface. 3-Secure Socket Layer (SSL): DeviceManager supports SSL encryption for secure communication between the management interface and storage systems. This helps protect sensitive data and ensures secure remote management. 4-Audit Logs: DeviceManager logs user activities and system events, providing an audit trail for security and compliance purposes.\nAdministrators can review these logs to track changes, identify security issues, and investigate any suspicious activities. DeviceManager can be used to perform these tasks on OceanStor Pacific systems. The general process involves the following steps: Obtaining the Firmware/Software: Download the latest firmware or software package from the official Huawei support website or other authorized sources. Uploading the Package: In DeviceManager, navigate to the firmware/software upgrade section and upload the package to the storage system. Initiating the Upgrade: Select the appropriate upgrade task and start the upgrade process. DeviceManager will guide you through the process, which typically involves verifying the package, validating compatibility, and initiating the upgrade. Monitoring the Upgrade: DeviceManager provides real-time progress updates during the upgrade process. You can monitor the status, view logs, and ensure that the upgrade completes successfully. DeviceManager supports integration with other management tools or platforms , allowing for a unified storage management experience. It offers standard APIs (such as RESTful APIs) that enable integration with third-party tools and platforms, including orchestration frameworks, monitoring systems, and automation solutions. This integration facilitates centralized management, monitoring, and automation of storage resources within a broader IT infrastructure. When it comes to troubleshooting and diagnosing issues in OceanStor Pacific storage environments, DeviceManager offers several features: 1- Real-time Monitoring: DeviceManager provides real-time performance monitoring and alerts for various system components, including controllers, storage pools, volumes, and disks. It allows administrators to identify bottlenecks, track performance metrics, and detect abnormalities. 2- Diagnostics and Logs: DeviceManager provides diagnostics tools to identify and troubleshoot storage-related issues.\nHi there! This time, I will share with you an important article about End-to-end data encryption, WORM, secure snapshot, and air gap are all important technologies that help enhance the security and integrity of data in storage systems. End-to-end data encryption: This technology involves encrypting data both while it is in transit and at rest to prevent unauthorized access. End-to-end encryption ensures that even if an attacker gains access to the data during transmission or while it is stored on a device, they will not be able to read it without the decryption key. What Is It and How Does It Work? Start to finish email encryption gives the most significant level of classification and security for your email correspondence. We should peruse this article to figure out how it functions. The advantages of end-to-end encryption: Guarantees your information is secure from hacks: Guarantees your information is secure from hacks: With start to finish encryption, you are the one in particular who has the confidential key to open your information. It doesn't make any difference in the event that the server is penetrated; your information is protected. Protects your privacy: When you use providers like Google and Microsoft, your data is decrypted on their servers. This means they can read it. And if they can access your data, so can hackers. Protects admins: Admins arent honey pots. They dont control data access, so they cant be leveraged as a single point of vulnerability.\nWORM (Write Once Read Many): This technology ensures that data cannot be modified or deleted once it has been written to a storage device. This is particularly useful in regulatory environments where data needs to be kept for a specified period of time and cannot be altered. WORM technology ensures that data is tamper-proof, maintaining its integrity. The accompanying developments license more itemized characterization of the media and the assurances: Software WORM As the name demonstrated, this class of insurance depends simply on programming usefulness. Network capacity frameworks are a genuine illustration of the class of these supposed SoftWORMs. FileLock from Grau Information and Snap Lock from NetApp are a portion of the better known items. System WORM The class of framework WORMs incorporates insurance which accomplishes its security utilizing the firmware of inside regulators or processors. For instance, the regulators of specific USB sticks can guarantee WORM-ability along these lines. Hardware WORM Capacity media which accomplish their security on the actual level are known as equipment WORMs. Disc R and DVD-R are a genuine illustration of this. The outer layer of the stockpiling media is intended to be composed on just a single time. Secure snapshot: This technology captures a point-in-time copy of data, allowing administrators to quickly recover from data corruption, human error, or other disasters. Secure snapshots use encryption and access controls to ensure that the data is protected from unauthorized access or modification. A secure snapshot is a term commonly used in the context of data backup and recovery.\nIt refers to a point-in-time copy of data that is taken and stored in a secure location, ensuring that the data is protected from loss, corruption, or unauthorized access. To create a secure snapshot, the data is first backed up to a separate location, such as an offsite data center or cloud storage. The backup process should use encryption to ensure the data is protected while in transit and at rest. Once the backup is complete, the data is then locked down to prevent any modifications or tampering. This ensures that the secure snapshot represents an accurate and complete copy of the data at a specific point in time. Secure snapshots are an important tool for businesses and organizations to protect critical data and ensure business continuity in the event of a disaster or system failure. By having a secure and up-to-date copy of their data, they can quickly restore operations and minimize downtime. AIR GAP: This technology involves creating a physical separation between a storage system and the rest of the network to prevent unauthorized access or cyberattacks. This can be achieved by using a separate network or by physically disconnecting the storage system from the network. Together, these technologies provide multiple layers of protection to ensure that data is secure, tamper-proof, and available when needed. They are particularly useful in environments where data security and integrity are critical, such as healthcare, financial services, and government. What are the types of air gap backup?\nHello all, Today we will talk about one-stop data solution for orders, contracts, and user profiles. As the company's digital transformation campaign begins, enterprise data is bound to explode. In addition, cross-analysis of various systems will lead to higher data usage costs, We urgently need a one-stop data solution, including the following: P-level storage scale : Centralized data management, including the original structured data storage (200 TB) and the access of more and more unstructured data (such as user behavior logs, images, videos, and documents) after digital transformation, will embed big data applications into more and more service scenarios. T-level computing capability : Large-scale pre-processing and test calculation. For example, more and more super wide tables (possibly thousands of dimensions) can be defined based on orders, contracts, and user profiles, and T-level barcode information scanning can be performed. Heterogeneous data access : Data storage will be diversified. For example, tables from the original OGG are stored in Oracle, and barcode information that supports key-V fast query is stored in HBase. Cross-database data can be analyzed using query engines Spark and Hive. Directly read localized metadata to implement cross-analysis. However, the actual data may be stored in multiple environments, such as HDFS, HBase, or Oracle. Large-throughput data pipe : Massive service data can be quickly aggregated to a data lake for downstream big data analysis and calculation and model prediction. If the timeliness cannot keep up with the prediction accuracy, the value will be lost.\nTrend analysis Upgraded SSD tech promotes all-flash storage to diverse industries The evolution of SSD technologies has enabled all-flash storage to meet a wider range of service scenarios in various industries. NVMe SSDs adopt the NVMe standard to offer twice the performance of traditional SAS SSDs. Equally, NVMe over Fabrics (NVMe-oF) is an enhanced protocol employed by SSDs to provide sub-ms latency for storage networks. Enabling improved performance with every generation, all-flash storage is perfect for real-time ultra-low latency scenarios such as transaction systems. The global wear-leveling technology uses algorithms to evenly write data to cells of all SSDs in the storage system, greatly extending the SSD service life and enabling enterprises to use all-flash storage in core service scenarios. Figure 1: Architecture and performance comparison between traditional and NVMe storage Similarly, data reduction technology is used to reduce data volumes without compromising the data, thereby reducing the number of disks and devices required in a storage environment. Todays data reduction ratios are as high as 4:1 or 5:1, helping slash the purchase costs of all-flash storage and allowing all-flash storage to enter more non-core service deployments. These technologies are a catalyst for all-flash storage to be perfectly deployed in enterprises core transaction and production systems, and can also be used in decision-making, operational support, and backup systems. Inexpensive all-flash: Quad-level cells (QLCs) and extra cell layers in 3D NAND NAND cells are the core component of enterprise-level SSDs, and determine the cost of SSDs.\nMost mainstream vendors now adopt NAND cells with 176 layers, and have released the 200-layer (nearly double that of 2018) design roadmap. For example, the Micron roadmap states the cell layers in a 3D NAND device will exceed 200, 300, and even 400, which will in turn slash the price of every TB of SSDs A combination of many stacking layers with TLC/QLC/PLC can significantly reduce the price of a single SSD. IDC predicts that by 2025, the cost per unit of SSD capacity will be lower than 10K RPM HDDs and higher than large capacity HDDs used for cold data storage. Figure 2: SSD price trend per unit capacity predicted by IDC Breakthroughs in cell technology and PCIe 5.0 are improving the capacity of SSDs, with many enterprises now adopting 15.36 TB SSDs or even 31 TB SSDs. Thanks to lower procurement costs, large-capacity SSDs will help develop storage environments in data centers. Worldwide drive for all-flash storage The ratio of all-flash storage in developed countries is on average over 45%, including 56.3% in the US, 54.7% in Australia, 54.3% in Netherlands, and 50% in Sweden. But now, countries that traditionally have a low all-flash usage ratio are adopting it more. In 2021, the annual growth rate of the all-flash storage market in Indonesia reached 54%, while in Poland, China, and Mexico, it reached 34%, 24%, and 14%, respectively. This trend is expected to continue in the future.\nWhat we suggest Plan all-flash storage tailored to enterprise current and future data volumes and requirements Digital transformation is the cause of huge data growth and service pressure. When making their storage construction plans, enterprises should evaluate the current and future IT system requirements in advance. Then, they can ensure a premium experience and strategically approach the new round of all-flash storage competition among vendors. All-flash storage is an excellent choice for enterprises technical teams to improve performance. By working with storage providers to evaluate the future data volumes and service pressure trends, enterprises can formulate all-flash storage strategies, and analyze benefits and O&M cost changes after the strategies are implemented. Seize the opportunity to replace legacy storage with all-flash models HDDs are common in many enterprise storage environments, but most are approaching the end of the warranty period. Enterprises that are undergoing digital transformation urgently need better, more performant storage devices. This is an excellent opportunity for storage enterprises to promote all-flash storage. The zero-interruption service migration is a key selling point of mainstream vendors, and a must-have for enterprises. All-flash storage: excellent performance, rock-solid reliability, and large capacity for much lower CAPEX and OPEX Data is the core of digital transformation. Data center consolidation helps gather data scattered on many devices to minimal devices. This reduces purchase, maintenance, management, and power consumption costs, and facilitates data value mining and enables service growth.\nDear All, Today we are going to learn about Huawei Distributed Storage High Performance. DHT routing technology In Huawei distributed storage, the block service uses the distributed hash table (DHT) routing algorithm. Each storage node stores a small proportion of data, and the data is routed and stored using the DHT routing algorithm. This DHT algorithm features balance and monotonicity. Balance: Data is distributed to all nodes as evenly as possible, thereby balancing loads among nodes. Monotonicity: When new nodes are added to the system, the system redistributes data among nodes. Data migration is implemented only on the new nodes, and the data on the existing nodes is not significantly adjusted. Traditional storage systems typically employ the centralized metadata management mechanism, which allows metadata to record the disk distribution of the LUN data with different offsets. For example, the metadata may record that the first 4 KB of data in LUN1+LBA1 is distributed on LBA2 of the 32nd disk. Each I/O operation initiates a query request for the metadata service. As the system scale grows, the metadata size also increases. However, the concurrent operation capability of the system is subject to the capability of the server accommodating the metadata service. In this case, the metadata service may become a performance bottleneck of the system. During system initialization, Huawei distributed storage system sets partitions for each disk based on the value of N and the number of disks. For example, the default value of N is 3600 for two-copy backup.\nIf the system has 36 disks, each disk has 100 partitions. The partition-disk mapping is configured during system initialization and dynamically adjusted based on the number of disks. The partition-disk mapping table occupies only a small space, and Huawei distributed block storage nodes store the mapping table in the memory for rapid routing. Huawei distributed block storage does not employ the centralized metadata management mechanism and therefore does not have performance bottlenecks incurred by the metadata service. Huawei distributed block storage logically divides a LUN by every 1 MB of space. For example, a LUN of 1 GB space is divided into 1024 slices of 1 MB space. When an application accesses block storage, the SCSI command carries the LUN ID, LBA ID, and I/O data to be read/written. The OS forwards the message to the VBS of the local node. The VBS generates a key based on the LUN ID and LBA ID. The key contains rounding information of the LBA ID based on the unit of 1 MB. The result calculated using DHT hash indicates the partition. The specific disk is located based on the partition-disk mapping recorded in the memory. The VBS forwards the I/O to the OSD to which the disk belongs.\nFor example, if an application needs to access the 4 KB data identified by an address starting with LUN1+LBA1, Huawei distributed storage first constructs \"key=LUN1+LBA1/1M\", calculates the hash value for this key, performs modulo operation for the value N, gets the partition number, and then obtains the disk of the data based on the partition-hard disk mapping. Each OSD manages a disk. During system initialization, the OSD divides the disk into slices of 1 MB and records the slice allocation information in the metadata management area of the disk. After receiving an I/O from the VBS, the OSD searches for the data fragment information on the disk based on the key, reads or writes the data, and returns the data to the VBS. In this way, the entire data routing process is completed. The DHT routing technology helps Huawei distributed storage quickly locate the specific location where data should be stored based on service I/Os, avoiding searching and computing in massive data. This technology uses Huawei-developed algorithms to ensure that data is balanced among disks. In addition, when hardware is added or removed (due to faults or capacity expansion), the system automatically and quickly adjusts the hardware to ensure the validity of data migration, automatic and quick self-healing, and automatic resource balancing. Dynamic intelligent partitioning and static disk selection algorithms During data persistence, Huawei distributed storage adopts two-layer algorithms to optimize the performance and reliability of distributed storage. One is to create Plogs and select the dynamic intelligent partitioning algorithm of PT for Plogs.\nThe other is to select the local static disk selection algorithm of OSDs for PT. The dynamic intelligent partitioning algorithm introduces an adaptive negative feedback mechanism to achieve superb reliability and performance. Its major improvements and objectives are as follows: Write reliability is not degraded. If the partition corresponding to a Plog falls into a faulty disk, the Plog is discarded and a new Plog is selected to write data. Loads are balanced and hotspots are eliminated. In random access scenarios, polling or distributed hash algorithms cannot fully ensure balanced data layouts and disk access performance. For example, in some storage systems based on the CRUSH hash algorithm, the utilization difference between OSDs reaches 20%, causing continuous occurrence of hotspot disks. In addition, disk fault recovery, slow disk hotspots, and QoS. traffic control affect system performance. The Plog Manager of Huawei distributed storage periodically collects the available capacity and I/O load of disks, nodes, and partitions, and intelligently identifies hotspots and fast and slow disks. The local static disk selection algorithm aims to optimize the local balancing of the mapping between a partition and an OSD. Optimization is achieved in the following scenarios: Data balancing performance is optimized without affecting reliability when nodes are added, reducing invalid data migration. In the algorithm, cyclic selection of partitions by partition group rather than conventional disk[1]based balancing is used. Intra-group balancing significantly reduces invalid data migration without degrading reliability.\nThe invalid data migration ratio is only 1% to 3% of that of the CRUSH algorithm, and the actual invalid data migration ratio is only 0.05% to 0.8%. In multi-copy scenarios, the balancing of primary partitions is optimized. Common algorithms only focus on the balancing of initial primary copies. Once a node or a disk becomes faulty, primary copies are selected again for many data blocks. Whether the new primary copies are evenly distributed affects the current service performance and the performance of subsequent incremental data synchronization. The existing solution is static fixed selection of primary copies, that is, selecting secondary copy 1 first and then secondary copy 2 after a primary copy becomes faulty. Huawei distributed storage adopts the dynamic primary selection solution. In the solution, it selects a primary disk based on the workloads of the disks where the secondary copies reside to prevent some disks from becoming hotspots after a fault occurs. The improved algorithm enhances the data balancing speed by up to 28.8% in actual tests. In EC scenarios, the number of supported concurrent data reconstruction tasks is improved and performance is increased. The correlation between PT groups and global disks is also used for small-scale calculation. The smaller the number of data blocks read from a single normal disk, the more the disks involved in load balancing. With the improvement, the disk read latency is shortened by up to 45% during system restoration.\nIntelligent CPU partitioning algorithms Intelligent CPU partitioning algorithms are categorized into the CPU grouping algorithm and CPU core splitting algorithm. The algorithms group CPUs and divide cores based on service types, identify I/O types (host I/Os, replication I/Os, and background I/Os), and intelligently schedule I/Os based on priorities to ensure stable read and write latency CPU grouping algorithm: Read and write I/Os and other I/Os are deployed in different groups to avoid mutual interference. Dedicated cores are used for key services to reduce latency. Multiple cores are shared and allocated for load balancing among multiple services. CPU core splitting algorithm: A request is executed on the same core to implement the \"Run-to-Complete\" scheduling principle. No thread switchover is performed on the same core to ensure operation atomicity. This avoids frequent multi-core switchovers and improves the CPU cache hit ratio. EC intelligent aggregation technology Erasure coding (EC) increases the computing overhead, and a poor EC design brings more write penalties. Therefore, the performance of products using EC may be significantly lower than that of products using multi-copy storage. Write penalty of EC: This section uses 4+2 redundancy as an example. If the size of data to be written is less than 32 KB (for example, only 16 KB), 16 KB of data is written for the first time. During the second write, the 16 KB data written earlier must be read and combined before being written to disks. This causes the read overhead. This problem does not occur when full stripes are delivered.\nThe intelligent aggregation EC based on append write ensures EC full-stripe write at any time, reducing read/write network amplification and disk amplification by several times. Data is aggregated at a time, reducing the CPU computing overhead and providing ultimate peak performance. In the cache, data of multiple LUNs is aggregated into a full stripe, reducing write amplification and improving performance. Huawei distributed storage provides intelligent I/O aggregation to use different policies for different I/Os, ensuring the read/write performance. Large I/Os are formed into EC stripes and directly written to disks without being cached, saving cache resources. When SSDs are used as cache media, the service life of SSDs can be extended. Small I/Os are written to the cache and an acknowledgement is returned immediately. In the log cache, small I/Os of different LUNs are aggregated into large I/Os to significantly increase the probability of aggregation and improve performance. Adaptive global deduplication and compression Huawei distributed storage systems support adaptive inline and post-process global deduplication and compression, which can provide ultimate space reduction and reduce users' TCO with proper resource consumption. Adaptive inline and post[1]process deduplication indicates that inline deduplication automatically stops when the system resource usage reaches the threshold. Data is directly written to disks for persistent storage. When system resources are idle, post-process deduplication starts. After the deduplication is complete, the compression process starts. The compression is 1 KB aligned. The LZ4 algorithm is used to support HZ9 deep compression to obtain a better compression ratio.\nDeduplication and compression can be enabled or disabled as required. Huawei distributed storage supports global deduplication and compression, as well as adaptive inline and post-process deduplication. Deduplication reduces write amplification of disks before data is written to disks. Huawei distributed storage adopts the opportunity table and fingerprint table mechanism. After data enters the cache, the data is sliced into 8 KB data fragments. The SHA-1 algorithm is used to calculate 8 KB data fingerprints. The opportunity table is used to reduce invalid fingerprint space, thereby reducing cost in memory. Multi-level cache technology The write cache process is as follows: Step 1: Data is written to the RAM-based write cache (Memory Write Cache). Step 2: Data is written to the SSD WAL cache (for large I/Os, data is directly written to HDDs) and a message is returned to the host indicating that the write operation is complete. Step 3: When the memory write cache reaches a specified watermark, data starts to be flushed to disks. Step 4: Data of large I/Os is written to the HDDs directly. Data of small I/Os is first written to the SSD write cache and then to the HDDs after the small I/Os have been aggregated into large I/Os. Note: If the data written in step 1 exceeds 512 KB, it is directly written to the HDDs, as described in step 4. The read cache process is as follows: Step 1: Data is read from the memory write cache. If the read I/O is hit, data is returned.\nOtherwise, the system proceeds to step 2. Step 2: Data is read from the memory read cache. If the read I/O is hit, data is returned. Otherwise, the system proceeds to step 3. Step 3: Data is read from the SSD write cache. If the read I/O is hit, data is returned. Otherwise, the system proceeds to step 4. Step 4: Data is read from the SSD read cache. If the read I/O is hit, data is returned. Otherwise, the system proceeds to step 5. Step 5: Data is read from the HDDs Note: Pre-fetched data, such as sequential data, is cached to the memory read cache. Hotspot data identified during the read process is cached to the SSD read cache. Append only plog technology Huawei distributed storage supports HDD and SSD media at the same time. HDDs and SSDs have great differences in technical parameters such as bandwidth, IOPS, and latency. Therefore, I/O patterns applicable to both media are different. Huawei distributed storage leverages Append Only Plog technology for unified management of HDDs and SSDs. The Append Only Plog technology provides the optimal disk writing performance model for media. Small I/O blocks are aggregated into large ones, and then large I/O blocks are written to disks in sequence. This write mode complies with the characteristics of disks. Intelligent load balancing technology Huawei distributed storage evenly distributes data and metadata on nodes and automatically balances loads, eliminating metadata access bottlenecks and ensuring system performance during capacity expansion.\nDear All, Today we are going to learn aboutSmartMigration SmartMigration is an essential technology for the seamless migration of service data, both within a storage system and between different storage systems, without causing any disruptions. As storage technology advances and digitalization continues to transform various industries, there is a growing need to replace outdated storage systems, re-allocate storage resources, and adjust service performance. This has created a significant demand for non-disruptive service data migration within and between storage systems. However, in critical areas, service interruption may result in data loss and significant economic impact. To address this issue, SmartMigration was developed. It is designed to migrate service data within a storage system without disrupting host services, and enables data migration between Huawei storage systems and other compatible storage systems. SmartMigration Screenshot from Storage GUI as a reference SmartMigration principally applies to migration of data on LUNs for optimizing service performance or facilitating device upgrade. SmartMigration is implemented in the following two phases Service Data Synchronization After creating a SmartMigration task, you need to use the source and target LUNs to create a pair. Service data synchronization between the source and target LUNs involves an initial synchronization and data change synchronization LUN Information Exchange The LUN information exchange involves exchanging only the data volume IDs of the source and target LUNs. LUN IDs of the source and target LUNs remain unchanged. Splitting Splitting is performed on a single pair.\nDear All, Today we are going to learn aboutSmartVirtualization. SmartVirtualization is an innovative virtualization solution created by Huawei, which enables seamless connectivity between heterogeneous storage systems. Specifically, this feature is designed to enable OceanStor Dorado V6 series storage systems to utilize and manage storage resources of other Huawei or third-party storage systems, regardless of any differences in their software or hardware architectures. By overcoming incompatibility issues between various storage systems, SmartVirtualization empowers users to effectively manage their storage environment and leverage storage resources from both legacy and modern storage systems, thus providing a significant return on investment for customers SmartVirtualization offers several benefits, including: Excellent compatibility: It enables a local storage system to work seamlessly with various heterogeneous storage systems, making it easier to manage storage resources centrally. Low storage space occupation: The local storage system doesn't have to create complete copies of physical data when using the storage space provided by external LUNs from a heterogeneous storage system. Therefore, the local storage system won't waste its storage capacity. Diversified value-added functions : The SmartVirtualization technology allows a local storage system to configure value-added functions like SmartMigration for external LUNs to ensure data security and reliability. SmartVirtualization-based Data Migration Solution offers non-disruptive migration that supports online takeover for heterogeneous SAN migration. This is one of the most significant advantages of Huawei's migration solution over those of other vendors. It is commonly used when OS native Multipath software or Huawei multipath software is installed. Dedicated migration tools help automate the migration process. Steps.\nConnect the source storage to the target storage and map the LUNs to be migrated to the target storage. On the target storage, create eDevLUNs with the online takeover property and map them to hosts. Host multipath software aggregates the I/O paths of source LUNs and eDevLUNs, switching host I/O to the paths of the target storage. Create SmartMigration tasks on the target storage to migrate data from the eDevLUNs to the internal LUNs on the target storage. Disconnect the source storage from the target storage and live network after service data acceptance. Application scenarios Heterogeneous SAN migration (online takeover compatible) Homogeneous SAN migration Host resource usage Data copy between storage devices does not use host resources. Migration speed 5 MB/s to 1000 MB/s per task. The upper limit of the migration speed is customizable. A maximum of 32 migration tasks can be performed concurrently in one storage at a total migration speed of 3 GB/s. The migration speed can be automatically adjusted based on service cycle. Migration tasks can be paused and are resumable. Downtime Zero downtime Compatibility Requirements for Non-disruptive Migration The main compatibility items to be considered include the operating system, host multipath software, cluster software, and source storage model. The following table is used only for analysis. Contact Huawei migration service team for detailed version or model evaluation. The range of table is still being updated.\\ Working Principle of SmartVirtualization Huawei's eDevLUN technology is composed of both data and metadata, which are mapped to ensure optimal performance.\nHello Everyone, In the face of high-performance computing, big data analysis, surge-typeIOhigh-concurrency, low-latency applications, the existingTCP/IPsoftware, hardware architecture and technical features of highCPUconsumption cannot meet the application requirements.This should be reflected inthe excessive processing delay of tens of microseconds,multiple memory copies, interrupt processing, context switching, complexTCP/IPprotocol processing,excessive network delay,store-and-forward mode, and packet loss cause additional delay. After reading the article, you will find out whyRDMAcan better solve this series of problems. What isRDMA? RDMAis a remote memory direct access technology.RDMAwas originally dedicated to the Infiniband architecture . With the emergence ofRoCE and iWARP under the general trend of network convergence , this enables RDMA with high speed, ultra-low latency, and extremely lowCPUusageto be deployed on the most widely used Ethernet at present. RDMAC (RDMA Consortium) andIBTA(InfiniBand Trade Association) lead the development ofRDMA. RDMACis a supplement ofIETFand mainly definesiWRAPandiSER.IBTAis the standard maker ofinfinibandand supplementsthe standardization ofRoCE v1 & v2.IBTAexplains the characteristic behaviors thatRDMAshould have during the transmission process, and the transmission-relatedVerbsinterface and data structure prototypes are completed by another organization,OFA(Open Fabric Alliance). Compared with the internal busIOof traditionalDMA,RDMArealizes the direct transmission ofBuffer between the application software of two endpoints through the network; compared with traditional network transmission,RDMAdoes not need the intervention of operating system and protocol stack.RDMAcan easily realize ultra-low latency and ultra-high throughput transmission between endpoints, and basically does not requireCPU,OSand other resources to intervene, and does not need to consume too many other resources for network data processing and moving.\nInfiniBand: InfiniBanduses the following technologies to ensure low-latency (sub-microsecond) network forwarding, using theCut-Throughforwarding mode to reduce forwarding delay;credit-based flow control mechanism to ensure no packet loss; hardware offloading;Bufferas small as possible, Reduce the delay of packet buffering. iWARP : iWARP (RDMA over TCP/IP)utilizes a matureIPnetwork; inherits the advantages ofRDMA;TCP/IPhardware implementation costs are high, but if the traditionalIPnetwork is used, packet loss will have a great impact on performance. The RoCEperformance is equivalent to that ofthe IBnetwork;the DCBfeature ensures no packet loss; the Ethernet needs to supportthe DCBfeature; the delay of the Ethernet switchis slightly higher than that ofthe IB switch. RoCEv2: RoCEv2has made some improvementstoRoCE , such as the introduction ofIPto solve the scalability problem, enabling cross-layer networking; the introduction ofUDPto solveECMPload sharing and other problems. Extendthe RDMA APIinterface to be compatible with existing protocols/applications.The OFED (Open Fabrics Enterprise Distribution)protocol stackis released bythe OpenFabric Alliance and is divided intoLinuxandWindowsversions, which can be seamlessly compatible with existing applications.By combining existing applications withRDMA, the performance is doubled. The transmission interface layer (Software Transport Interface)betweenthe application andRNIC(RDMA-aware network interface controller)is calledVerbs.OFA (Open Fabric Alliance)provides a seriesof Verbs API forRDMAtransmission.OFAhas developedthe OFED(Open Fabric Enterprise Distribution) protocol stack, which supports multipleRDMAtransport layer protocols. OFEDnot only provides the basic queue message service ofRNIC (implementingRDMA andLLP (Lower Layer Protocol))downward, but also providesULP(Upper Layer Protocols) upward. ThroughULP ,the upper layer application does not need to directly interface withthe Verbs API, but uses Because of the connection betweenULPand applications, common applications can run on theRDMAtransport layer without modification.\nHi there! This time, I am sharing with you a very challenging article about IOPS. In this article you will learn aboutWhat IOPS Means? IOPS Performance Characteristics. How to Measure IOPS? IOPS is defined as the number of I/O operations per second.One of the main characteristics for evaluating the performance of a planned or existing storage system, RAID array, HDD or SSD drive.In other words, this is the number of blocks that can be written to or read from the device per unit of time.The more IOPS, the more productive the system. The value of IOPS depends on many factors: Disc structure. Disk setup. Array construction. Array setup. Configuration and type of RAID array. The number of disks in the array. The presence and size of the cache. hybrid discs. Disc quality. Throughput of interfaces. Block size. RAID controller. Conditions for running a program that measures IOPS. Presence of other background tasks. It is customary to distinguish several types of IOPS: Sequential read IOPS. Serial write IOPS. Random read IOPS. Random write IOPS. TOTAL IOPS - The total value of read and write IOPS (different for sequential and random access). Performance Specifications: The main measured quantities are linear (sequential) and random (random) access operations as shown in below figure. Linear read / write operations: In this measure the parts of files are read sequentially, one after another, we mean the transfer of large files (more than 128 K).\nRandom Operations: In this operation read data randomly from different areas of the media, usually associated with a block size of 4 KB. Below are the main features: Parameter Description Total IOPS (Total IOPS) Total IOPS (both read and write) Random Read IOPS Average random reads per second Random Write IOPS Average random writes per second Sequential Read IOPS Average linear reads per second Sequential Write IOPS Average linear writes per second Approximate IOPS: Approximate IOPS values for hard drives. Device Type IOPS Interface 7,200 rpm SATA drives HDD ~75-100 IOPS SATA 3Gb/s 10,000 rpm SATA drives HDD ~125-150 IOPS SATA 3Gb/s 10,000 rpm SAS drives HDD ~140 IOPS SAS 15,000 rpm SAS drives HDD ~175-210 IOPS SAS Approximate IOPS for SSDs. Device Type IOPS Interface Intel X25-M G2 MLC SSD ~8,600 IOPS SATA 3Gb/s OCZ Vertex 3 SSD ~60,000 IOPS (4K Random Write) SATA 6Gb/s OCZ RevoDrive 3 X2 SSD ~200,000 IOPS (4K Random Write) PCIe OCZ Z-Drive R4 CloudServ SSD ~1,400,000 IOPS PCIe The workload characteristic is generally considered as the %age of reads and writes that the application generates or requires.For example, if we consider in a VDI environment, the IOPS percentage is 80-90% for writes and 10-20% for reads.The characteristics of the workload is the most important as well as critical factor, since it determines the choice of the optimal RAID for the environment.It is ultimate to say that the Write-intensive applications are considered good for RAID 10, while read-intensive applications can be placed on RAID 5.\nFor example, let's take a disk: Seagate ST500DM002-1BC142 To calculate IOPSwe use the equation: IOPS = 1/(avgLatency + avgSeek) IOPS = 1/(0.00416 + 0.0085) = 78,9889415 In total, the maximum IOPSis 79. In a storage system design, calculating the performance of a disk system is very critical to the performance of a given system and most systems use RAID technologyto provide storage redundancy. In the below discussion we will discussed to how IOPSare calculated for RAIDarrays. Calculating the maximum IOPSread value ( maxReadIops) for a RAIDarray: maxReadIops = numDisks * diskMaxIops Accordingly, for an array of 4 disks, the maximum IOPS value will be as follows:read maxReadIops = 4 * 79 maxReadIops = 316 Calculating the maximum write IOPSvalue ( maxWriteIops) is different for RAIDarrays. RAIDarrays have a write penalty, and the RAID type determines the severity of the penalty. provides This penalty is a result of the redundancy that RAID, since the array must necessarily write data to multiple drives/locations to ensure data integrity. In the table below we are discussing about the most used RAIDtypes and their write penalties as shown below.\nWhat is Data Replication? Data replication refers to the process of creating and maintaining copies of data in multiple locations or systems to ensure redundancy, availability, and reliability. It involves duplicating data from one location or system (referred to as the \"source\") to one or more additional locations or systems (referred to as the \"targets\" or \"replicas\"). Data replication is commonly used in various computing and storage scenarios, including databases, file systems, cloud storage, and distributed systems. Storage Based Replication Techniques: Full Volume replication (Cloning) Full volume replication, also known as cloning, is a data replication technique used in computer systems to create an exact copy or replica of an entire volume or disk, including all its data and metadata. This replication technique can be used for various purposes, such as creating backups, creating test or development environments, or setting up disaster recovery solutions. Pointer based Virtual replication (Snapshot) Pointer-based virtual replication, also known as snapshot-based replication, is a data replication technique used in computer systems to create a point-in-time copy of data using pointers or references, rather than physically duplicating the data. Snapshots are created by capturing the state of a volume or disk at a specific moment in time, and they can be used for various purposes, such as data protection, data recovery, and data analysis. Storage Based Remote Replication Techniques: Synchronous Replication Asynchronous Replication Multi-site Replication Summary: Data replication is the process of creating and maintaining multiple copies of data in different locations or storage systems to ensure data availability, redundancy, and reliability.\nThe user is reporting a task in preparing status. No error is shown but only this status where the incremental backup plan stucks. This is an agent backup (no VM backup). The issue started around Jan 20th and the task previously worked, it was configured to see the behavior when recovering files in a granular manner. This is the only task with this behavior. The host was offline when the backup task was executed. XXXXXX_FS_118 2023-01-13 17:00:00 Error StartFailed Failed to start backup or recovery, cause: 1. Host is offline 2. Connecting public service timed out or failed Error details: Host offline 2023-01-13 17:00:00 Info This plan is started automatically by applying the SLA policy. Backup Task Cannot Continue to Be Executed. Please fix it following: During the execution of a standard or advanced backup plan, if the host breaks down or is shut down, or the host service process does not exist, the backup plan cannot continue to be executed. As a result, the progress of the backup plan is suspended. In this case, you can use the O&M tool to stop and end the plan. Log in to the Data Protection Appliance node. Run the su - root command and enter the root account's password to switch to the root account. Run the cd /opt/CDMServer/script/ command to go to the /opt/CDMServer/script/ directory. Run the ./stoptask_delpolicy_Tool.sh --help command to view the method of using the O&M tool.\nAccording to the feedback from the user, the DeviceManager of the storage device reports an alarm indicating that the bandwidth for the unmap exceeds the threshold. Storage version: OceanStor Dorado 5000 V6 6.1.2.SPH16 [Alarm information] 0xF03360026 Fault Critical Recovered 2023-03-21 09:25:06 controller (0A): The Unmap command bandwidth (20156.0 MB/s) is greater than the threshold (1024.0 MB/s) [Root Cause] The unmap alarm parameter is set improperly. You need to modify the alarm threshold. [Positioning Roadmap] 1. Log in to the storage device, collect storage logs, and check whether an alarm is generated on the storage controller at the same time. The alarm persists for two to three minutes and then disappears automatically. According to the communication with the frontline personnel, the alarm is automatically cleared when the customer does not perform any operation. 2. Check the alarm code and confirm that the alarm is caused by the load exceeding the preset threshold. 3. Check the performance logs and find that the fault is related to the host. The LUNs belong to the same host. The customer confirms that no service is abnormal during the period. 4. Check the performance logs. It is found that other I/O performance parameters in the period are within the normal range. 5. Check the customer's alarm threshold settings and find that the current threshold is stored. 6. Considering the traffic burr of the unmap command operation on the host, ask the customer to change the number of sampling times from 12 to 60 according to the suggestion of R&D engineers.\nDear Members, In 2014, Gartner released the very first Magic Quadrant for all-flash arrays, reporting that all-flash storage accounts for over 50% of the primary storage market (2021Q4). Now the evolving all-flash storage technologies are driving a larger all-flash storage market. Trend analysis Upgraded SSD tech promotes all-flash storage to diverse industries The evolution of SSD technologies has enabled all-flash storage to meet a wider range of service scenarios in various industries. NVMe SSDs adopt the NVMe standard to offer twice the performance of traditional SAS SSDs. Equally, NVMe over Fabrics (NVMe-oF) is an enhanced protocol employed by SSDs to provide sub-ms latency for storage networks. Enabling improved performance with every generation, all-flash storage is perfect for real-time ultra-low latency scenarios such as transaction systems. The global wear-leveling technology uses algorithms to evenly write data to cells of all SSDs in the storage system, greatly extending the SSD service life and enabling enterprises to use all-flash storage in core service scenarios. Figure 1: Architecture and performance comparison between traditional and NVMe storage Source: Huawei Similarly, data reduction technology is used to reduce data volumes without compromising the data, thereby reducing the number of disks and devices required in a storage environment. Todays data reduction ratios are as high as 4:1 or 5:1, helping slash the purchase costs of all-flash storage and allowing all-flash storage to enter more non-core service deployments.\nHello Everyone! What is Backup? Multi-node distributed parallel architecture: torch distribution The process is as follows: Initializes the process group. Creates a distributed parallel model. Each process has the same model and parameters. Creates a sampler for data distribution to enable each process to load different parts of data in a mini-batch. Adjacent parameters are organized into buckets. Each process does its own forward propagation and computes its gradient. After all parameter gradients in a bucket are obtained, communication is performed immediately for gradient averaging. Each GPU updates model parameters. The detailed flowchart is as follows: Advantages of Multi-Node Distributed Data Parallel Fast communication Balanced load Fast running speed 1- Unified backup: Advanced Options: 2- Replication: Data backups need to be reconstructed/hydrated and restored before the data can be used. Unlike data backups, replicated data doesnt require reconstruction/hydration and can be used immediately if when required. Data replication is typically used to replicate entire volumes or file systems, databases, and applications, data backups may consist of files or folders, database data files, and application files depending on organizational requirements for backup-restore procedures. Both backup and replication are data protection features. The former is focused on reducing Recovery Point Objectives (RPOs) and preventing data loss. While the latter is built to reduce Recovery Time Objectives (RTOs) ensure business continuity and minimize downtime. Data replication involves making copies of all or specific parts of the data so that you have multiple versions of the same thing on hand if something happens with one version.\nDatabase Management System (DBMS) is software for storing and retrieving users data while considering appropriate security measures. It consists of a group of programs that manipulate the database. The DBMS accepts the request for data from an application and instructs the operating system to provide the specific data. In large systems, a DBMS helps users and other third-party software store and retrieve data. DBMS allows users to create their own databases as per their requirements. The term DBMS includes the user of the database and other application programs. It provides an interface between the data and the software application. Let us see a simple example of a university database. This database is maintaining information concerning students, courses, and grades in a university environment. The database is organized as five files: The STUDENT file stores the data of each student The COURSE file stores contain data on each course. The SECTION stores information about sections in a particular course. The GRADE file stores the grades which students receive in the various sections The TUTOR file contains information about each professor. To define DBMS: We need to specify the structure of the records of each file by defining the different types of data elements to be stored in each record. We can also use a coding scheme to represent the values of a data item. Basically, your Database will have 5 tables with a foreign key defined amongst the various tables.\n1960 Charles Bachman designed the first DBMS system 1970 Codd introduced IBMS Information Management System (IMS) 1976- Peter Chen coined and defined the Entity-relationship model, also known as the ER model 1980 Relational Model becomes a widely accepted database component 1985- Object-oriented DBMS develops. 1990s- Incorporation of object-orientation in relational DBMS. 1991- Microsoft ships MS access, a personal DBMS, and that displaces all other personal DBMS products. 1995: First Internet database applications 1997: XML applied to database processing. Many vendors begin to integrate XML into DBMS products. Here are the characteristics and properties of a Database Management System: Provides security and removes redundancy Self-describing nature of a database system Insulation between programs and data abstraction Support of multiple views of the data Sharing of data and multiuser transaction processing Database Management Software allows entities and relations among them to form tables. It follows the ACID concept ( Atomicity, Consistency, Isolation, and Durability). DBMS supports a multi-user environment that allows users to access and manipulate data in parallel. DBMS vs. Flat File Following are the various category of users of DBMS Here is the list of some popular DBMS systems: MySQL Microsoft Access Oracle PostgreSQL dBASE FoxPro SQLite IBM DB2 LibreOffice Base MariaDB Microsoft SQL Server Below are the popular database system applications: https://www.guru99.com/images/1/122118_0515_WhatisDBMSA1.pngTypes of DBMS The main Four Types of Database Management Systems are: Hierarchical database Network database Relational database Object-Oriented database In a Hierarchical database, model data is organized in a tree-like structure. Data is Stored Hierarchically (top-down or bottom-up) format.\nData is represented using a parent-child relationship. In Hierarchical DBMS, parents may have many children, but children have only one parent. The network database model allows each child to have multiple parents. It helps you to address the need to model more complex relationships like the orders/parts many-to-many relationship. In this model, entities are organized in a graph which can be accessed through several paths. Relational DBMS is the most widely used DBMS model because it is one of the easiest. This model is based on normalizing data in the rows and columns of the tables. Relational model stored in fixed structures and manipulated using SQL. In the Object-oriented Model data is stored in the form of objects. The structure is called classes which display data within it. It is one of the components of DBMS that defines a database as a collection of objects that stores both data members values and operations. DBMS offers a variety of techniques to store & retrieve data DBMS serves as an efficient handler to balance the needs of multiple applications using the same data Uniform administration procedures for data Application programmers are never exposed to details of data representation and storage. A DBMS uses various powerful functions to store and retrieve data efficiently. Offers Data Integrity and Security The DBMS implies integrity constraints to get a high level of protection against prohibited access to data.\nA DBMS schedules concurrent access to the data in such a manner that only one user can access the same data at a time Reduced Application Development Time DBMS may offer plenty of advantages, but it has certain flaws- The cost of Hardware and Software of a DBMS is quite high, which increases the budget of your organization. Most database management systems are often complex, so training users to use the DBMS is required. In some organizations, all data is integrated into a single database that can be damaged because of electric failure or corruption in the storage media. Using the same program at a time by multiple users sometimes leads to data loss. DBMS cant perform sophisticated calculations Although DBMS system is useful, it is still not suited for the specific task mentioned below: Not recommended when you do not have the budget or the expertise to operate a DBMS. In such cases, Excel/CSV/Flat Files could do just fine. For Web 2.0 applications, its better to use NoSQL DBMS DBMS definition: A database is a collection of related data which represents some aspect of the real world The full form of DBMS is Database Management System. DBMS stands for Database Management System. It is software for storing and retrieving users data by considering appropriate security measures. DBMS Provides security and removes redundancy DBMS has many advantages over traditional Flat File management system Some Characteristics of DBMS are Security, Self-describing nature, Insulation between programs and data abstraction, Support of multiple views of the data, etc.\nDifference between OceanStor series and the FusionCube series Hi, The OceanStor series and the FusionCube series are two product lines offered by Huawei Technologies, but they serve different purposes and target different areas within the IT infrastructure. Here's a comparison of these two series: Product Focus: OceanStor Series: The OceanStor series focuses on storage solutions and offers a wide range of storage systems, including all-flash arrays, hybrid storage arrays, and software-defined storage solutions. The OceanStor series is designed to provide high-performance, scalable, and reliable storage capabilities for enterprise-level customers, supporting various workloads and data-intensive applications. FusionCube Series : The FusionCube series, on the other hand, is a hyper-converged infrastructure (HCI) solution. It combines compute, storage, and networking resources into a single integrated appliance. FusionCube is designed to simplify infrastructure deployment and management, providing a converged platform for virtualization, cloud computing, and high-performance computing (HPC) workloads. Use Cases: OceanStor Series : The OceanStor series is well-suited for a wide range of storage requirements, including database storage, virtualization, cloud storage, big data analytics, and mission-critical enterprise applications. It provides advanced features such as data deduplication, data compression, snapshot and cloning capabilities, and support for multiple storage protocols. FusionCube Series: The FusionCube series is designed for organizations seeking a hyper-converged infrastructure solution. It integrates compute, storage, and networking resources into a single appliance, providing a scalable and easy-to-manage platform for virtualization, private cloud deployment, and consolidation of IT resources. FusionCube enables organizations to quickly deploy and scale their infrastructure while reducing complexity and improving resource utilization.\nHi there! This time, I will share with you an important article about Disk-level reliability. It is one of four levels of reliability to the Most Stable Huawei High-End All-Flash Storage Architecture. In this article, you will learn about Disk-level reliability in Huawei All-Flash Storage Systems. Disk-level reliability: SSD reliability is measured by examining the MTBF (Mean Time Between Failure) & AFR (Annualized Failure Rate). The industry MTBF benchmark is between approximately 2 and 2.5 million hours. First time in history Huawei raises this bar well beyond the limit and reached 3 million hours between failures on its homegrown disks. How does Huawei extend the life of its SSDs? Huawei has maintained and extended long-term cooperation with its vendors, such as Samsung, Micron, and Toshiba, to ensure that components are manufactured according to Huaweis solution design and objectives. Another important reason is the extensive cooperation achieved between arrays and disks, which combines a series of reliability designs such as optimization in graphene dissipation technology (GDT), global wear leveling, and global anti-wear leveling. 1. Global wear leveling design: At the beginning of the SSD lifecycle, service loads or data distribution are spread in an equal manner throughout SSDs to avoid overloading specific disks. This leads to the idleness of some disks beyond their limits and the premature retirement of others. 2.\nSolid State Drives (SSDs) have become increasingly popular in recent years, offering faster read/write speeds and improved reliability compared to traditional Hard Disk Drives (HDDs). There are several types of SSDs available, including M.2, SATA, PCIe, and NVMe SSDs. In this article, we will explore each of these SSD types and their features. M.2 SSDs are small, thin solid-state drives that are designed to fit into M.2 slots on motherboards. These SSDs use the PCI Express (PCIe) interface to communicate with the motherboard and are available in both SATA and NVMe versions. M.2 SSDs are commonly used in laptops and compact desktops due to their small form factor. SATA SSDs are solid-state drives that use the Serial ATA (SATA) interface to communicate with the motherboard. These SSDs are available in both 2.5-inch and 3.5-inch form factors, making them compatible with most desktops and laptops. SATA SSDs are typically slower than other SSD types but are still faster than traditional HDDs. PCIe SSDs are solid-state drives that use the PCI Express (PCIe) interface to communicate with the motherboard. These SSDs offer faster read/write speeds than SATA SSDs and are available in both HHHL (Half-Height Half-Length) and FHHL (Full-Height Half-Length) form factors. PCIe SSDs are commonly used in high-performance desktops and servers. NVMe (Non-Volatile Memory Express) SSDs are solid-state drives that use the PCIe interface and a special protocol designed specifically for solid-state storage devices. NVMe SSDs offer faster read/write speeds than SATA and PCIe SSDs, making them ideal for high-performance desktops and servers.\nDuplicate data costs more to store, backup, and retrieve. This data is mostly from secondary storage systems with several copies. Organizations with more data have a harder difficulty meeting RTO and RPO (recovery point objectives). Data deduplication eliminates duplicates. Deduplication helps backup software because most of a file system's backup data doesn't change. Virtual environments benefit from combining system files of different virtual machines into a single storage area. Block-level deduplication is a technique for extracting and examining single files, getting rid of duplicate files, and producing and getting rid of redundant data segments. It can be variable-length or variable, but is often done at fixed block boundaries. A cryptographic alpha-numeric hash for the shard is generated using blockchain, and its existence is validated against a hash table or hash database. If not, the hash is thrown away and a new reference is added to the database's or hash table's hash list. The data set for deduplication is first chopped into parts. Several data blocks form a chunk. Patents cover how and where the procedure separates portions. After creating a series of chunks, the dedupe mechanism compares them to all preceding chunks. A deterministic cryptographic hashing technique provides a hash to compare parts. As each update modifies chunk hashes, the system deems two chunks identical if their hashes match. Data deduplication saves space. Only unique data blocks are stored after data deduplication. Fingerprintsdata block digital signaturesare used.\nSo, the inline deduplication engine will fingerprint incoming data blocks and store them in a hash store as the system publishes them (in-memory data structure). After calculating the fingerprint, the procedure performs a hash store lookup. The system then analyses data blocks that match cache memory's duplicate fingerprint (donor block). If a match is found in the hash store, one of two things may occur: If a match is found, the new data block (recipient) is compared to the donor block, which serves as verification. Without writing the recipient block to the disc, the system verifies the data between the two blocks. The system then modifies the metadata to keep track of the sharing details. If the donor block is not present in the cache memory, the system prefetches it from the disc to perform a byte-by-byte comparison with the recipient block in the cache. If there is an exact match, the system marks the recipient block as a duplicate without writing it to the disc, but it does update the metadata to document the sharing details. Similarly, the background duplication engine operates. It searches each and every data block in mass. It compares block fingerprints and conducts byte-by-byte comparisons in order to eradicate false positives and remove duplicates. The procedure does not result in data loss. The reduction in the requirements for equipment, power, cooling, and floor space in a data center results in a significant reduction in the capital and running costs of the data center.\nHardware RAID vs. software RAID As with RAID controllers, RAID is implemented through either hardware or software. Hardware-based RAID supports different RAID configurations and is especially well suited for RAID 5 and 6. Configuration for hardware RAID 1 is good for supporting the boot and application drive process, while hardware RAID 5 is appropriate for large storage arrays. Both hardware RAID 5 and 6 are well suited for performance. Hardware-based RAID requires a dedicated controller be installed in the server. RAID controllers in hardware are configured through card basic I/O system or Option ROM (read-only memory) either before or after the OS is booted. RAID controller manufacturers also typically provide proprietary software tooling for their supported OSes.Software-based RAID is provided by several modern OSes. It is implemented in a number of ways, including: As a component of the file system; As a layer that abstracts devices as a single virtual device; and As a layer that sits above any file system. This method of RAID uses some of the system's computing power to manage a software-based RAID configuration. As an example, Windows supports software RAID 0, 1 and 5, while Apple's macOS supports RAID 0, 1 and 1+0. Benefits of RAID Advantages of RAID include the following: Improved cost-effectiveness because lower-priced disks are used in large numbers. Using multiple hard drives enables RAID to improve the performance of a single hard drive. Increased computer speed and reliability after a crash, depending on the configuration.\nReads and writes can be performed faster than with a single drive with RAID 0. This is because a file system is split up and distributed across drives that work together on the same file. There is increased availability and resiliency with RAID 5. With mirroring, two drives can contain the same data, ensuring one will continue to work if the other fails. Downsides of using RAID RAID does have its limitations, however. Some of these include: Nested RAID levels are more expensive to implement than traditional RAID levels, because they require more disks. The cost per gigabyte for storage devices is higher for nested RAID because many of the drives are used for redundancy. When a drive fails, the probability that another drive in the array will also soon fail rises, which would likely result in data loss. This is because all the drives in a RAID array are installed at the same time, so all the drives are subject to the same amount of wear. Some RAID levels -- such as RAID 1 and 5 -- can only sustain a single drive failure. RAID arrays, and the data in them, are vulnerable until a failed drive is replaced and the new disk is populated with data. Because drives have much greater capacity now than when RAID was first implemented, it takes a lot longer to rebuild failed drives.\nIf a disk failure occurs, there is a chance the remaining disks may contain bad sectors or unreadable data, which may make it impossible to fully rebuild the array. However, nested RAID levels address these problems by providing a greater degree of redundancy, significantly decreasing the chances of an array-level failure due to simultaneous disk failures. When should you use RAID? Instances where it is useful to have a RAID setup include: When a large amount of data needs to be restored. If a drive fails and data is lost, that data can be restored quickly, because this data is also stored in other drives. When uptime and availability are important business factors. If data needs to be restored, it can be done quickly without downtime. When working with large files. RAID provides speed and reliability when working with large files. When an organization needs to reduce strain on physical hardware and increase overall performance. As an example, a hardware RAID card can include additional memory to be used as a cache. When having I/O disk issues. RAID will provide additional throughput by reading and writing data from multiple drives, instead of needing to wait for one drive to perform tasks. When cost is a factor. The cost of a RAID array is lower than it was in the past, and lower-priced disks are used in large numbers, making it cheaper. History of RAID The future of RAID RAID is not quite dead, but many analysts say the technology has become obsolete in recent years.\nPEM (Privacy Enhanced Mail) is a file format that is . In this article, we will explore what PEM files are, how they are used, and why they are important. PEM is a file format that is used to . PEM files are encoded in base64 and consist of one or more items, each enclosed in \" -----BEGIN [item]----- \" and \" -----END [item]----- \" delimiters. The [ item ] can be a The PEM format is , making it easy to transfer data between different systems and platforms. PEM files have several uses in secure communications and data transmission. Some of the common uses of PEM files include: Certificates in PEM format are used to t and are commonly used in SSL/TLS (Secure Sockets Layer/Transport Layer Security) for secure web connections. Private keys in PEM format are Certificate chains in PEM format are PEM files are used in email encryption, where they are . PEM files are used in VPNs (Virtual Private Networks), which provide by encrypting all communication between the remote user and the network. When working with PEM files, it is important to ensure that they are kept secure and protected from unauthorized access. Private keys, in particular, should be kept secret and protected with a strong passphrase or password. In addition, it is important to verify the authenticity of any certificates or certificate chains before using them to establish a secure connection. PEM files are a versatile and widely used file format for storing and transmitting sensitive information in a secure manner.\nWhat are the benefits of using Huawei's OceanStor storage systems, and how do they help organizations meet their storage needs in a cost-effective and scalable way? Hi, Huawei's OceanStor storage systems offer several benefits for organizations that need to manage large amounts of data in a cost-effective and scalable way, including: High Performance: OceanStor storage systems use advanced technologies such as solid-state drives (SSDs) and NVMe to deliver high performance and low latency, which can help organizations improve application performance and reduce processing time. Scalability: OceanStor storage systems are highly scalable and can support a wide range of workloads and data volumes, from small businesses to large enterprises. Organizations can easily expand their storage capacity as needed without the need for significant hardware upgrades or changes. Reliability: OceanStor storage systems offer advanced data protection features such as RAID, snapshot, and backup and recovery, which help organizations ensure data integrity and availability. The systems also include redundant components and hot-swappable drives to minimize downtime and ensure continuous operations. Cost-Effective: OceanStor storage systems are designed to be cost-effective, offering high performance and scalability at a lower cost compared to traditional storage solutions. They also feature energy-efficient technologies that help reduce power and cooling costs. Easy Management: OceanStor storage systems include user-friendly management tools that simplify storage administration and monitoring. Organizations can easily configure and manage their storage systems using a web-based interface or command-line interface. Huawei's OceanStor storage systems provide organizations with a highly reliable, scalable, and cost-effective solution for managing their data storage needs.\nIntroduction: Huawei HiSuite is a robust and adaptable software suite that facilitates simple device management, backup, and restoration for Huawei users. Huawei's HiSuite was designed to simplify the process of managing many devices and to add useful functionality without adding complexity. This article will examine the features and advantages of Huawei HiSuite, explaining why it has quickly become a must-have app for Huawei users everywhere. Effortless Device Management: Huawei HiSuite allows customers to manage their Huawei mobile devices from a computer. Users can quickly and easily move, rename, and remove files on and off their device and computer with the help of this program. HiSuite streamlines the process of managing your devices, whether you're looking to create an online photo album, control your music collection, or store essential documents in the cloud. Convenient Backup and Restore: HiSuite's ability to back up and restore data is a notable feature. Users have the option of backing up all of their data or just certain types of information, such contacts, messages, images, and more. These copies of important information can be safely kept on the computer. HiSuite allows for simple restoration of backed-up data, guaranteeing a seamless transfer without any loss of data in the event of accidental data loss, device upgrading, or switching to a new Huawei device. Data Synchronization and Transfer: HiSuite acts as a bridge between Huawei devices and computers, allowing for seamless data synchronization and transfer.\nHow to Use the chmod Command on Linux What is chmod Command on Linux: The chmod command on Linux is used to change the permissions of files or directories. \" chmod\" stands for \"change mode\". It is a command line utility that allows users to modify the access permissions to their files and directories. In Linux, file permissions decide who can read, write, and execute files. The permissions of three types that: read (r), write (w), and execute (x). These permissions are assigned to three groups of users: owner (u), group (g), and others (o). The chmod command allows users to modify these permissions for each group of users. For example, a user can use the chmod command to grant read, write, and execute permissions to the owner of a file, but only read and execute permissions to the group and others. The chmod command can be used in different ways. It can modify permissions by specifying numeric modes, such as 777 or 644, or by using symbolic modes, such as u+rwx or g-w. The chmod command is an important tool for managing file and directory permissions on Linux systems. How to Use the chmod Command on Linux: The chmod command on Linux is used to change the permissions of files or directories. It is a powerful command that can modify permissions for the owner, group, and others. Here are the basic steps to use the chmod command: (1). Open a terminal window on your Linux machine. (2).\nIdentify the file or directory you want to modify. You can use the ls command to list the files in the current directory and their permissions. (3). Determine the permission you want to change. Permissions are represented by three digits, each digit representing the permission for the owner, group, and others. The three digits correspond to the permissions of read (4), write (2), and execute (1). For example, chmod 777 myfile will give the owner, group, and others full permission to read, write, and execute myfile . (4). Use the chmod command to modify the permissions of the file or directory. The syntax is chmod [options] mode file . Here are some examples: chmod 755 myfile will give the owner read, write, and execute permission, and others read and execute permission. chmod u+w myfile will add write permission for the owner of myfile . chmod g-r myfile will remove read permission for the group of myfile . There are many other options and ways to use the chmod command, so it's a good idea to consult the manual pages for more information. You can access the manual pages by typing man chmod in the terminal. Summary: The chmod command on Linux is used to change the permissions of files or directories. It can modify permissions for the owner, group, and others. Permissions are represented by three digits, each digit representing the permission for the owner, group, and others.\nHi there, Are you aware? What three steps must storage systems take to implement SmartTier? Many thanks Hi, SmartTier is a storage tiering technology that allows storage systems to automatically move data between different tiers of storage based on its usage and performance requirements. To implement SmartTier, storage systems typically need to perform the following three steps: Identify the data access patterns: Storage systems need to collect and analyze information about the frequency and type of access to the data stored in the system. This information can be gathered through various means, such as monitoring tools, application profiling, or machine learning algorithms. The goal is to understand which data is frequently accessed and requires high performance, and which data is rarely accessed and can be stored on lower-tier storage. Define the storage tiers: Storage systems need to define different tiers of storage based on their performance characteristics, such as disk speed, capacity, and cost. The tiers can range from high-performance solid-state drives (SSDs) to low-cost hard disk drives (HDDs), tape, or cloud storage. The goal is to create a hierarchy of storage that matches the performance and cost requirements of different types of data. Automate data movement: Storage systems need to implement an automated mechanism to move data between different storage tiers based on its access patterns and performance requirements. This can be done through various means, such as policies, rules, or machine learning algorithms.\nHi Everyone! Measurement of Data Storage : Definition: Measurement of data storage refers to the process of quantifying the amount of digital information that can be stored in a storage medium, such as a hard drive, solid-state drive, tape drive, or cloud storage. It involves using standardized units of digital information to express the size or capacity of a storage medium, which helps in estimating the amount of data that can be stored or transferred. Data storage is typically measured in units of digital information, such as bits, bytes, kilobytes (KB), megabytes (MB), gigabytes (GB), terabytes (TB), petabytes (PB), exabytes (EB), zettabytes (ZB), and yottabytes (YB). These units represent different levels of storage capacity, with each unit being larger than the previous one. Here's a brief overview of commonly used units for measuring data storage: Bit (b): A bit is the smallest unit of digital information and can have a value of either 0 or 1, representing binary data. Byte (B): A byte is a group of 8 bits and is the basic unit used to represent a single character or symbol in most computer systems. Kilobyte (KB): 1 KB is equivalent to 1,024 bytes or 8,192 bits. Megabyte (MB): 1 MB is equivalent to 1,024 KB or 1,048,576 bytes. Gigabyte (GB): 1 GB is equivalent to 1,024 MB or 1,073,741,824 bytes. Terabyte (TB): 1 TB is equivalent to 1,024 GB or 1,099,511,627,776 bytes. Petabyte (PB): 1 PB is equivalent to 1,024 TB or 1,125,899,906,842,624 bytes.\nWhat is Data Migration? Data migration refers to the process of transferring data from one system, application, or storage medium to another. This can involve moving data from one physical location to another, such as from an on-premises data center to a cloud-based storage platform. It can also involve moving data between different types of applications or databases, such as migrating data from an older version of a software application to a newer one. The process of data migration typically involves several steps, including assessing the existing data to be migrated, preparing the new system or application for the data transfer, mapping the data to be moved to the appropriate fields or structures in the new system, and performing the actual data transfer. Data migration can be a complex process, particularly when large amounts of data are involved or when the data being migrated is critical to business operations. To ensure that data is migrated successfully and without disruption to operations, it is important to plan and execute the process carefully and thoroughly, often with the help of specialized tools or professional services. Types of data migration: Application Migration: The migration of apps to a different vendor application or platform can occur. This method has its inherent complexity since applications communicate and each has its data model. Not designed to be portable applications. Management software, operating systems, and the configurations of virtual machines all vary from those in the environment in which the application was created or implemented.\nThe use of middleware product to bridge technology gaps which involve an effective application migration. Storage Migration: The method is used to make data validation and reduction efficient, by finding outdated or faulty information. Storage migration is justified by technology refreshments. This requires the transfer of storage blocks and files on disk, tape, or cloud from one storage device to another. The method is broken down with various storage migration products and software. The relocation of storage often provides an opportunity to fix orphaned storage or inefficiency. Cloud migration: Cloud migration is a significant technology trend because Capex offers versatility, scalability, and decreased on-demand scalability for on-site infrastructure. Providers of public cloud provide a range of storage, database, and application migration services. Database Migration: Database migration takes place if a database provider has to be updated, database software modified or a database migrated to the cloud. The underlying data, which can impact the application layer if there is a protocol or data language shift, through the shift in this form of migration. Database migrations deal with data alteration without modifying the schema. The assessments of the database capacity, as well as the need for storage, testing of software, and data confidentiality, are some of the main tasks. During the migration process, compatibility issues can occur, so it is necessary to first test the process. Data migration involves 3 basic steps: Extract data Transform data Load data Moving important or sensitive data and decommissioning legacy systems can put stakeholders on edge.\nHaving a solid plan is a must; however, you dont have to reinvent the wheel. You can find numerous sample data migration plans and checklists on the web. For example,Data Migration Pro, a community of data migration specialists, has acomprehensive checklistthat outlines a 7-phase process: Premigration planning. Evaluate the data being moved for stability. Project initiation. Identify and brief key stakeholders. Landscape analysis. Establish a robust data quality rules management process and brief the business on the goals of the project, including shutting down legacy systems. Solution design. Determine what data to move, and the quality of that data before and after the move. Build & test. Code the migration logic and test the migration with a mirror of the production environment. Execute & validate. Demonstrate that the migration has complied with requirements and that the data moved is viable for business use. Decommission & monitor. Shut down and dispose of old systems. This may appear to be an overwhelming amount of work, but not all these steps are needed for every migration. Each situation is unique, and each company approaches the task differently. Best Practices for Data Migration: Regardless of which implementation method you follow, there are some best practices to keep in mind: Back up the data before executing: In case something goes wrong during the implementation, you cant afford to lose data. Make sure there are backup resources and that theyve been tested before you proceed.\nStick to the strategy: Too many data managers make a plan and then abandon it when the process goes too smoothly or when things get out of hand. The migration process can be complicated and even frustrating at times, so prepare for that reality and then stick to the plan. Test, test, test: During the planning and design phases, and throughout implementation and maintenance, test the data migration to make sure you will eventually achieve the desired outcome. 6 Key Steps in a Data Migration Strategy: Each strategy will vary in the specifics, based on the organizations needs and goals, but generally, a data migration plan should follow a common, recognizable pattern: 1. Explore and Assess the Source: Before migrating data, you must know (and understand) what youre migrating, as well as how it fits within the target system. Understand how much data is pulling over and what that data looks like. There may be data with lots of fields, some of which wont need to be mapped to the target system. There may also be missing data fields within a source that will need to pull from another location to fill a gap. Ask yourself what needs to migrate over, what can be left behind, and what might be missing. Beyond meeting the requirements for data fields to be transferred, run an audit on the actual data contained within.\nIf there are poorly populated fields, a lot of incomplete data pieces, inaccuracies, or other problems, you may reconsider whether you really need to go through the process of migrating that data in the first place. If an organization skips this source review step, and assumes an understanding of the data, the result could be wasted time and money on migration. Worse, the organization could run into a critical flaw in thedata mappingthat halts any progress in its tracks. 2. Define and Design the Migration: The design phase is where organizations define the type of migration to take on big bang or trickle. This also involves drawing out the technical architecture of the solution and detailing the migration processes. Considering the design, the data to be pulled over, and the target system, you can begin to define timelines and any project concerns. By the end of this step, the whole project should be documented. During planning, its important to considersecurity plansfor the data. Any data that needs to be protected should have protection threaded throughout the plan. 3. Build the Migration Solution: It can be tempting to approach migration with a just enough development approach. However, since you will only undergo the implementation one time, its crucial to get it right. A common tactic is to break the data into subsets and build out one category at a time, followed by a test. If an organization is working on a particularly large migration, it might make sense to build and test in parallel. 4.\nMemory Management: Importance of Paging and Segmentation In computer systems, memory management is a crucial aspect of optimizing the use of resources. Two techniques used in memory management are segmentation and paging. These techniques enable the system to effectively manage the available memory space and allocate it to different processes as needed. Segmentation Segmentation is a technique that divides the memory space into segments or blocks of varying sizes. Each segment is allocated for a specific purpose, such as storing a program code, data or stack. Segmentation allows the system to allocate memory dynamically, which means that it can allocate and deallocate segments as needed, depending on the requirements of the running processes. Segmentation provides several advantages over other memory management techniques. Firstly, it allows for more efficient use of memory by only allocating the amount of memory needed for each segment. Secondly, it provides a mechanism for protecting memory from unauthorized access. Finally, it allows for easy sharing of memory between multiple processes. However, segmentation also has some drawbacks. One of the major issues is external fragmentation, which occurs when the available memory is divided into small segments that are too small to be useful for any future allocations. This results in the inefficient use of memory, as the system cannot allocate these small segments to new processes. Paging To overcome the issues of external fragmentation, the technique of paging is used. Paging divides the memory space into fixed-sized blocks called pages. Each page is a fixed size and is numbered.\nThe process's memory is divided into blocks of the same size as the pages, which are called page frames. The system uses a page table to keep track of which pages are assigned to each process and where they are located in the memory. When a process requests a memory allocation, the system allocates page frames from the available memory space and updates the page table accordingly. Paging provides several advantages over segmentation. Firstly, it eliminates the issue of external fragmentation by dividing the memory into fixed-sized blocks, which can be easily allocated to new processes. Secondly, it provides better protection against unauthorized access, as each page can be marked as read-only, read-write or execute-only. Finally, it allows for more efficient use of memory, as the system can easily allocate and deallocate memory pages as needed. However, paging also has some drawbacks. One of the major issues is internal fragmentation, which occurs when the process's memory requirement is not a multiple of the page size. This results in some unused space in the last page of the process's memory, which is inefficiently used. Additionally, paging requires extra overhead to maintain the page table, which can impact system performance. In conclusion, both segmentation and paging are important techniques used in memory management. Segmentation provides a dynamic allocation of memory space for each process, while paging provides a fixed allocation of memory space in the form of pages.\nDear All, Today we are going to learn aboutApplication-Layer Performance Tuning. Application-Layer Performance Tuning. Application-layer performance tuning refers to the process of optimizing the performance of an application at the software level. This can involve various techniques, such as code optimization, resource allocation, and data access pattern optimization, to improve the application's efficiency and responsiveness. Application-layer performance tuning aims to reduce latency, increase throughput, and improve the overall user experience by optimizing the application's performance. It is often done by analyzing the application's performance metrics and identifying the bottlenecks or areas that require optimization. By improving the application's performance, it can better utilize system resources, provide faster response times, and handle larger workloads efficiently. The storage system supports a wide range of service applications. The appropriate tuning methods vary with application types. The tuning solution at the application layer is complex and changeable, but it also follows the basic process of performance tuning.\nI/O Models in Typical Application Scenarios Online transaction processing (OLTP): small random I/Os, sensitive to response latency Online analytical processing (OLAP): multi-channel sequential I/Os in data warehouse systems, sensitive to bandwidth Virtual desktop infrastructure (VDI): small random I/Os in desktop cloud scenarios with high hit ratios, sensitive to response latency SPC-1: foremost authoritative and well-recognized SAN performance testing benchmark model developed by the Storage Performance Council (SPC) with small random I/Os, sensitive to IOPS and latency OLTP Services OLTP stands for Online Transaction Processing, which refers to a class of applications that allow users to interact with a database in real-time by processing individual transactions. OLTP services are used to perform day-to-day business operations, such as inventory management, order processing, and financial transactions. These services require high availability, high throughput, and low response times to meet the demands of real-time transaction processing. OLTP systems typically involve frequent reads and writes to the database, often with concurrent access by multiple users. To ensure data consistency, OLTP systems use techniques such as locking, concurrency control, and transaction management. Examples of OLTP services include e-commerce websites, online banking systems, and airline reservation systems. It has the following I/O characteristics: OLTP Monitoring Items OLAP Services OLAP stands for Online Analytical Processing, which refers to a class of applications that allow users to analyze large volumes of data from multiple perspectives. OLAP services are used for data analysis and decision-making, providing a comprehensive view of the data to support business intelligence and analytics.\nDear All, Today we are going to learn about Storage Performance Tuning Overview. Performance Tuning Storage performance tuning refers to the process of optimizing the performance of a storage system to meet specific requirements. This can involve adjusting various parameters, such as block size, cache size, and access patterns, to improve the system's performance. The goal of storage performance tuning is to ensure that the storage system can handle the workload efficiently, with minimum latency and maximum throughput. This can help to improve the overall performance of the application that relies on the storage system and provide a better user experience. Applications for modern information processing, such as multimedia, voice recognition, image processing, knowledge bases, and databases, must handle vast amounts of intricate data. The storage systems required for these applications have significant demands. To fulfill stricter service requirements and improve system performance, it is necessary to optimize the storage system's performance. This can be achieved by tuning the system's performance through adjusting the relationships between its various elements, which is commonly referred to as performance tuning. Having a comprehensive understanding of the system is crucial for this task.\nPerformance Indicators IOPS I/Os per second Number of I/Os that can be processed per second Used to measure I/O processing capabilities of storage devices Bandwidth Bandwidth Data volume that can be processed per second Measured in MB/s or GB/s Used to measure storage system throughput Amount of data that a storage device can process every second Latency Time interval from an I/O request being initiated to the I/O request being processed Measured in ms Indicators: average response time and max. response time Fluctuation Rate Values needed to measure the fluctuation rate: max. value, min. value, and mean square error Frequently used method to calculate the fluctuation rate: Mean square error/Average value x 100% What Are Performance Issues? Insufficient IOPS or bandwidth, unable to satisfy customer service needs High I/O latency causing slow responses Unstable performance levels Varying performance levels between different versions System Workflow and Bottlenecks Performance Tuning Process Performance Tuning Precautions Iterative Progress The tuning process is iterative and progressive. The result of each tuning is used in subsequent system running. Running performance indicators should be monitored continuously. Design Orientation System performance depends on good design. In the process of system design and development, performance should always be considered. Tuning skills are only an auxiliary tool . Multi-aspect Check A performance problem may occur on any part of the I/O path. Service performance is limited by the performance bottlenecks in a system. It is therefore important to check all parts of the I/O path to find out where the problem is and eliminate it .\nDear All, Today we are going to learn about Storage Data Migration. Data migration is the process of moving data from one storage system to another, such as from an old storage device to a new one or from an on-premises storage system to the cloud. Storage data migration, specifically, refers to the process of moving data from one storage system to another while preserving the integrity, security, and accessibility of the data. There are several reasons why organizations may need to migrate their data to a new storage system. For example, they may need to upgrade their storage infrastructure to support new applications or workloads, consolidate storage systems to reduce costs, or move to a cloud-based storage solution to improve scalability and flexibility. Storage data migration can be a complex and challenging process that requires careful planning and execution to ensure that the data is transferred accurately and securely. Some of the key considerations for storage data migration include: Data compatibility: Ensuring that the new storage system is compatible with the data format and software applications used by the old system. Data integrity: Ensuring that the data is transferred accurately and without corruption. Security: Ensuring that the data is protected during the transfer and after it has been migrated to the new system. Downtime: Minimizing downtime during the migration process to avoid disrupting business operations.\nA storage method known as virtual memory gives the user the impression that their main memory is quite large. By considering a portion of secondary memory as the main memory, this is accomplished. By giving the user the impression that there is memory available to load the process, this approach allows them to load larger size programs than the main memory that is available. The Operating System loads the many components of multiple processes in the main memory as opposed to loading a single large process there. By doing this, the level of multiprogramming will be enhanced, which will increase CPU consumption. Virtual memory is now very popular in the modern world. In this scheme, whenever a large number of pages must be loaded into the main memory for execution but there is not enough memory to hold that many pages, the OS instead searches for RAM areas that have not been used recently or that have not been referenced and copies those into the secondary memory to make room for the new pages in the main memory. It gives the impression that the computer has boundless RAM because everything is done automatically. Let us assume 2 processes, P1 and P2, contains 4 pages each. Each page size is 1 KB. The main memory contains 8 frame of 1 KB each. The OS resides in the first two partitions.\nIn the third partition, 1st page of P1 is stored and the other frames are also shown as filled with the different pages of processes in the main memory. The page tables of both the pages are 1 KB size each and therefore they can be fit in one frame each. The page tables of both the processes contain various information that is also shown in the image. The CPU contains a register which contains the base address of page table that is 5 in the case of P1 and 7 in the case of P2. This page table base address will be added to the page number of the Logical address when it comes to accessing the actual corresponding entry. Swapping, a short-term procedure that combines RAM and hard drive space, is how virtual memory operates. On a computer, RAM is the actual memory that houses the operating system, running applications, and open documents. Virtual memory can shift data from RAM to a location known as a paging file when RAM is low. Memory can be made available by this procedure, enabling a computer to finish the task. On rare occasions, a user could see a notification stating that there is not enough virtual memory. This indicates that either additional RAM must be installed or the paging file's size must be raised. Generally, this procedure is managed automatically by operating systems like Windows. It can also be changed manually if the default size of the virtual memory isnt large enough. RAM data is always changing.\nA user might, for instance, just have one program or one document open, or they might occasionally have a lot of apps and documents open. A device can run more data and programs simultaneously the more Memory it has. There may be times when a device has too many open programs and insufficient Memory to support them. Freeing up RAM by moving data that isn't currently being used from RAM to the hard drive. Nonetheless, the device will operate slower than if it were using RAM if virtual memory is used. This is due to the fact that the processor must wait while information is transferred between the RAM and the hard drive. Hard disks are a type of secondary storage that has slower access times and can slow down a device's processing speed. If the RAM is expanded, this can be prevented. Given below are the advantages of using Virtual Memory: Virtual Memory allows you to run more applications at a time. With the help of virtual memory, you can easily fit many large programs into smaller programs. With the help of Virtual memory, a multiprogramming environment can be easily implemented. As more processes should be maintained in the main memory which leads to the effective utilization of the CPU. Data should be read from disk at the time when required. Common data can be shared easily between memory. With the help of virtual memory, speed is gained when only a particular segment of the program is required for the execution of the program.\nThe process may even become larger than all of the physical memory. Given below are the drawbacks of using Virtual Memory: Virtual memory reduces the stability of the system. The performance of Virtual memory is not as good as that of RAM. If a system is using virtual memory then applications may run slower. Virtual memory negatively affects the overall performance of a system. Virtual memory occupies the storage space, which might be otherwise used for long term data storage. This memory takes more time in order to switch between applications. Let's understand the working of virtual memory using the example shown below. Assume that an operating system uses 500 MB of RAM to hold all of the running processes. However, there is now only 10 MB of actual capacity accessible on the RAM. The operating system will then allocate 490 MB of virtual memory and manage it with an application called the Virtual Memory Manager (VMM). As a result, the VMM will generate a 490 MB file on the hard disc to contain the extra RAM that is necessary. The OS will now proceed to address memory, even if only 10 MB space is available because it considers 500 MB of actual memory saved in RAM. It is the VMM's responsibility to handle 500 MB of memory, even if only 10 MB is available. Virtual memory is a part of the system's secondary memory that acts and gives us an illusion of being the main memory.\nGrid computing is a system of computers from different administrative domains working together. Grid computing simplifies complex tasks that a single computer system couldn't do. Grid computing is different from computer clusters because each node in grid computing has a unique role. Distinct nodes perform different activities, although a single grid can serve multiple purposes. Grids can be any size, usually huge. Many sectors employ grid computing techniques and apps. This page describes Grid Computing's usage. This article contains such information. Distributing work to multiple systems can address a problem. Grid computers can be in various nations or regions. Grid computing is used when great processing performance is needed across time. This can take months or years and requires constant high computing power. Grid Computing enables on-demand supercomputers. Grid computing helps businesses with shifting demand. Grid Computing processes data simultaneously by spreading it to grid nodes. Data processing and distribution are linked. Collaborative supercomputing is a grid computing application. Collaboration using supercomputing is collaborative supercomputing. Grid Computing saves time and is more efficient in the movie industry. Grid computing helps add effects and speed up filmmaking. Gaming uses Grid Computing. Grid computing helps process faster and better as online game traffic increases. The government handles a lot of data. Grid computing makes processing so much data easy. Grid computing distributes and processes data in pieces. How does it work? The functionality of grid computing Is not that much complicated but it makes the task completion time fast.\nGrid computing runs on every computer in the data grid. The software manages the system and organizes grid tasks. Each computer is assigned subtasks so they can work simultaneously. Subtask outputs are gathered and aggregated to accomplish a bigger task. The program lets each machine connect across the network with the others to share information on what subtasks each is executing and how to consolidate and deliver outputs. Figure 1 | How the Data grid is connected with Different Applications A typical grid network has three machine types: I. A control node is a server or set of servers that manage a network's resources. II. A provider or grid node contributes to the network resource pool. III. A user's computer uses network resources to execute a task. Figure 2 | How the Central controller manages the client application and grid nodes When a computer requests network resources, the control node grants access. When idle, it should contribute to the network. Depending on its demands, a node computer might be a user or a supplier. Homogeneous networks use similar platforms and the same OS, while heterogeneous networks use different platforms and OSs. Grid computing is distinguished by this feature. Middleware is used to control a network's resources. The control nodes are essentially its executors. As a grid computing system should only employ unused computer resources, the control node ensures no provider is overwhelmed. Middleware authorizes network processes.\nIn grid computing, a provider allows a user to run anything on its machine, posing a tremendous security risk for the network. Middleware should prevent unnecessary network tasks. Here we are going to explore what are some amazing features of Grid Computing, Grid computing uses specific software on all grid computers. The software manages all grid tasks. The software divides the primary task into subtasks and sends them to computers. All computers can work on subtasks simultaneously. All computers' subtask results are combined to perform the main task. The software lets computers share information about subtasks. The computers can then consolidate and complete the main work. Grid computing is a subset of distributed computing, where a virtual supercomputer merges the resources of numerous independent computers. Grid computers contribute processing power, network bandwidth, and storage capacity to accomplish high-powered processes. Grid architecture resembles a single computer. In grid computing, each task is divided into small parts and spread between nodes. Parallel processing of each fragment speeds up a difficult task. Grid computing is supported by an open set of standards and protocols like open grid services architecture (OGSA) that allow communication across heterogeneous systems and settings. Grid computing lets firms pool resources for heavy tasks or share them across networks for collaboration. Enterprises can optimize computers and resources from anywhere.\nLets get deeper and explore the types of Grid Computing Figure 3 | Different types of Data Grid Computational grids account for the largest share of grid computing usage across industries today, and the trend is expected to stay the same over the years to come. When a task takes longer than planned, use a computational grid. In this example, the main work is split into many subtasks, each running on a distinct node. After subtasks are completed, the main task's result is integrated. By splitting a task, it can be completed O(n) times quicker than on a single machine. Real-world scenarios use computational grids. A computational grid can speed up report generation for an online marketplace. As clients value time, the organization may provide reports in seconds instead of minutes using computational grids. Such grids outperform traditional systems. In this type, the Data grids spread data across machines. Data grids allow placing data on a network of computers or storage, like computational grids. Despite dividing, the grid treats them as one. Multiple users can access, change, or transmit dispersed data using data grid computing. Each website can keep its data on a data grid. The grid facilitates coordinated data sharing among users. Such a grid encourages teamwork and knowledge sharing. It offers seamless collaboration to address challenges. This sort of computing supports teamwork. As workers can easily access each other's work and essential information, it boosts productivity and innovation, which helps enterprises. It bypasses geographical obstacles and allows remote workers to collaborate.\nWith a collaborative grid, all users can access and simultaneously work on text-based documents, graphics, and design files. It helps manage image and text blocks. This grid pattern allows image and text blocks to be continuously added while previous batches are processed. It's a simple grid computing framework that parallelizes text, manuscripts, and images. It disaggregates storage, GPUs, memory, and networking in a system or chassis. IT teams might combine resources to support apps or services. A modular grid combines hardware and software for specific applications. Server rack chassis may house CPU and GPU drives. Interconnecting them with a high-speed, low-latency fabric creates an application-optimized server setup. Applications require a set of computing resources and services. When apps expire, computing support is removed and resources are freed for other programs. Original equipment manufacturers (OEMs) play a vital role in application-specific modular grid computing. Grid computing is a distributed collaborative network, akin to cloud computing. Grid computing is used to address many mathematical, analytical, and physical problems. All different approaches and types of grid computing contribute well to task completion according to the requirements and fulfill the task within the least possible time duration.\nDirect-attached storage (DAS) systems implement the most well-known type of connection. With DAS, the server has a personal connection to the storage system and is almost always the sole user of the device. In this case, the server gets block access to the data storage system, that is, it accesses the data blocks directly. The disadvantage of the direct connection method is the short distance between the server and the storage device. A typical DAS interface is 12Gbit SAS. Storage systems of this type began to lose their popularity and were replaced by equipment with SAN connection. Network Attached Storage (NAS), also known as file servers, expose their network resources to clients over the network in the form of shared files or directory mount points. Clients use network file access protocols such as Server Message Block (SMB) or NFS. The file server, in turn, uses block access protocols to its internal storage to process file requests by clients. Since the NAS works over the network, the storage can be very far away from clients. A variety of NAS systems provide additional features such as storage imaging, deduplication or compression, and more. A storage area network (SAN) provides customers with block access to data over a network (such as Fibre Channel or Ethernet). Devices in a SAN do not belong to a single server, but can be used by all SAN clients. It is possible to divide disk space into logical volumes, which are allocated to individual host servers.\nThese volumes are independent of the SAN components and their placement. Clients access the data store using a block access type, as with a DAS connection, but because the SAN uses a network, storage devices can be located far away from customers. Currently, the SAN architecture uses SCSI (Small Computer System Interface) and NVMe protocols to transmit and receive data. A Fibre Channel (FC) SAN encapsulates SCSI or NVMe protocols in Fibre Channel frames. Storage area networks (SANs) that use standard TCP/IP packets work with iSCSI (Internet SCSI) and iWARP (Internet Wide Area Remote Direct Memory Access (RDMA) Protocol. Prior to the advent of All Flash storage, there was a lot of discussion about the choice of FC or iSCSI for building a storage network. Some companies focused on the low cost of initial iSCSI SAN deployments, while others opted for the high reliability and availability of Fibre Channel SANs. Although low-end iSCSI solutions are cheaper than Fibre Channel, with increasing performance and reliability requirements, the cost advantage of iSCSI SAN disappears. Smaller companies are more likely to choose iSCSI because of the low entry price threshold, and they get the opportunity to further scale the SAN. Basic and niche solutions use a 1/10GBase-T connectivity environment (video surveillance, archives, backup storage, recovery). It is possible to create low-cost installations based on 10GbE interfaces, but when expanding, low-cost network switches often become a bottleneck. High-speed, low-latency installations based on 10/25GbE interfaces have no economic advantages over FC, but can be selected according to personal preference.\nHello community, Multiple BASIS FOR COMPARISON MAGNETIC DISK OPTICAL DISK The Magnetic disk is made of a set of circular platters. These platters are initially built up of non-magnetic material i.e., aluminum or aluminum alloy referred to as substrate then the substrate is coated with a magnetic film and mounted on a common spindle. The disks are placed inside a rotary drive where the magnetised surface rotates close to the read and write heads. Every head is comprised of a magnetising coil and a magnetic yoke . It stores the digital information on the concentric tracks by applying the current pulse of appropriate polarity to the magnetic coil. The number of bits stored on each track does not change by using simplest constant angular velocity . Multiple zoned recording is used to increase the density in which the surface is partitioned into a number of zones and the zones located near the center contain fewer bits than the zones farther from the center. However, this strategy is not optimal. The Optical disk is a storage device in which optical (light) energy is used. In the initial stages, the designers created a compact disk in the mid- 1980s which use the digital representation for the analog sound signals. The CD was capable of providing great quality sound recording by taking 16-bit samples of analog signals at the speed of 44,100 samples per second and also it can detain up to 75 minutes where a total amount of stored bits needed is approx 3 x 109 (3 gigabits).\nThe importance of data visualization in data analytics in the current data-driven environment cannot be overstated. For gleaning valuable insights from massive amounts of data, this crucial tool has become vital. Businesses that want to make wise judgments and advance their development must be able to translate large amounts of data into manageable, understandable graphics. This article will cover the importance of data visualization and how it can be used to analyze giant, complex data sets. Data visualization is the process of collecting data, analyzing data, providing insights into that data, and exploring more information about that data. A game-changing tool for businesses of all sizes is data visualization. The use of graphical representations like charts, graphs, and maps, enables people to use massive datasets efficiently and quickly. Data visualization must be a part of data analytics. It allows us to effectively analyze enormous amounts of data and communicate our findings to others. It enables practical insights into others and the interpretation of massive amounts of data. Any organizational data analytics approach must now include data visualization because it gives businesses the crucial insights they require in an easily comprehensible format. It assists in revealing trends in the data that may have gone missing in the past. Seizing opportunities and reducing risks can help teams move more quickly by giving them the information they need to make better business decisions. By swiftly and clearly showing data, understanding internal organizational dynamics, and taking necessary action, data visualization also helps firms communicate their findings to stakeholders.\nIn data analytics, data visualization is crucial because it allows us to spot patterns, trends, and outliers that may not be apparent in the raw data. Charts, graphs, and dashboards are examples of data visualization tools that make it easier for humans to analyze and make sense of complex data sets. This is crucial when working with huge data sets because analyzing the data without visuals can be daunting. We can also determine associations between variables with the aid of data visualization tools, which is useful in helping us come to wise conclusions. A scatter plot, for instance, can be used to show the relationship between two variables, such as sales and marketing expenditure. This can assist companies in determining whether expanding their marketing budgets will enhance sales. Several Types Of Data Visualization Tools: There are many different kinds of data visualization tools available, and each tool is best suited for a particular kind of data. Charts, graphs, and dashboards are a few common data visualization tools. Data visualization with charts is quick and easy. There are many various kinds of charts, including pie charts, bar charts, and line charts. Line charts are good for displaying trends over time, whereas bar charts are useful for comparing data across many categories. Pie charts help display data proportions. Another tool for data visualization that helps display relationships between variables is the graph. Network diagrams, heat maps, and scatter plots are a few examples of typical graph types.\nStorage Types Centralized Storage By \"centralized\", it is meant that all resources are centrally deployed and are used to provide services over a unified interface. Centralized storage means that all physical disks are centrally deployed in the disk enclosure and are used to provide storage services externally through the controller. Centralized storage typically refers to disk arrays. In terms of technical architectures, centralized storage is classified into SAN and NAS. SANs can be classified into Fibre Channel SAN (FC SAN), Internet Protocol SAN (IP SAN), and Fibre Channel over Ethernet SAN (FCoE SAN). Currently, FC SAN and IP SAN technologies are mature, and FCoE SAN is still in the early stage of its development. A disk array combines multiple physical disks into a single logical unit. Each disk array consists of one controller enclosure and multiple disk enclosures. This architecture delivers an intelligent storage space featuring high availability, high performance, and large capacity. Distributed Storage Unlike centralized storage, distributed storage does not store data on one or more specific nodes. It virtualizes all available space distributed on each host of an enterprise to a virtual storage device. In this way, the data stored in this virtual storage device is also distributed over the storage network. As shown in Figure, distributed storage uses general-purpose servers rather than storage devices. A distributed storage system does not have any controller enclosure or disk enclosure. All disk storage resources are delivered by general-purpose x86 servers.\nClients are delivered by the distributed storage system to identify and manage disks, as well as establish data routing and process read/write I/Os. The distributed storage client mode has advantages and disadvantages. In terms of capacity expansion, an x86 server with a client installed can be a part of the distributed storage system. This mode delivers great scalability. However, in addition to the applications running on the server, the client software installed on the server also consumes compute resources. When you plan a distributed storage system, you must reserve certain amounts of compute resources on servers you intend to add to this system. Therefore, this mode has certain requirements on the hardware resources of the server. In a traditional centralized storage system, data is read and written by controllers. However, the number of controllers is limited. In a distributed storage system, servers with clients can read and write data, breaking the limit on the number of controllers and improving the read and write speed to some extent. However, the paths for reading and writing data need to be calculated each time data is read and written. If there are too many clients, the path calculating is complicated. When the optimum performance is reached, adding more clients cannot further improve the performance. For high data availability and security, the centralized storage system uses the RAID technology. RAID can be implemented by hardware and software. All disks must be deployed on the same server (hardware RAID requires a unified RAID card, and software RAID requires a unified OS).\nStorage replication is the process of copying data from one storage location to one or more additional storage locations, in order to ensure data redundancy and availability in case of hardware failures or other issues. There are several types of storage replication, including: Local replication: This involves creating a copy of data on the same storage device or on a different device within the same data center. Remote replication: This involves creating a copy of data on a different storage device in a different geographic location, typically using a wide area network (WAN) to connect the two locations. Synchronous replication: This type of replication ensures that both the primary and replicated data are kept in sync, so that there is no data loss in case of a failure. Synchronous replication is typically used for high availability applications where downtime is not acceptable. Asynchronous replication: This type of replication copies data to the secondary storage device with a delay. Asynchronous replication is typically used for disaster recovery applications where some data loss is acceptable. Multi-site replication: This involves replicating data across multiple data centers, in order to provide higher levels of availability and disaster recovery. This type of replication is typically used by large organizations with multiple locations. Storage replication is an important part of data protection and disaster recovery planning. By replicating data to multiple locations, organizations can ensure that their data is protected in case of hardware failures, natural disasters, or other unexpected events.\nBlu-ray Characteristics: To understand Blu-ray in detail, it can be categorized in the following sections: Longevity Readability Authenticity Low-Cost Operation Green Media Longevity: Unlike magnetic tape and HDDs, there is no contact between the optical pickup (OPU) and the disc, so it maintains excellent durability even after repeated reading and writing. Blu-ray discs have an archival life of , in contrast to an HDD's 5 to 10-year and magnetic tape's 15-20-year archival life. This allows data to be stored for longer periods of time without the need for data migration. Readability: The Blu-ray Disc is the de facto standard for large-capacity optical discs, and is supported by about 170 major corporations, assuring long-term readability. A large variety of applicable equipment offers a wide range of reading environments. Blu-ray Disc Advantages as an Archive Media Authenticity: Unlike media such as magnetic tape, the WORM (Write Once Read Many) With its long-term storage ability, the because data migration is unnecessary. BDs have a serial number and an ID (recorded in the Burst Cutting Area (BCA) ), making it easy to verify the authenticity of the media itself. Low Cost Operation: Unlike a RAID system, where the motor is constantly rotating even when it is not in use, offline archiving is possible with Blu-ray discs without the need for constant powering, to Air conditioning is not required for media storage, so there are . Data migration is not required, so the costs related to migration, such as replacing devices, are not incurred.\nLinear Multimedia: Non-Linear Multimedia: In Non-Linear multimedia, the end-user is allowed the navigational control to rove through multimedia content at his own desire. The user can control the access of the application. Non-linear offers user interactivity to control the movement of data. For example computer games, websites, self-paced computer-based training packages, etc. Applications of Multimedia Multimedia indicates that, in addition to text, graphics/drawings, and photographs, computer information can be represented using audio, video, and animation. Multimedia is used in: Education In the subject of education, multimedia is becoming increasingly popular. It is often used to produce study materials for pupils and to ensure that they have a thorough comprehension of various disciplines. Edutainment, which combines education and entertainment, has become highly popular in recent years. This system gives learning in the form of enjoyment to the user. Entertainment The usage of multimedia in films creates a unique auditory and video impression. Today, multimedia has completely transformed the art of filmmaking around the world. Multimedia is the only way to achieve difficult effects and actions. The entertainment sector makes extensive use of multimedia. Its particularly useful for creating special effects in films and video games. The most visible illustration of the emergence of multimedia in entertainment is music and video apps. Interactive games become possible thanks to the use of multimedia in the gaming business. Video games are more interesting because of the integrated audio and visual effects. Business Marketing, advertising, product demos, presentation, training, networked communication, etc.\nare applications of multimedia that are helpful in many businesses. The audience can quickly understand an idea when multimedia presentations are used. It gives a simple and effective technique to attract visitors attention and effectively conveys information about numerous products. Its also utilized to encourage clients to buy things in business marketing. Technology & Science In the sphere of science and technology, multimedia has a wide range of applications. It can communicate audio, films, and other multimedia documents in a variety of formats. Only multimedia can make live broadcasting from one location to another possible. It is beneficial to surgeons because they can rehearse intricate procedures such as brain removal and reconstructive surgery using images made from imaging scans of the human body. Plans can be produced more efficiently to cut expenses and problems. Fine Arts Multimedia artists work in the fine arts, combining approaches employing many media and incorporating viewer involvement in some form. For example, a variety of digital mediums can be used to combine movies and operas. Digital artist is a new word for these types of artists. Digital painters make digital paintings, matte paintings, and vector graphics of many varieties using computer applications. Engineering Multimedia is frequently used by software engineers in computer simulations for military or industrial training. Its also used for software interfaces created by creative experts and software engineers in partnership. Only multimedia is used to perform all the minute calculations.\nComponents of Multimedia Multimedia consists of the following 5 components: Text Characters are used to form words, phrases, and paragraphs in the text. Text appears in all multimedia creations of some kind. The text can be in a variety of fonts and sizes to match the multimedia softwares professional presentation. Text in multimedia systems can communicate specific information or serve as a supplement to the information provided by the other media. Graphics Non-text information, such as a sketch, chart, or photograph, is represented digitally. Graphics add to the appeal of the multimedia application. In many circumstances, people dislike reading big amounts of material on computers. As a result, pictures are more frequently used than words to clarify concepts, offer background information, and so on. Graphics are at the heart of any multimedia presentation. The use of visuals in multimedia enhances the effectiveness and presentation of the concept. Windows Picture, Internet Explorer, and other similar programs are often used to see visuals. Adobe Photoshop is a popular graphics editing program that allows you to effortlessly change graphics and make them more effective and appealing. Animations A sequence of still photographs is being flipped through. Its a set of visuals that give the impression of movement. Animation is the process of making a still image appear to move. A presentation can also be made lighter and more appealing by using animation. In multimedia applications, the animation is quite popular. The following are some of the most regularly used animation viewing programs: Fax Viewer, Internet Explorer, etc.\nHello community, will someone kindly tell me what HUAWEI OceanStor storage systems utilize as their default multipath load balancing strategy? cheer, Multipath Connectivity: UltraPath UltraPath is a Huawei-developed multipathing software. It can manage and process disk creation/deletion and I/O delivery of operating systems. UltraPath provides the following functions: Masking of redundant LUNs In a redundant storage network, an application server with no multipathing software detects a LUN on each path. Therefore, a LUN mapped through multiple paths is mistaken for two or more different LUNs. UltraPath installed on the application server masks redundant LUNs on the operating system driver layer to provide the application server with only one available LUN, the virtual LUN. In this case, the application server only needs to deliver data read and write operations to UltraPath. UltraPath then masks the redundant LUNs, and writes data into LUNs without damaging other data. Optimum path selection In a multipath environment, an application server with UltraPath accesses a LUN on the storage system through an optimum path, thereby obtaining the highest I/O speed. Failover and failback Failover When a path fails, UltraPath fails over its services to another functional path. Failback UltraPath automatically delivers I/Os to the first path again after the path recovers from the fault. I/O Load balancing UltraPath provides load balancing within a controller and across controllers. For load balancing within a controller, I/Os poll among all the paths of the controller. For load balancing across controllers, I/Os poll among the paths of all these controllers.\nPath test UltraPath tests faulty and idle paths: Faulty paths UltraPath frequently tests faulty paths to detect the path recovery as soon as possible. Idle paths UltraPath tests idle paths to identify potentially faulty paths early on, preventing unnecessary I/O retries. The test frequency is kept low to minimize impact on service I/Os. MPIO Windows Microsoft Multi-Path IO (MPIO) allows storage vendors to develop multipathing solutions that contain the hardware-specific information needed to optimize connectivity with storage systems. MPIO can be used independently. This software helps balance loads among multiple paths, and implement path selection and failover between storage systems and hosts. MPIO supports the following policy settings: Failover Only This policy does not perform load balancing. This policy uses a single active path, and the rest of the paths are standby paths. The active path is used for sending all I/Os. If the active path fails, then one of the standby paths is used. When the failed path is reactivated or reconnected, the standby path that was activated returns to standby. Round Robin This load balancing policy allows the Device Specific Module (DSM) to use all available paths for MPIO in a balanced way. This is the default policy that is chosen when the storage controller follows the active-active model and the management application does not specifically choose a load balancing policy. Round Robin with Subset This load balancing policy allows the application to specify a set of paths to be used in a round robin fashion, and with a set of standby paths.\nThe DSM uses paths from a primary path pool for processing requests as long as at least one of the paths is available. The DSM uses a standby path only when all the primary paths fail. For example, given 4 paths: A, B, C, and D, paths A, B, and C are listed as primary paths and D is the standby path. The DSM chooses a path from A, B, and C in round robin fashion as long as at least one of them is available. If all three paths fail, the DSM uses D, the standby path. If paths A, B, or C become available, the DSM stops using path D and switches to the available primary paths. Least Queue Depth This load balancing policy sends I/O down the path with the fewest currently outstanding I/O requests. For example, consider that there is one I/O sent to LUN 1 on Path 1, and the other I/O is sent to LUN 2 on Path 1. The cumulative outstanding I/O on Path 1 is 2, and on Path 2 is 0. Therefore, the next I/O for either LUN will process on Path 2. Weighed Paths This load balancing policy assigns a weight to each path. The weight indicates the relative priority of a given path. The larger the number, the lower ranked the priority. The DSM chooses the least-weighted path from among the available paths. Least Blocks This load balancing policy sends I/O down the path with the least number of data blocks currently being processed.\nWhich of the following three technologies for disaster recovery is most popular? A. Disaster recovery using the host layer as a foundation B. Network layer-based disaster recovery C. Recovery from disasters using arrays D. Application-layer-based disaster recovery Dear user, t's difficult to say which of these technologies is the most popular for disaster recovery as it depends on various factors such as the type and size of the organization, their specific IT infrastructure and requirements, and the level of risk they are willing to tolerate. That being said, network layer-based disaster recovery is a commonly used approach as it focuses on replicating data and applications across multiple servers and networks to ensure availability in the event of a disaster. This approach typically involves using techniques such as load balancing and clustering to distribute traffic and workload among multiple servers. However, other approaches such as host layer-based and application-layer-based disaster recovery can also be effective depending on the organization's needs and requirements. Host layer-based disaster recovery involves backing up and restoring entire server environments, while application-layer-based disaster recovery focuses on replicating specific applications and their associated data. Recovery from disasters using arrays typically refers to storage arrays, and while they can be an important component of a disaster recovery strategy, they are not a standalone solution. Instead, storage arrays are typically used in conjunction with other disaster recovery technologies such as network or host layer-based approaches. A. Disaster recovery using the host layer as a foundation B.\nThe associated storage technology is changing together with the arrival of the big data era. Which of the following statements is true? A. From a local file system to a cluster file system and a distributed file system, the file system evolves. B. Traditional NAS will waste resources due to PB-level data, complicated volume management, and uneven system capacity distribution. C. The mass storage system is defined by a trickier management system and file system. D. A mass storage system's capacity to expand on a vast scale is one of its characteristics. Dear user, Option A is true: From a local file system to a cluster file system and a distributed file system, the file system evolves with the arrival of the big data era. As data becomes larger and more distributed, traditional file systems may not be sufficient to handle the workload. Cluster and distributed file systems are designed to handle big data by dividing data across multiple servers or nodes, allowing for greater scalability and better performance. Option B may be partially true: Traditional NAS (Network Attached Storage) may not be optimal for handling PB-level data due to its limited scalability and uneven system capacity distribution. However, whether it will waste resources or have complicated volume management depends on the specific NAS system in question. Option C is not entirely clear what is meant by \"mass storage system.\nHi, CIFS (Common Internet File System) is a protocol used to share files and folders over a network, typically between Windows computers. It was originally developed by Microsoft as an extension of the Server Message Block (SMB) protocol for sharing files and printers in a local area network (LAN). Thanks. Common Internet File System CIFS : It is a system also known as SMB, and it is a network protocol most commonly used to share documents on a local area network LAN, it is also used to facilitate file sharing across multiple platforms for a large number of concurrent users without the need to install new software. The CIFS protocol allows users to manipulate files as if they were available on the local computer. Various operations are supported, such as reading, writing, creating, deleting, renaming, and the only difference between the protocols is that the files are not on the local computer and are in fact on a small server. CIFS is considered to be a sufficiently high-level network protocol, and in the OSI model it is best represented at the application or presentation layer. TCP (NBT)\". The Common Internet File System supports a specific set of commands that computers can access remotely and remotely read and write files, provides both anonymous document transfers and authenticated access, and is used to avoid unauthorized access to specific folders and files. It also has a file lock that prevents multiple users from editing the same file at the same time.\nFibre Channel is a high-speed networking technology designed for use in storage area networks (SANs). It supports several different topologies for connecting devices such as servers and storage devices together. The most common Fibre Channel topologies are: Fibre Channel (FC) is a high-speed network technology that is commonly used for storage area networks (SANs) in enterprise environments . FC supports various topologies that determine how devices are connected and communicate with each other. Point-to-Point Topology: This topology connects two devices together using a single Fibre Channel link. It is typically used for connecting a server to a storage device. Arbitrated Loop Topology: In this topology, devices are connected in a loop using Fibre Channel switches. The devices are connected to the switches using point-to-point links, and the switches are connected to each other to form a loop. The switches arbitrate for access to the loop, and only one device can transmit at a time. Switched Fabric Topology: This topology uses Fibre Channel switches to connect devices together in a non-looped, switched network. Each device is connected to a switch using a point-to-point link, and the switches are interconnected to form a fabric. This topology provides high performance and scalability, making it the most common Fibre Channel topology used today. Hi, Fibre Channel is a high-speed networking technology designed for use in storage area networks (SANs). It supports several different topologies for connecting devices such as servers and storage devices together.\nAfter the FusionCube is powered off and reinstalled, the FusionStorage service cannot be started after the FusionCube is powered on again. An internal error code 11200000 is reported when the service is started through the CLI. Product version: 1. Check the operation time and determine the log time to be viewed. /var/log/oam/fsm/client/run/log-fsm-client.log 2. Check the corresponding log. An error message is displayed, indicating that the communication between the MDC process and the MDC process is abnormal. /var/log/oam/fsm/manager/run/log-fsm-DSMWeb.log If archived /var/log/oam/fsm/manager/bak/log-fsm-DSMWeb. date.X.log.zip 3. Check the node process information. The ZooKeeper, MDC, and VBS services are not started. 4. Log in to each node and run the df -h command. The ZooKeeper disk is mounted. 5. Check the storage side. Generally, the network port name on the CNA side is VLANxxx . Ping other nodes find that can't connect. 6. Use the ethtool to check whether the corresponding logical and physical ports are online. 7. Check the configuration on the FusionCompute and switch sides. It is found that the working modes of the two sides are different. The host is 802.3ad LACP, and the switch is not configured with active/standby link aggregation. 8. Confirm with frontline engineers that the required mode is active/standby. After the port mode of FusionCompute is changed to active/standby, the port cannot be accessed. 9. Check the MAC address learned by ARP entries on the switch. eth6 and eth7 are connected to the storage port of the switch. On the FC side, eth6 and eth7 are defined as replication links.\nKB vs MB source:PCStrike.com In this post, we learn about KB and MB. We frequently hear words like KB and MB while discussing data storage. But what do these expressions actually signify, and how do they differ from one another? Kilobytes are abbreviated as KB, and megabytes as MB. These two phrases describe the volume of digital data that can be sent or stored. 1,024 bytes comprise a kilobyte (KB). A byte, the fundamental building block of digital information, stands for a single character or symbol, like a letter or integer. Kilobytes are frequently used to calculate the size of small files like text or images. source: mbtogb.com/1-kb-to-mb On the other side, 1,024 kilobytes, or 1,048,576 bytes, make up a megabyte (MB). Larger files like movies, music, and software applications are measured in megabytes. So what makes KB and MB different from one another? Their size is the primary distinction. A megabyte may store or send more data because it is bigger than a kilobyte. A typical MP3 music, for instance, can be 34 MB in size, whereas a written page might just be a few kilobytes. The usage of KB and MB is another distinction. Kilobytes are frequently used to calculate the size of small data sets, like email attachments or tiny image files. On the other hand, megabytes are used to describe bigger quantities of data, such as the size of a movie or a substantial software program.\nHi Everyone! This time, I will share with you an important article about DR center architecture for major industries such as finance, securities, telecommunications, electric power, and petroleum: Disaster Recovery (DR) center architecture for major industries such as finance, securities, telecommunications, electric power, and petroleum may vary based on their specific needs and requirements. However, here is a general overview of the DR center architecture for each of these industries: Finance: The DR center for finance industries should have redundant infrastructure to ensure continuous operation of critical business applications, such as trading and banking systems. The center should also have a backup generator for power and a backup communication link to maintain connectivity during an outage. Additionally, the center should have a backup data storage facility to ensure the availability of data. Securities: The DR center for securities industries should have a geographically diverse location to minimize the impact of natural disasters. It should also have a secure and reliable network infrastructure to maintain trading and order management systems. The center should have a backup data center that can take over in case of an outage, and the data should be replicated in real-time to ensure that there is no loss of data. Telecommunications: The DR center for telecommunications should have redundant equipment and connectivity to ensure uninterrupted communication services. The center should have a backup power source and a backup data center with mirrored data to ensure service continuity in case of a disaster.\nThe center should also have a disaster recovery plan that includes processes to restore service in case of an outage. Electric Power: The DR center for electric power should have a redundant power supply and control systems to ensure continuous operation of power generation and distribution systems. The center should also have a backup communication network to maintain connectivity with the main control center. Additionally, the center should have a disaster recovery plan that includes procedures for restoring power in case of an outage. Petroleum: The DR center for petroleum should have a redundant infrastructure to ensure continuous operation of critical business applications, such as oil drilling and refining systems. The center should also have a backup generator for power and a backup communication link to maintain connectivity during an outage. Additionally, the center should have a backup data storage facility to ensure the availability of data. In all cases, the DR center architecture should include redundant equipment, a backup power source, and backup data storage facilities. It should also have a disaster recovery plan that includes procedures for restoring operations in case of an outage. The specific requirements for the DR center architecture will vary depending on the industry and the critical business systems involved. There are some common elements that are typically included in DR center architectures for major industries, such as: Redundancy: DR centers should have redundant systems for critical applications, data, and infrastructure to ensure high availability and minimize downtime in case of a disaster.\nWhat is Erasure Coding? IT administrators who design storage systems must plan ahead so that mission critical data is not lost when any type of failure occurs. The storage systems comes in different shapes but they have one thing in common that is the chance of failing and losing data. The utilization of erasure coding prevents the loss of data due to system failure or disaster. In simple words Erasure coding (EC) is one of the methods of data protection through which the data is broken into sectors. Then they are expanded and encoded with redundant data pieces and stored across different storage media. Erasure coding adds the redundancy to the system that tolerates failures. How Erasure Coding works? Erasure coding takes original data and encodes it in such a way that, when you require the data it only needs the subset of the pieces to recreate the original information. Lets take an example, consider that the original value of the object or data is 95, we divide it in such a way that x= 9 and y=5. The encoding process will create a series of equations. Suppose in this case it will create equations like: x+y= 14 x-y= 4 2x+y= 23 When you want to recreate the object you require any two of those three equations, to be able to be decoded. So when you solve the equations together, you will be able to get the values for x and y.\nBasic Concept of Disk I/O: The term I/O literally means input and output.The operating system moves from the upper layer to the lower layer, and there are I/O operations between each layer.For example, the CPU has I/O, memory has I/O, VMM has I/O and the underlying disk also has I/O.This is I/O in the broadest sense.Top-level I/O can generate multiple disk IOs, that is, top-level I/O is sparse, while low-level I/O is dense. Disk I/O, as the name suggests, is disk I/O.Input refers to writing data to disk and output refers to reading data from disk.We usually use ATA, SATA, FC, SCSI, and SAS drives, as shown in Figure 1. Among these types of drives, servers commonly use SAS and FC drives, and some high-end storage systems also use SSD drives.The performance of each disk type is different. Physical disk architecture and common types of disks. Performance Index: SAN (storage area network) and NAS storage (network storage) usually have two metrics:IOPSand throughput (bandwidth) - the two metrics are independent and interrelated.The most important measure of storage system performance isIOPS.Next, I will explain the meaning of these two parameters. What is IOPS?\nIOPS(Input / Output Per Second) - This is the number of input and output data (or the number of reads and writes) per second, and this is one of the main indicators for measuring disk performance.IOPS refers to the number of I/O requests that a system can process in a unit of time.I/O requests are usually requesting for read or write operations.For applications with frequent random reads and writes, such as OLTP (online transaction processing), IOPS is a key metric. In short, Disk IOPS is the number of I/O operations, reads, and writes performed by a disk in one second. What is Throughput? Another important metric is Throughput, which refers to the amount of data that can be successfully transferred per unit of time.For many sequential read-and-write applications such as VOD (Video on Demand), In short, Disk throughput i.e. disk I/O traffic per second, i.e. the size of disk writes operations plus read data. Relationship between IOPS and throughput: I/O throughput per second = IOPS * average I/O SIZE.This can be seen from the formula: the larger the I/O SIZE, the higher the IOPS, and the higher the I/O throughput per second.Hence, we think that the higher the IOPS and throughput, the better.In fact, for a disk, these two parameters have their maximum values, and there is a certain relationship between these two parameters.\nIOPS can be broken down into the following metrics: Total IOPS, mixed read-write disk IOPS, and sequential random I/O load, this is the most consistent with the real I/O situation, and most applications pay attention to this figure. Random read IOPS, 100% random read IOPS. Random Write IOPS, 100% Random Write IOPS. Sequential read IOPS, 100% sequential read IOPS under load. Sequential write IOPS, IOPS at 100% sequential write load. The following figure shows a typical NFS test result: IOPS performance testing tools mainly includeIometer, IoZone, FIO, etc. can be used to test disk IOPS in various situations.For the application system, you must first determine the data load characteristics, and then select a reasonable IOPS index for measurement and benchmarking, and then select the appropriate media and software system accordingly. There is a linear relationship between IOPS and Throughput, and the variable that determines their changes is the size of each I/O. It can be seen from the figure that when the I/O to be transmitted is relatively small, the transmission time required for each I/O will be relatively small, and the number of I/O transmitted per unit time will be large. While the total time to transmit the actual data is relatively low due to the processing of the packet headers. When the I/O size is relatively large, as shown in the figure below, the time to transmit each I/O increases and the number of IOPS decreases. But compared to a higher percentage of I/O channels used to transmit actual data, Throughput increases significantly.\nThe relationship between Throughput and IOPS is shown as: Throughput MB/s = No of IOPS * KB per IO / 1024 Assume a 10, 10K SAS disks, each providing 140, IOPS, for a total of 1400 max IOPS. Theoretically, these disks handle different IO sizes, and the throughput that can be achieved is different. Simply put, whichever first reaches the limit of the physical disk, IOPS or Throughput, determines the performance threshold of the physical disk. As can be seen from the calculation formula below, the unit I/O size can double the throughput, but it fails to reach the theoretical \"bandwidth\" of 10 SAS disks of 1GB/S (each disk has a bandwidth of 100MB/s). Obviously, because the I/O of most applications will not be so large, you will see that the throughput of the storage array is much lower than the theoretical value provided by the manufacturer. This is justified that the IOPS first reaches the performance threshold so that the throughput cant be further improved. Of course, there are also special applications, such as streaming media servers, etc., where the I/O size of 2MB can be used on the application side, so the throughput utilization rate will obviously be higher, and the IOPS requirement is relatively low.\nThroughput MB/s = 1400 x 64 /1024 = 87.5 MB/s Throughput MB/s = 1400 x 128 /1024 = 175 MB/s Throughput MB/s = 1400 x 256 /1024 = 350 MB/s To sum up, when planning storage performance and dealing with storage performance issues, it is necessary to comprehensively look at the two parameters of IOPS and throughput. The viewpoints of this article are summarized as follows: The throughput counted by the performance tool will never reach the theoretical \"bandwidth\" of the nodes in the actual I/O flow, because the performance tool will not count the I/O packet header information, but the actual data transmission volume. Whichever first reaches the limit of the physical disk, IOPS or Throughput, determines the performance threshold of the physical disk. However, it is the size of the I/O that determines which reaches the performance threshold first. If the performance monitoring tool shows that the IOPS is low or the throughput is lower than expected, do not directly think that there is a problem with the storage performance, but figure out the I/O size of the application before making a follow-up judgment. Another important factor in storage performance is disk response time (Response Time). The content of this article is based on the premise that storage can provide response time within the accepted access.\nDo We Need to Store Data for Decades or Centuries? HI HI, Greetings! HI April, Today, I would like to explain the importance of data stored for Decades or Centuries. Let's get into the article. When keeping data for decades or centuries, it is critical to consider the data's long-term preservation. Hardcopy or print, microfilm/microfiche, magnetic tape, and optical storage can all provide long-term storage alternatives. Each method, however, has advantages and disadvantages, and it is critical to select the method that best meets the demands of the data being saved. Proper storage conditions and frequent maintenance can also aid in the preservation of stored data. A Datastore is a necessary component of modern software systems that enables data storage and retrieval. It offers a structured and organized approach to data management, allowing for efficient and effective access to data for a range of purposes such as analysis, reporting, and decision-making. Here are some of the reasons why Datastore are essential: 1. Data administration: Datastore enables firms to efficiently manage and organize data. It acts as a centralized data store, making it easier to access, change, and share data across multiple applications and systems. 2. Scalability: As a company grows, so do its data management requirements. Datastore is a scalable solution for storing and managing huge volumes of data, allowing for increased demand while maintaining speed. 3. Data protection: Datastore guarantees that data is securely kept and safeguarded against unauthorized access.\nIt includes access controls, encryption, and other security measures that aid in the prevention of data breaches and the protection of sensitive information. 4. Data analysis: Businesses can use Datastore to analyze data and gain important insights. It encourages the use of analytics technologies like business intelligence software and machine learning algorithms to assist firms in making better decisions based on data-driven insights. 5. Application development: A Datastore is an essential part of application development. It serves as the framework for developing and delivering data-driven applications, allowing developers to focus on generating new solutions that satisfy business goals. Storing digital data for decades or centuries is a difficult endeavor since digital data is prone to erosion, destruction, and obsolescence over time. Here are some of the difficulties related to keeping digital data for extended periods of time: 1. Obsolescence of technology: Digital data storage technology is continually changing, and today's data storage medium and equipment may become obsolete in the future. This might make accessing or reading data saved on obsolete devices or media challenging. 2. Data tampering: Digital data can become corrupted owing to a variety of variables such as bit rot, magnetic fields, and environmental factors. This might lead to data loss or corruption over time. 3. Security risks: Hacking, viruses, and other cyber attacks are all potential hazards to digital data. These hazards can jeopardise the integrity and confidentiality of stored data, exposing it to theft, alteration, or destruction. 4.\nMigration of data: To avoid data loss due to technological obsolescence, data must be moved on a regular basis to fresh storage media or devices. This procedure can be time-consuming, costly, and risky, as data might be lost or corrupted throughout the migrating process. 5. Long-term storage: To guarantee that digital data stays accessible and useable over time, it must be properly stored, backed up, and maintained. This necessitates continuing commitment and effort, such as routine backups, monitoring, and improvements. The greatest long-term digital storage media is determined by a number of characteristics, including storage capacity, durability, accessibility, and cost. The following are some of the most widely used long-term digital storage media: 1. HDDs (Hard Disc Drives): HDDs are the most widely used digital storage media, with great storage capacity and quick data access. They are, however, susceptible to mechanical breakdown and may not be suited for long-term preservation. 2. SSDs (Solid State Drives): SSDs are more durable and have faster access times than HDDs. They store data in flash memory and are less prone to mechanical failure. They are, however, still susceptible to data loss due to wear and tear, and their data retention rates are unknown. 3. Magnetic Tapes: For decades, magnetic tapes have been utilized for long-term storage. They have a large storage capacity and are quite inexpensive. They do, however, necessitate specialized equipment to read and write data and may not be appropriate for small-scale storage.\n4.Optical Discs: Optical discs, such as CDs, DVDs, and Blu-ray discs, have a large storage capacity and a long lifespan. They are similarly simple to use and read, although scratches and light exposure can shorten their lifespan. 5. Cloud Storage: Cloud storage provides a handy and dependable solution to store data for extended periods of time. It offers high accessibility and backup possibilities while doing away with the requirement for actual storage media. It is, however, costly for large-scale storage, and its long-term stability and security are not fully established. Furthermore, archival-grade optical discs like M-DISC are specifically built for long-term data archiving. They employ a unique data layer composed of materials that are resistant to damage over time, such as UV light and moisture. There are two common causes of \"disc rot.\" According to the various makers, these discs can survive up to 1000 years or more. Obviously, proving this assertion is impossible, but they can make an educated guess through testing. A Datastore is a crucial component of modern software systems, allowing businesses to handle and store data in an efficient, secure, and scalable manner, as well as providing a platform for data analysis and application development. M-DISC optical discs, for example, are designed to last hundreds or even thousands of years. Tape storage may be trusted for decades, whereas hard drives and SSDs can be trusted for 5-10 years, depending on how well they are treated and stored.\n(Rest API Model) What is REST API? REST API is an architectural style that is used to create web services that can be accessed via HTTP. The style is based on the transfer of resources from one system to another. These resources are identified by URIs (Uniform Resource Identifiers). The transfer of data between systems is done using standard HTTP methods like GET, POST, PUT, and DELETE. REST API is highly scalable, easy to use, and widely adopted in the web development community. REST APIs use a set of uniform resource identifiers (URIs) to identify resources. A resource is any information that can be accessed using a URI. (REST API) How does REST API work in Storage? Storage is a critical component of any web-based application or service. Data must be stored in a secure and reliable manner. REST API has become a popular way to access and manipulate data stored in a variety of storage systems. Here's how the REST API works in storage. (REST API work) 1. Resource identification: The first step in using REST API in storage is to identify the resources that need to be accessed. In the case of storage, resources are files, folders, or databases that contain data. These resources are identified using URIs. REST APIs can be used to perform file operations such as retrieving files, creating new files, updating existing files, or deleting files.\nFor example, a client can send a GET request to retrieve a file from a storage system using a specific URI that identifies the file. The server processes the request and sends back the file content as the message body of the response. 2. HTTP Methods: Once the resources have been identified, the next step is to perform actions on them using HTTP methods. REST API supports several HTTP methods, including GET, POST, PUT, and DELETE. These methods are used to read, create, update, or delete resources. (HTTP Methods) 3. Data formats: REST API supports several data formats, including JSON (JavaScript Object Notation), XML (Extensible Markup Language), and plain text. These data formats are used to transfer data between systems. JSON has become the most popular data format for REST API because it is easy to read and write, lightweight, and widely supported. (Rest API in Action) 4. Authentication: Access to resources stored in storage must be authenticated to ensure the security of the data. REST API supports several authentication mechanisms, including HTTP Basic Authentication, OAuth, and JSON Web Tokens (JWT). These mechanisms are used to authenticate users and grant access to resources. REST APIs can be used to perform access control operations such as granting or revoking access to files or directories. For example, a client can send a PUT request to update the access control list (ACL) associated with a file using a specific URI that identifies the file. The server processes the request and updates the ACL accordingly. 5.\nDear All, Today we are going to learn about KB vs MB. KB and MB are units of measurement used to quantify the amount of digital information stored on a computer or other electronic device. While both of these units refer to the amount of data storage, there are significant differences between them. KB, short for kilobyte, is the smallest unit of measurement used for data storage. One kilobyte equals 1,024 bytes. A byte is the basic unit of measurement for digital data and represents a single character such as a letter, number, or symbol. Therefore, a kilobyte can store approximately 1,000 to 1,200 characters. MB, on the other hand, stands for megabyte. One megabyte equals 1,024 kilobytes or 1,048,576 bytes. This unit is used to describe larger amounts of data storage, such as documents, images, and music files. For example, a typical MP3 file might be around 3-4 MB in size. Unit of measurement Abbreviation Equivalent The main difference between KB and MB is the amount of data they can store. KB is used for small amounts of data, such as text documents or simple images, while MB is used for larger files such as videos or high-resolution images. Another important difference between KB and MB is the rate at which data is transferred. The speed at which data can be transferred from one device to another is measured in bytes per second or bits per second. The higher the number, the faster the data transfer rate.\nHello all, Have a good day. Today we talk about 3 types of storage: File Storage, Block Storage, Object Storage. Advantages: easy management and interoperability with applications Disadvantages: support for expansion but with many restrictions Application scenarios: enterprises' internal application integration and file sharing File storage is used to store unstructured data. Dedicated file systems are added to block storage devices to implement file sharing. Generally, file storage is used to store video, audio, and image data from TV stations, finance, oil exploration, biomedicine, and HPC (big data). Advantages: direct access, minimized overhead, and highest efficiency Disadvantages: highest cost and poor scalability Application scenarios: enterprise databases, such as Oracle Block storage is used to store structured data, that is, data is directly read and written by reading or writing one or more addresses from or into storage space. More generally, block storage stores data of databases such as SAP and Oracle, common office mails (Exchange), tables, and financial data. Flat structure and nearly unlimited capacity expansion More intelligent self-management Standard Internet protocols and cross-region transmission capabilities Application scenarios: Internet-oriented storage, archiving, and backup Object storage, also known as object-based storage (OBS), is a network storage architecture. The difference between OBS and block storage or file storage lies in the interfaces (S3 interfaces) provided by OBS. OBS only generates an ID for the metadata of stored data and stores the ID, regardless of the data type. This storage architecture is mainly used in the application scenarios that have low requirements on performance but high requirements on capacity.\nBackup is the process of producing a copy of your system's data that you can use for recovery if your original data is lost or corrupted. Backup can also be used to retrieve copies of previous files that have been removed from your system. Many businesses and organizations use backup to secure their sensitive data, making it an important component of a company's Disaster Recovery Plan and Business Continuity Strategy. Types of Data Backup Solutions: Local backups(Using external hard drives, DVDs, or USB drives) Cloud backups (Using online services such as Dropbox, ICloud ,Huawei Cloud or Google Drive) Network backups (using a network-attached storage device or a customised backup server). Identify which data to backup: You should identify the data that is necessory for your business and personal life, this includes important documents, financial records, family photos, and other irreplaceable data that must be backed up regularly. Go For Best Backup Solution : On the basis of your needs and budget, you have to decide a backup solution. You must consider the number of backups, the amount of storage required and level of security is responsible for the solution. Backup Schedule : According to your daily routine, needs and time you must schedule backups regularly and keep your data protected.This can be daily,weekly or monthly backups depending on your data changes. Backups Testing: Checking your backups are working correctly by testing them periodically. Restoring your backups data is to ensure that it is complete and accurate.\nDear all, This case is about the process how to handle the NTP service synchronization failure in FusionStorage. Storage node is EulerOS. FusionStorage 8.0.1; EulerOS Release 2.0 (SP3). During NTP synchronization on storage nodes, three nodes fail to be synchronized and the status of two nodes is Abnormal . The status of one node is Unknown . 1. Log in to the three nodes that fail to be synchronized and check whether the OS version is EulerOS Release 2.0 (SP3). 2. Run the below command to check the running status of the NTP service on each node. The NTP service status of the two nodes that report Abnormal is dead . Command reference: # systemctl status ntpd # systemctl restart ntpd 3. The NTP service of the node that reports Unknown is failed and the NTP service cannot be started no matter whether the service is restarted, stopped, or started. Command reference: # systemctl status ntpd # systemctl restart ntpd # systemctl stop ntpd # systemctl start ntpd 4. According to the error information displayed in the query status of the NTP service, other NTPD services are running. 5. Confirmed with the engineers that the node tried to start the ntpdate service except the ntp service. Therefore, it was suspected that the two services conflicted. The ntpdate service was also in a failed state. 6. Failed to restart the ntpdate service. 7. Check the running status of the OS process. It is found that multiple ntpd services are running at the same time.\nHi there! This time, I will share with you an important article about Huawei OceanProtect DR Solution & its key technologies. In this article, you will learn about OceanProtect Product DR: SAN and NAS integration, gateway-free active-active architecture, RAID-TP, and other technologies. OceanProtect DR Solution: Huawei is committedto building and has provided multi-layer andcomprehensive DR solutionsin terms of OceanProtect DR productscoveringactive/active, active/passive,and geo-redundant 3DC deployment,helping you construct DR systems tailored for variousindustries to ensure business continuity. Zero service disruption: OceanProtect provides Gateway-free active-active architecture for both SAN and NAS ensuring always-on services. Zero fault impact on hosts: OceanProtect provides a failover feature within seconds in the event of production storage failures, without affecting the upper layer of services. Visualized management: OceanProtect provides simplified O&M with global topology, and one-click DR solution drill and failover The solution isfully designed to offer full DR of hot data andit integrates DR for SAN& NAS and providestension-free device upgrades to maximize return on investment (ROI). solid-state drives Gateway Free Active-Active Architecture: gateway-free active-active or 3DC solutions three-data-center (3DC) multi-DC four-copy solution. RAID-TP: OceanProtect Design Principle - Distributed Architecture: The entire series adopts the distributedsystem architecture. Failover isperformed within seconds so thatservices are not affected. Backup tasks are not redone, which ensures that each backup task is completed within the specified time window.Symmetric client access balances data to all controllers, providing ultimate performance. Controller load balancing and automatic balancing of scale-out, failover, andfailback services are supported,achieving on-demand expansion andsimple O&M.\nHi there! This time, I will share with you an important article about Huawei OceanProtect Backup Storage & its key technologies. In this article, you will learn about OceanProtect Product introduction, hardware, software architecture, and advantages. OceanProtect Product Introduction: Huawei OceanProtect backup storage is a first line of defense and robust storage system with the fastest recovery capability, which leads the dedicated backup storage industry into the flash-to-flash-to-anything era for \"combat\" servers. These new All-Flash backup storage systems are focused on the backup of \"warm\" data, which allows you to quickly restore the performance of business-critical systems and applications in the event of a failure or attack. OceanProtect completely ensures uninterrupted services without data loss and delivers long-term data retention in the intelligent world. It is very helpful inquickly and reliably backup and restoring data at a low TCO, using E2E acceleration and an active-active architecture. It is also a trusted choice for demanding scenarios primarily financial institutions, energy, health care, telecommunications, production management, retail, etc. According to experts, the lack of disaster tolerance, the most important element of which is just backup . More than half of the cases lead to bankruptcy within 2-3 years after the first IT systems crash and there are a lot of reasons for such a fall - from natural disasters and the human factor to unintentional (equipment failure) or intentional (attack) interference in the operation of storage systems. Product Overview: Here is a detailed overview of the available products.\nThe below table depicts the deep drive summary of the OceanProtect backup storage. OceanProtect Hardware Architecture: 1- Hardware Architecture of OceanProtect X9000 Storage Controller: Active/Active hardware architecture adopted for redundancy of all components, ensuring no single point of failure: The front panel has 04 controllers, each of which has an independent BBU and fan, 192 cores, and two 2.5-inch system disks. The rear panel supports up to 28 I/O cards, which are globally shared among 04 controllers. 04 power modules form 02 power planes. The fans, power modules, BBUs, and I/O cards can be maintained online. 2- Hardware Architecture of OceanProtect X8000 Storage Controller: Active/Active hardware architecture adopted for redundancy of all components, ensuring no single point of failure: The power modules, BBUs, and I/O cards are hot-swappable. Controllers must be removed for fan replacement. One 1 m cabinet is required for installing a controller enclosure with a length of 0.82 m. The DIMM slots beside the IOB are empty. 3- Huawei OceanProtect X8000/X9000 Disk Shelf: 4-port (Mini-SAS) SAS-3 interface modules are used to connect disk shelves, 25/100GbE with RDMA for controllers, and FC8/16/32 and 10/25/40/100GbE modules with RDMA for hosts. Ethernet controllers support offloading the TCP/IP stack, relieving the CPU from unnecessary workload. There are enough slots for modules to combine controllers with redundant connections without using an external switch. Support for Fiber Channel and iSCSI is available for SAN, and NFSv3/4.1, SMB/CIFS 2.0/3.0 and NDMP for NAS. OceanProtect Software Architecture: The overall flow of data and backup process is shown in the below figure.\nRAID (Redundant Array of Independent Disks) is a technology that combines multiple physical hard drives into a logical unit to provide improved data redundancy, availability, and performance. There are several RAID levels, each with its own set of benefits and drawbacks. The most commonly used RAID levels are: RAID 0 : Striping without parity. Data is striped across two or more drives to improve performance, but there is no redundancy. RAID 0 is not fault-tolerant, and a single drive failure will result in data loss. RAID 1 : Mirroring. Data is mirrored on two or more drives for redundancy. RAID 1 provides high fault tolerance, but it is less efficient in terms of disk usage, as only half of the total disk capacity is available for data storage. RAID 5 : Striping with distributed parity. Data is striped across three or more drives, with parity information distributed across all drives. RAID 5 provides fault tolerance and improved performance, and it uses disk space efficiently, as only one drive's worth of space is used for parity. However, performance may degrade during a drive rebuild after a failure. RAID 6 : Striping with dual parity. Data is striped across four or more drives, with two sets of parity information. RAID 6 provides higher fault tolerance than RAID 5, as it can tolerate the failure of two drives simultaneously. However, it uses more disk space for parity, and performance may also degrade during a drive rebuild.\nHello all, This post is talking about how to make your post easy to be searched by Google. Writing a good English article is not just about creating quality content, but also optimizing it for SEO (Search Engine Optimization). Here are some key steps to follow when creating an SEO-friendly article: Conduct keyword research : Before you start writing, it's essential to identify the primary keywords and phrases that your target audience is searching for. Use keyword research tools like Google AdWords Keyword Planner, SEMrush, or Ahrefs to identify relevant keywords with high search volume and low competition. Use the primary keyword in the title : Your article title should include the primary keyword, preferably at the beginning, as it's the first thing that readers and search engines will see. Include the primary keyword in the URL: The URL of your article should include the primary keyword and be concise and descriptive. Write high-quality content : Focus on creating high-quality, informative content that meets the needs of your target audience. Avoid keyword stuffing or using irrelevant keywords, as this can harm your SEO. Use subheadings and formatting : Use subheadings to break up your content into sections and make it easier for readers and search engines to understand. Use bold and italic formatting to highlight important points and make your content more readable. Optimize images : Use descriptive filenames and alt tags for your images, as this helps search engines understand the content of your article and can also drive traffic from image searches.\nInclude internal and external links : Link to relevant internal pages within your website and external authoritative sources to provide additional value to your readers and improve your SEO. Optimize meta tags : Use relevant meta tags, such as title tags and meta descriptions, to provide a brief summary of your article and help search engines understand its content. By following these steps, you can create an SEO-friendly article that provides value to your audience and helps you rank higher in search engine results pages. We know our post has only one title on the top of the post, but if we have subtitles in the paragraph, how do we add our subtitles? This figure shows how to modify the title tag. A page just needs only one Heading 1, our Title already adds H1 tag, so in our Body we just need to use Heading 2 or Heading 2. Never use Heading 1 in your post Body. When we add pictures to our post make sure the description is relevant to the picture. This figure shows this picture is about wifi 6. When we add videos to our post make sure the title, the description, and the thumbnail are relevant to the video. This figure shows this video is about how to add a video. Refer to video example: If you have a command in the paragraph, make sure it in Bold . If you have a long code or log to cope, make sure it in code form.\nHere are common protocols used in storage are: Internet Small Computer Systems Interface (iSCSI): It transfers SCSI blocks and commands over TCP/IP networks. It allows organizations to leverage their IP networks to transfer block storage data. It has benefits such as interoperability, lower cost and ease of management Fibre Channel (FC): It is a high-speed network technology that transfers SCSI blocks and commands over a dedicated network of fiber optic cables. It provides high performance, reliability and security for mission-critical applications. It requires specialized hardware and software to operate. Fibre Channel over Ethernet (FCoE): It is a protocol that encapsulates FC frames over Ethernet networks. It aims to combine the benefits of FC and Ethernet, such as high performance, lower cost and simplified management. It requires converged network adapters and switches that support both protocols. Network File System (NFS): It is a client-server protocol for distributed file systems that allows users to access or share data between devices on the same network. It uses remote procedure calls (RPCs) to execute requests between clients and storage servers. It is mainly used in Unix and Linux environments. Server Message Block (SMB): It is a client-server protocol for distributed file systems that allows users to access or share data between devices on the same network. It uses TCP/IP or NetBIOS over TCP/IP to communicate between clients and storage servers. It is mainly used in Windows environments Hello, dear. There are several protocols commonly used in storage: 1. SCSI (Small Computer System Interface) 2. SATA (Serial ATA) 3.\nA client fails to connect to a storage system through the front-end service IP address of the storage system and thus cannot use service functions. Additionally, the front-end service network port does not send any alarm. The front-end network cable is incorrectly connected. The front-end service gateway is faulty. Possible cause 1: The front-end network cable is incorrectly connected. a. Check whether the network cables of the front-end service network of the storage system are correctly connected to the switch ports by referring to the and the actual network planning. -If yes, go to possible cause 2 . -If no, go to b . b. Connect the network cables to the switch ports correctly. Then check whether the client can connect to the storage system through the front-end service IP address of the storage system. -If yes, no further action is required. -If no, go to possible cause 2 . Possible cause 2: The front-end service gateway is faulty. a. Log in as user omuser to the storage system using the management IP address. Run the ssh command and use the back-end IP address of the node with the inaccessible front-end IP address to skip to the node. Run the su command to switch to user root and run the ping command to check whether the gateway can be pinged ( indicates the address of the front-end service gateway). -If yes, check the connectivity between the client and gateway and fix the network faults.\nThe chmod command is used to in Linux. The command stands for , and it allows you to of files and directories. The chmod command uses a to set the permissions of a file or directory. Here's how to use the chmod command: Numeric mode The numeric mode of chmod uses a three-digit number to set permissions. Each digit represents a different set of permissions: The the permissions for the owner of the file or directory. The the permissions for the group that the file or directory belongs to. The the permissions for everyone else. read (4), write (2), and execute (1). For example, to set for the owner of a file, you would use the number 6 (4 + 2). To set for everyone else, you would use the number 5 (4 + 1). To use the numeric mode, you would enter the following command: For example, to set read and write permissions for the owner of a file called example.txt , you would enter the following command: Symbolic mode The symbolic mode of chmod uses The symbols are: u for the of the file or directory g for the that the file or directory belongs to o for else a for users + to permissions.\nWhat role does multi-path software play? A. Changing the fault route B. IO traffic load balancing C. recovery of a fault path D. Data restoration Dear user, UltraPath provides the following functions: Masking of redundant LUNs In a redundant storage network, an application server with no multipathing software detects a LUN on each path. Therefore, a LUN mapped through multiple paths is mistaken for two or more different LUNs. UltraPath installed on the application server masks redundant LUNs on the operating system driver layer to provide the application server with only one available LUN, the virtual LUN. In this case, the application server only needs to deliver data read and write operations to UltraPath that masks the redundant LUNs, and properly writes data into LUNs without damaging other data. Optimum path selection In a multipath environment, the owning controller of the LUN on the storage system mapped to an application server is the prior controller. With UltraPath, an application server accesses the LUN on the storage system through the prior controller, thereby obtaining the highest I/O speed. The path to the prior controller is the optimum path. Failover and failback Failover When a path fails, UltraPath fails over its services to another functional path. Failback UltraPath automatically delivers I/Os to the first path again after the path recovers from the fault. I/O Load balancing UltraPath provides load balancing within a controller and across controllers. For load balancing within a controller, I/Os poll among all the paths of the controller.\nBlob storage is a type of cloud storage for unstructured data. A \"blob,\" which is short for Binary Large Object, is a mass of data in binary form that does not necessarily conform to any file format. Blob storage keeps these masses of data in non-hierarchical storage areas called data lakes. Scalable : Storage capacity for blobs is essentially limitless. Additionally, saving data for later retrieval is still simple and quick even as the amount of data being stored increases. Cloud-native : The cloud is where blob storage is housed. Blob storage is thus a logical fit for businesses creating in or moving to the cloud. This implies that, like all cloud services, blob storage can be accessed online from any place. Programming language agnostic : Developers may typically access blobs using a variety of languages thanks to blob storage providers. Cost-effective: Pricing for blob storage typically has levels. Large volumes of data can be stored more affordably overall if the majority of it is not frequently accessible since material that is rarely viewed is in a much lower tier. Some of the major use cases for blob storage include: Media : Image, video, and audio data take up a lot of space, and sometimes needs to be stored but not necessarily accessed regularly. Logs : Software constantly generates a stream of events while it runs, which can be stored in logs for subsequent examination. These data can quickly grow in quantity.\nThis data can be quickly and affordably stored in an unstructured format using blob storage. Blob storage is less economical for this use scenario, though. Any log data query will incur egress expenses. Backups and disaster recovery : The majority of businesses must maintain thorough backups, especially for recovering from ransomware attacks. Blob storage is a good choice for backing up big data sets because this data is duplicated in production and rarely accessed. There are three primary blob types: append, block, and page blobs. Append Blobs Append blobs, which also consist of blocks, are created expressly for use with append operations. An add blob is most frequently used to store and update log files. Blocks can be added to the end of an append blob, but they cannot be changed or eliminated from an existing block. Just as with block blobs, an append blob may contain up to 50,000 blocks, each up to 4 MiB. Block Blobs Block blobs, which are separated into blocks, are typically used for the storage of binary, text, and media data. Blocks can be any size up to 4000 MiB (mebibytes), although they can be any size (in the most current Azure version). A block blob can only contain up to 50,000 blocks, which results in a maximum block size of 4.75 TiB. (tebibytes). Blocks can be added to, removed from, and individual blocks within a block blob can be changed or replaced. The upload time for block blobs is also optimized by parallel uploading of individual blocks.\nPage Blobs A page blob is designed to be read and written to. An assembly of 512-byte pages is referred to as a page blob, and its largest possible size is 8 TiB. Page blobs can be used to store things like operating systems and data for disaster recovery. Also keep in mind that since all blobs are cloud-based and come encrypted, regardless of kind. In general, cloud encryption will encrypt data as it moves between cloud-based applications or storage and users at their various locations. There are two types of costs associated with blob storage: transaction costs (i.e. costs for accessing the data) and storage costs. Transaction costs are per block, that is a write operation on one block is a single transaction. Blob storage pricing is divided into tiers, based on the frequency of access to the data. This allows for more cost-efficient storage, particularly where the user has large amounts of data that require very infrequent access. Hot The most frequently accessed data is kept in hot storage. Of the three categories, it has the lowest storage cost but the highest storage cost. Hot blob storage is constantly accessible. Cool Data that is periodically but not frequently accessed should be kept in cool storage. Cool storage is suitable, in particular, when data is not accessed more than once every 30 days. Compared to hot storage, cool storage offers lower storage costs but greater transaction costs. Data in cool storage is always online, just like in hot storage.\nDear All, Today we are going to discuss about Huawei All Flash Storage High Reliability High Reliability Multiple cache copies To ensure the low latency of write I/Os on the host, Huawei all-flash storage system stores the data written by the host in the cache (memory) and then writes the cached data to disks in the background. To ensure the reliability of cache data, Huawei all-flash storage system provides up to three copies of cache data. By default, the storage system provides two copies of cache data. For data with the same LBA, the storage system creates a pair relationship between two controllers within a controller enclosure. Upon receiving write requests from the host, the system writes the data to both the caches on the paired controllers and returns acknowledgement to the host after data is successfully written to both caches to ensure data consistency. In the event that a controller is faulty, data cached on the other controller can be destaged or accessed, ensuring zero data loss. Intra-disk dynamic RAID For data on an HSSD, RAID groups are created by physical dies to reduce the probability of data loss caused by silent corruption. Each package on an HSSD consists of multiple physical dies. RAID 4 is used for redundancy of data written to the dies, preventing data loss in the event of a single die failure. The uncorrectable bit error rate (UBER) of SSDs in the industry is about 10-17.\nWith the support of intra-disk RAID, the UBER of HSSDs is reduced to 10-18 (decreasing by one order of magnitude). For example, if intra-disk RAID is not used, a bad block occurs when 11 PB of data is written to an SSD. If intra-disk RAID is used, a bad block occurs when 110 PB of data is written to an SSD. Wear levelling and antiwear levelling The SSD controller uses software algorithms to monitor and balance the P/E cycles on blocks in the NAND flash. This prevents over-used blocks from failing and extends the service life of the NAND flash Intra-disk wear leveling: HSSDs support dynamic and static wear leveling. Dynamic wear leveling enables the SSD to write data preferentially to less-worn blocks to balance P/E cycles. Static wear leveling allows the SSD to periodically detect blocks with fewer P/E cycles and reclaim their data, ensuring that blocks storing cold data can participate in wear leveling. HSSDs combine the two solutions to ensure wear leveling. Global wear leveling: The biggest difference between SSDs and HDDs lies in that the amount of data written to SSDs is no longer unlimited, and the service life of an SSD is inversely proportional to the amount of data written to the SSD. Therefore, an all-flash storage system requires load balancing between SSDs to prevent overly-used SSDs from failing. FlashLinkTM uses controller software and disk drivers to regularly query the SSD wear degree from the SSD controller.\nIn addition, FlashLinkTM evenly distributes data to SSDs based on LBAs/fingerprints to level the SSD wear degree. Anti-wear leveling: When SSDs are approaching the end of their service life as their wear degrees have reached 80% or above, multiple SSDs may fail simultaneously if global wear leveling is still in use, resulting in data loss. In this case, the system enables anti-global wear leveling to avoid simultaneous failures. The system selects the most severely worn SSD and writes data to it as long as it has idle space. This reduces that SSD's life faster than others, and you are prompted to replace it sooner, avoiding simultaneous failures. LDPC and FSP algorithms HSSDs use the LDPC algorithm and the FSP technology to ensure data reliability Enhanced Low-Density Parity-Check (LDPC) algorithm: provides higher error correction capability than that required by flash chips to ensure device reliability. LDPC refers to a kind of linear codes defined through a check matrix. LDPC consists of four modules Encode, Decode, Soft-bit Logic, and DSP Logic. When data is written to the NAND flash, the system generates the LDPC parity data and writes it to the NAND flash with the raw data. When data is read from the NAND flash, the LDPC parity data is used to check and correct the data. In case an error occurs in data in the NAND flash, the HSSD enables LDPC hard decoding to correct the error. If the error cannot be corrected, the HSSD enables shift read to save data.\nIf shift read fails, the HSSD attempts to enable soft read to save data. If soft read also fails, the HSSD enables read retry to recover data. If the data still fails to be recovered, the data on other dies is used to perform the XOR operation to recover user data. Intelligent FSP algorithm: Based on the characteristics of 3D TLC media, the intelligent FSP algorithm provides faster and more reliable data storage services DIF DIF stands for Data Integrity Field. Data path protection refers to the protection of data integrity and correctness on I/O paths. In addition to ECC and CRC for key memories (such as DDR), SSDs can enable LBA check and DIF to protect data. DIF is the protection information that is added to the end of the user data sector. This information ensures the data consistency between the host and the SSD. Data inspection algorithm After data has been stored in NAND flash for a long term, data errors may occur due to read interference, write interference, or random failures. Risks can be detected and handled in advance through inspection, preventing data loss. HSSDs use read inspection and write inspection to prevent data retention errors. Read inspection traverses data in the NAND flash quickly and observe data transitions. Data experiencing high BIT transitions will be relocated in time. Read inspection prevents data errors due to random failure or read interference. The interval between write inspections changes with the temperature.\nWrite inspection checks the data retention and relocates data that has been stored for an excessively long time. Write inspection prevents errors due to long-time storage. Background inspection helps to identify risks in advance, preventing most NAND flash errors and improving data reliability. E2E protection information (PI) During data transmission within a storage system, data passes through multiple components over various channels and undergoes complex software processing. Any problem during this process may cause data errors. The scenario where a problem is not immediately detected but found in subsequent data access is called silent data corruption. ANSI T10 Protection Information (PI) provides a method of verifying data integrity during access to a storage system. It ensures that errors occurred during transmission are detected and rectified in time, preventing silent data corruption. The check is implemented based on the PI field defined in the T10 standard. This standard adds an 8-byte PI field to the end of each data block to implement data integrity check. In most cases, T10 PI is used to ensure data integrity within a storage system. Huawei all-flash storage system supports T10 PI. Upon reception of data from a host, the storage system inserts an 8-byte PI field to every 512 bytes of data before performing internal processing such as forwarding to other nodes or saving the data to the cache. After the data is written to disks, the disks verify the PI fields of the data to detect any change to the data between reception and destaging to the disks.\nAs shown in the figure in the training materials, the green point indicates that a PI is inserted into the data. The blue points indicate that a PI is calculated for the 512- byte data and compared with the saved PI to verify data correctness. When the host reads data, the disks verify the data to prevent changes to the data. If any error occurs, the disks notify the upper-layer controller software, which then recovers the data by using RAID. To prevent errors on the path between the disks and the front end of the storage system, the storage system verifies the data again before returning it to the host. If any error occurs, the storage system reads the data from the disks in transparent mode or recovers the data using RAID to ensure end[1]to-end data reliability from the front-end interface modules to the back-end disks. Multi-layer redundancy and fault tolerance design Huawei all-flash storage system uses the SmartMatrix multi-controller architecture that supports linear expansion of system resources. The controller enclosure uses the IP interconnection design and supports linear IP scale-out between controller enclosures. The management plane, control plane, and service plane are physically separated (in different VLANs), and served by different components. Each plane can independently detect, rectify, and isolate faults. Faults on the management plane and control plane do not affect services. Service plane congestion does not affect system management and control. All components in Huawei all-flash storage system work in redundancy mode, eliminating single points of failure.\nHuawei all-flash storage system provides multiple redundancy protection mechanisms for the entire path from the host to the storage system. Service continuity is guaranteed even if multiple field replaceable units (FRUs) allowed by the redundancy scheme are faulty simultaneously or successively. High reliability Design Three copies across controller enclosures: For data with the same LBA, Huawei all-flash storage system creates a pair between two controllers to form a dual[1]copy relationship and creates a third copy in the memory of another controller. If there is only one controller enclosure, three copies are stored on different controllers in this controller enclosure, preventing data loss when any two controllers become faulty at the same time. If there are two or more controller enclosures, the third copy can be stored on a controller in another controller enclosure. This prevents data loss when any two controllers are faulty at the same time or a single controller enclosure (with four controllers) is faulty Continuous cache mirroring: This prevents data loss or service interruption when a controller is faulty and a new controller fault occurs before that controller recovers. This ensures service continuity to the maximum extent. Generally, data blocks 1 and 2 on controller A and data blocks 1 and 2 on controller B form are mutually mirrored, respectively. If controller A is faulty, data blocks on controller B are mirrored to controllers C and D, ensuring data redundancy.\nDear All, Today we are going to learn about Huawei All Flash Storage High Performance Protocol offload with DTOE Traditional NIC: The CPU must spend great resources in processing each MAC frame and the TCP/IP protocol (checksum and congestion control). TOE: The NIC offloads the TCP/IP protocol. The system only processes the actual TCP data flow. High latency overhead still exists in kernel mode interrupts, locks, system calls, and thread switching. DTOE's advantages: Each TCP connection has an independent hardware queue to avoid the lock overhead. The hardware queue is operated in user mode to avoid the context switching overhead. In addition, the polling mode reduces the latency, and better performance and reliability can be achieved. I/O acceleration Huawei all-flash storage systems support end-to-end NVMe and provide high[1]performance I/O channels. NVMe over FC, NVMe over RoCE v2 (planned), and NVMe over TCP/IP (planned) are supported for the networks between hosts and storage systems. NVMe over RoCE v2 is supported for the networks between storage controllers and disk enclosures. NVMe provides reliable NVMe commands and data transmission. NVMe over Fabrics extends NVMe to various storage networks to reduce the overhead for processing storage network protocol stacks and achieve high concurrency and low latency. Huawei uses self-developed ASIC interface modules, SSDs, and enclosures for high[1]speed end-to-end NVMe channels. This takes full advantage of the unique designs in protocol parsing, I/O forwarding, service priority, and hardware acceleration. Huawei-developed ASIC interface module offloads TCP/IP protocol stack processing for 50% lower latency.\nIt directly responds to the host from its chip for fewer I/O interactions and evenly distributes I/Os. In addition, it supports lock-free processing with multi-queue polling. Huawei-developed ASIC SSD and enclosure prioritize read requests on SSDs for prompt response to hosts. Smart disk enclosures have CPUs, memory, and hardware acceleration engines to offload data reconstruction for a lower latency. They also support lock-free processing with multi-queue polling. Intelligent multi-level cache Data IQ identifies the access frequency of metadata and data, and uses the DRAM cache to accelerate reads on LUN and pool metadata. Reads on file system metadataand data are accelerated by using DRAM for the hottest data and SCM cache for the second hottest data. This reduces 30% of latency. CPU Grouping/Partitioning Multi-core technology: Huawei-developed CPUs are used to provide the most number of CPUs and CPU cores in the same controller in the industry. Host I/O requests are distributed to vNodes based on the intelligent distribution algorithm. Services are processed in vNodes in an end-to-end manner, avoiding cross-CPU communication overheads, cross-CPU remote memory access overheads, and CPU conflicts. In this way, the storage performance increases linearly as the number of CPUs grows. All CPU cores are grouped in the vNode. Each service group corresponds to a CPU core group. The CPU cores in a service group run only the corresponding service code. In this way, different service groups do not interfere with each other. Different services are isolated and run on different cores through service grouping, avoiding CPU contention and conflicts between service groups.\nIn a service group, each core uses an independent data structure to process service logic. This prevents the CPU cores in a service group from accessing the same memory structure, and implements lock-free design between CPU cores Large-block sequential writes Hot and cold data separation Hot data and cold data are identified in the storage system. The cooperation between SSDs and controllers improves the garbage collection performance, reduces the number of erase times on SSDs, and extends the service life of SSDs. Data with different change frequencies is written to different SSD blocks, which can reduce garbage collection. Metadata is modified more frequently than user data, so the metadata and user data are written into different SSD areas. The data in garbage collection is also different from the newly written data in terms of coldness and hotness, and they are also written into different SSD areas. In an ideal situation, garbage collection would expect all data in a block to be invalid so that the whole block could be erased without data movement. This would minimize write amplification. I/O priority adjustment: Resource priorities are assigned to different I/O types to ensure I/O processing based on the SLAs. A highway has normal lanes for general traffic, but it also has emergency lanes for vehicles which need to travel faster. Similarly, priority adjustment lowers latency by setting different priorities for different types of I/Os by their SLAs for resources Smart disk enclosure The latest generation of Huawei-developed smart disk enclosures is used.\nTherefore, it has become crucial for enterprises to invest in premium security solutions that protect their data from malicious actors. As more user files are stored in storage systems, these systems must include robust antivirus safeguards. Antivirus scanning is an important method for removing threats from your system before they have a chance to strike. Huawei storage comes with a NAS antivirus feature that provides incognito security protection for your data and enhances the security of the entire system. By working with a third-party antivirus engine for both on-access and on-demand file scanning, it allows you to configure scanning policies and servers. This feature does not process isolated files, but rather uses the isolation function provided by the antivirus software. If the system fails to scan certain files, you can export the antivirus logs to check for the files and determine the cause. On-access scanning On-access scanning is triggered by file access over CIFS to prevent clients from accessing virus-infected files. After on-access scanning is enabled for a file system, virus scanning is executed in real time when you open or close a file after writing data. If a virus is detected in the file, the system will deny the access request. Otherwise, the system will deem the file to be safe or the threat is removed, and youll be able to proceed with subsequent operations. You can configure a scanning policy to specify the maximum scannable file size, scanning period, and types of files that you do not wish to scan.\nOn-demand scanning On-access or on-demand which best meets your needs? In applications that require high performance, it is recommended that you configure on-demand scanning, as it will have only a limited effect on your services. On-access scanning is ideal when performance is not prioritized, as it will cause a slight read latency when scanning is triggered upon the opening of a file. This helps ensure that there are no viruses on the file. If you open a file multiple times but do not modify it, virus scanning is triggered only at the time of first access. Once the file is scanned, a flag will be added to the file indicating that its current status is safe, and will remain there until the file is modified. Any modification to the file will automatically remove this flag, and scanning will be executed the next time it is opened. You can also configure a policy to trigger virus scanning upon the closing of a file, which prevents this latency from occurring, but is not as safe as scanning upon opening. Antivirus server management In addition to on-access and on-demand scanning, Huawei storage also allows you to add or delete antivirus servers in the NAS antivirus feature. On a typical Huawei storage network providing NAS antivirus services, the storage system is connected via TCP/IP to the Windows antivirus server, which is responsible for removing viruses.\nBusinesses that work with significant amounts of data must be continually mindful of the storage and access costs related to their operations. On-site storage needs ongoing monitoring, regular backups, and advanced IT security capabilities. Small business owners and IT executives are turning to the cloud for useful data management features. Nevertheless, different use cases come at varying costs. Understanding the distinctions between cold and hot storage is necessary to comprehend these costs. In contrast to hot data, which is often accessed, cold data is information that is only sometimes accessible. Organizations are understanding the benefits of using cold data storage devices instead of high-performance main storage because they are significantly more affordable, easy to set up and operate, and less likely to experience drive failure as unstructured data accumulates at previously unheard-of rates. Determining when data should be considered hot and preserved on primary storage or can be labelled as cold and moved off to a secondary storage device is the actual challenge with cold data for many organizations. For this reason, it's critical to comprehend the distinctions between various forms of data in order to create the most cost-effective cold data management strategy for your business. Information can be put on inexpensive hardware that doesn't necessarily fulfill the performance requirements needed for continuous use or access using cold data storage. Businesses who desire long-term storage for data that they won't be accessing frequently have a better alternative with this method.\nUsers can easily store inactive data that they only occasionally need in a single location or across numerous locations inside different storage mediums. Users typically desire systems that can endure fires, floods, and other natural disasters and have high capacity and resilience. Redundancy is even more crucial in cold storage systems than speed. Any type of data can be cold storage data, including: -Media files -Compliance data -Regulatory data Backup data, archives, and important documents can all be stored long term without the need for complicated software or other tools that would be required for continuous access or use. Dormant data can be kept on-site or in the cloud by IT professionals and small business owners. The most crucial step is to locate a secure, reasonably priced area that can be reached when necessary. Businesses who are concerned about the security of their cold data can employ offline storage on their own physical disks. Many cloud-based solutions provide safe high-volume storage boxes at discounted prices when in-house equipment is not an option. Cold data storage services provide data scalability at a fraction of the cost if you need to save low-priority data for a long time. Businesses that deal with huge amounts of data frequently incur high storage expenses since pricey disks, backups, and management software are required. Data tracking is made more challenging by this combination of platforms, especially as businesses continue to expand. Decision-makers have more growth flexibility and low-cost options thanks to cold storage as the business environment is always changing.\nCloud storage is a type of data storage that is hosted on the internet, rather than on a physical server or hard drive. It is a way of storing data in a virtual environment, allowing users to access their data from any device with an internet connection. Cloud storage is a form of distributed computing, where data is stored on multiple servers in different locations. This means that if one server fails, the data is still available from other servers. This makes cloud storage more reliable than traditional storage solutions, as it is less likely to suffer from data loss due to hardware failure. Cloud storage is a great way to store and share data, as it is secure, reliable, and cost-effective. It is also highly scalable, meaning that it can grow with your business as your data needs increase. Cloud storage is also more secure than traditional storage solutions, as it is hosted on secure servers and is protected by encryption. This means that only authorized users can access the data, and that the data is protected from unauthorized access. Cloud storage is also more cost-effective than traditional storage solutions, as it does not require the purchase of hardware or software. Instead, users pay for the amount of storage they need, and can scale up or down as their data needs change. This makes cloud storage a great option for businesses that need to store large amounts of data, but dont want to invest in expensive hardware or software.\nCloud storage is also highly flexible, as users can access their data from any device with an internet connection. This means that users can access their data from anywhere, at any time, making it ideal for businesses that need to access their data from multiple locations. Cloud storage is also highly scalable, meaning that it can grow with your business as your data needs increase. This makes it a great option for businesses that need to store large amounts of data, but dont want to invest in expensive hardware or software. Cloud storage is also highly secure, as it is hosted on secure servers and is protected by encryption. This means that only authorized users can access the data, and that the data is protected from unauthorized access. Cloud storage is also highly reliable, as it is hosted on multiple servers in different locations. This means that if one server fails, the data is still available from other servers. This makes cloud storage more reliable than traditional storage solutions, as it is less likely to suffer from data loss due to hardware failure. Types of cloud storage include public cloud storage, private cloud storage, hybrid cloud storage, and M Community-cloud) storage. Public cloud storage is a type of cloud storage that is hosted on a public cloud providers servers. Private cloud storage is a type of cloud storage that is hosted on a private cloud providers servers. Hybrid cloud storage is a type of cloud storage that combines public and private cloud storage.\nDear All, Today we are going to learn about RESTful API. In today's digital world, APIs have become an integral part of web development. RESTful APIs are a popular type of API that enable developers to access web services using standard HTTP protocols. In this article, we will explore what RESTful API is, how it works, and its benefits. What is RESTful API? REST stands for Representational State Transfer . It is a software architectural style that is used to create scalable web services. RESTful APIs are designed to enable communication between different systems using HTTP(S) protocols. RESTful APIs are based on the concept of resources, which can be any type of data or functionality. These resources can be accessed using a unique URI (Uniform Resource Identifier). How does RESTful API work? The main principle of RESTful API is to use HTTP methods like GET, POST, PUT, DELETE to interact with resources. Each method corresponds to a specific action, such as retrieving data, creating new data, updating existing data, or deleting data. When a client sends a request to a server, the server responds with a representation of the requested resource, usually in JSON or XML format. RESTful APIs are stateless, which means that the server does not keep track of the client's state. Each request is self-contained, and the client must provide all the necessary information for the server to complete the request. This makes RESTful APIs highly scalable and easy to maintain.\nBenefits of RESTful API One of the biggest advantages of RESTful APIs is their flexibility. They can be used to access any type of resource, from text to images to video. RESTful APIs are also platform-independent, which means that they can be used on any device that has access to the internet. RESTful APIs are also highly scalable. Because they are stateless, they can handle a large number of requests without consuming a lot of server resources. They are also easy to cache, which can improve performance. How RESTful API works in Huawei Storage Solutions RESTful API (Representational State Transfer Application Programming Interface) plays an important role in Huawei storage solutions. RESTful APIs are a standardized way for software applications to communicate with each other, enabling seamless integration between different systems. In Huawei storage solutions, RESTful APIs are used to provide programmatic access to the storage infrastructure, enabling IT professionals to automate common tasks and streamline management workflows. One of the primary roles of RESTful API in Huawei storage solutions is to provide access to the storage system's management interface. IT professionals can use RESTful APIs to create, modify, and delete storage resources, such as volumes, LUNs, and storage pools. This enables IT professionals to automate common management tasks and quickly provision storage resources to meet changing business needs. Another important role of RESTful API in Huawei storage solutions is to enable integration with third-party applications.\nRESTful APIs provide a standardized way for third-party applications to access the storage system's resources, making it easy to integrate Huawei storage solutions with other enterprise software solutions. This can include backup and recovery software, virtualization platforms, and cloud storage solutions. RESTful APIs can also be used to monitor the performance of Huawei storage solutions. IT professionals can use RESTful APIs to access performance data and monitor system health. This enables IT professionals to proactively identify and resolve performance issues before they impact business operations. Finally, RESTful APIs can be used to enable automation and orchestration of storage management workflows. IT professionals can use RESTful APIs to create scripts or workflows that automate common storage management tasks, such as provisioning storage resources, configuring data protection, and managing storage performance. This can help organizations reduce the time and effort required to manage their storage infrastructure, freeing up IT resources for other critical tasks. In summary, RESTful APIs play a critical role in Huawei storage solutions. By providing programmatic access to storage resources, enabling integration with third-party applications, and automating common management tasks, RESTful APIs help organizations streamline storage management and optimize storage performance. In conclusion, RESTful APIs are an essential part of modern web development. They provide a flexible, scalable, and platform-independent way to access web services. By using HTTP methods to interact with resources, RESTful APIs are easy to understand and maintain. As web applications continue to grow in complexity, RESTful APIs will continue to play a crucial role in enabling communication between different systems.\nComparison of software-defined storage vs. storage virtualization ( Software-defined storage) SDS vs. storage virtualization : SDS and storage virtualization are different sides of the same coin. Both abstract storage and simplify managing storage resources, but SDS is geared toward hardware independence, whereas storage virtualization is better for centralized management. The key difference between SDS vs. storage virtualization is how they abstract storage resources. SDS separates storage functions from the hardware, whereas storage virtualization separates capacity from hardware to create a storage pool. Software-defined storage: Storage management approaches ) SMP The rapid growth of unstructured data, scale-out storage expansion, virtualization of hardware devices and the growth of cloud-based storage have all supported SDS adoption. The SDS applications are available. There are also open-source products including Ceph, FreeNAS, and OpenStack Swift. SDS provides flexibility for managing storage resources, ease of administration, dynamic scalability, and enhanced feature automation. An Organization can manage just about any arrangement of storage devices using an SDS application. They can use older legacy storage devices to save on costs instead of buying new equipment. Centralized storage management through SDS can also use data deduplication, encryption, and compression technologies. SDS software does not have to be from the same vendor as the storage systems. However, some commodity hardware may not be compatible with certain SDS configurations, so to determine the compatibility. SDS can also be difficult to manage in large environments, and organizations may need more personnel to handle the different types of hardware.\nStorage virtualization centrally manages and aggregates multiple storage devices so that they appear to be in a pool of storage capacity. The storage pool can use standard configuration and architecture servers or VMs. Virtualized block- and file-based storage are two common implementations of virtualized storage . The former option is unique to NAS systems. By contrast, file-based storage uses various protocols , such as SMB, CIFS, and NFS. The mapping of storage resources is a key part of storage virtualization. The Organizations can use maps to locate stored data. To respond to read and write requests, the virtualization software refers to its map to locate the requested data or to store data on a specific device. Storage virtualization was considered somewhat challenging to implement and manage because. It is originally run on a host system and other devices used in the storage pool. That has changed as updated and enhanced systems, which are much more flexible and adaptable to user requirements - become available. Like SDS, important benefits of storage virtualization include ease of administration, scalability, efficient storage utilization, reusing older legacy systems, and deploying features such as caching and replication across the storage pool. The main limitation of storage virtualization is the chance of vendor lock-in, although product improvements have minimized lock-in issues. Comparing SDS and storage-virtualization Both approaches to storage management abstract storage management activities from hardware platforms. Each provides increased flexibility of feature use and more independence from specific storage vendors, devices, and the infrastructures.\nBrief History of Docker Docker was invented by Solomon Hykes as a solution to the challenges of packaging and deploying applications on different infrastructures and it quickly became popular among developers and organizations for its benefits and ease of use. key concepts and features of Docker: Overall, Docker provides a flexible and powerful platform for building, deploying, and running containerized applications. The ability to package an application and its dependencies in a single container, and easily deploy it on any infrastructure, makes it a popular choice for developers and DevOps engineers. Images: An image is a snapshot of an application and its dependencies that can be used to create new containers. Images are created using a Dockerfile, which is a script that specifies the steps required to build the image. Containers: A container is a running instance of an image. You can start, stop, and delete containers as needed. Each container has its own file system, network interfaces, and process space, and is isolated from other containers. Docker Hub: Docker Hub is a public registry where you can find and share images. It allows you to easily find images for popular applications and services, and also allows you to share your own images. Volumes: Volumes are a way to store data outside of the container's file system. This can be useful for storing data that you want to persist between container restarts, or for sharing data between multiple containers.\nNetworks: Docker allows you to create and manage networks, which allow containers to communicate with each other and with the host machine. You can use the built-in bridge, host, and overlay networks, or create your own user-defined networks. Docker Compose: Docker Compose is a tool that allows you to define and run multi-container applications using a simple YAML file. It makes it easy to start, stop, and manage all the containers in your application as a single unit. Docker Swarm: Docker Swarm is a native clustering solution for Docker that allows you to turn a group of Docker hosts into a single virtual host. It allows you to easily scale your applications, and also provides built-in load balancing and service discovery. Kubernetes: Kubernetes is a container orchestration platform that can be used to manage and scale containerized applications. It allows you to automate the deployment, scaling, and management of containers. several reasons why developers and organizations use Docker Overall, Docker provides a number of benefits for developers and organizations, including isolation, portability, scalability, efficiency, and ease of use. These benefits can help you save time and money, and make it easier to build, deploy, and manage applications. how to use docker on your own machine you will need to first install the Docker engine, which is the software that runs the containers. Here are the basic steps to get started: Download and install Docker: Go to the Docker website (https://www.docker.com/) and download the installer for your operating system.\nRun the installer and follow the prompts to install Docker. Make sure to install the latest version of Docker. Start the Docker daemon: Once the installation is complete, start the Docker daemon by running the following command: Copy code dockerd Verify that Docker is running: Verify that the Docker daemon is running by running the following command: Copy code docker info You should see information about the Docker environment, including the number of containers and images. Run a container: To run a container, you can use the docker run command. For example, to run the hello-world container, you would run the following command: Copy code docker run hello-world Stop and remove container: To stop a running container, you can use the docker stop command followed by the container ID. To remove a container, you can use the docker rm command followed by the container ID. Manage images: To list the images on your system, you can use the docker images command. To download an image from a registry, you can use the docker pull command followed by the image name. Access container: To access a running container and run command inside it, you can use the docker exec command followed by the container ID and command you want to run. These are the basic steps to get started with using Docker on your own machine. You can find more detailed information about using Docker in the official Docker documentation at https://docs.docker.com/.\nWhat is Redirect-on-write (ROW)? Redirect-on-write (ROW) is a technique used in computer storage systems, such as file systems or virtual disks, to optimize the performance of write operations. In a system that uses ROW, when a file or disk block is modified, instead of overwriting the original data in place, the system first makes a copy of the original data and writes the modified data to a new location. The new data is then redirected to the new location, and the old data is marked as free space. This way, the original data remains intact until it is no longer needed, and the new data can be accessed more quickly. The advantage of using ROW is that it can improve the performance of write operations by reducing the amount of data that needs to be moved and written. It also helps to prevent data loss or corruption, as the original data is preserved until the new data is fully written and verified. Working Principle: ROW is commonly used in copy-on-write (COW) file systems and virtual disk systems, such as those used in virtualization technologies like VMware and Hyper-V. The working principle of Redirect-on-Write (ROW) involves copying the data that needs to be modified to a new location before modifying it. This process involves the following steps: Redirection on Write: Step 1 The storage system receives a write request to modify a block of data in a file or a virtual disk.\nStep 2 Instead of overwriting the original data in place, the system creates a new copy of the data in a different location in the storage device. Step 3 The system then redirects the write request to the new copy of the data, so that all subsequent write requests will modify the new copy. After Write: Step 1 The system then updates any pointers or metadata that reference the original data to point to the new copy of the data. Step 2 The original data is marked as free space, which can be used for new write requests in the future. Advantages: The primary advantage of ROW is that it avoids the need to read and copy the original data before modifying it, which can improve the performance of write operations. Additionally, ROW can help to prevent data corruption or loss in case of a power failure or other system failure during the write operation, as the original data remains intact until the new copy has been fully written and verified. However, ROW may also result in increased storage usage due to the creation of additional copies of data. Redirect-on-Write (ROW) can offer several advantages, including: Reduced risk of data loss or corruption: Because ROW involves copying the data that needs to be modified to a new location before modifying it, the original data remains intact until the new copy is fully written and verified.\nThis can reduce the risk of data loss or corruption in case of a power failure or other system failure during the write operation. Improved performance of write operations: ROW can improve the performance of write operations by reducing the amount of data that needs to be moved and written. This can be especially significant in storage systems that frequently modify data or use small block sizes. Simplified data recovery : Because the original data is preserved until the new copy is fully written and verified, it can be easier to recover data in case of a data corruption or other data loss event. Reduced need for garbage collection : Because ROW avoids overwriting data in place, it can reduce the amount of garbage collection needed to maintain free space in the storage device. This can help to improve the performance and lifespan of solid-state drives (SSDs) or other flash-based storage devices. Improved storage utilization : Because ROW avoids overwriting data in place, it can reduce the risk of data fragmentation and enable more efficient use of storage space. Disadvantage: While Redirect-on-Write (ROW) can offer several advantages, such as improving the performance of write operations and reducing the risk of data loss or corruption, it also has some potential disadvantages, which include: Increased storage usage : ROW requires creating additional copies of data before modifying it, which can increase the amount of storage space required. This can be especially significant in storage systems that use small block sizes or that frequently modify data.\nIncreased write amplification : Because ROW requires copying data to a new location, it can increase the number of write operations needed to modify a file or a virtual disk. This can result in increased write amplification, which can reduce the performance and lifespan of solid-state drives (SSDs) or other flash-based storage devices. Reduced read performance: Because data is copied to a new location before it is modified, read operations may be slower because the system needs to determine which version of the data to access. This can be especially true in systems that use a large number of snapshots or versions of data. Increased complexity : ROW can be a complex technique to implement and manage, especially in systems that use complex data structures or that require fine-grained control over data access. This complexity can make it more difficult to troubleshoot issues or optimize performance. Conclusion: In conclusion, Redirect-on-Write (ROW) is a technique used in computer storage systems to optimize the performance of write operations while reducing the risk of data loss or corruption. ROW works by copying data to a new location before modifying it, which enables the original data to remain intact until the new copy is fully written and verified. While ROW can offer several advantages, such as improving the performance of write operations and simplifying data recovery, it also has some potential drawbacks, such as increased storage usage and write amplification.\nThis post shows you how to generate the CSR and for which certificate scenario and also how to import it. Check it out! Need to configure an Object S3 compatible repository on a Veeam Backup & Replication server and for this you needto add an SSL certificate signed from enterprise PKI for this usage. Do you need help to know how to generate the CSR and for which certificate scenario and also how to import it? For this specific scenario, to use S3 Compatible Storage with Veeam, and if the requirement is to use own certificate signed by the Enterprise PKI, then you should create a CSR file with OpenSSL for Object storage service security certificate. Details about certificates:https://support.huawei.com/hedex/hdx.do?docid=EDOC1100251369&id=safe_conifg_000013&lang=en Q1. Do I need to connect with SSH protocol on one node off the array to get the CSR? A1. You do not need to login to any node to use OpenSSL, you can use any Linuxhost and executeOpenSSLcommand to generate the CSR file. Q2. How can I get the CSR file generated? A2. You can use the FTP tool to access the Linux host and retrieve the CSR from the directory where was saved. Q3. Once the CSR was signed by our dedicated service, how can I upload the certificate on the Pacific array? A3. Once the certificate is generated by the CA, you can use DeviceManager to import the certificate. For importing the certificate use: CSR creation for a specific certificate scenario.\nHi everyone, I recommend you a perfect app for wifi device called Huawei Hilink, I strongly recommend to download it. It shows you full details of your wifi. You can reset password and change name password. You can see the charge of wifi battery and your data usage in app. And there are so many other features. Cool . Love it . Please add data usage more details like how much data is used on youtube, how much data is used on chrome, how much data is used on pubg, and how much data is used on play store. It will be the most perfect app then.\nThe basic difference between OLTP and OLAP is that OLTP is an online database modifying system, whereas, OLAP is an online database query answering system. There are some other differences between OLTP and OLAP which I have explained using the comparison chart shown below. BASIS FOR COMPARISON OLTP OLAP OLTP is an Online Transaction Processing system . The main focus of OLTP system is to record the current Update, Insertion and Deletion while transaction. The OLTP queries are simpler and short and hence require less time in processing , and also requires less space . OLTP database gets updated frequently . It may happen that a transaction in OLTP fails in middle, which may effect data integrity . So, it has to take special care of data integrity. OLTP database has normalized tables (3NF). ATM OLAP is an Online Analytical Processing system . OLAP database stores historical data that has been inputted by OLTP. It allows a user to view different summaries of multi-dimensional data. Using OLAP, you can extract information from a large database and analyze it for decision making. OLAP also allow a user to execute complex queries to extract multidimensional data. In OLTP even if the transaction fails in middle it will not harm data integrity as the user use OLAP system to retrieve data from a large database to analyze. Simply the user can fire the query again and extract the data for analysis.\ncombines servers and storage into a distributed infrastructure platform that uses intelligent software to create flexible building blocks that replace traditional infrastructure consisting of separate servers, storage networks, and storage arrays. More specifically, it combines commodity data center server hardware with locally attached storage (spinning disk or flash) and is supported by a distributed software layer to eliminate common pain points associated with legacy infrastructure. HCI converges the entire data center stack, including computing, storage, storage networking, and virtualization. More specifically, it combines commodity data center server hardware with locally attached storage (spinning disk or flash) and is supported by a distributed software layer to eliminate common pain points associated with legacy infrastructure. Complex and expensive legacy infrastructures have been replaced by distributed platforms running on industry-standard commodity servers, enabling enterprises to fine-tune their workloads and scale flexibly as needed. Each node includes processors with SSDs and HDDs. Software running on each node distributes all operational functions across the cluster for superior performance and resiliency. Hardware platform configurations can accommodate any workload by independently scaling various resources (CPU, RAM, and storage) and can be configured with or without GPUs for graphics acceleration. All nodes include flash memory to optimize storage performance, and all-flash nodes can be used to provide maximum I/O throughput and minimum latency for all enterprise applications. In addition to the distributed storage and compute platform, the HCI solution includes a management pane that enables you to easily manage HCI resources from a single interface.\nThis eliminates the need for separate management solutions for servers, storage, storage networking, and virtualization. Here's a brief overview of how HCI works: Hardware : HCI systems typically consist of a cluster of x86 servers, each with its own processor, memory, and storage. These servers are interconnected with each other through a high-speed network, which provides the communication channels for the HCI system. Software : The HCI software layer is the key component that integrates the hardware resources and presents them as a unified system. This layer is typically based on a hypervisor that runs on each server and manages the virtual machines (VMs) and their associated storage and networking resources. Virtualization : HCI uses virtualization to abstract the hardware resources and present them as virtualized resources that can be allocated dynamically to VMs. This means that the storage, computing, and networking resources can be allocated and adjusted as needed to support the workload requirements. Management : The HCI software layer provides a centralized management interface that allows administrators to manage the entire HCI system from a single console. This interface allows administrators to monitor and configure the storage, computing, and networking resources, as well as the VMs that run on the system. Scaling : HCI systems can be easily scaled by adding additional x86 servers to the cluster. The HCI software layer automatically distributes the workload across the new servers and integrates them into the existing system. This allows organizations to scale their data center resources as needed to meet changing business requirements.\nSince the 1990s, data center infrastructures have been designed around SAN storage to protect data and power critical databases, and became common with the explosion of virtualization in the early 00s. But as organizations become more dependent on technology, traditional SAN-based infrastructures can no longer meet IT needs. It is complex, bulky, and cannot scale as flexibly or efficiently as IT teams need to keep up with changing business priorities. The world's largest networking companies faced the limitations of traditional infrastructure long before the broader market emerged and developed distributed system technologies to address their challenges of scalability, reliability, and operational efficiency. In 2009, engineers from several of these network-sized companies realized that the technology they developed to solve their own operational challenges was applicable to the entire market. Bringing these technologies to the reality of enterprise computing required a new approach, and the concept of HCI was born. Today, HCI is the infrastructure of choice for companies that want to remain competitive and evolve with the changing realities of the technology world. Organizations are increasingly leveraging public cloud services such as Amazon Web Services (AWS), Microsoft Azure, , and Google Cloud to deploy IT applications to run their business. Public cloud services are flexible and dynamic, enabling organizations to dynamically adapt to changing business needs. But despite the increased agility, cloud computing has its own challenges. Building and deploying applications in the public cloud requires a different mix of expertise than traditional IT teams, increasing the degree of specialization of an already highly siloed organization.\nIn addition, leveraging public cloud resources is more expensive than on-premises infrastructure and presents control and security challenges. are powered by many of the same distributed systems technologies as public clouds, enabling IT organizations to build private clouds that bring the benefits of cloud computing into the organization's data center. Hyperconverged infrastructure services can also be extended into the public cloud, enabling a true hybrid cloud infrastructure, enabling applications to be deployed and managed using the same tools and procedures, while easily migrating data and services across clouds. There are many benefits to the simplicity of moving from complex legacy infrastructure to hyperconverged, but the main reasons organizations are making the transition include lower costs, more stable performance, a smaller data center footprint, more efficient and productive IT teams, and maximizing ROI on infrastructure. Turnkey infrastructure - Integrates server, storage, network, and virtualized resources, as well as end-to-end systems management and operations management functions. Rapid deployment - Deploy infrastructure in minutes so IT teams can focus on the applications and services that power the business. 100% software-driven - supporting a wide range of different hardware platforms - including three of the world's four most popular server platforms. Freedom of choice - Use built-in virtualization or use your preferred hypervisor and deploy on a wide selection of server vendors or purchase pre-integrated appliances from OEMs. Superior performance and resiliency - HCI software running on each node distributes all operational functions to the entire cluster.\nMicro Electro Mechanical Systems (MEMS) are miniaturized devices that combine mechanical and electrical components on a single chip. They are fabricated using microfabrication techniques similar to those used for integrated circuits. MEMS can be used for sensing, actuating, or processing information in various applications such as navigation, communication, biomedical, and automotive. Some of the advantages of MEMS are their small size, low power consumption, high performance, and integration capability. Micro Electromechanical Systems (MEMS) is a technology that involves the fabrication of tiny mechanical and electromechanical devices that are integrated with electronics. MEMS devices typically have a size range of a few micrometers to several millimeters and can perform a wide range of functions, from sensing and actuation to data processing and communication. Some common examples of MEMS devices include accelerometers, gyroscopes, pressure sensors, microphones, and microfluidic devices. These devices are typically made using microfabrication techniques such as photolithography, etching, and deposition, which allow for precise control over the device dimensions and properties. The main advantage of MEMS technology is its ability to combine mechanical and electronic functionality in a single device, leading to a variety of benefits such as: Miniaturization: MEMS devices can be made very small and lightweight, making them ideal for use in portable and wearable electronics. Low power consumption: MEMS devices typically require very little power to operate, making them well-suited for battery-powered applications. High sensitivity and accuracy: MEMS sensors and actuators can be designed to have high sensitivity and accuracy, making them ideal for use in measurement and control systems.\nDynamic Resiliency Engine (DRE) is a new technology used to improve the reliability and data protection of enterprise-class storage systems. It is an alternative to traditional RAID-based data protection schemes, and it offers faster rebuild speeds and increased bandwidth by using a different layout or approach. The DRE technology allows an enterprise-class storage system to sustain a higher number of drive failures and still maintain data integrity. The DRE technology is based on the principle of Dynamic Resiliency, which allows an enterprise-class storage system to sustain a higher number of drive failures and still maintain data integrity. DRE allows a storage system to recover quickly from multiple drive failures and maintain the highest possible level of data protection. Traditional data protection methods use RAID groups with fixed layouts to safeguard data stored on a volume. These methods have limitations in terms of bandwidth and rebuild speed, which are determined by the number of drives that participate in the group. Additionally, the RAID level chosen will also impact the speed of rebuilds in the event of drive failures. For example, a RAID 6+2 configuration can provide fast read speeds with the use of 6 drives, but can only sustain the rebuild speed of 2 drives in the event of a dual drive failure.\nData consistency and reliability are dependent on the following factors: Bit Error Rate Number of drivers Data volume to be rebuilt As we increase the number of drives and data volume, it becomes difficult to manage and maintain data integrity in case of disk failure especially if we are using traditional RAID schemes. The Dynamic Resiliency Engine (DRE) is a software-based solution for data redundancy that is much more distributed, automated, and efficient than traditional RAID methods. It provides the same level of data protection as RAID 6 and/or RAID 5, but with increased resiliency and at a lower cost. System utilizes algorithms to partition every drive into multiple virtual chunks, and then uses these chunks across multiple drives to create redundancy extents. This approach allows for a more efficient and cost-effective way to protect data. The Dynamic Resiliency Engine (DRE) automatically consumes the drives within a storage appliance and creates the necessary redundancy using all the drives. This process improves overall performance and allows for scalability as more drives are added to the appliance. With DRE, data written to a volume can be distributed across any number of drives within the appliance, and as new drives are added, the data is automatically reallocated for optimal performance. Unlike traditional RAID protection methods, DRE technology does not require dedicated spare drives. Instead, spare capacity is distributed across the entire storage appliance, and a small amount of space is set aside on each drive for use in case of a drive failure.\nA single drive's worth of spare space is reserved for every resiliency set within the appliance. When a drive fails, only the portion of the drive that contains data is rebuilt, which allows for more efficient management of spare capacity and shorter rebuild times as only the data that was written to the drive needs to be rebuilt. The DRE-enabled storage appliance can tolerate multiple drive failures, even within the same resiliency set, as long as the failures occur at different times (i.e. a second drive fails after the rebuild of the first failed drive is complete). During the initial configuration of an appliance, single drive failure tolerance for a 25-drive resiliency set or a double drive failure tolerance for a 50-drive resiliency set can be decided. When deploying a multi-appliance cluster, you can mix different drive failure tolerances across appliances. For example, Appliance A could be set to single drive failure tolerance while Appliance B is set to double drive failure tolerance. As the storage system grows over time, the DRE dynamically increases resiliency sets. For instance, an appliance that has been configured for single-drive-fault-tolerance, meaning that the system is set up with a 25-drive resiliency set, will split into two sets when a 26th drive is added. Additionally, resiliency sets can expand across physical enclosures and include a mix of drive sizes, depending on the number of drives in the appliance. The Dynamic Resiliency Engine (DRE) technology improves rebuild times by distributing sparing across multiple drives within the appliance simultaneously.\nData was stored on storage racks using the LTO (Linear Tape-Open) technique in the 1990s. The tapes needed to be retrieved and mounted onto a tape reading device in order to access the data. Because it is affordable and secure, this method is still occasionally employed today. The majority of businesses cannot scale it since it is slow. Today, HDD (Hard Disk Drives), which are made to accept and store massive amounts of data at cheaper costs, are frequently used for cold storage. The ideal choice for your business will rely on your particular data storage requirements and corporate standards. Businesses must legally store data they no longer need as they gather more and more data. They avoid wasting resources and jeopardizing the accessibility of more crucial data by doing this. It's obvious that a corporation needs a cold storage solution the more data it generates. For instance, a company stores all of its data in the cloud. At the top of their resource allocation is hot storage. Real-time analytics and financial transactions are among its components. The business employs a cold storage system to reduce unused costs. They move the unnecessary data to cold storage devices so that cloud computing resources aren't used and the data recovery process is slowed down. However, certain sectors of the economy need more varied cold storage solutions than others. For instance, the healthcare sector generates a huge volume of patient data. This information consists of data sets on imaging, medical histories, and vaccinations.\nWhich storage option should you choose? NAS is used to provide centralized storage for data that distributed teams, such as remote workers, need to access, and can act as a private cloud to give multiple users access to data on multiple hard drives. It is also used in larger enterprises as a backup target using NAS arrays, and for small virtualization environments for VMs. NAS is dedicated file storage that enables multiple users and heterogeneous client devices to retrieve data from a centralized disk capacity. Users on a (LAN) access the shared storage via an Ethernet connection. Each NAS resides on the LAN as an independent network node, defined by its unique IP address. The devices consolidate storage in one place and support a cloud tier and tasks, such as archiving and backup. The Network attached storage and storage area networks. SANs are the two main types of networked storage. The NAS handles unstructured data like audio, video, websites, text files, and Microsoft Office documents. The SANs are designed primarily for block storage inside databases and are also known as structured data. Blocked storage for enterprise applications. NETWORK-ATTACHED (NAS) STORAGE USED FOR? The purpose of NAS is to enable users to collaborate and share data more effectively. It is useful to distribute teams that need remote access or work in different time zones. NAS connects to a wireless router, making it easy for distributed workers to access files from any desktop or mobile device with a network connection.\nOrganizations commonly deploy NAS environment as a storage filer or the foundation for a personal or private cloud. Some NAS products are designed for use in large enterprises. Enterprise NAS gear is designed with more high-end data features to aid storage management and usually comes with at least four drive bays. A (cloud network attached storage) Direct-attached storage (DAS) Cluster. Enterprises had to configure and manage hundreds or even thousands of file servers. To expand storage capacity, NAS appliances are outfitted with more or larger disks; this is known as scale-up NAS. Appliances are also clustered together for scale-out storage. In addition, most NAS vendors partner with cloud storage providers to give customers the flexibility of redundant backup. Collaboration is a virtue of network-attached storage, it can also be problematic. Network-attached storage relies on hard disk drives (HDDs) to serve data. I/O contention can occur when too many users overwhelm the system with requests simultaneously. Newer systems use faster solid-state drives (SSDs) or flash storage as a tier alongside HDDs or in all-flash configurations. NAS USE CASES AND EXAMPLES: The type of HDD selected for a NAS device will depend on the applications used. Sharing Excel spreadsheets or Word documents with co-workers is a routine task, as is performing periodic data backups. the using of NAS to handle large volumes of streaming media files requires larger capacity disks, memory, and powerful network processing. Users rely on network-attached storage to do the following: manage security systems and security updates. manage consumer-based IoT components.\nIn storage LLD (detailed design) is based on the determined target network architecture, clearly designing the scope of the network and functional deployment plan; the specific design content was paid attention by the customers within the scope of the contract, including clarifying the interactive process of the important business, the function allocation of all services on each network element, the peripheral docking relationship of the design network element, the docking requirements and compliance with the norms, the quantitative calculation of docking resources. A- True B- False The Correct Answer is this Question is A- True In storage LLD (detailed design), the detailed design of the network is based on the determined target network architecture. It involves clearly defining the scope of the network and functional deployment plan, which includes essential details such as the interactive process of the business, function allocation of services on each network element, peripheral docking relationship of the design network element, docking requirements, and compliance with norms, and quantitative calculation of docking resources. These design contents are typically paid attention to by customers within the scope of the contract. Therefore, option A is correct and true. In storage You can create an LLD by using one of the following methods: In Create LLD Without a Template , click Customize Devices to Create LLD . In Create LLD Without a Template , click Import BOQ to Create LLD . In Create LLD Using a Template , click Active-Active Data Center and customize a template or choose an existing template. A- True.\nRAID stands for Redundant Array of Inexpensive Disks ,There are two possible RAID methods: Hardware RAID and Software RAID. Depending on how your RAID is configured, it can increase your computers speed while giving you a single drive with a huge capacity. RAIDs can also increase reliability. There are several RAID levels each one optimized for a specific situation like total storage size redundancy and mirroring data : RAID Level 0 striping RAID 1 mirroring RAID 5 striping with parity RAID 6 striping with double parity RAID Level 10 combining mirroring and striping Raid is kind of a method for improving the performance and reliability of your storage media by using multiple drives. So that the data is either divided between disks to distribute load, or duplicated to ensure that it can be recovered once a disk fails. Hardware RAID was the initial type of RAID available, where a specially built RAID controller handles the drives so that the processes are almost transparent to the host computer. So Hardware RAID presents logical disks that are already configured to the system (or the SAN), mirrored and ready to go. Also Configuration is still required, but that configuration takes place outside the system Under software RAID, that configuration is performed in the system. And operation is isolated from the host computers resources. Software RAID is a newer type of RAID and its cheaper than hardware RAID, where no specialized hardware is needed, and the host computer is responsible for the drives.\nHello Everyone, What is HDPA Storage in Huawei and how does it works? Regards FA Hi, HDPA (Huawei Dynamic Power Allocation) is a technology developed by Huawei for its storage systems that optimizes power usage and efficiency. It works by dynamically allocating power to different components of the storage system based on their workload and utilization. In a traditional storage system, power is allocated to components such as hard disk drives (HDDs) and fans based on their maximum power consumption. This can lead to inefficiencies, as some components may not be fully utilized, while others may be overburdened and consume more power than necessary. With HDPA, power is allocated based on real-time workload and utilization data. This allows the storage system to adjust power consumption dynamically, ensuring that each component receives the appropriate amount of power based on its workload. For example, if a particular HDD is not being heavily utilized, its power consumption can be reduced to save energy. Conversely, if a particular HDD is under heavy load, it can be allocated more power to ensure that it operates at maximum efficiency. HDPA also includes features such as power capping, which allows administrators to set maximum power limits for the storage system. This can help to prevent power spikes and ensure that the storage system operates within its power budget. Overall, HDPA is designed to improve power efficiency and reduce energy costs for Huawei storage systems.\nWhat are the features of DME? What kind of logs can be monitor into it? Do we monitor users into this device as well? What is it purpose? Hello, dear. DME Data is the unified management software integrating software and hardware in data centers. It controls, manages, and analyzes VMs, datastores, servers, switches, and storage devices. Based on Huawei's unified intelligent management platform, DME Data provides real-time O&M and closed loop problem solving based on alarms, a wide rang of report analysis capabilities based on unified datasets, and automatic O&M capabilities based on policies and AI. In the southbound and northbound ecosystem, DME Data can connect to customers' existing O&M platforms and various cloud management platforms through protocols such as RESTful, SNMP, Telnet, and Redfish. In addition, DME Data supports typical third-party devices, including servers, switches, and storage devices, through the open standard interfaces in the industry. In terms of service scenarios, DME Data provides the following benefits: DME Data provides a unified management interface for virtual inventories, preventing administrators from switching between multiple software interfaces. In addition, it provides a simple and intuitive view to quickly learn about the health status and performance change trend of virtual inventories such as VMs and datastores, facilitating virtual resource management. DME Data provides automatic lifecycle management, which frees administrators from tedious work, avoids human errors, and enables them to focus on problem resolution and experience accumulation. For example, DME Data can automatically execute repeated work based on a customized template.\nWhat is installation procedure of huawei storages? Are they different from one another? The installation procedure for Huawei storage systems may vary depending on the specific model and configuration of the system. However, there are some general steps that are typically involved in the installation process: 1. Site preparation: Before installing the storage system, you need to prepare the site and ensure that it meets the necessary requirements. This may include providing adequate power and cooling, ensuring proper ventilation, and installing any necessary mounting hardware. 2. Unpacking and inspection: Once you have prepared the site, you can unpack the storage system and inspect it for any damage or missing components. 3. Hardware installation: The next step is to install the hardware components of the storage system, including any disk shelves, controllers, and network connections. This may involve connecting cables and power cords, installing modules or cards, and mounting hardware in racks or cabinets. 4. Initial configuration: After the hardware is installed, you need to perform the initial configuration of the storage system. This may involve setting up network settings, configuring storage pools and volumes, and setting up any additional features or services. 5. Testing and verification: Once the system is configured, you should test it to ensure that it is working properly and that all components are communicating correctly. 6. Integration and data migration: After the system is verified, you may need to integrate it with other systems or applications and migrate any data to the new storage system.\nThere are six types of data migration: Storage migration is where an organization moves data from one physical storage location to another Application migration is called for when a business changes software applications or vendors, which requires the data to be moved to a new computing environment Business process migration occurs when business applications and their associated data are moving to a new environment; this is often driven by a company reorganization, merger, or acquisition Data center migration involves existing infrastructure and the data it holds being moved to a new location, or the data being moved onto new infrastructure In an ideal world, migrating data to a new platform, location, or architecture may be accomplished with little to no downtime, no data loss, and little to no human data modification or re-creation. In complicated migrations involving large datasets, the ETL process of extracting the data, transforming it, and then loading the data can be extremely useful. There are three phases to every data migration project:- 1. Planning is the most important part of any data migration effort. Key considerations when developing your data migration strategy include data sources and destinations, security, and cost. 2. Migration is the active stage of the process, when data is moved from the source to the destination. 3. Post-migration is the last step, when you check to confirm that the migration was executed correctly. When creating your data migration plan, you can consider either the big bang data migration approach or trickle data migration approach: Big bang data migration .\nThis method moves all the migrated data in one effort. Advantages of this approach include lower cost, a quicker move, and less complexity. However, this type of data migration requires all systems to be offline for the duration of the migration. Additionally, big bang data migration carries a failure risk that increases in relation to the amount of migrated data. The combination of these two factors makes the big bang approach best for smaller businesses with less data to move and the ability to be fully offline during the migration. Trickle data migration . This method migrates data in incremental phases, with the old and new systems running in parallel until the entire migration process is complete. Advantages of this approach include a reduced risk of error or failures, and no requirement for system-wide downtime. However, due to its piecemeal nature, this type of data migration strategy is more complicated and requires more time to plan and execute. Because big businesses with large amounts of data typically cant afford downtime but do have the resources for a complex process trickle data migration is likely the best path for them to pursue. Companies have the option of creating their own data migration scripts or using pre-made on-premises or cloud-based technologies. Self-scripted data migration is a homegrown DIY method that may work for small applications but struggles to scale. If all of the data storage is located within a single site, on-premises tools perform effectively.\nWhat is LDAP? LDAP (Lightweight Directory Access Protocol) - is an application protocol used over IP network to manage and access the distributed directory information service. The primary purpose of a directory service is to provide a systematic set of records, usually organized in a hierarchical structure. Overview of LDAP and Role of a Specialized Server: In order to commence an LDAP session, a client needs to connect to the server known as the Directory System Agent, which is set by default to use TCP port 389. After the connection is established, the client and server exchange packets of data. Basic encoding rules are used to transfer information between the server and the client. Structure of LDAP: Although the structure of LDAP seems relatively complex, it is fairly simple to understand. The server is capable of holding a sub-tree and its children, beginning from a particular entry. Operations on LDAP: There arte plethora of operations that can be performed on theLightweight Directory Access Protocol. Here are the most prominent ones. Add - this is used to insert a new entry into the directory-to-server database. If the name entered by a user already exists, the server fails to add a duplicate entry and instead shows an 'entryAlreadyExists\" message. - on connection with the LDAP server, the default authentication state of the session is anonymous. There are basically two types of LDAP authentication methods - the and the . Delete - as the name suggests, this operation is used to delete an entry from the directory.\nData migration is the process of transferring data from one storage system or computing environment to another. Your company may need to start a data migration project for a variety of reasons. For instance, you might be consolidating or decommissioning a data center, replacing servers or storage devices, etc. In the whole process of transferring on-premises IT infrastructure to a cloud computing environment, data migration is also a crucial stage. Finding a safe, affordable, and effective way to transfer your data to its new storage place is important whether you're transitioning to a public cloud, private cloud, hybrid cloud, or multi cloud environment. To ensure that your project goes smoothly, adhere to the following best practices: - Understand the data and what its used for : Who currently utilizes the data, who will in the future, and how will they use it? For instance, the storage and formatting requirements for data used for analytics may be significantly different from those for data kept for regulatory compliance. Throughout the relocation process, be sure to collect data from all pertinent stakeholders and business units. - Assess the source and target environments carefully : Will both environments run the same operating system? Would other formatting or database schemas need to change? Are there any difficulties that need to be resolved before the migration (such as redundancy concerns or an abundance of \"dirty\" data)? - Verify business requirements and potential impact early in the process : Which kind of migration schedule is required?\nWhen will a data center's lease expire if it is being decommissioned? How much data security must you uphold during the migrating process? What level of data loss or corruption is acceptable, if any? How might the company be impacted by delays or unforeseen obstacles? Though the benefits of modernizing IT systems outweigh the risks associated with data migrationespecially over the long termdata migration can be stressful and risky. Here are some of the risks to account for: - Security : Before migrating, make sure all data is safely encrypted. Verify the security of the shipper's freight and logistics services for offline migrations that require shipping data storage devices. - Long transfer times : Online transfer times can be difficult to forecast with absolute precision. System hardware constraints or network bottlenecks may limit the quantity of data that can be read from or written to them, or both. - Higher-than-expected costs : Planning mistakes frequently lead to unanticipated expenses. For instance, there will be extra fees for internet transfers that take longer than intended. Additional fees may also apply if you maintain a vendor-provided storage equipment longer than you originally committed to (this could happen as a result of data transfers or shipping delays). The successful and secure movement of data to a different application, storage system, or cloud is ensured by data migration. Although migrating data from one platform to another can be risky and expensive, there are several advantages for an organization.\nHere, are the important landmarks from the history of DBMS: 1960 Charles Bachman designed the first DBMS system 1970 Codd introduced IBMS Information Management System (IMS) 1976- Peter Chen coined and defined the Entity-relationship model, also known as the ER model 1980 Relational Model becomes a widely accepted database component 1985- Object-oriented DBMS develops. 1990s- Incorporation of object-orientation in relational DBMS. 1991- Microsoft ships MS access, a personal DBMS, and that displaces all other personal DBMS products. 1995: First Internet database applications 1997: XML applied to database processing. Many vendors begin to integrate XML into DBMS products. The main Four Types of Database Management Systems are: Hierarchical DBMS In a Hierarchical database, model data is organized in a tree-like structure. Data is Stored Hierarchically (top-down or bottom-up) format. Data is represented using a parent-child relationship. In Hierarchical DBMS, parents may have many children, but children have only one parent. Network Model The network database model allows each child to have multiple parents. It helps you to address the need to model more complex relationships like the orders/parts many-to-many relationship. In this model, entities are organized in a graph which can be accessed through several paths. Relational Model Relational DBMS is the most widely used DBMS model because it is one of the easiest. This model is based on normalizing data in the rows and columns of the tables. Relational model stored in fixed structures and manipulated using SQL. Object-Oriented Model In the Object-oriented Model data is stored in the form of objects.\nThe structure is called classes which display data within it. It is one of the components of DBMS that defines a database as a collection of objects that stores both data members values and operations. It uses a digital repository established on a server to store and manage the information. It can provide a clear and logical view of the process that manipulates data. DBMS contains automatic backup and recovery procedures. It contains ACID properties which maintain data in a healthy state in case of failure. It can reduce the complex relationship between data. It is used to support manipulation and processing of data. It is used to provide security of data. It can view the database from different viewpoints according to the requirements of the user. Controls database redundancy : It can control data redundancy because it stores all the data in one single database file and that recorded data is placed in the database. Data sharing : In DBMS, the authorized users of an organization can share the data among multiple users. Easily Maintenance : It can be easily maintainable due to the centralized nature of the database system. Reduce time : It reduces development time and maintenance need. Backup : It provides backup and recovery subsystems which create automatic backup of data from hardware and software failures and restores the data if required.\nRESTful API is an interface that two computer systems use to exchange information securely over the internet. Most business applications have to communicate with other internal and third-party applications to perform various tasks. For example, to generate monthly payslips, your internal accounts system has to share data with your customer's banking system to automate invoicing and communicate with an internal timesheet application. RESTful APIs support this information exchange because they follow secure, reliable, and efficient software communication standards. RESTful APIs include the following benefits: Scalability Systems that implement REST APIs can scale efficiently because REST optimizes client-server interactions. Statelessness removes server load because the server does not have to retain past client request information. Well-managed caching partially or completely eliminates some client-server interactions. All these features support scalability without causing communication bottlenecks that reduce performance. Flexibility RESTful web services support total client-server separation. They simplify and decouple various server components so that each part can evolve independently. Platform or technology changes at the server application do not affect the client application. The ability to layer application functions increases flexibility even further. For example, developers can make changes to the database layer without rewriting the application logic. Independence REST APIs are independent of the technology used. You can write both client and server applications in various programming languages without affecting the API design. You can also change the underlying technology on either side without affecting the communication. Before REST, developers integrated APIs via SOAP.\nDevelopers manually created an XML document with a Remote Procedure Call (RPC) in the body to place a call. After that, they POST their SOAP envelope to the endpoint after specifying the endpoint. Roy Fielding and a team of programmers decided to establish a standard in 2000 so that any server could communicate with every other server. In his 2000 doctoral dissertation at the University of California, Irvine, he defined REST and the architectural limitations mentioned above. Software integration is made simpler for developers by these uniform guidelines. In 2000, Salesforce became the first business to offer an API as a component of its \"Internet as a Service\" offering. When eBay created a REST API, it opened up its market to any website that could access its API, but few developers were actually able to use the complex XML API. Another well-known e-commerce behemoth saw this, and in 2002, Amazon unveiled its API. In August 2004, Flickr released its own RESTful API, allowing bloggers to quickly embed photographs on their websites and social network feeds. Both Facebook and Twitter revealed their APIs in 2006, caving in to developer pressure to build \"Frankenstein\" APIs after scraping the websites. Developers were able to access data space via AWS's REST API in minutes when it helped launch the cloud in 2006, and the demand for public APIs swiftly increased. Since then, RESTful APIs have gained popularity among developers, who now use them to enhance the functionality of their websites and applications.\nREST APIs are regarded as the \"backbone of the internet\" nowadays. A RESTful API performs the same fundamental task as accessing the internet. When a client needs a resource, it uses the API to communicate with the server. In the server application API documentation, API developers outline how the client should utilize the REST API. The standard procedures for any REST API call are as follows. A request is made to the server by the client. The client formats the request in accordance with the API documentation so that the server can interpret it. The server verifies the client's identity and validates that the client is authorized to submit that request. The request is received by the server, which then handles it internally. The client receives a response from the server. If the request was successful or not is indicated in the response to the client. Any information that the client asked for is also included in the response. Depending on how the API is designed by the developers, the REST API request and response details differ widely. Besides the design and architecture constraints, individuals will have to confront some challenges with REST APIs. Some concepts which may be challenging can include: Endpoint consistency -- paths of endpoints should be consistent by following common web standards, which may be difficult to manage. API versioning -- endpoint URLs shouldn't be invalidated when used internally or with other applications.\nLong response times and too much data -- the amount of returned resources can increase in size in time, adding to increased load and response times. Navigation paths and user input locations -- because REST uses URL paths for input parameters, determining URL spaces can be challenging. Security -- which has a lot of aspects to keep an eye on, including the use of: HTTPS; blocking access from unknown IP addresses and domains; validating URLs; blocking unexpectedly large payloads; logging requests; and investigating failures. Authentication -- use common authentication methods such as HTTP basic authentication (which allows for a base64-encoded username:password string), API keys, JSON Web Tokens and other access tokens. OAuth 2.0, for example, is good for access control. Requests and data -- requests may have more data and metadata than needed or more requests may be needed to obtain all the data. APIs can be adjusted for this. API testing -- can be a long process to set up and run. Each part of the process can be either long or challenging. Testing can also be done in the command line with the utility Curl. Parts of the testing process that may be challenging include: -Initial setup -Schema updates -Test parameter combinations -Sequence API calls -Validate test parameters -System integration Define error codes and messages. -With error codes, it is more of a common practice to use standard HTTP error codes. These are recognized by clients and developers more often.\nHuawei hybrid flash storage adopts the structure of RAID 2.0+ software architecture, which realizes the virtualization of the underlying media and the virtualization of the upper-level resources, and at the same time solves the problem of rapid data reconstruction and intelligent allocation of resources. A. True B. False This is the correct statement. The correct answer to this question is True A.True Refer to the post: Robust reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. Huawei RAID 2.0+ is a brand-new RAID technology developed by Huawei to overcome the disadvantages of traditional RAID and keep in line with the storage architecture virtualization trend. RAID 2.0+ implements two-layer virtualized management instead of traditional fixed management. Based on the underlying disk management that employs block virtualization (Virtual for Disk), RAID 2.0+ uses Smart-series efficiency improvement software to implement efficient resource management that features upper-layer virtualization (Virtual for Pool). Block virtualization is to divide disks into multiple contiguous storage spaces of a fixed size called a chunk (CK). This is the correct statement. The correct answer to this question is True A.True Refer to the post: Robust reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction.\nA multinational company has a huge business, with customer business data scattered in multiple locations, and the distance between locations is greater than 1000 km. The company hopes to centrally manage the backup data of each site, so that users can analyze and mine all data without affecting normal business, and any business site suffers a disaster, the company can take over the business and perform data recovery. In this case, which disaster recovery backup solution the enterprise should choose? A. Central disaster recovery and backup solution, synchronous remote replication is selected between the business site and the central site. B. Two-site three-center solution, with asynchronous remote replication between sites. C. Two-site three-center solution, with synchronous remote replication between sites. D. Central disaster recovery and backup solution, asynchronous remote replication is selected between the business site and the central site. In this scenario, the enterprise should choose A. Central disaster recovery and backup solution, synchronous remote replication is selected between the business site and the central site. The Correct answer is A Since the company has customer business data scattered in multiple locations and the distance between locations is greater than 1000 km, it would be best to have a centralized backup solution where all the backup data can be managed from a single location. This will allow users to analyze and mine all the data without affecting normal business operations, and in case of a disaster at any business site, the company can take over the business and perform data recovery.\nA company has a variety of services such as database applications, OLTP applications, mail service applications, and Web server applications. In which of the following service scenarios does the SmartCache feature not significantly improve performance? A. Mail service reading business B. Web server log business C. Database analysis business D. OLTP data query business B. Web server log business is the service scenario where the SmartCache feature may not significantly improve performance. The SmartCache feature is a caching technology used in Huawei's OceanStor storage system that improves the performance of read-intensive workloads. It caches frequently accessed data in high-speed media, such as solid-state drives (SSDs), to reduce the number of reads from the slower, hard disk drives (HDDs). In the Web server log business scenario (option B), the data being accessed is not typically accessed repeatedly, so there is little benefit to caching it. Therefore, the SmartCache feature may not significantly improve performance in this scenario. B. Web server log business Web server log business is the service scenario where the SmartCache feature may not significantly improve performance. The SmartCache feature is a caching technology used in Huawei's OceanStor storage system that improves the performance of read-intensive workloads. It caches frequently accessed data in high-speed media, such as solid-state drives (SSDs), to reduce the number of reads from the slower, hard disk drives (HDDs). In the Web server log business scenario (option B), the data being accessed is not typically accessed repeatedly, so there is little benefit to caching it.\nThen go to Step 5 . If the alarm is not generated, go to Step 6 . Step 5 Ensure that the current system does not have an alarm indicating that the s3_ha_agent process exits. Then check whether the alarms indicating that the HA system cannot perform active/ standby switching are cleared. If the alarms indicating that the HA system cannot perform active/standby switching are cleared, no further action is required. If the alarms still exist, go to Step 6 . Step 6 Check whether an alarm indicating that an HA node exits is generated. If the alarm is generated, handle the alarm by taking recommended actions. Then go to Step 7 . If the alarm is not generated, go to Step 8 . Step 7 Ensure that the current system does not have an alarm indicating that an HA node exits. Then check whether the alarms indicating that the HA system cannot perform active/standby switching are cleared. If the alarms indicating that the HA system cannot perform active/standby switching are cleared, no further action is required. If the alarms still exist, go to Step 8 . Step 8 The following operations may cause data loss. If you continue to perform the operations, go to Step 9 . If you choose not to perform the following operations, contact technical support engineers for help. Step 9 Based on alarm parameters, find out the active and standby HA nodes. Use a KVM to log in to the operating system of a node as user root.\nHi there! This time, I will share with you an important and highlighted feature of the Huawei OceanProtect backup solution. In this article, you will learn about OceanProtect backup storage product feature D2D2T to F2F2X. What isD2D2T? The traditional data tiering strategy is Disk-to-Disk-to-Tape (D2D2T),which means thestorage systems use the disk in the primary andsecondary storage systems, and tape in backup system. The main painpoints of D2D2T are: This storage is bundled with explosive data volumes and limited budget growth. Long backup time, unchanged backup window, and inadequate support for the adoption of newapplications and scenarios. It is very time-consuming, and recovery isunpredictable. InD2D2Tmajor issue is low reliability. This interruption cause a mandatory restart of backup at night or the next working day and impacts application performance. What isF2F2X? Huawei introduced anew Stoarge for backup and DR solutions. This storage system is based on active-active architecture andFlash-to-Flash-to-Everything (F2F2X) features.All-Flash storage systemsare equippedwith high performance to build DC-DRCcenters with unified SANandNASservices. The all-flash storage system cannowbe used as backup storage with shorterdata backuptime,and a restore window, and the data could be archived to anything likeobject storage and could with highflexibility. The main advantages of F2F2X are: Rapid Backup: The OceanProtect Backup Storage provides upto 155 TB/h,3x higherthan the industry's next best. Fast Recovery: The OceanProtect Backup Storage provides upto 48 GB/s,5x higherthan the industry's next best. Efficient Reduction: The OceanProtect Backup Storage provides upto72:1data reduction ratio,20% better than the industry's next best.\nThere are two types of ransomware attacks: Strong encryption algorithms, such as AES and RSA, are used to encrypt user data. Users must pay the ransom to obtain the keys to restore and access data. If the ransom is not paid within a specified period, the file data will be lost permanently. Sensitive and important user data is stolen. Attackers threaten to disclose the data to the public unless the user pays the ransom. HyperDetect handles the first type of ransomware attack. It provides ransomware file interception, real-time ransomware detection, and intelligent ransomware detection to protect data from ransomware threats. Ransomware File Interception The system intercepts the writes of files infected by known ransomware based on the FileBlock function. The model library is upgraded independently to update the interception rules. Users or administrators can add or delete blacklist items and update interception rules on the management UI at any time. Real-time Ransomware Detection Analyzes file I/O behaviors (such as read, write, renaming, and deletion) based on file operation logs to quickly identify abnormal files. Performs encryption detection on the abnormal files to detect files infected by ransomware. If a file encrypted by ransomware is detected, the system creates a secure snapshot for the file system immediately to minimize the impact of ransomware. In addition, an alarm is sent to the data protection administrator for confirmation and data recovery.\nDear All, Today we are going to learn about Huawei All Flash Storage High Security. Software integrity protection The software package uses an internal digital signature and a product package digital signature. After the software package is sent to the customer over the network, the upgrade module of the storage system verifies the digital signatures and performs the upgrade only after the verification is successful. This ensures the integrity and uniqueness of the upgrade package and internal software modules. Trusted and secure boot of hardware Secure boot is to establish a hardware root of trust (which is tamperproofing) to implement authentication layer by layer. This builds a trust chain in the entire system to achieve predictable system behavior. Huawei all-flash series storage system builds secure boot based on the hardware root of trust (RoT) to ensure that the software loaded during the boot process is not tampered with by hackers or malware. Software verification and loading process for secure boot: Verifying the signed public key of Grub: BootROM verifies the integrity of the signed public key of Grub. If the verification fails, the boot process is terminated. Verifying and loading Grub: BootROM verifies the Grub signature and loads Grub if the verification is successful. If the verification fails, the boot process is terminated. Verifying the status of the software signature certificate: Grub verifies the status of the software signature certificate based on the certificate revocation list. If the certificate is invalid, the boot process is terminated.\nDear All, Today we are going to learn about FC and IP SAN FC Network Fibre Channel Routing (FCR) provides connectivity to devices in different fabrics without merging the fabrics. Different from E_Port cascading of common switches, after switches are connected through an FCR switch, the two fabric networks are not converged and are still two independent fabrics. The link switch between two fabrics functions as a router. FC Router: a switch running the FC-FC routing service. EX_Port: a type of port that functions like an E_Port, but does not propagate fabric services or routing topology information from one fabric to another. Backbone fabric: fabric of a switch running the FC router service. Edge fabric: fabric that connects a Fibre Channel router. Inter fabric link (IFL): the link between an E_Port and an EX-Port, or a VE_Port and a VEX-Port FC HBA Zoning A zone is a set of ports or devices that communicate with each other. A zone member can only access other members of the same zone. A device can reside in multiple zones. Basic Zones are configured to control the access permission of each device or port. Traffic Isolation Zones: When there are multiple ISLs (E_Ports), an ISL only transmits the traffic destined for ports that reside in the same traffic isolation zone. IP SAN and FC SAN To solve the poor scalability issue of DAS, storage devices can be networked using FC SAN to support connection to more than 100 servers.\nWhat Is a Storage Area Network (SAN)? A Storage Area Network (SAN) is a specialized, high-speed network that provides network access to storage devices. SANs are typically composed of hosts, switches, storage elements, and storage devices that are interconnected using a variety of technologies, topologies, and protocols. SANs may span multiple sites. A SAN presents storage devices to a host such that the storage appears to be locally attached. This simplified presentation of storage to a host is accomplished through the use of different types of virtualization. SANs are often used to: Improve application availability (e.g., multiple data paths), Enhance application performance (e.g., off-load storage functions, segregate or zone networks, etc. ), Increase storage utilization and effectiveness (e.g., consolidate storage resources, provide tiered storage, etc. ), and improve data protection and security. SANs perform an important role in an organization's Business Continuity Management (BCM) activities (e.g., by spanning multiple sites). SAN s are commonly based on a switched fabric technology. Examples include Fibre Channel (FC), Ethernet, and InfiniBand. Gateways may be used to move data between different SAN technologies. Fibre Channel is commonly used in enterprise environments. Fibre Channel may be used to transport SCSI, NVMe, FICON, and other protocols. Ethernet is commonly used in small and medium sized organizations. Ethernet infrastructure can be used for SANs to converge storage and IP protocols onto the same network. Ethernet may be used to transport SCSI, FCoE, NVMe, RDMA , and other protocols. InfiniBand is commonly used in high performance computing environments.\nDear team, I would like to know the \"best practices\" for the recommended size of the LUNs to map on the ESXi as datastore for performance reasons. Thanks. Dear friend, The maximum LUN size supported by VMware is 64 TB. Customers must consider the size of each LUN and the number of LUNs to be configured. Before VMware introduced the Atomic Test & Set (ATS), large LUNs were not recommended because VMware used SCSI-2 reservation to obtain disk locks in the past. In the VMware cluster, when the LUN is large and multiple VMs are running on the LUN, only the owner node of the SCSI-2 reservation can access the LUN during the process of obtaining the disk lock. In this case, other hosts in the cluster cannot access the LUN. Although the process of obtaining the disk lock is very short, frequent metadata operations still have a significant impact on performance. This problem is solved after the ATS is introduced. The ATS obtains the disk lock from one sector. In addition, multiple hosts can initiate metadata operations to one virtual machine file system (VMFS) concurrently, which greatly improves the efficiency and performance. Therefore, it is a good choice to run multiple VMs on a large LUN. In VDI scenarios, a single LUN can provide storage space for dozens of or even hundreds of VMs. However, customers need to consider other costs, such as disaster recovery (DR) and backup of large LUNs.\nDear Members, This article continues our series examining the digital transformation trends in different industries. In previous issues we discussed sectors like finance and manufacturing, and today well focus on how digital transformation is revolutionizing healthcare. Digital transformation challenges to overcome At the core of every digital transformation overhaul is digital tech. Over the last decade, cutting-edge technology has fueled innovations in the healthcare industry, supporting the growth of e-workflows and high-quality medical services. Smart hospitals will be at the core of connected healthcare ecosystems, but current development levels are inadequate. Pain points like resource waste, information silos, and other issues limit the ability to mine or share data and prevent effective monitoring and diagnosis. Next-gen IT systems in healthcare Many in the healthcare industry need to restructure or even overhaul existing IT infrastructure to meet challenges with data growth and service quality. Hospital informatization refers to upgrading the Hospital Information System (HIS) a common model centered on hospital management to a Clinical Information System (CIS) model designed specifically for clinical data. This shift will be the catalyst for a Global Medical Information Service (GMIS) system, in which data and resources can be aggregated, integrated, and shared between medical consortiums and hospitals and their cloud-based big data platforms (data lakes). Understanding digital hospital systems A digital hospital system is a comprehensive information system comprising hospital service software, medical equipment, and IT platforms. Hospitals undergoing a digital revolution will better integrate resources, optimize processes, reduce operating expenses, and improve the quality of services and management tasks.\nHello all, Let's talk about Wear Leveling for SSD. I will give you an overview of Wear Leveling. Wear leveling is a flash memory controller feature that spreads the wear and tear of data transfers and useage evenly across the NAND flash memory, making your SSD last longer. Wear leveling is a technique that some SSD controllers use to increase the lifetime of the memory. The principle is simple: evenly distribute writing on all blocks of an SSD so they wear evenly. All cells receive the same number of writes, to avoid writing too often on the same blocks. Flash memory in Solid State Drives (SSDs) allows only a certain number of reading and writing processes. It usually ranges from 10,000 to 100,000. If we write 100 GB of data daily on a SSD with 400 GB of space, wear leveling ensures that the 100 GB of data is not always at the same location in the physical flash blocks. The data will distribute evenly over all the physical cells of the SSD. Wear leveling mechanisms allow the flash storage device to evenly distribute the P/E cycles among all blocks. It prevents the premature wearout of overused blocks, so all blocks can be used to the maximum. Wear leveling extends the life span and improves the reliability and durability of the storage device. Single Level Cell (SLC) Flash approx. 100 000 cycles Multi Level Cell (MLC) Flash approx. 10 000 - 3500 cycles Triple Level Cell (TLC) Flash approx. 5000 cycles Quad Level Cell(QLC) Flash approx.\nDear friend, As you know, the old eService platform has two main functions, the 1st function is called Call home service which can automatically create SR for alarms, so that Huawei support can inform customers or handle the hardware issues in time. The 2nd function is eService cloud, customer can access the eService cloud portal or service mobile APP to view the device status, health status, SR status, etc. very easy. Started from Oct, 2022, the eService cloud was updated as DME IQ, a totally new brand, which provides more powerful functionalities than the old eService cloud portal. Customers can directly switch over from eService cloud portal to DME IQ portal. URL: The mobile-app can download from Apple store from Android store. Strictly speaking, direct device connection and eService client, dont have relationship with DME IQ, they are just the way to activate Huawei eService service, the storage devices activated eService via direct device connection and eService client both can access DME IQ web portal. Please note, the 1st contact person of the device/site is the device administrator who can directly view the devices on DME IQ portal. The 2nd contact person is also the device administrator but he/she needs to manually bind the devices with the support-e account. If other user accounts need to view the device, need to device administrator account gives the permission to them. Direct device connection and eService client have some difference, you can compare and decide which one is more suitable for you.\nDears, what happened if a storage node goes down in hyper metro? BR, Tayyab Rehman (tayyab0101) The data of LUNs on both storage systems is synchronized in real time. Both storage systems are accessible to hosts. If one storage system malfunctions, the other one continues providing services for hosts. A host delivers a read I/O to the HyperMetro I/O processing module. The HyperMetro I/O processing module enables the local storage system to respond to the read request of the host. If the local storage system is operating properly, it returns data to the HyperMetro I/O processing module. If the local storage system is not operating properly, the HyperMetro I/O processing module enables the host to read data from the remote storage system. Then the remote storage system returns data to the HyperMetro I/O processing module. The HyperMetro I/O processing module returns the requested data to the host regards, Kashif Ali The data of LUNs on both storage systems is synchronized in real time. Both storage systems are accessible to hosts. If one storage system malfunctions, the other one continues providing services for hosts. A host delivers a read I/O to the HyperMetro I/O processing module. The HyperMetro I/O processing module enables the local storage system to respond to the read request of the host. If the local storage system is operating properly, it returns data to the HyperMetro I/O processing module. If the local storage system is not operating properly, the HyperMetro I/O processing module enables the host to read data from the remote storage system.\nhi, We we size QLC drives using Dorado? Hi, Dorado is a series of high-performance all-flash storage arrays offered by Huawei. QLC (Quad-Level Cell) drives are a type of solid-state drive (SSD) that can store more data per cell than other types of SSDs. When sizing QLC drives using Dorado, there are a few factors to consider: Capacity: QLC drives typically have a higher capacity than other types of SSDs, which can be an advantage in terms of cost and storage density. When sizing QLC drives with Dorado, it's important to consider the capacity requirements of the application and ensure that the QLC drives can provide enough storage capacity. Performance: QLC drives typically have lower performance than other types of SSDs, particularly in terms of write performance. When sizing QLC drives with Dorado, it's important to consider the performance requirements of the application and ensure that the QLC drives can provide sufficient performance. Endurance: QLC drives typically have lower endurance than other types of SSDs, which can be a concern in write-intensive applications. When sizing QLC drives with Dorado, it's important to consider the endurance requirements of the application and ensure that the QLC drives can provide enough endurance to meet the needs of the application. Cost: QLC drives are generally less expensive than other types of SSDs, which can be an advantage in terms of cost-effectiveness. When sizing QLC drives with Dorado, it's important to consider the cost requirements of the application and ensure that the QLC drives provide a cost-effective storage solution.\nWhat is the primary purpose of KVM? a) To manage Virtual machines b) To manage Logical volumes c) To manage physical storage d) None of above Dear friend, KVM stands for Kernel-based Virtual Machine. Its primary purpose is to allow multiple virtual machines to run on a single physical machine at the same time. This allows for greater resource utilization, as well as increased flexibility and efficiency in managing and allocating resources. KVM is used for a variety of purposes, including virtualization of servers, desktops, and other devices. It is often used in data centers and cloud computing environments, as well as in development and testing environments. KVM, which stands for Kernel-based Virtual Machine, is an open-source virtualization solution for Linux. Its primary purpose is to allow multiple virtual machines (VMs) to run on a single physical host, using the host's kernel as the foundation for each VM. This approach enables KVM to provide high performance, security, and reliability. KVM works by using hardware virtualization capabilities provided by modern CPUs to create multiple virtual environments that can run different operating systems and applications simultaneously. Each virtual machine is isolated from the others, providing a secure environment for running applications. KVM is commonly used in data centers and cloud computing environments where resources need to be efficiently utilized, and different applications need to be isolated from each other. KVM is also popular in development and testing environments, where developers can easily create multiple virtual machines to test their applications on different operating systems and configurations.\nHello everyone, This post will show you the difference between Active-Passive controller, Asymmetric Logical Unit Access(ALUA), and Symmetric Active-active. Basically, the storage controller does three things: Reads data from hard disks or writes data to hard disks (including SSDs and hard disks). Data processing, including metadata (metadata) and data services (data services) Receives data read/write requests from the host side. Generally, there are two storage controllers for redundancy design. In the case of active-passive architecture, the p controller is responsible for the above three tasks, while the passivecontroller is normally idle. The only thing it does is wait for the preferred controller to fail and take over the work of the preferred controller. From the perspective of resource utilization, it is a waste of resources. NETAPP storage and Pure storage use this architecture. In the ALUA architecture, storage LUNs and two controllers are accessed using the SCSI protocol, and each LUN has a owning controller. Therefore, each LUN has a preferred access path (primary path) for the host, that is, access from a controller is preferentially accessed. If the host accesses the LUN from a non-preferred path, performance will be affected. For example, the preferred controller of LUN A is controller A, and the host accesses data of LUN A from controller B. In this case, controller B needs to forward data, resulting in performance deterioration. In this architecture, two controllers work at the same time. This avoids idle resources of a single controller in the A-P architecture, but the two controllers do not work together.\nWhat is HyperVault? Hi Dear, Based on file systems, HyperVault enables data backup and recovery within a storage system and between different storage systems. Data backup involves local backup and remote backup. With file systems' snapshot or remote replication technology, HyperVault backs up the data at a specific point in time to the source storage system or backup storage system based on a specified backup policy. Data recovery involves local recovery and remote recovery. With file systems' snapshot rollback or remote replication technology, HyperVault specifies a local backup snapshot of a file system to roll back it or specifies a remote snapshot of the backup storage system for recovery. HyperVault has the following characteristics: Time-saving local backup and recovery: A storage system can generate a local snapshot within several seconds to obtain a consistent copy of the source file system, and roll back the snapshot to quickly recover data to that at the desired point in time. Incremental backup for changed data: In remote backup mode, full backup at an initial time and permanent incremental backup save bandwidth. Flexible and reliable data backup strategy: HyperVault supports self-defined backup policies and threshold for the number of copies. A copy of invalid backup data will not affect follow-up backup tasks. Thanks Hi, HyperVault is a file systembased data backup and recovery technology. HyperVault uses snapshot and remote replication technologies to back up data from the source to the backup storage system quickly based on the specified backup policy.\nDear All, Today we are going to learn about HyperSnap Technology. Technical Highlights of HyperSnap for Block Technical highlights: Quick generation: A storage system can generate a snapshot within several seconds to obtain a consistent copy of the source data. Minimal storage space consumption: A snapshot is not a full physical data copy, so does not occupy a large amount of storage space. A very small amount of storage space is required for a snapshot with a large data source Working Principles of HyperSnap for Block Data sent request to be written to L2 of the source LUN is written to a new space P5. The Actual space P2 is Linked by the snapshot. Data requested to be written to L0 of snapshot 1 is written to a new space P6, requiring no additional read and write overhead. When data is written to L2 of the source LUN again, the requested data is written to a new space P7. The Actual space P5 is released because it is not linked by a snapshot. A new snapshot 2 is created and activated HyperSnap for File By generating a consistent image of the source file system at a certain point in time, the file system snapshot function enables users to quickly obtain a data duplicate identical to the source file system. The source file system Services will be not interrupted during this process A snapshot is available immediately after being generated. Data read, write, and modification on the source file system do not affect the snapshot data.\nLatency is the delay between a users action and a web applications response to that action, often referred to in networking terms as the total round trip time it takes for a data packet to travel. Since networks communicate with one another in milliseconds (ms), latency cannot be completely avoided. It is dependent on a number of network-related factors and may change if any of them are altered. There are four main components that affect network latency, including: Transmission medium : The physical path between the start point and the end point. The type of medium can impact latency. For instance, old copper cable-based networks have a higher latency than modern optic fibers. Propagation : The further apart two nodes are the more latency there is as latency is dependent on the distance between the two communicating nodes. Theoretically, latency of a packet going on a round trip across the world is 133ms. In actuality, such a round trip takes longer, though latency is decreased when direct connections through network backbones are achieved. Routers : The efficiency in which routers process incoming data has a direct impact on latency. Router to router hops can increase latency. Storage delays : Accessing stored data can increase latency as the storage network may take time to process and return information. By taking care of the aforementioned elements and making sure they are functioning properly, latency can be decreased. Additionally, it can be decreased by utilizing specialized networks that guide communication between nodes and streamline the network path.\nPrivate networks are offered to customers by Content Delivery Network (CDN) service providers like StackPath, enabling them to avoid using the public internet. These private networks lessen latency by giving data packets faster routes to follow. Interrupt latency is the length of time that it takes for a computer to act on a signal that tells the host operating system (OS) to stop until it can decide what it should do in response to an event. Fiber optic latency is how long it takes for light to travel a specified distance through a fiber optic cable. For every kilometer (km) covered, a latency of 3.33 microseconds (s) naturally occurs, according to the speed of light. In reality, however, the per-kilometer latency of fiber optic cables is about 4.9 s -- this is because light travels slower in a cable. Bends or other imperfections in the cable could make the latency higher. Internet latency times are dependent upon distance. The longer a packet has to travel across a global wide area network (WAN), the higher the latency. WAN latency can be an important factor in determining internet latency. A WAN that is busy directing other traffic will produce a delay, whether the resource is being requested from a server on the local area network (LAN), another computer on that network or elsewhere on the internet. Audio latency is the delay between sound being created and heard.\nIn sound created in the physical world, this delay is determined by the speed of sound, which varies slightly depending on the medium the sound wave travels through. Sound travels faster in denser mediums: It travels faster through solids, less quickly through liquids and slowest through air. In audio, the acceptable midrange of latency is around 8 to 12 s. Latencies of 30 milliseconds (ms) are generally noticed by the listener. Operational latency can be defined as the sum time of operations if they are performed in a linear workflow. In parallel workflows, the latency is determined by the slowest operation performed by a single task worker. Mechanical latency is the delay from input into a mechanical system or device to the desired output. This delay is determined by Newtonian physics-based limits of the mechanism (excepting quantum mechanics). Computer and OS latency is the combined delay between an input or command and the desired output. Contributors to increased computer latency include insufficient data buffers and mismatches in data speed between the microprocessor and input/output (I/O) devices. By adjusting, modifying, and upgrading computer hardware, software, and mechanical systems, latency can be decreased. By adopting methods like prefetching, which anticipates the requirement for data input requests, multithreading, or using parallelism across many execution threads, latency within a computer can be eliminated or hidden. Uninstalling pointless apps, improving networking and software setups, updating or overclocking hardware are all additional ways to decrease latency and boost speed.\nFault tolerance allows a system to function after a failure. Accidents happen. Fires, floods, and major storms can destroy data centers, availability zones, and regions. That doesn't mean an outage. Multi-region apps are resilient. Single-region architecture is machine-resistant. Single-AZ architecture survives AZ failure. Only a multi-region application architecture can tolerate region failure. Multi-region architecture is crucial for high-availability applications. Low latency with distributed users? Multi-Region application architecture keeps data close to customers worldwide to reduce latency. Example: Keep latencies under 100 milliseconds. That is the maximum instantaneous sensation. Due to the speed of light, global user bases make it hard to keep latencies under 100 ms. Multi-region application architecture is the only approach to reduce latency and improve user experiences. Some companies wait until a region has enough users to add a database. Risky strategy. Sydney users will certainly suffer excessive latency if your primary data centers are in US-East. Your app won't expand if consumers are unhappy. Other companies do the reverse. European online sports gambling companies added US data centers after gambling became legal in some US states. These companies scaled their application architecture to the new location to ensure new users had a positive experience. When designing a schema for multi-region architecture, you must segment your database on a column, partition it, and set zone restrictions. After that, adding new regions with ALTER statements is considerably easier. Multiple servers and locations host your database and application. Most cloud providers make adding regional servers easy. Just deploy the database and app to that server.\nWhat Is an M.2 SSD? A Basic Definition The shape of a stick of gum is the M.2 form factor for SSDs (solid-state drives). Although these SSDs are more expensive than standard 2.5-inch SSDs, they are typically speedier. M.2 SSDs are increasingly being used in thin gaming laptops (opens in new tab) because they are more space-efficient than 2.5-inch SSDs or hard drives. M.2 SSDs have storage capacities of up to 2TB. More capacity is available in other form factors. M.2 SSDs might be shorter or longer than the typical size of 22 x 80mm (W x L). By examining the four or five-digit number in an M.2 SSD's name or on its printed circuit board, you may determine its size (PCB). Its width is represented by the first two numbers, and its length by the remaining numerals (example: M.2 Type-2280). A longer SSD has more room for NAND chips, but it does not necessarily have greater storage. Below are common M.2 SSD sizes: M.2 Type-2280 (22 x 80mm) M.2 Type-2230 (22 x 30mm) M.2 Type-2242 (22 x 42mm) M.2 Type-2260 (22 x 60mm ) M.2 Type-22110 (22 x 110mm) What Does an M.2 SSD Look Like? In addition to the two varieties of M.2, connectors also differ in a few ways. It's crucial that you purchase the proper M.2 SSD for your motherboard's connection. There are three different layouts, each with a different notch and gap in the edge connector. B: The notch is six pins from the left.\nThe user accidentally deletes the ACL permission, and no user has the permission to modify the ACL. As a result, the user has no permission to access files in the CIFS share. Step 1 Create a local user and set the primary group to Administrators. Step 2 Use the local user created in step 1 to mount the CIFS share. Step 3 Modify the ACL permission. Problems may occur during this process When a CIFS share is mounted as a local user, the following error message is displayed: This indicates that the current client has used a user to mount the share with the same logical IP address. A Windows host does not allow multiple CIFS sessions to be established for the same IP address. Change the logical IP address or disconnect the currently mounted CIFS share and run the net use /del command to clear the connection. The customer still fails to use the administrator user. In developer mode, run the show statistic cifs_connection command to find the current login session, check whether the login user is a local user, and check whether the 99999 group exists. If not exists, it indicates that the user does not belong to the administrators group in the current session. The possible cause is that the administrator group is manually modified but the session is not interrupted and the administrator group is remounted. Disconnect the session, check the configuration, and mount it again. The administrator privilege is lowered and the administrator does not have permission to modify the password.\nA system on a chip includes of hardware, defined in Structure, and software operating the microcontroller, microprocessor, or DSP cores, peripherals, and interfaces. Architectural co-design tries to build SoC hardware and software simultaneously. Design flow must consider optimizations and restrictions. Most SoCs are constructed using pre-qualified hardware component IP core specifications for the hardware pieces and execution units outlined above, along with software device drivers that may govern their operation. Protocol stacks that drive USB are important. Hardware blocks are assembled using CAD tools, specifically electronic design automation tools; software modules are integrated using an IDE. SoC components are commonly designed in C++, MATLAB, or SystemC and converted to RTL designs using HLS tools such as C to HDL or flow to HDL. HLS \"algorithmic synthesis\" products allow designers to model and synthesize system, circuit, software, and verification levels in C++, irrespective of HDL time scales. Other components can stay software and be developed and implemented on soft-core processors as HDL IP cores. Once the SoC architecture is determined, any additional hardware pieces are written in register transfer level (RTL), which defines circuit behavior, or synthesized from a high-level language using high-level synthesis. Hardware description language connects these pieces to produce the SoC design. Glue logic connects components and converts between suppliers' interfaces. Let us have a detailed look on its Architecture: SoC architectures rely on processor cores. Many SoCs currently use Arm Cortex-A, Cortex-M, and Cortex-R cores. Other SoC cores include Synopsys ARC, Cadence Tensilica Xtensa, and RISC-V cores.Multiple-core SoC architectures are growing.\nApplications are partitioned across many processor cores in symmetric multiprocessing. In asymmetric multiprocessing, some cores manage I/O while others execute executive responsibilities. Each SoC architecture poses programming and communication issues. Memory types and configurations vary per SoC architecture. SRAM is utilized for processor registers and quick level 1 (or L1) caches, while DRAM is the SoC's lower-level main memory. Developers may need off-chip DDR memory for memory-intensive applications like embedded vision. In designing these apps, the SoC's memory bandwidth can be significant. SoC BlocksetTM can assess Simulink memory bandwidth. SoC architectures have many peripherals to address popular communications protocols. CAN, SPI, USB, UART, and I2C are popular interfaces. SoC microcontrollers feature PWM, ADC, and D/A converters (DAC). SoC Blockset simulates peripherals in Simulink during algorithm development, while SoC Builder automates peripheral configuration. SoC modules must send instructions and data. Since early SoCs, bus-based communication has been used. Arm's AMBA is common bus architecture. The semiconductor industry has largely implemented AMBA AXI. Interconnection networks have replaced bus-based communication in SoC architectures. Each subsystem has its own clock domain in network-on-a-chip architecture. Xilinx, Intel, and Microchip have created programmable SoC architectures for FPGAs. These programmable SoC devices combine CPU cores with FPGA logic. Programmable SoCs let clients construct hardware/software apps using processor software and IP core libraries. Xilinx introduced the Zynq-7000 SoC design with a dual-core Arm Cortex-A9 core, then the Zynq UltraScale+ MPSoC and RFSoC families with quad-core Arm Cortex-A53 and dual-core Arm Cortex-R5F processors. Intel created SoC FPGAs.\nHigh-performance flash storage carries mission-critical business systems for various industries. If a problem occurred, enterprises would be hit hard. Qualix Group released figures to show the impacts of business interruption. In transportation, a one-minute stoppage would result in average losses of 150,000 USD, while that for banks would be 270,000 USD. The same one-minute stoppage for a telecommunications company would cost an average of 350,000 USD, while manufacturing would be hit with a 420,000 dollar loss, and security traders would top the list, losing 450,000 USD. Therefore, ensuring mission-critical business continuity is a top priority for all-flash storage systems. Reliability, designed in an end-to-end way, is no easy task. Looking from media to systems and solutions, lets examine how Huawei OceanStor Dorado all-flash storage can provide high performance and reliability for customers. Disk-level reliability SSD reliability is measured by examining the mean time between failure (MTBF) and annualized failure rate (AFR). The industry MTBF benchmark is between 2 and 2.5 million hours. Huawei raises the bar well beyond this, reaching 3 million hours between failures on its homegrown disks. How does Huawei accomplish this feat and extend the life of its SSDs? Huawei has maintained long-standing cooperation with its vendors, such as Samsung, Micron and Toshiba, to ensure that components are manufactured according to Huaweis solution design objectives. Another reason is the extensive cooperation achieved between arrays and disks, which combines a series of reliability designs (such as optimization in graphene dissipation technology (GDT), global wear leveling, and global anti-wear leveling). 1.\nGlobal wear leveling design At the beginning of the SSD lifecycle, service loads are spread in a balanced manner throughout SSDs to avoid overloading specific disks. This leads to the idleness of some disks and premature retirement of others. 2. Global anti-wear leveling design (Huawei-patented) At the end of the SSD lifecycle, when the wear on an SSD exceeds 80%, the anti-leveling mechanism takes the gradient wear to more than a 2% difference to avoid simultaneous disk failure. The service life of the system can be prolonged by gradually replacing the disks, ensuring sufficient time for system upgrade. All-flash storage focuses on performance and efficiency. Like a giant container ship on the sea, all-flash storage continuously pursues higher speeds and a larger capacity. With continuous and stable operations, Huawei OceanStor Dorado all-flash systems can deliver 99.9999% reliability, providing the public with a lightning-fast, rock-solid platform. 3. System software optimization First, at the algorithm layer, Huawei is the first vendor to commercialize the LDPC algorithm in SSDs. After years of optimization, Huawei now supports a 4 K ultra-long code algorithm. This brings the error correction granularity to twice of other SSD providers in the industry. Second, at the flash chip layer, the number of erase cycles is limited in an SSD. The service life of an SSD can be prolonged if the number of erase cycles can be increased through algorithms.\nHuaweis innovative adaptive program & erase (APE) technology automatically controls the erase strength and frequency of flash chips based on the amount of read and write data. In this way, the number of erase cycles can be effectively extended without changing costs or media granules, prolonging the SSD service life. Third, at the data protection layer, while the storage controller system has RAID protection, SSDs also support two-dimension RAID groups with interleaving parity at channel and CE levels, ensuring chip-level failure data protection. The disk RAID and system RAID groups work together to conduct automatic data recovery if multiple chips of a single disk are faulty. Then, after being recovered, SSDs will be operational again. System-level reliability Achieving reliability is complex. In addition to the hardware structure design and software fault tolerance mechanism, the storage system must tolerate physical and logical faults and support quick recovery. This will prevent data loss caused by system faults and ensure businesses continue running stably. 1. Magnitude 9.0 earthquake-resistant design The irregular seismic waves and intensified shaking caused by huge earthquakes will affect the stability and service life of electronic equipment. Huawei OceanStor Dorado all-flash storage has passed the magnitude 9 earthquake-resistant test run by China Telecommunication Technology Labs (TTL). This makes Huawei the only company to have done so and satisfy the TILs IT standards. Once an exception is detected, the system can also diagnose and rectify the fault quickly enough to prevent business interruption. 2. Tolerance of three-disk failures Disk capacity increases linearly with disk reconstruction time.\nTraditional RAID 5 or RAID 6 technologies allow 5 hours for the reconstruction of 1 TB of data, and 80 hours for 16 TB. However if one or two more disks become faulty during reconstruction, systems running RAID 5 or RAID 6 are unable to cope, severely disrupting business. Therefore, traditional RAID technologies cannot ensure system reliability, causing data loss and business interruption. Huaweis innovative RAID-TP software technology is based on the Erasure Code (EC) algorithm. Parity bits support 1-, 2-, 3-dimensions and can tolerate 1 to 3 simultaneous disk failures. This means that in the case of three disk failures, the system will not suffer from data loss or service interruption. Currently, only products from Huawei, NetApp, and Nimble can tolerate the simultaneous failure of three disks. Although NetApp and Nimble can tolerate simultaneous failures of three disks, they both use traditional RAID architecture with fixed data disks and hot spare disks. For these companies, hot spare disk reconstruction for 1 TB of data takes 5 hours. OceanStor Dorado employs a global virtualization system able to reconstruct the data in just 30 minutes, fulfilling the requirements of ultra-large capacity profiles. 3. End-to-end data integrity protection and tolerance of silent data corruption In data access, any errors that occur can cause issues for data integrity when data is transferred through multiple components, channels, and complex software. However, such errors can only be detected in subsequent data checks and access. This phenomenon is called silent data corruption.\nOften overlooked, silent data corruption has greatly impacted services, such as databases, that require absolute data integrity. Launched by Huawei, Emulex, and Oracle, the data integrity solution changes the traditional condition where hosts and storage systems protect data independently. This has been achieved by implementing end-end protection across applications, hosts, storage systems, and disks. As a result, this solution prevents silent data corruption for mission-critical businesses and eliminates potential down times. 4. Intelligent prefetch When a disk detects block faults or even severe die failures, the storage system receives failure reports from SSDs and uses redundant data in RAID groups to rapidly reconstruct and repair damaged data, reducing data loss risks and ensuring system reliability. Huawei all-flash storage systems can accurately query internal data, such as SSD data, and use innovative prediction algorithms to monitor and predict the service life of disks. The personnel in charge of customer businesses will be told that their disks need replacing before the disks become faulty or one month before the service life is exhausted. Solution-level reliability Huawei OceanStor Dorado all-flash storage supports multiple data protection technologies, such as snapshot, clone, remote replication, and active-active data protection. This allows it to implement data protection solutions from local or intra-city to remote disaster recovery. This solution provides high availability and non-disruptive storage data services for customers, preventing data loss caused by logical or physical disasters. 1. Lossless snapshot Traditionally, COW-based snapshot technology requires data to be written to a location after being read and migrated to a new location.\nTherefore, such snapshot processes involve one read, two writes, and one metadata update. COW-based snapshot affects system performance due to performance loss during each data migration. Huawei OceanStor Dorado all-flash storage implements lossless snapshot using ROW. When a snapshot is activated, data is written to the new location and the pointer of the mapping table is modified. Only one data write and one metadata update are involved, with data operation complexity being only 1/3 of that seen for COW-based snapshot. In addition, no extra data migration is required when the ROW snapshot is activated, resulting in no compromises in performance regarding production businesses. In addition, OceanStor Dorado storage supports second-level periodic snapshot, which is superior to the minute- or hour-level snapshots used by competitors all-flash storage. OceanStor Dorado snapshot provides users with a more intensive and powerful continuous data management (CDM) solution, enabling real-time data protection. 2. Gateway-free active-active architecture Huawei OceanStor Dorado storage adopts a gateway-free active-active layout, removing the gateways on both sides. This immediately reduces customer procurement costs and lowers possible failures, achieving reduced latency, improved reliability, and accelerated performance. In addition, the overall networking is greatly simplified, with the number of deployment steps halved, thereby shortening the delivery cycle. Active-active architecture HyperMetro is deployed on two arrays in an active-active profile. Data on the active-active LUNs at both ends is synchronized in real time, and both ends process read and write I/Os from application servers to provide the servers with parallel active-active access.\nHello, everyone! The post will share with you what is RAID 2.0+. The term Redundant Array of Independent Disks (RAID) was first defined by the University of California, Berkeley in 1987. The basic idea of RAID is to combine multiple independent physical disks based on a certain algorithm to form a virtual logical disk that provides a larger capacity, higher performance, and better data error tolerance. This technology bred the external storage market. RAID 2.0+ employs the two-layer virtualization management mode rather than the traditional management mode. In addition to underlying disk management, RAID 2.0+ uses the Smart series software to efficiently manage upper-layer resources. Each CK is 64 MB physical space divided from disk space in a storage pool. CK is the basic unit of a RAID group. A disk group(DG) is a set of disks of the same type in a disk domain. The disk type can be SSD, SAS, or NL-SAS. A logical drive(LD) is a disk that is managed by a storage system and corresponds to a physical disk. A chunk group (CKG) is a logical storage unit that consists of CKs from different disks in the same DG based on the RAID algorithm. lt is the minimum unit forallocating resources from a disk domain to a storage pool. Each CKG is divided into logical storage spaces of fixed and adjustable size called extents. Extent is the minimum unit (granularity) for migration and statistics of hot data. lt is also the minimum unit for space application and release in a storage pool.\nHello, can someone please share some insite on \"Hyper Metro\" from his personal deployement experience. that how useful is this product and what is the difficulty level to deploy. Thanks Based on my understanding, \"Hyper Metro\" is a feature or technology offered by Huawei that allows for synchronous replication of data between two geographically distant storage systems, often with zero data loss and very low recovery times in the event of a failure. From a deployment perspective, the usefulness of \"Hyper Metro\" depends on the specific needs and requirements of your organization. If you have a need for highly available and resilient storage, especially across geographically distributed locations, \"Hyper Metro\" can be a valuable solution. It can provide a robust disaster recovery and business continuity solution that ensures that your data remains available in the event of a site-level outage or failure. In terms of difficulty level to deploy, it can depend on a variety of factors, such as the specific storage vendor and the complexity of your environment. Generally speaking, setting up \"Hyper Metro\" requires careful planning and coordination between the storage systems and network infrastructure involved. You may need to configure networking, storage, and other components to ensure that they work together properly. Overall, deploying \"Hyper Metro\" can be a complex process, but if done correctly, it can provide significant benefits in terms of data availability and business continuity.\nOverview of REST API Representational State Transfer (REST) is an architectural style used in developing web services. RESTful web services enable communication between different systems using the Hypertext Transfer Protocol (HTTP) with a focus on resource-oriented architecture. RESTful web services have become the preferred way for many organizations to expose their services to other systems and applications. In this article, we will discuss the basics of REST APIs, their features, and benefits. What is a REST API? A RESTful API is an Application Programming Interface that uses HTTP methods to interact with resources and return responses in a standardized format such as JSON, XML, or HTML. REST APIs are designed to be stateless, meaning that the server does not store any information about the client's previous requests. Instead, each request is treated independently and contains all the necessary information required to complete the request. Features of REST API Resource identification: REST APIs use Uniform Resource Identifiers (URI) to identify resources, which can be a document, image, or any other data. HTTP methods: REST APIs use HTTP methods such as GET, POST, PUT, DELETE, and PATCH to perform different operations on resources. Stateless: REST APIs are designed to be stateless, which means that the server does not store any information about the client's previous requests. Representations: REST APIs return representations of resources in a standardized format such as JSON, XML, or HTML. Caching: REST APIs use caching to improve performance by reducing the number of requests to the server.\nDear Members, The digital transformation of the financial industry has come a long way in recent years, with fintech companies adapting to these market changes. Todays contactless technologies are much more commonplace in digital strategies, cementing both continuity and customer experience. Here are two simple examples: 70% of all sales of a Spanish digital bank took place on digital channels in Q1 2022. A major insurer in the Chinese mainland implemented a video-based insurance platform for investigating claims, helping complete the process in an average of just five minutes and settle 93.2% of auto insurance claim payments in one hour. Have you wondered what digital channels, contactless capabilities, and online services mean for the future of the finance field? While business models are prioritizing digital technologies to innovate their practices, this industry-wide trend has caused the growth of mass unstructured small files (such as camera identification and authentication info), especially on banks data exchange and cheque imaging systems. The small files housed by a single cheque imaging system can reach hundreds of terabytes or even several petabytes. Online banking and the surge in unstructured data Whereas structured data comprises clearly defined data types with patterns that make them easily searchable, unstructured data is the opposite. Data types are diverse, including formats like audio, video, and social media posts, which makes performing queries hard. The research group states that up to 80% of banking data is encrypted in an unstructured format, spanning online forms, images, signatures, blueprints, and PDF documents, and this figure is set to grow.\nGood day everybody! This post contains an overview of HyperSnap. Please check out more details as you read further down the post. HyperSnap is a virtual snapshot that is an identical copy of the source data at a specific point in time. The virtual snapshot is associated with the source LUN/file system. Copy-On-Write for V5 block: Redirect-On-Write for V5 file systems and Dorado block: Zero performance loss The solution only modifies pointers each time a snapshot changes, and does not migrate the data of the last snapshot, which ensures zero performance compromise. Scheduled snapshots in seconds Supports two timing policies: Create snapshots at fixed intervals, weekly, daily, or at fixed moments. The minimum interval for periodic snapshots is 30 seconds. Supports configuration of 512 periodic snapshot plans. Each plan supports 128 source LUNs, and each source LUN supports 256 periodic snapshots. Each LUN only supports one periodic snapshot plan. Readable and writable snapshots Supports readable and writable snapshots by default. Multiple snapshot copies The solution creates a read-only copy of a snapshot at a specific point in time. Cascading snapshots Snapshot cascading refers to creating a child snapshot for a parent snapshot. The child snapshot shares the data of its parent snapshot. Supports cross-level rollback of cascading snapshots. Cross-level rollback refers to multi-level cascading snapshots that share a source volume, and, including the source volume, can roll back each other regardless of their cascading levels. Supports a maximum of 8 cascading levels, does not support the deletion of mid-level snapshots.\nSince the first commercial 5G services were switched on in the first half of last year, many more 5G networks have gone live throughout the world. According to the tracking by research house , by the end of Q2 this year over 70 mobile operators across more than 40 countries have launched 5G, bringing fast mobile or fixed wireless broadband connectivity to millions of consumers. Huawei forecasts that 5G is on track to reach mass market adoption faster than any previous mobile generation with close to 2 billion subscribers by the end of 2024. Meanwhile, the mobile industry has also started embracing the advanced phase of 5G, in particular the standalone mode, which will help operators realize 5Gs potential of billions of additional value through serving industrial and other business use cases. Such a background makes it a good time to take stock of 5Gs performance in its first year and start looking ahead towards what should be expected, and what should be avoided, in the years to come. To this end, Huawei has recently conducted an industry survey to gauge the sentiment towards 5G so far, and to gather the industrys views on the new technologys successes and challenges. The enthusiastic response of the industry is encouraging. By the time the survey was closed in mid-July, a total of 344 participants had responded to the survey and shared their views on a range of questions related to 5G Technologies, 5G Security, and 5G Expectations.\nThe three biggest groups of companies represented in the survey are vendors and system integrators, each at 24%, followed by mobile network operators (MNOs) or mobile virtual network operators (MVNOs), at 21%. In terms of the participants job functions, 22% of the total respondents were C-level executives and VPs, including those heading their organizations IT departments. This was followed by mid-level management (19%), engineers and developers (18%), and sales and marketing personnel (17%). The survey has attracted plenty of Huawei veterans. Over a quarter of all respondents (26%) had been in the telecoms industry for more than 25 years, and just under a quarter (23%) had been in the trade between 20 and 25 years. Geographically, the largest proportion of respondents are based in Europe (40%), followed by the group from North America and Asia Pacific, each at 21%. Incidentally, these three groups combined also represent the majority of the markets where 5G has gone live. The demographics of the respondents showed that the large majority of the respondents have an intimate understanding of the technologies, a close relationship with the market and customers, and a rich experience in the telecoms industry. The responses to the survey can therefore be read with a high level of confidence as representing the true attitude of the telecoms industry towards 5G. The survey opened by asking the participants to rate the overall performance of 5G so far against their expectations. The good news is 41% of all respondents felt that 5G has either met or exceeded their expectations.\nAbout a third of respondents, primarily from the markets where 5G is yet to launch, understandably said they could not tell. The less good news is that about a quarter (24%) of survey participants felt underwhelmed by 5Gs performance. How would you describe 5Gs overall performance since the first commercial services went live in 2019? 5Gs Overall Performance Better than I expected 11% Meeting my expectations............................................ 30% Less impressive than I expected ................................................................ 24% Cant tell yet ...................................................................... 31% Other..................................................................................... 4% The rationale behind the mixed responses became clearer when the participants responded to questions on their greatest expectations for 5G and what leading challenges they see 5G facing. Three quarters (74%) of the respondents expected to see 5G enable new use cases to generate additional value, for example high-quality video experience, or industrial automation. Fifty-six percent saw 5G delivering stronger network customization capability for operators to meet new and unique customer demands, for example end-to-end network slicing, or deployment of private networks. When it comes to challenges 5G is likely to face, over half (54%) of all the respondents believed the failure to deliver on 5Gs high promises would be the biggest worry. There is a high degree of overlap between those choosing this option and those who felt 5G has been less impressive than expected. Which is understandable, considering, for example, consumers have not experienced anything close to gigabit per second speed.\nDear all, FusionCube Center cannot obtain the monitoring data of the switch. How can I handle this? Thanks. Dear Axe, Here is the procedure for this alarm. 1. Locate the row that contains the alarm and click . View Additional Info and record information about the alarm. 2. Check whether the SNMP user name and password set on FusionCube Center are the same as the SNMP user name and password of the switch. If yes, go to 5. If no, go to 3. 3. Change the SNMP user name and password on FusionCube Center. 4. Wait 5 minutes and check whether the alarm is cleared. If yes, no further action is required. If no, go to 5. 5. Check whether the CX310 or CX311 is used as the switch. Check the switch authentication protocol and encryption protocol. Check the switch authentication protocol and encryption protocol in FusionCube Center. If yes, go to 9. If no, go to 6. 6. Check whether the SNMP authentication protocol and encryption protocol set on FusionCube Center are the same as the SNMP authentication protocol and encryption protocol of the switch. If yes, contact Huawei technical support. If no, go to 7. 7. Change the SNMP authentication protocol and encryption protocol on FusionCube Center. 8. Wait 5 minutes and check whether the alarm is cleared. If yes, no further action is required. If no, contact Huawei technical support. 9. On the CX310 or CX311, check whether the SNMP authentication protocol is SHA and encryption protocol is AES .\nAn alarm is generated on the FusionCompute platform :ALM-15.1004006 HA Resource in a Cluster Is Going to Be Insufficient 1. Checking the HA Configuration of the Cluster. This site enabled Tolerate cluster host failures. 2. Check the configuration and quantity of VMs. The Max CPU resources is 10 (Mhz) and max memory resource is 100(GB) 3. Check the host specifications. There are eight hosts in this cluster. The total memory of each host is less than 400 GB. 4. Refer to the following document for calculation. 5. This is because automatic computing is selected. Automatic computing considers all VMs as the maximum specifications (10 vCPUs, 100 GB) of VMs on the live network. This calculation has a large error when only a small number of VMs with large specifications are used. 6. Based on the live network specifications, we can calculate that the resources of at least one host are redundant. 15 VM as 80 GB Memory flavor. Use 4 nodes Memory 2 VM as 90 GB Memory flavor. Use 0.5 nodes Memory 2 VM as 92GB Memory flavor. Use 0.5 nodes Memory 4 VM as 100GB Memory flavor. Use 0.5 nodes Memory 2 VM as 10GB Memory flavor and 2 VM as 5 Memory flavor and 2 VM as 6GB Memory flavor. Combine as 0.5 hosts Remaining resources host =8(total host)-4-0.5-0.5-0.5-0.5=2 Therefore, sufficient resources are available for VM HA after a fault occurs. Automatic computing considers all VMs as the maximum specifications.\nNetwork performance and your company's operations can both be negatively impacted by latency. As businesses depend more on Internet of Things (IoT) services and cloud-based applications, this will become more and more important. Basic corporate operations are impacted by latency, and the consequences hurt any attempt at future-proofing. High-latency measurements cause inefficiencies and prevent systems from performing at their best, as seen in instances like automated manufacturing and smart sensors. Latency increasingly has the potential to mean the demise of internet-based services and/or users' access to them. Researching hardware, bandwidth, and connectivity is also crucial when evaluating the efficiency of an online application and its capacity to handle latency. Collaboration and productivity are closely related to latency. Many of the latency problems consumers have with alternate options, such as older networks that have been pieced together over time or copper infrastructure, are mitigated by a well-planned commercial fiber connection. 1. Distance In most cases, distancein this case, the distance between your computer and the servers your computer is requesting information fromis the primary cause of latency. 2. Propagation delay Let's quickly discuss propagation. When we talk about the internet, propagation is the act of sending your data packets to a server. In physics, propagation is \"the sending out or spreading of light or sound waves, movement, etc.\". Your data packet the server Now let's talk about propagation delay, which is the amount of time it takes for data packets to travel there.\nPropagation delay is just one factor that determines how much latency you encounter (although it excludes the time it takes to travel the entire distance back to your computerround-trip that's time). 3. Internet connection type How high or low your latency is might also depend on the type of internet connection you use. DSL, cable, and fiber internet often have reduced latency compared to satellite internet, which typically has higher latency. Latency by connection type: DSL: 2442 ms Cable: 1527 ms Fiber: 1015 ms Satellite: 594612 ms 4. Whats on a website Have you ever clicked on a link and had to wait several minutes for the page to load because it contained way too many GIFs, advertising, or enormous images? Your web browser must download all of the files and advertisements on a website if it has numerous huge files, such as HD photos or movies, or numerous third-party advertisements. Additionally, there will be some latency due to distance if those files or advertisements are stored on a server that is far from your computer. 5. Wi-Fi vs. Ethernet cable Use an Ethernet cable to connect to the internet if you wish to minimize latency as much as is humanly possible. Does Wi-Fi affect latency? Yes, Wi-Fi is fantastic, but because your wireless signal is more prone to noise, any lost data packets will likely need to be sent again. Wi-Fi also needs to navigate a few more hurdles, such as encryption procedures, in order to communicate with your computer.\nFurthermore, wireless transmissions typically lose strength over distance more quickly than an Ethernet connection does. 6. Your router Whether you connect by Ethernet or Wi-Fi, an outdated, sluggish router might hinder the speed at which your computer communicates with the modem at your internet service provider. This is especially true if you have a lot of devices and people connecting to your router at once, or if your router can't support the internet speed you're paying for. Although switching to a new router might reduce your latency, it most likely won't make a significant difference. Any latency of 100 ms or less is regarded as acceptable. You can still play the majority of online games without much frustration even at 100 ms. If you're playing a first-person shooter (FPS) game like Call of Duty or any other game where timing is important, low latency is very important. There are some things you can do to fix high latency (besides cursing your internet connection). Take a look: Turn off any downloads, and be sure to check for anything thats downloading in the background. Close any unused applications or browser tabs. Check for malware. We once had a bug on our computer that was using up most of our bandwidth. Use an Ethernet cable to connect your device to your router or modem, if at all possible. If you cant use an Ethernet cable, you may want to invest in a mesh Wi-Fi system. Update your routers and modems firmwareoutdated firmware can even cause slow internet speeds.\nBased on the changes in the all-flash array market in 2017 and 2018, DCIG summarizes six trends, which will affect the purchase decisions of enterprises in 2019. 1. The number of new vendors increases, and the market is still being integrated. Although new storage providers continue to enter the all-flash array market and focus on NVMe over Fabrics, the overall market trend is continuous integration. HPE acquired Nimble Storage and Western data acquired Tegile. Large enterprises in the industry have performed at least one all-flash purchase case. As a result, some providers are in the process of \"rationalizing\" their all-flash portfolio products. For example, HPE decides to position the all-flash array of Nimble as a \"level-2 flash memory\". Starting from 3PAR StoreServ storage, the Nimble HPE prediction and analysis platform is configured for the entire data center product series of HPE. Dell's EMC seems to position VMAX as the main product oriented to mission-critical workloads. Unity is oriented to enterprise organizations that focus on simplified operations, XtremIO is targeted at/dev for VDI/, and SC is oriented to low-cost organizations. Almost all flash array vendors provide at least one hyper-converged infrastructure product. These hyper-converged products compete with all-flash arrays for marketing and data center infrastructure budgets. This will bring extra pressure on all-flash-memory array suppliers and may push forward further consolidation of the market. 2. The flash memory capacity increases significantly. According to the DCIG report, the average capacity of more than 100 all-flash-memory arrays reaches 4.4PB. Compared with the 2017-18, the capacity increases by five times.\nThe product provides a maximum of 70PB all-flash capacity, which is seven times higher than that of the 70PB. 3. The storage density increases significantly. According to DCIG, the storage density of all-flash-memory arrays is increasing. Half of the density reaches 50TB/RU (Rack Unit). The storage density of some all-flash arrays can reach 200TB/RU. The combination of all-flash performance and high storage density means that all-flash-memory arrays can meet the performance and capacity requirements of traditional HDD storage systems and first-generation all-flash-memory arrays. This makes it possible for many institutions to solve the problem of reducing the cost of important data centers. Some organizations have eliminated data centers, and some have delayed the construction of new data centers. 4. The increase in the capacity and density of flash memory must match the new components that improve disk array performance. The components are as follows: Intel new-generation multi-core CPU 32GB FC and 25/40/100GbE GPU ASICS Reducing the Storage Task Load The NVMe SSD is connected to the SSD. Each component can unlock more performance of flash memory. Enterprises should assess the integration of these components to unlock the performance of flash memory and their applications. 5. Unified storage is a new norm. DCIG said that more than half of all flash arrays support unified storage. Supporting multiple concurrent protocols also creates opportunities for integrating and accelerating more types of workload. 6. Most all-flash-memory arrays are targeted at public cloud storage.\nQ: In the AD domain environment, which CIFS share statement is correct? The storage system supports AD domain clients can only access the CIFS shared directory with the same name. When an AD domain client accesses the CIFS file share provided by the storage system, all authentication is done through the AD domain controller. An AD domain client can view the CIFS directory of other domain clients An AD domain client can view the NFS and CIFS directories of other domains Hi, Answer is A & B, In an Active Directory (AD) domain environment, the correct statement for a Common Internet File System (CIFS) share depends on the specific requirements of the organization. However, here are some general guidelines: Use domain user accounts or domain groups to control access to the CIFS share. This provides a centralized way of managing access and allows for easy modifications to access permissions as needed. Set the appropriate permissions on the share and the underlying file system to ensure that users have the necessary access to the files they need, while preventing unauthorized access. Consider using access-based enumeration (ABE) to hide files and folders that users do not have access to. This can improve security and simplify the user experience by only showing them the files they have permission to access. Ensure that the CIFS share is properly secured by using SSL/TLS encryption for data in transit, and implementing appropriate measures to protect against unauthorized access to the share itself.\nDear All, Today we are going to learn about FLashLink Clod/Hot Data Partitioning Metadata and data are stored separately in the controller and system to accelerate data access. This is just like how highways are divided into lanes allowing different speeds New data is written to new locations. The original data is set to invalid. When the amount of garbage reaches the threshold, valid data is migrated to a new stripe. The original CKG is released. Hot/Cold partitions: Controllers automatically detect data layouts inside SSDs. Partitioning of hot and cold data is implemented within the controller and SSDs simultaneously. Sequential layout of hot and cold data in different partitions Effectively reducing the amount of garbage inside SSDs FlashLink : Smooth GC with Multi-stream to reduce WA by 60% Metadata is changed frequently, sometadata flows are assigned to thesame block list to reduce the amount of data moved during GC and improve GC efficiency. User data is changed less frequently, and user data flows are also assigned to the same block list. Data to be moved can then be located sooner during GC and GC efficiency can be improved User data that has remainedunchanged for a long time is also less likely to be changed in the future. Such data flows are assigned to the same block list as well so that fewer blocks need to be scanned during GC and GC efficiency can be improved.\nInfoEqualizer can intelligently distribute client access requests among storage nodes, improving service performance and reliability. Load balancing based on domain names 1. The client accesses thefile system using the level-1 or a level-2 domain name, forexample, fsx.tx.com. 2. The cluster resolves the domain name and returns the corresponding IP address based on the load balancing policy. 3. Service access Partition-based management InfoTier enables files to be stored on different tiers based on file properties. A tier consists of one or more node pools. A node pool consists of multiple nodes. A node pool is divided into multiple DiskPools. A partition is created for each DiskPool. InfoAllocator is a resource control technology. It restricts resources (including storage space and file number) available for users or user groups in a directory. InfoAllocator enables OceanStor 9000 administrators to: Plan storage space or file number consumed by a user or a group in a proper way. Manage storage space or file number that a user or a group consumes. Make statistics on and check storage space or file number consumed by a user or a group. TheInfoStamperis a directory-based snapshot feature provided byOceanStor 9000. It can create snapshots for any directory (excluding the root directory) in a file system instantly, enabling users to back up critical data without affecting services. In addition, theOceanStor 9000can periodically create snapshots for source data. A periodic snapshot policy enables theOceanStor 9000to create data copies that preserve the state of source data at multiple points in time, providing continuous protection for the source data.\nThe amount of unstructured data being generated is increasing at a rate of up to 60% per year and is anticipated to make up 80 to 90% of all data by 2025 as a result of the digital transformation of nearly everything. Organizations face new data storage issues as the volume of unstructured data generated globally rises. Most of this data is unstructured and coldthat is, it is dormant and only sometimes accessible. In a recent survey by Forbes, 95% of organizations said they needed to manage unstructured data, and more than 40% said they needed to do so frequently. Here are a few illustrations of the enormous amount of data being gathered and processed. A weekly production of 4000 wafers by a semiconductor company results in over a billion image scans, creating petabytes of data that must be stored for anywhere between six months and several years. Every hour of operation, an autonomous vehicle can generate up to 2 TBs of data, which must be kept for several years in case safety needs to be examined or modified. Each sequenced human genome is distinct and requires 100 GB of storage. Each has tremendous untapped worth for both ongoing and future medical research, which is a very strong motive to keep this information on hand for decades rather than just years. Unstructured and inactive data is a problem that cannot be ignored as the gap between data growth and storage budgets deepens.\nThe hero of archival storage known as step in cold storage is frequently disregarded, but that is about to change. It's no secret that keeping data on hand may be expensive for businesses. The NVMe and solid-state disk technologies used for high performance access to hot data are significantly more expensive than cold data storage, on the other hand. The major cause of this is that different storage systems' performance and cost trade-offs benefit cold data. The cost-effective storage of cold data on lower-performing and less expensive storage equipment, whether on-site or in the cloud, enables organizations to store larger volumes of their expanding data sets. Some of the biggest cloud solution providers in the world invented modern cold storage archives, but thanks to new designs and services, cold storage solutions may now be implemented in a company's data center, colocation space, or hosted IT environment. Recent developments have raised the bar for performance and affordability expectations by simultaneously maximizing performance, data durability, and storage efficiency. Without access costs, data is more readily available and can be retrieved in a matter of minutes rather than hours or days. In contrast to the antiquated method of storing numerous copies, modern erasure coding algorithms are now optimized expressly for cold storage. To comply with data sovereignty and data residency standards, data can now be maintained within internal security perimeters.\nWhat are the various methods that provides Synchronization? Dear friend, The main purpose of data synchronization is to ensure that two or more locations contain the same up-to-date data. Suppose the data is modified (edited or deleted) in some way in one place. In this case, the synchronization process edits or deletes the corresponding data elsewhere, thus maintaining consistency. There are many ways to synchronize data, but all of these technologies can be divided into several types: File Sync works best for home backing up external hard drives or updating mobile data on flash drives, Dropbox, or similar products. It is faster than manual copying, less error prone, and prevents duplicates of the same files. Some backup software also supports real-time file synchronization. However, you always have to consider the limitation that synced files must be physically housed in a portable storage device. Version control , also known as VCS, allows you to track and manage changes to the code base over time and store those changes in a database. It enables many contributors to modify the same file simultaneously without overwriting or conflicting with each other's work. It ensures that every team member has access to the latest code and provides recovery options for any earlier versions of the application. Today, most hardware and software development teams use version control. Distributed file systems (DFS) differ from typical file systems, such as NTFS, by providing the ability to access the same file data from multiple locations.\nDear All, Today we are going to learn about HyperCDP Overview With an ever-increasing amount of data, traditional data backup solutions are facing the following challenges: Large amount of backup data and rapid data growth Small backup window Requirement for zero impact on production system performance Ever higher requirements on the recovery point objective (RPO) and recovery time objective (RTO) Currently, Huawei OceanStor Dorado V6 storage systems provide writable snapshots. A single LUN supports up to 1,000 snapshots, but this does not meet the requirements of mission-critical applications for continuous data protection. To address these requirements, Huawei provides HyperCDP, which creates high-density snapshots on a storage system to provide continuous data protection. Functions and Working Principles HyperCDP creates high-density snapshots on Huawei OceanStor Dorado V6 to provide continuous data protection. HyperCDP has the following advantages: HyperCDP provides intensive and persistent data protection. A single LUN supports 60,000 HyperCDP objects. The minimum interval is 3 seconds. HyperCDP provides data protection at an interval of seconds, with zero impact on performance and while occupying little space. Scheduled tasks are supported. You can specify HyperCDP schedules by day, week, month, or a specific interval. HyperCDP consistency groups are supported. A HyperCDP object cannot be directly mapped to a host for read and write. You can create a duplicate, convert it into a writable snapshot, and map it to the host. Read and Write Principles HyperCDP objects cannot be directly mapped to hosts for read and write.\nThey must be converted to writable snapshots and then mapped to hosts HyperCDP Rollback Without Data Modification Rollback using a HyperCDP object is a process of copying the data in the HyperCDP object to the source LUN. After the rollback is started, the source LUN can be used immediately (data on the source LUN is the HyperCDP data). HyperCDP Rollback with Data Modification HyperCDP Consistency Group The data, logs and modification information of large and medium-sized databases are stored on different LUNs. If data on one of these LUNs is unavailable, data on the other LUNs is also invalid. The HyperCDP consistency group ensures the consistency of application data during restoration. Like individual HyperCDP objects, you can create, delete, roll back, or stop rolling back a HyperCDP consistency group as required. You can also create or rebuild duplicates for HyperCDP consistency groups HyperCDP Schedule You can specify HyperCDP schedules by day, week, month, or a specific interval. These schedule policies can retain different numbers of HyperCDP objects, and multiple policies can be used together in a schedule. A HyperCDP schedule supports multiple LUNs and LUN consistency groups, while a LUN or LUN consistency group can be added to only one HyperCDP schedule.\nAIX 6.1 31.00.090 The /opt/Ultrapath/log/syslogAlert.log file is continuously printed after UltraPath is installed. This print is the print of UltraPath internal message interaction. Log files are managed by the syslog mechanism of the system. A maximum of three copies of log files can be dumped and compressed. UltraPath logs are recorded in host logs, which contain a large number of Info logs. If UltraPath logs are also recorded in Info logs, link exceptions occur. Therefore, you need to find UltraPath-related logs from massive Info information to reduce the fault locating efficiency. To improve fault locating and facilitate problem handling, you need to set the log level of UltraPath to alart information that appears less frequently. UltraPath implements the kernel message exchange mechanism through the kernel functions kmsgsnd and kmsgrcv provided by the AIX operating system. This message mechanism is important. The initialization and loading of the UltraPath driver also depends on this mechanism. Therefore, to trace the message interaction, the interaction process is recorded in syslogs to prevent fault locating. Logs at the alert level are important for locating specific problems. Logs at the alert level are saved in the /opt/UltraPath/log/syslogAlert.log file. If the level is set to info, a large number of logs at the info level are generated in the log file, which may overwrite some key information. You can stop the user-mode program of the scheduled routine test in the background to effectively reduce the frequency of the print. Stopping the program does not affect services.\nData management as a service is a cloud service that centralizes company data. \"As a service\" refers to a pay-per-use business model that doesn't require customers to buy or manage data management infrastructure. In this strategy, the customer backs up data to DMaaS. In the case of cloud data sources, a basic authentication process may be the first step. DMaaS can be given utilizing on-premises infrastructure or the vendor's private cloud, but all infrastructure must be maintained by the vendor to be deemed a service. Logistically and financially, this method of data management as a service is impractical. No company requires customers to install or manage virtual or physical infrastructure to supply or use the service. Companies employing such services notify the vendor (typically via their interface) of their unique demands, such as how many users and how much storage each user needs. The vendor will provide and manage the service's infrastructure. Data management as a service employs cloud services to scale, analyze, and access a company's data. This central data collection provides data protection and other functions. File, application, and database servers, including VM databases, are data sources. Most firms store data in the cloud, on desktops, laptops, and mobile devices. A comprehensive data management solution secures and manages all data sources in a single cloud-based system, although some choices only protect and manage a portion. Proactive compliance, data analytics, legal hold, and centralized search are further services. Many services benefit from a centralized view of all data sources.\nIf an electronic discovery request demands a corporation to find and hold all of an employee's work, a single search across servers, laptops, and cloud instances makes that considerably easier. Services such as backup and archiving, disaster recovery, analytics, and security are increasingly being bundled into a single DMaaS solution. To spot outliers and prevent cyberattacks in progress, these systems employ methods like machine learning. Some of the key factors when you need a good DMaaS: When you need a software-as-a-service (SaaS) platform with a centralized dashboard and an intuitive interface that allows users to access and search data regardless of where it is stored. Data from diverse cloud providers should be supported by the platform. The option to subscribe to many data management services from a single source, rather than using multiple providers for diverse purposes, such as backup, disaster recovery, archiving, files, and development/test environments. Users should be able to connect to the services immediately and begin using them. When you want the data to be managed via a single graphical user interface (GUI) across many deployment options (on-premises, cloud, and edge). Data might be relocated and retrieved from anywhere in those settings. Data classification services to meet compliance and privacy concerns, or machine learning skills to detect intrusions like ransomware, are two examples of cutting-edge cloud services that the platform should be able to access. with data management as a service (DMaaS), your business can subscribe to a variety of data management solutions that can be used for different purposes.\nSmartTier is to automatically store data on different tiers according to specific policies for intelligent data management. SmartTier of OceanStor Pacific series allows users to set tiering policies for namespaces or dtrees. Data can be stored onto different storage media automatically or manually and can be migrated between different tiers based on specific policies, meeting users' diversified requirements for file processing performance, storage capacity, and costs. This helps properly utilize storage space, improve access performance of the storage system, and reduce the overall deployment cost. Feature Principles SmartTier is based on disk pools. That is, data of namespaces can be migrated between disk pools. Storage spaces are classified into three tiers: hot, warm, and cold. Each tier can contain multiple disk pools. The system automatically balances loads among multiple disk pools in a storage tier, including load balancing and capacity balancing. It is recommended that the tier grade of a disk pool be set as follows: When SSDs or SSD cards and NVMe SSDs are used as the main storage, the tier grade of the disk pool is hot. When SAS disks are used as the main storage, the tier grade of the disk pool is warm. When SATA disks are used as the main storage, the tier grade of the disk pool is cold. Smart QOS: I/O traffic control is implemented by LUN-based I/O queue management, token allocation, and dequeue management. LUN-based I/O queue management uses a token mechanism to allocate storage resources.\nTypes of Databases Databases are typically divided into relational and non-relational databases. Among the top 10 databases, youll see both relational and non-relational databases. One type of database is not better than the other, but they suit different needs. Relational databases A relational database (aka SQL database), stores data in tables and rows also referred to as records. This type of database links information from different tables through keys. A key is a unique value in a table that is also known as the primary key. When this key is added to a record located in another table, its called foreign key in this second table. This connection between primary and foreign keys creates a relationship between records within both tables. Some popular relational database management systems (RDBMS) are Oracle, MySQL, SQL Server, and PostgreSQL. Heres a basic schema that shows how a relational database works. To query data in a RDBMS, we use Structured Querying Language (SQL). With SQL we can create new records, update them, and more. This makes the RDBMS good for apps that need transactional functionality, data mining, and complex reporting. Non-Relational databases A non-relational database (aka NoSQL database), stores data without tables, rows, or keys. In other words, a non-relational database stores data in a non-tabular form. This adds some flexibility and helps satisfy specific requirements of the type of data being stored. You can think of a non-relational database as a collection of documents. A document can contain a lot of detailed information about a customer.\nHuawei OceanStor Architecture: Huawei OceanStor is a family of enterprise-level storage solutions that includes all-flash storage, hybrid storage, and software-defined storage (SDS) products. The architecture of Huawei OceanStor can vary depending on the specific product, but generally, it features a distributed architecture with intelligent data management capabilities. Here are some key features of Huawei OceanStor architecture: Distributed architecture Huawei OceanStor uses a distributed architecture that enables it to scale out as more storage is added. The architecture features a distributed file system that allows data to be distributed across multiple nodes and controllers. This improves performance and resiliency by ensuring that there is no single point of failure. Intelligent data management Huawei OceanStor uses intelligent data management techniques to improve storage efficiency and reduce costs. These techniques include data deduplication, compression, and thin provisioning. Additionally, Huawei OceanStor supports tiered storage, which enables data to be automatically moved to the most appropriate storage tier based on its access patterns. High-performance hardware Huawei OceanStor uses high-performance hardware components, such as NVMe flash drives and high-speed interconnects, to deliver low-latency and high-throughput storage performance. Multi-protocol support Huawei OceanStor supports multiple storage protocols, including block, file, and object storage. This enables it to meet the storage needs of a wide range of applications. Virtualization and cloud integration Huawei OceanStor supports virtualization technologies, such as VMware and Hyper-V, and can be integrated with cloud platforms, such as Huawei Cloud, AWS, and Azure. Overall, Huawei OceanStor architecture is designed to deliver high-performance, highly available, and scalable storage solutions for enterprise-level applications.\nSolutions targeting by Oceanstor: Huawei OceanStor is a family of storage solutions that target a wide range of storage needs for different industries, applications, and workloads. Here are some of the solutions targeting by OceanStor: All-Flash Storage: OceanStor all-flash storage solutions are designed for mission-critical applications that require high performance, low latency, and high availability. They are suitable for applications such as database, virtualization, and online transaction processing. Hybrid Storage: OceanStor hybrid storage solutions combine the advantages of flash and hard disk drives to provide high performance and cost-effective storage for a wide range of workloads. They are suitable for applications such as virtual desktop infrastructure, file sharing, and backup and recovery. Software-Defined Storage (SDS): OceanStor SDS solutions offer flexible, scalable, and cost-effective storage solutions that can be deployed on commodity hardware. They are suitable for private, hybrid, and public cloud environments, and can be integrated with leading virtualization and cloud platforms. Object Storage: OceanStor object storage solutions provide scalable, secure, and cost-effective storage for unstructured data, such as images, videos, and documents. They are suitable for applications such as big data analytics, content delivery, and archiving. Data Protection and Disaster Recovery: OceanStor data protection and disaster recovery solutions offer comprehensive data protection and disaster recovery capabilities for mission-critical applications. They include backup and recovery, snapshot, remote replication, and high availability features. Overall, Huawei OceanStor offers a wide range of storage solutions that target different industries, applications, and workloads, providing customers with highly available, scalable, and cost-effective storage solutions.\nOcearnStor Products Lineup: Here are some of the main products in the Huawei OceanStor lineup: All-Flash Storage: OceanStor Dorado All-Flash Storage provides innovative hardware, multi-level reliability, and easy management to accelerate your DX journey. OceanStor Dorado 8000/18000 OceanStor Dorado 5000/6000 OceanStor Dorado 2000/3000 Hybrid Flash Storage: Enjoy your smooth transition to all-flash. OceanStor Hybrid Flash Storage converges resources and simplifies cloud migration to meet demanding performance and reliability requirements. New-Gen OceanStor 18510/18810 New-Gen OceanStor 6810 New-Gen OceanStor 5310/5510/5610 Scale-Out Storage: OceanStor Scale-Out Storage provides efficient processing of diverse data and online services to help unlock the value of mass data. OceanStor Pacific 9950 OceanStor Pacific 9550 OceanStor Pacific 9540 Data Protection: Huawei OceanProtect provides all-scenario data protection covering full disaster recovery, quick backup and restore, and warm archiving. OceanProtect Backup Storage Huawei Data Protection Appliance OceanStor BCManager Hyper-Converged Infrastructure (HCI): The rapid development of the digital economy has caused many enterprises to focus on developing innovative and agile services. As services mature, they increase in volume, making it difficult to implement all elements of the IT infrastructure at the same time. FusionCube 1000 Hypervisor & Data FusionCube 1000 Cabinet Data Center Storage Consolidation Tool Suite: Huawei data center storage consolidation tool suite quickly consolidates storage resources of multiple data centers, improving O&M efficiency and reducing management costs. Data Management Engine DME IQ Intelligent Cloud O&M Platform \"OceanStor Micro\" is a product line of small-capacity storage devices developed by Huawei Technologies Co., Ltd.\nIntroduction for object storage technology Users who frequently access the Internet and use mobile devices often need object storage techniques. The core of object storage is to separate the data path from the control path. Object storage does not provide access to original blocks or files, but to the entire object data via system-specific APIs. You can access objects using HTTP/RESTbased uniform resource locators (URLs), like you access websites using browsers. Object storage abstracts storage locations as URLs so that storage capacity can be expanded in a way that is independent of the underlying storage mechanism. This makes object storage an ideal way to build a large-scale system with high concurrency. As the system grows, object storage can still provide a single namespace. This way, applications or users do not need to worry about which storage system they are using. By using object storage, you do not need to manage multiple storage volumes like using a file system. This greatly reduces O&M workloads. Object storage has many advantages in processing unstructured data over traditional storage and delivers the advantages of both SAN and NAS. It is independent of platforms or locations, offering scalability, security, and data sharing: It can distribute object requests to large-scale storage cluster servers. This enables an inexpensive, reliable, and scalable storage system for massive amounts of data. Other advantages of object storage are as follows: Security: data consistency and content authenticity. Object storage uses special algorithms to generate objects with strong encryption.\nThe Dedicated Distributed Storage Service (DSS) offers you storage pools that are physically isolated from other storage pools to ensure the highest level of security. DSS provides extremely dependable, durable, low-latency, and stable storage resources using data redundancy and cache acceleration methods. By integrating flexibly with diverse compute services, such as Dedicated Computing Cluster (DCC), Elastic Cloud Server (ECS), and Bare Metal Server (BMS), DSS is suited for a variety of use cases, including high-performance computing (HPC), online analytical processing (OLAP), and mixed loads. n the distributed model, instead of keeping data in one spot, it is stored frequently among numerous physical servers called nodes. These nodes can be placed in the same region or even across continents. This type of network is properly called a distributed data store. Distributed data store systems differ from standard data storage in that your data is duplicated (in whole or in part) across numerous servers in a storage network. This creates redundancy for data availability. If a single server is offline or lost, the totality of your data is backed up and distributed over numerous other nodes. Unique algorithms are utilized to distribute and store users data across the node network. This strategy creates two different sorts of dataprimary and secondary data. Primary data is when a node is provided the original, full data collection. Secondary data is when a different node is provided only part of the primary data set as a backup. Which nodes acquire secondary data sets relies on the platforms methodology and technique.\nA storage strategy is a method of storing data that serves a certain purpose. For example, if you need to store volumes and images for a cloud platform like OpenStack, you might choose to store data on moderately performant SAS drives with SSD-based journals. Data management is not \"one-size-fits-all.\" Every organization has data storage and availability issues. A robust data management system strategy must consider these requirements. Organizations should understand the business value of their data and how to employ storage solutions to increase it. About 80% of data collected by organizations is unstructured. Unstructured data can be any size, shape, or form, unlike structured data, which is easy to read, search for, and understand. Thus, huge data management and analysis are difficult. Any downtime can be costly. A good data strategy extends beyond choosing a server location. Disaster recovery, hardware failure, or human error require data backup and easy restoration. Creating a disaster recovery strategy is a solid start and ensures data and linked systems are available with little disruption. Every disaster recovery strategy needs cloud-based disaster recovery and virtualization. They can work together to assure you that no customer will ever encounter more downtime than they can afford at any one moment. By relying entirely on the cloud storage provider, the organization can outsource the storage issue. By employing online data storage, the firm can decrease the expenditures connected with internal resources.\nData deduplication is a procedure that eliminates redundant data copies and dramatically reduces storage capacity requirements. It enables the storing of a single unique instance of all data within a database, with no duplicates taking up unnecessary space. Once the redundant copies of data have been deleted, data deduplication allows you to compress the single copies of data that remain in order to save even more space. To identify duplicate parts of data, one method for deduplicating data employs cryptographic hash algorithms. A collision occurs when two separate bits of information produce the same hash value. The likelihood of a collision is primarily determined by the hash length.As a result, there is fear that data corruption may occur if a hash collision happens and further measures of verification are not employed to determine whether or not there is a difference in data. For guaranteed data integrity, both in-line and post-process architectures may provide bit-for-bit validation of original data. Standard hash functions are employed. The process's computational resource intensity can be a disadvantage of data deduplication. Some systems use both weak and strong hashes to improve performance. Weak hashes are much faster to calculate, but they are more prone to hash collisions. Systems that use weak hashes will then compute a strong hash and use it to determine whether or not the data is the same. It is important to note that the system overhead associated with calculating and seeking up hash values is mostly a result of the deduplication operation.\nDear all, Do you know what are the types of Artificial Intelligence? I will give you a view of those 4 types.The four A.I. types are: Reactive Machines Limited Memory Theory of Mind Self Aware Reactive Machines perform basic operations. This level of AI is the easiest. These types react to certain inputs and to certain outputs. No learning happens. This is the first phase of any artificial intelligence system. Machine learning, which takes a face as input and outputs a box around the face to identify it as a face, is a simple, reactive machine. The model does not store input and does not perform learning. Limited memory types refer to the AI's ability to store previous data and/or predictions and use that data to make better predictions. With limited memory, machine learning architectures become more complex. Each machine learning model requires a limited amount of memory to be created, but models can be deployed as reactive machine types. There are three main machine learning models that enable this limited memory type: These models learn to make better predictions through many cycles of trial and error. This kind of model is used to teach computers how to play games like Chess, Go, and DOTA2. Researchers intuited that past data would help predict the next items in sequences, particularly in language, so they developed a model that used what was called the Long Short Term Memory.\nFor predicting the next elements in a sequence, the LSTM tags more recent information as more important and items further in the past as less important. The E-GAN has memory such that it evolves at every evolution. The model produces a kind of growing thing. Growing things dont take the same path every time, the paths get to be slightly modified because statistics is a math of chance, not a math of exactness. In the modifications, the model may find a better path, a path of least resistance. The next generation of the model mutates and evolves towards the path its ancestor found in error. In a way, the E-GAN creates a simulation similar to how humans have evolved on this planet. Each child, in perfect, successful reproduction, is better equipped to live an extraordinary life than its parent. While every machine learning model is created using limited memory, they dont always become that way when deployed. Limited Memory A.I. works in two ways: A team continuously trains a model on new data. The A.I. environment is built in a way where models are automatically trained and renewed upon model usage and behavior. For a machine learning infrastructure to sustain a limited memory type, the infrastructure requires machine learning to be built-in to its structure. More and more common in the ML lifecycle is Active Learning. The ML Active Learning Cycle has six steps: Training Data. An ML model must have data to train on. Build ML Model. The model is created. Model Predictions.\nIntroduction to Block Storage Block storage commonly uses an architecture that connects storage devices and application servers over a network. This network is used only for data access between servers and storage devices. When there is an access request, data can be transmitted quickly between servers and backend storage devices as needed. From a client's perspective, block storage functions the same way as disks. One can format a disk with any file system and then mount it. A major difference between block storage and file storage is that block storage provides storage spaces only, leaving the rest of the work, such as file system formatting and management, to the client. Block storage uses evenly sized blocks to store structured data. In block storage, data is stored without any metadata. This makes block storage useful when applications need to strictly control the data structure. A most common usage is for database. Databases can read and write structured data faster with raw block devices. Currently, block storage is usually deployed in FC SAN and IP SAN based on the protocols and connectors used. FC SAN uses the Fibre Channel protocol to transmit data between servers (hosts) and storage devices, whereas, IP SAN uses the IP protocol for communication. The FC technology can meet the growing needs for high-speed data transfer between servers and large-capacity storage systems. With the FC protocol, data can be transferred faster with low protocol overheads, while maintaining certain network scalability.\nFile Storage has the following advantages: Offers long-distance data transfer with a high bandwidth and a low transmission bit error rate. Based on the SAN architecture and massive addressable devices, multiple servers can access a storage system over the storage network at the same time, eliminating the need for purchasing storage devices for every server. This reduces the heterogeneity of storage devices and improves storage resource utilization. Protocol-based data transmission can be handled by the HBA, occupying less CPU resources. In a traditional block storage environment, data is transmitted over the fibre channel via block I/Os. To leverage the advantages of FC SAN, enterprises need to purchase additional FC components, such as HBAs and switches. Enterprises usually have an IP network-based architecture. As technologies evolve, block I/Os now can be transmitted over the IP network, which is called IP SAN. With IP SAN, legacy infrastructure can be reused, which is far more economical than investing in a brand new SAN environment. In addition, many remote and disaster recovery solutions are also developed based on the IP network, allowing users to expand the physical scope of their storage infrastructure. Internet SCSI (iSCSI), Fibre Channel over IP (FCIP), and Fibre Channel over Ethernet (FCoE) are the major IP SAN protocols. iSCSI encapsulates SCSI I/Os into IP packets and transmits them over TCP/IP. iSCSI I widely used to connect servers and storage devices because it is cost-effective and easy to implement, especially in environments without FC SAN.\nElastic Volume Service Elastic Volume Service (EVS) offers scalable block storage for cloud servers such as Elastic Cloud Servers (ECSs) and Bare Metal Servers (BMSs). EVS disks offer high reliability and excellent performance. They can be used for distributed file systems, development and testing environments, data warehouse applications, and high-performance computing (HPC). EVS disks are like the hard disks on your local computer, except on the cloud. They need to be attached to cloud servers before you can use them. You can initialize EVS disks, create file systems, and then use them for persistent data storage. Alternatively, you can create backups and snapshots for your EVS disks to improve data reliability. EVS disks include as extreme SSD, ultra-high I/O, general purpose SSD, high I/O, or common I/O types, each type offering different performance characteristics. EVS disks differ in performance and price. Choose the disk type most appropriate for your applications. EVS has the following advantages: Various disk types: EVS disks come in various specifications and can be attached to ECSs as data disks or system disks. You can select EVS disks based on your budget and service requirements. Elastic scalability: The size of a single EVS disk can be anything from 10 GB to 32 TB. As services migrate to the cloud, if a disk no longer meets you needs, you can expand disk capacity in increments as small as 1 GB without stopping services. Aside from the limit on individual disk capacity, space can also be limited by quotas.\nCompany ABC hyper-convergence architecture \"Business-centric Scale-Out architecture\" is adopted while considering this scenario. Huawei Provides Fusion Cube for convergence of resources at medium to enterprise level. As, HCI is a long-standing feature of the data center. Here, networked server systems are provided with software for compute, network, and storage virtualization. To this end, standard servers can be provided with a suitable software stack or preconfigured appliances used. The main benefits relate to simple scaling and uniform operations. Enterprises are generating growing amounts of data across different branches, which often have complex environments, are located in different geographical areas, and manifest unique industry attributes. Therefore, data centers deployed in enterprise branches must be able to adapt to diverse environments and be easy to manage. The global goal of achieving carbon neutrality is causing enterprise IT infrastructure to become greener and intensive. Enterprises are increasingly looking to deploy HCI in their data centers thanks to its agile, elastic and efficient resources, and simplified management. Here are some of the ways adopting hyper-convergence architecture can help organizations: Simplified management: By consolidating storage, computing, and networking into a single system, hyper-convergence can simplify management and reduce the complexity of the data center. Scalability: Hyper-convergence systems are highly scalable, which means that they can easily adapt to changing workloads and accommodate growth without requiring significant hardware upgrades. Improved performance: Hyper-convergence can provide faster and more reliable performance by reducing latency and bottlenecks in the system.\nDear All, Today, I'll introduce Huawei OceanStor Pacific distributed storage purpose-built for emerging applications. OceanStor Pacific improves the efficiency for hybrid workloads, supporting seamless multi-protocol interworking, and providing high bandwidth, IOPS, and OPS, 20% higher than similar products in the industry. OceanStor Pacific also provides optimal ROI, and always-on services. First, let me introduce the SmartBalance system design for hybrid workloads. Likewise, Huawei SmartBalance intelligently senses data types and I/O flows to effectively improve hybrid-workloads efficiency. It adopts two ground-breaking innovations. The first we use is converged indexing for unstructured data. It enables native seamless interworking between file, object, and HDFS storage protocols. The second innovation is data flow adaptive to large and small I/Os, achieving optimal bandwidth, IOPS, and OPS performance. Object storage systems use a flat indexing structure, which enables a single bucket to support hundreds of billions of objects. While file systems use a tree indexing structure, which facilitates file management. However, the number of files supported by a single directory is limited. To combine the advantages of both file and object storage, we have designed converged indexing for unstructured data. This unique design supplements the tree structure with single-layer index slices, enabling single-layer indexing to support hundreds of billions of objects. The converged indexing allows the standard file system, Hadoop distributed file system, and object protocol to simultaneously access the same data without copying it. And all this is achieved without using any physical or logical gateway, and without any function or performance loss either.\nHello, everyone! This post will share with you the HyperMetro and the HyperMetro working principles. HyperMetro is also called an active-active feature. Two data centers are backups for each other in the running status. If a device is faulty in a data center or even the entire center is faulty, the other data center will automatically take over services, solving the problems of traditional DR centers in the switchover. This ensures high data reliability and service continuity and improves the resource utilization of the storage system If the link between two data centers is down or one data center is faulty, data cannot be synchronized between the two data centers in real time. In this case, only a HyperMetro pair or a site of HyperMetro consistency group can continue providing services. For data consistency, HyperMetro adopts an arbitration mechanism to determine service priority in data centers. HyperMetro provides two quorum modes: Static priority mode : applied to scenarios where no quorum server is configured. Quorum server mode ( recommended ): applied to scenarios where a quorum server is configured. Fault Type Result A link between storage systems are down. A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A continue providing services while LUNs in data center B stop providing services. Data center B is faulty. A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A continue providing services while LUNs in data center B stop providing services. Data center A is faulty.\nA HyperMetro pair is in the to-be-synchronized status. LUNs in data center A cannot be accessed and LUNs in data center B stop providing services. A HyperMetro replication link is down and a link between a host and data center B is down. A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A continue providing services while LUNs in data center B stop providing services. Data center B is faulty and the link between the host and data center B is down. A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A continue providing services while LUNs in data center B stop providing services. Links between the host and data centers A and B are concurrently down. A HyperMetro pair is in the normal status. A host fails to access LUNs in both data center A and B. Fault Type Result The quorum server is faulty. A HyperMetro pair is in the normal status. LUNs in data center A and data center B continue providing services A link between a storage system and the quorum (example of the storage system in data center A) is down. A HyperMetro pair is in the normal status. LUNs in data center A and data center B continue providing services. A storage system is faulty (example of the storage system in data center A). A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A are invalid, but LUNs in data center B continue providing services.\nA link between storage systems is down. A HyperMetro pair is in the to-be-synchronized status. LUNs in data center A continue providing services while LUNs in data center B stop providing services. A storage system and the quorum server (example of the storage system in data center A) are concurrently faulty. A HyperMetro pair is in the to-be-synchronized status. Data center A is faulty and LUNs in data center B stop services. Industry Feature Healthcar With the development of hospital services, the growing numbers of beds and new outpatient buildings pose higher requirements on service continuity. Once critical departments such as out-patient, in-patient, and electronic medical records (EMR) are interrupted, medical treatment will be delayed and hospitals will suffer from great economic loss and inestimable damage to their reputation. In addition, an out-patient building is close to an in-patient network information center in the same hospital, and two hospitals in the same city are physically close to each other. HyperMetro can meet their requirements. Finance In the finance industry, banking services, 24-hour ATM services, POS services, and e-bank services are developing quickly as bank services develop. These services require that banking systems process around-the-clock services. For reliability and stability, banks require a solution to store for reused and meet their service construction requirements (RPO = 0, RTO = 0) to ensure business continuity. Service interruptions damage banks' reputations and pose huge pressure on technical departments. Social security In the social security industry, service continuity requirements are high. Monthly settlement and year-end carry-over require 24/7 online operation.\nStorage OceanStor 5600 V5 upgrade risk evaluation. Risks that must be rectified before the upgrade: 1. The current patch V500R007C30SPH187 is a special patch and is incompatible with later general patches. Therefore, need to uninstall the patch and then install the V500R007C30SPH138 patch. . 2. The next BBU periodic discharge test time is 2022-02-23, . Possible risks: 1. For 7 Windows hosts and other hosts that are not collectedInformation. Please check the configuration according to the Host Connectivity Guide. Before the upgrade, make sure that redundant paths exist between hosts and storage devices. (Pathredundancy means for each host needs to have path to storage controller 0A and 0B.) For Vmware ESX: ForWindows: 2. For NAS service, if the switch port already enabled discarding gratuitous ARP packets and DAI validation features, the IP failover may not be recognized by the service side and lead to service interruption. Please check the attachment file to do the check. (Checking and Modifying the ARP SecurityFunctions on Ethernet Switches for NAS Services) 3. If the Oracle database is not installed on the host, ignore this part. Vmware ESX: Risk 1: The Oracle database service times out. (By referring to and in the upgrade guide.) Check method: If a third-party UltraPath or Huawei UltraPath earlier than V1R8 is installed on the host, if the Oracle version is11.2.0.3.0&11.2.0.4.0&12.1.0.1, the ASM disk group is normal or high, and the value of the ASM disk timeout parameter is less than 120 seconds, check the Initiator Timeout Parameter in an iSCSI Network.\nWhat is disk encription technology and methodology used in Huawei Flash Storage? OceanStor Dorado V6 series storage systems support disk encryption, which provides secure storage services without impacting storage performance. The disk encryption function has the following characteristics: Data in all disks is encrypted transparently without affecting other features such as mirroring, snapshot, deduplication, and compression. Automatic key lifecycle management and the Key Management Interoperability Protocol (KMIP) are supported, ensuring the openness of key management systems. If you enable Data Encryption when creating a storage pool, disk encryption is enabled. The storage system activates the AutoLock function on self-encrypting drives (SEDs) and uses the authentication keys (AKs) allocated by the key management server. SED access is protected by the AutoLock function and only the storage system itself can access its SEDs. When the storage system accesses an SED, it acquires an AK from the key management server. If the AK's hash value is consistent with that on the SED, the SED decrypts the data encryption key (DEK) for data encryption/decryption. If the AKs' hash values are different, all read and write operations will fail. If you do not enable Data Encryption when creating a storage pool, disk encryption is disabled and the AutoLock function of SEDs is deactivated. In this case, the SEDs use the default AKs and access to the SEDs is not restricted. The SEDs can be read and written normally. Data written to the SEDs is encrypted using DEKs, regardless of whether Disk Encryption is enabled.\nfor reference please read the below article ; I have selected OceanStor Dorado V6 series storage for your question reference. OceanStor Dorado V6 series storage systems support disk encryption, which provides secure storage services without impacting storage performance. The disk encryption function has the following characteristics: Data in all disks is encrypted transparently without affecting other features such as mirroring, snapshot, deduplication, and compression. Automatic key lifecycle management and the Key Management Interoperability Protocol (KMIP) are supported, ensuring the openness of key management systems. If you enable Data Encryption when creating a storage pool, disk encryption is enabled. The storage system activates the AutoLock function on self-encrypting drives (SEDs) and uses the authentication keys (AKs) allocated by the key management server. SED access is protected by the AutoLock function and only the storage system itself can access its SEDs. When the storage system accesses an SED, it acquires an AK from the key management server. If the AK's hash value is consistent with that on the SED, the SED decrypts the data encryption key (DEK) for data encryption/decryption. If the AKs' hash values are different, all read and write operations will fail. If you do not enable Data Encryption when creating a storage pool, disk encryption is disabled and the AutoLock function of SEDs is deactivated. In this case, the SEDs use the default AKs and access to the SEDs is not restricted. The SEDs can be read and written normally.\nthe overview of Data Recovery. This is extremely helpful for IT & Non-IT users. My first article of this new year as well. Introduction of data recovery : The process of recovering lost, inaccessible, corrupted, or destroyed data is known as data recovery. It can be done using media files on a computer or hard drive as well as secondary storage. It usually starts to be helpful when retrieving data from secondary storage proves to be difficult. This procedure is frequently used to recover data from storage media, including external or internal hard disk drives, solid-state drives, CDs, DVDs, USB flash drives, and other electronic devices. Usually, it is necessary when the storage files are physically harmed. It can also happen if the document's structural integrity is compromised, preventing the host OS from compiling the data. The techniques of data recovery: These methods are based on the kind of storage-related damage that happens. Recycle bin, formatted file, raw, and hard disk lost data recovery problems are all resolved by it. These methods are divided into logical and physical categories. Logical: Physical: It also covers the resurfacing of concealed data. Some forensic computer specialists also recover encrypted data instead of lost data. Recovery specialists can readily recover the data utilizing remote access software via the Internet and do not necessarily need physical access to the software. The importance of data recovery: The data recovery technology successfully retrieves and fixes lost, damaged, deleted, or corrupted data and returns it safely.\nWhat is the EOS of Dorado V6? Hello, friend! EOX release lifecycle milestone Release name Milestone EOS OceanStor Dorado 18000 V6 6.0.0 Dec 31,2022 OceanStor Dorado 18500 V6 6.0.0 Dec 31,2022 OceanStor Dorado 18800 V6 6.0.0 Dec 31,2022 OceanStor Dorado 3000 V6 6.0.0 Dec 31,2022 OceanStor Dorado 5000 V6 6.0.0 Dec 31,2022 OceanStor Dorado 5300 V6 6.0.0 Dec 31,2022 OceanStor Dorado 5500 V6 6.0.0 Dec 31,2022 OceanStor Dorado 5600 V6 6.0.0 Dec 31,2022 OceanStor Dorado 6000 V6 6.0.0 Dec 31,2022 OceanStor Dorado 6800 V6 6.0.0 Dec 31,2022 OceanStor Dorado 8000 V6 6.0.0 Dec 31,2022 Huawei would like to advise that you move or upgrade your products to the newer release in order to continue and enjoy Huaweis high level service. The following table lists the recommended replacement releases. Table 2 Replacement release End of Life release Replacement release OceanStor Dorado 18000 V6 6.0.0 OceanStor Dorado 18000 V6 6.1.0 OceanStor Dorado 18500 V6 6.0.0 OceanStor Dorado 18500 V6 6.1.0 OceanStor Dorado 18800 V6 6.0.0 OceanStor Dorado 18800 V6 6.1.0 OceanStor Dorado 3000 V6 6.0.0 OceanStor Dorado 3000 V6 6.1.0 OceanStor Dorado 5000 V6 6.0.0 OceanStor Dorado 5000 V6 6.1.0 OceanStor Dorado 5300 V6 6.0.0 OceanStor Dorado 5300 V6 6.1.0 OceanStor Dorado 5500 V6 6.0.0 OceanStor Dorado 5500 V6 6.1.0 OceanStor Dorado 5600 V6 6.0.0 OceanStor Dorado 5600 V6 6.1.0 OceanStor Dorado 6000 V6 6.0.0 OceanStor Dorado 6000 V6 6.1.0 OceanStor Dorado 6800 V6 6.0.0 OceanStor Dorado 6800 V6 6.1.0 OceanStor Dorado 8000 V6 6.0.0 OceanStor Dorado 8000 V6 6.1.0 For more details, see . Hope this helps!\nNetworking diagram for backing up VMs using OceanProtect X8000 and Veeam (VMware transport mode: Hot-Add) the cable connections between the controller enclosures and application servers, between the controller enclosures and disk enclosures, and between controller enclosures. Networking description: VMware vSphere: In this best practice, VMs are deployed on five ESXi physical hosts. Use vCenter to four 10GE distributed ports (active/standby mode) for each ESXi physical host. Each host uses four SSDs (3.84 TB for each SSD) as the production storage and the datastores of VMs. Four VMs are planned for each datastore. Each VM contains at least two types of disks, one system disk, and multiple data disks. 500 GB to 1 TB data is preset on each data disk. Backup proxy: In Hot-Add transport mode, each ESXi host uses four VMs as backup proxies. There are 20 backup proxies in total. Gateway server: Four gateway servers are connected to ESXi hosts and OceanProtect X8000 over a 10GE switch. Six 10GE optical fibers are used by each backup proxy to connect the 10GE switch. On each gateway server (Windows host), NIC teaming (with teaming mode set to static teaming and load balancing mode set to dynamic) is configured. Eth-Trunk (mode manual) is configured on the switch. OceanProtect X8000: OceanProtect X8000 is connected to four gateway servers through two 10GE switches. Each gateway server uses six 10GE optical fibers to connect to the 10GE switches.\nHello, everyone! The post will share with you the DAS, NAS, and SAN. Direct attached storage (DAS) is a data storage technology that connects a storage device to a computer through a SCSI interface, Fibre Channel, or SAS interface. DAS can be used to access and manage data and files. Compared with common external storage systems, a DAS system can be easily planned and implemented with lower investments. However, a DAS system has no independent operating system and cannot provide cross-platform file sharing. Different DAS systems are not connected. Data is managed in a distributed manner and cannot be shared. Challenges Description Low Scalability Limited number of ports that can be connected to a host. Limited number of addressable disks. Limited distance Inconvenient Maintenance The system needs to be powered off during maintenance. Insufficient Resource Sharing Front-end ports and storage space are difficult to share. Resource silos: For example, the DAS with insufficient storage space cannot share the remaining space of the DAS with excessive storage resources. Network-attached storage (NAS) connects storage devices to the live network and provides data and file services. The most commonly used network sharing protocols for NAS are Common Internet File System (CIFS) and Network File System (NFS). Improved efficiency Improved flexibility Centralized storage Simplified management High scalability High availability Security (user authentication and authorization) Cloud computing uses the NFS server as the internal shared storage. The file sharing service applies to scenarios such as enterprise file servers and media assets.\nThe storage area network (SAN) is a high-speed network dedicated to storage operations. It is usually independent of the computer local area network (LAN). A SAN connects a host and a storage device and provides a dedicated communication channel for any host and any storage device on the SAN. SAN separates storage devices from servers to share storage resources at the server level. SANs bring channel and network technologies into storage environments, providing a new networked storage solution that meets the requirements of throughput, availability, reliability, scalability, and manageability simultaneously. For more details, see Distributed Storage Networking Frontend service/Tenant network The front-end service/tenant network is used to interconnect the distributed storage with the customer network. It provides the tenant Ul for tenant users to complete operations such as resource application and usage query and processes service requests sent by tenant clients or APIs. Backend storage network The backend storagey/internal management network is used for internal interconnection between nodes. It provides heartbeat communication between high availability (HA) components such as the data service Subsystem (DSS), and internal communication and data interaction between components. Management network The management network is used to interconnect with the customer's maintenance network. lt provides a management UI for the system administrator to perform service operations such as System configuration, tenant management, resource management, and service provisioning, as well as maintenance operations such as alarm, performance, and topology management.\nHello, We are disengaging our Dorado 5000V3 So we wanted to follow this procedure in order to completely destroy the data: For this, we deleted all the LUNs, LUNGroup, HostGroup, MappingView, Hosts, StoragePool and finally DiskDomain thinking that this would allow us to use the \"Disk Data Destruction\" function available at disk level on the System menu of the DeviceManager, but a priori the type of disks we have does not allow us to use this function (NVMe SSD HSSD-D5XXXXXXXXX), the button remains greyed out. We also tried in CLI with the \"change disk erase disk_id_list=?\" command but we got an error in return: \"The disk whose data is to be destroyed is not a self-encrypting disk\" So far the best we have been able to do is: - Recreate a DiskDomain without hot spare policy and with all available disks on the array - Recreate a Storage Pool in RAID6 - Recreate a LUN without compression or dedup with a size corresponding to the entire available capacity - Attach this LUN directly to a host - Launch a command on the host that will clean up the LUN space using the DoD 5220.22-M method (in progress) However, we think this method is not optimal, because it is very slow (it takes 1 week to do the 3 erasing passes) and we have no certainty that all the disks will be erased.\nDear All, Today we are going to learn about FlashLink Raid-TP Huawei Triple Parity RAID-TP is a proprietary RAID storage technology developed by Huawei that provides triple parity data protection for their storage devices. It is designed to offer a high level of data protection while maintaining high storage efficiency. Triple Parity RAID-TP extends the traditional RAID 5 and RAID 6 configurations by adding an additional parity disk to provide triple data redundancy. This allows the system to recover data in case of up to three disk failures, which is significantly higher than traditional RAID 5 and RAID 6 configurations that only support single or double disk failure recovery. Triple Parity RAID-TP is typically used in enterprise environments where data protection is critical, such as financial institutions, healthcare organizations, and government agencies. The use of triple parity protection provides a high level of data protection, which is essential for these organizations to ensure business continuity and minimize the risk of data loss. Flash Link: Raid-TP As an increasing number of customers in the industry begin to use large-capacity SSDs, Huawei develops RAID-TP to prevent data loss due to dual-disk failures during SSD reconstruction. Customers are advised to use this technology when using SSDs with 8 TB or larger capacity. The reconstruction time increases with the SSD capacity. The time required for reconstructing 8 TB disks is 30 hours while that for 16 TB disks is 60 hours.\nDear all, This post describes how to upgrade 5500 V5 with remote replication. User wants to upgrade the storage 5500 V5 to V5R7C71. Product Model Current Version Target Version Model Controller Disk domain Storage pool LUN(Mapped) Disk enclosure Host Filesystem Value-added 5500 V5 (1) 2 2 3 28(18) 2 6*Vmware ESX +4*Windows Server 2012+1*Linux+1* Windows FC-SAN 4 Created Asynchronous Fs Remote Replication Number:1 5500 V5 (2) 2 2 3 25(13) 2 6*Vmware ESX +1*Windows Server 2012+1*Linux FC-SAN 4 Created Asynchronous Fs Remote Replication Number:1 1. Please ensure that the tool ArrayUpgrade has the latest common version before the upgrade operation, re-evaluate and ensure that there are no new risks that affect the upgrade, then perform the upgrade. If the tool has not been updated to the latest version, import the tool package. SmartKit_22.0.0.12_Tool_ArrayUpgrade.zip 2. Split the remote replication before the upgrade. 3. For some (CIFS)NAS business, during upgrade there may be business hang or interrupt during the upgrade. The client retry of the business will recover after upgrade. 1. Information about all hosts is not collected. Please check the configuration according to the Host Connectivity Guide. Before the upgrade, make sure that redundant paths exist between hosts and storage device. (Path redundancy means that each host needs to have path to storage controller 0A and 0B.) For Vmware ESX: For Windows: For Linux: 2. If the version of ESXi host is 5.5.x or 6.0.x, the driver version of EmulexLPe12000/12002/15000/16000/16002/3100x/3200x should be above 11.1.145.18, otherwise the business may be interrupted during the upgrade.\nIf the Emulex driver version of the host is earlier than the target version, you are advised to login to the VMware official website: and download the latest driver to upgrade the HBA. (For details, see Step 1 to Step 7 of Check Method in 10.1.3 Checking Risky Emulex HBA Driver Versions in a Fibre Channel Network in the upgrade guide.) 3. If there is Redhat or CentOS running with NAS business and the kernel version is lower than 3.10.0-957.5.1.el7, there is a kernel bug which may cause the business interruption, please upgrade kernel version to 3.10.0-957.5.1.el7 or a later version before upgrading the storage array. The kernel bug link: 4. In the Suse11sp3 (Kernel version earlier than 3.0.101-0.47.106.59.1) + scenario where ALUA is configured, I/Os are zeroing due to system kernel errors. Replace the host system or upgrade the system kernel to 3.0.101-0.47.106.59.1 or later. 5. The next BBU periodic discharge test time is 2023-01-28, avoid the upgrade time window. 6. Information about hosts is not collected, the following risk exists. If the Oracle database is not installed on the host, ignore this part. Vmware ESX: The default timeout interval for the HBA card of the ESXi host is 10 seconds, which is not risky. If the timeout duration queried in the previous step is longer than 10 seconds, change the timeout duration to 10 seconds to reduce the impact on host I/Os during the upgrade. (By referring to 10.1.5.1 VMwareESX) Windows: Risk 1: The Oracle database service times out.\n(By referring to 10.1.4 Checking the Oracle Heartbeat Parameter, 10.1.5 Checking the HBA Timeout Parameter in a Fibre Channel Network.) Check method: If a third-party UltraPath or Huawei UltraPath earlier than V1R8 is installed on the host, if the Oracle version is 11.2.0.3.0&11.2.0.4.0&12.1.0.1, the ASM disk group is normal or high, and the value of the ASM disk timeout parameter is less than 120 seconds, check the HBA Timeout Parameter in a Fibre Channel Network. Workaround: Rectify the HBA timeout parameter in Linux (by referring to 10.1.5.2 Windows) or OracleASM disk timeout duration of the host (by referring to Modifying the Oracle ASM Timeout Parameter). Linux: Risk 1: The Oracle database service times out. (By referring to 10.1.4 Checking the Oracle Heartbeat Parameter, 10.1.5 Checking the HBA Timeout Parameter in a Fibre Channel Network) Check method: If a third-partyUltraPath or Huawei UltraPath earlier than V1R8 is installed on the host, if the Oracle version is 11.2.0.3.0&11.2.0.4.0&12.1.0.1, the ASM disk group is normal or high, and the value of the ASM disk timeout parameter is less than120 seconds, check the HBA Timeout Parameter in a Fibre Channel Network. Workaround: Rectify the HBA timeout parameter in Linux (by referring to 10.1.5.3 Linux) or Oracle ASMdisk timeout duration of the host (by referring to Modifying the OracleASM Timeout Parameter). Risk 2: Non-aggregated drive letter binding risk. If using a third-party Multipathing or a version earlier than Huawei UltraPathV100R008 is installed, the aggregated drive letter is inconsistent with the path drive letter.\nThe 2021 Innovative Data Infrastructure Forum with the theme of Hainer Data, Navigating the YB Era kicked off, and Huawei officially released six new data storage products. Huaweis new OceanStor Dorado all-flash storage Huawei stated that the brand-new OceanStor Dorado all-flash storage released at this forum will fully develop the NAS market on the basis of the leading SAN. Rich security features represented by quotas, QoS, and anti-ransomware realize the secure sharing of files across departments. Huaweis next-generation OceanStor hybrid flash storage OceanStor hybrid flash storage defines five integrations: storage and computing integration, multi-protocol integration, warm data integration, cross-generation integration, and multi-cloud integration, achieving high-performance, multi-functional compatibility, and providing more efficient data for inclusive diversified scenarios Pedestal. Huawei OceanProtect dedicated backup storage Huawei has fully upgraded OceanProtects dedicated backup storage products to achieve 3 times the industrys backup bandwidth, 5 times the recovery bandwidth, 72:1 data reduction rate, and a comprehensive anti-ransomware solution to help users achieve efficient backup and recovery, and build the last way to protect data Line of defense. Huaweis new OceanStor Pacific series of distributed storage The OceanStorPacific series of distributed storage is the only one in the industry that supports mixed loads and realizes a set of storage that supports diversified applications such as HPDA, big data, video, backup, and archive.\nHello everyone! This post will give you an overview ofAsymmetric Logical Unit Access (ALUA). Please find more information displayed below. Asymmetric Logical Unit Access (ALUA) is a multi-target port access model. In a multipathing state, the ALUA model provides a way of presenting active/passive LUNs to a host and offers a port status switching interface to switch over the working controller. For example, when a host multipathing program that supports ALUA detects a port status change (the port becomes unavailable) on a faulty controller, the program will automatically switch subsequent I/Os to the other controller. In an ALUA architecture, each LUN is owned by a specific controller. Customers need to plan the owning controllers of LUNs for load balancing. In an ALUA architecture, read and write requests to a LUN must be processed by the LUN's owning controller. When ALUA works, the host multipathing software classifies the physical paths to disks as Active Optimized (AO) and Active Non-optimized (AN) paths. The host preferentially delivers services to the storage system via the AO paths. An AO path is the optimal I/O access path between the host and the owning controller of a LUN. An AN path is the suboptimal I/O access path between the host and a non-owning controller. If an AO path fails, the host will deliver I/Os to another AO path. If all AO paths on the owning controller fail, the host will deliver I/Os to the AN paths on the non-owning controller, as shown in \"Path failure\" in Figure 1.\nIf a LUN's owning controller fails, the system will activate the other controller as the new owning controller, as shown in \"SP failure\" in Figure 1. Figure 1. Failover in non-HyperMetro scenarios When HyperMetro works in load balancing mode, the host multipathing software defines the paths to the owning controllers on both HyperMetro storage arrays as AO paths, and those to the other controllers as AN paths. The host accesses the storage arrays via the AO paths. If an AO path fails, the host will deliver I/Os to another AO path. If a LUN's owning controller fails, the system will activate the other controller to maintain load balancing. Figure 2. Load balancing mode When HyperMetro works in local preferred mode, the host multipathing software defines the paths to the owning controller on the local storage array as AO paths. This ensures that the host delivers I/Os only to the owning controller on the local storage array, reducing link consumption. If all AO paths fail, the host will deliver I/Os to the AN paths on the non-owning controller. If the owning controller of the local storage array fails, the system will activate the other controller to maintain the local preferred mode. Figure 3. Local preferred mode Old-version Huawei storage supports ALUA only in dual-controller configuration, but not in multi-controller or HyperMetro configuration. New-version Huawei storage supports ALUA in dual-controller, multi-controller, and HyperMetro configurations.\nStorage Type Version Remarks Old-version Huawei storage (namely, storage that does not support multi-controller ALUA or ALUA HyperMetro) OceanStor T V1/T V2/18000 V1/V300R001/V300R002/V300R003 /V300R005/Dorado V300R001C00 - New-version Huawei storage (namely, storage that supports multi-controller ALUA and ALUA HyperMetro) OceanStor V500R007C00 and later versions OceanStor V300R006C00 and later versions Dorado V300R001C01 and later versions V300R006C00: refers to only V300R006C00SPC100 and later versions. Dorado V300R001C01: refers to only V300R001C01SPC100 and later versions. ALUA is mainly applicable to a storage system that has only one prior LUN controller. All host I/Os can be routed through different controllers to the working controller for execution. ALUA will instruct the hosts to deliver I/Os preferentially from the LUN working controller, thereby reducing the I/O routing-consumed resources on the non-working controllers. If all I/O paths of the LUN working controller are disconnected, the host I/Os will be delivered only from a non-working controller and then routed to the working controller for execution. To prevent I/Os from being delivered to a non-working controller, you are advised to ensure that: LUN home/working controllers are evenly distributed on storage systems so that host service I/Os are delivered to multiple controllers for load balancing. Hosts always try the best to select the optimal path to deliver I/Os even after an I/O path switchover. It is recommended that Huawei storage use ALUA to interconnect with host native multipathing due to the following reasons: Huawei storage systems are not all-AA storage systems.\nIn multi-controller models, especially those with eight or more controllers, if ALUA is not used, a large number of I/Os will be forwarded between controllers, increasing I/O latency overhead. Huawei storage does not support cross-engine LUN switchover. Therefore, implicit ALUA is used. That is, LUN switchover is not supported by host multipathing. When the host multipathing uses the fixed or failover policy, enabling ALUA instructs the host to select a path of the working controller as the I/O path. When an I/O path fails, you can select a path from the remaining path on the working controller as a new I/O path. If ALUA is not used, you may select a path on a non-working controller as an I/O path. In the active-active scenario of Huawei storage, LUNs are in the shutdown state. For I/Os delivered by hosts from the downtime site, the storage system returns a LUN downtime error code. This error code is a non-standard protocol processing error code (only for Huawei-developed multipathing software). Many native multipathing software cannot identify this error code, which may cause service interruption. ALUA, however, is based on the SCSI standard protocol and has standard command sets and error codes. These error codes can adapt to various host multipath processing mechanisms to implement path failover and recovery. In the active-active remote scenario, enabling ALUA ensures that host services deliver I/Os only from the local site. If ALUA is disabled, the host may select the remote site to deliver I/Os.\nHuawei storage initiators have multiple third-party multipathing switchover modes to adapt to different host OSs and scenarios. The storage system must be compatible with earlier versions that do not support active-active multi-controllers. The earlier versions support only explicit and implicit ALUA in dual-controller scenarios. Some host multipathing software does not support ALUA (for example, HP 11.1i PVlinks), or customers do not want to use ALUA. Therefore, a mode selection of \"without ALUA\" is provided. l When ALUA is enabled, different hosts send RTPG messages to query the stored port group information. The expected content is different. For example, for Linux, only the RTPG command is required to return the port group information of the current link. For ESXi and AIX hosts, the port group information of all controllers needs to be returned. If HyperMetro is used, the port group information of controllers at all sites needs to be returned. When a HyperMetro LUN stops working, multipathing on different hosts processes the error codes returned by the storage system differently. The error codes are mainly caused by I/O switchover when a HyperMetro LUN is faulty and path reconnection when a HyperMetro LUN is restored. Even if the same operating system is used, different minor versions or upgrade versions store different data. Therefore, different error codes for HyperMetro LUN faults need to be returned based on the host version to adapt to the host processing mechanism.\nAs a non-full-AA storage system, when ALUA is enabled, the performance of a Huawei storage system is related to the service scenario and host routing policy configuration. To achieve optimal performance, ensure that: 1. All LUNs are evenly distributed on controllers. 2. Host services and applications should not be distributed on only one LUN. 3. Switch the working controller of the LUN to avoid excessive pressure on a single controller. In addition, the performance of front-end links should be prevented from reaching bottlenecks. When Round-Robin is used for host multipathing, links can be added to reduce link pressure. When the CPU pressure of the working controller of a LUN is high, resources are insufficient, or the transmission bandwidth of its link reaches the bottleneck, the I/O performance loss may be greater than the transmission performance loss of the mirror link. Therefore, the performance of non-ALUA in these scenarios may be better than that of ALUA enabled. Name Description ALUA Asymmetric Logic Unit Access Port group A group of ports with the same attributes on a distributed virtual switch (DVS) or virtual software switch (VSS). In a hypervisor, all DVS settings, such as Virtual Local Area Network (VLAN) and network flow control, are configured on a port group basis. All ports of each controller form a port group. Preferred path Optimal path for UltraPath to deliver I/Os based on LUNs Non-preferred path Alternate path for UltraPath to deliver I/Os based on LUNs. AO active/opitimal Indicates the optimal access status of the port group.\nDear all, This post will give you an overview of the Network File System (NFS). We will talk about what is NFS, the versions of NFS, and how it works... NFS (Network File System)is a protocol developed by Sun. IETF (Internet Engineering Task Force) is in charge of developing its new versions. This protocol is designed for file sharing between Linux and UNIX operating systems. NFS works based on the client/server architecture. A server provides clients with file system access, whereas client access shared file systems. NFS enables clients running different operating systems to share files over a network. Storage systems support NFS, enabling users to flexibly and easily use clients and configure desired environments. When being configured as an NFS server, a storage system provides shared file system access for clients that use NFS. NFS allows users to centrally store data in the storage system and access remote file systems in the same way as accessing local files over a network, reducing local disk space required. The NFS feature enables clients running a variety of operating systems to share files over a network. It applies to a wide range of network environments, including the non-domain environment, Lightweight Directory Access Protocol (LDAP) domain environment, and network information service (NIS) domain environment. NFS shares in a non-domain environment are commonly used for small- and medium-sized enterprises. Figure 1 shows the networking. On the network, the storage system serves as an NFS server and employs the NFS protocol to provide shared file system access for clients.\nAfter the clients map the shared files to the local directories, users can access the files on the server as if they are accessing local files. Clients whose IP addresses are configured in the storage system are allowed to access the shared file system. Figure 1 NFS share in a non-domain environment Domains enable accounts, applications, and networks to be centrally managed. In Linux, LDAP and NIS domains are available. LDAP is an open, extendable network protocol. The purpose of LDAP-based authentication applications is to set up a directory-oriented user authentication system, specifically, an LDAP domain. When a client user needs to access applications in an LDAP domain, the LDAP server compares the user name and password sent by the client with corresponding authentication information in the directory database for identity verification. NIS is a directory service technology that enables central management of system databases. It provides a yellow page function to support the centralized management of network information. NIS works based on the client/server architecture. When the user name and password of a user are saved in the NIS server database, the user can log in to an NIS client and maintain the database to centrally manage the network information on the LAN. As shown in Figure 2, when a client needs to access an NFS share provided by a storage system in a domain environment, the storage system uses the domain server network group to authenticate the accessible IP address, ensuring the reliability of file system data.\nFigure 2 NFS share in a domain environment Multiple clients can use the same file, which allows everyone on the network to use the same data and access that data on a remote host as if it were a local file. Computers share applications, which eliminates the need for local disk space and reduces storage costs. All users can read the same file, so the data is up to date and consistent and reliable. Mounting a file system is transparent to all users. Support for heterogeneous environments allows you to run mixed technologies from multiple vendors and use interoperable components. System administrator overhead is reduced due to data centralization. It's always nice to have fewer removable disks and drives around to reduce security issues. Our Oceastor Dorado V6storage systems support NFSv3, NFSv4.0, and NFSv4.1. NFS protocol versions: NFS Protocol Version Supported Storage System Version NFSv3 6.1.0 and later NFSv4.0 6.1.3 and later NFSv4.1 6.1.2 and later Performance of NFS protocol versions: NFS Protocol Version Performance NFSv3 NFSv3 does not maintain persistent sessions between hosts and storage devices. Concurrent stateless short connections are established for I/O interactions. Its performance is better than that of NFSv4.0 and NFSv4.1. NFSv4.0 NFSv4.0 enhances the connection security and maintains persistent session connections. Its performance is only about 30% of that of NFSv3 in small file scenarios, and is basically the same in large file scenarios. NFSv4.1 NFSv4.1 optimizes the performance in small file scenarios and supports concurrent I/O processing with persistent connections.\n/FFT uses fat file timing instead of NTFS. This means the granularity is a bit less precise. For across-network share operations this seems to be much more reliable - just don't rely on the file timings to be completely precise to the second. /Z ensures Robocopy can resume the transfer of a large file in mid-file instead of restarting. /XA:H makes Robocopy ignore hidden files, usually these will be system files that we're not interested in. /W:5 reduces the wait time between failures to 5 seconds instead of the 30 second default. #7 Copy all changes Use Robocopy to copy all changes to files in a directory called c:\\data to a directory that contains the date, like data_20091124. Create a batch file as follows. off set day=e:~0,2% set month=e:~3,2% set year=e:~6,4% Robocopy \"c:\\data\" \"c:\\backup\\data\\%-%month%-%year%\\\" /MAXAGE:1 #8 Mirror directory excl. deletion To mirror the directory \"C:\\directory\" to \"\\\\server2\\directory\" excluding \\\\server2\\directory\\dir2\" from being deleted (since it isn't present in C:\\directory) use the following command: Robocopy \"C:\\Folder\" \"\\\\Machine2\\Folder\" /MIR /XD \\\\server2\\ directory\\dir2\" Robocopy can be setup as a simply Scheduled Task that runs daily, hourly, weekly etc. Note that Robocopy also contains a switch that will make Robocopy monitor the source for changes and invoke synchronization each time a configurable number of changes has been made. This may work in your scenario, but be aware that Robocopy will not just copy the changes, it will scan the complete directory structure just like a normal mirroring procedure. If there are a lot of files & directories, this may hamper performance.\n[Shenzhen, China, February 14, 2023] DCIG, a leading technology analysis and research firm, recognized Huawei OceanStor Dorado 18000 and 8000 as top high-end storage arrays in the report. Based on research covering 14 products that met DCIG's criteria for high-end storage arrays, Huawei's OceanStor Dorado earned plaudits for continuous innovation in reliability, usability, and resilience. Huawei OceanStor Dorado 18000 and 8000 named the 2023-24 DCIG TOP 5 High-End Storage Arrays DCIG TOP 5 is an annual research report featuring a comprehensive analysis of the industry's most competitive products and solutions. It takes into consideration the main indicators that guide customers when procuring a product, namely, business benefits, efficiency, and data reliability. Huawei OceanStor Dorado 18000 and 8000 proved their worth in multiple ways. The storage systems set a new benchmark in reliability. Running the SmartMatrix full-mesh architecture, the systems are the industry's only offerings that ensure service continuity in the event of a failure of seven out of eight controllers or one out of two controller enclosures. Combined with the innovative FlashLink intelligent algorithm, the systems deliver ultra-fast file and block performance across controllers, enclosures, and SSDs. In terms of usability, OceanStor Dorado 18000 and 8000 provide SmartIDC built for data center storage infrastructure to help customers streamline and optimize the configuration and performance of storage systems, hosts, applications, and disaster recovery. In addition, the SmartVirtualization eliminates data silos by integrating heterogeneous storage systems, simplifying storage management, data migrations, and disaster recovery.\nDear All today we will discuss about Huawei Fusion Storage DHT Overview FusionStorage is a distributed storage software. FusionStorage can virtualize local HDD and SSD devices on general x86 servers into a large-scale storage pool using the distributed technology. FusionStorage provides upper applications and VMs with standard SCSI and iSCSI APIs. FusionStorage provides open APIs. Traditional Server SAN Architecture Engine bottleneck: dual controllers to 16 controllers, not supporting linear scaling Cache bottleneck: the cache capacity (generally in GB) Network bottleneck: 10GE and 8G bit/s FC Distributed Server SAN Architecture Distributed controllers: 4096 nodes can be involved. Distributed cache: The capacity of cache can reach to TB-level. Convergence-free P2P high speed network and 56 Gbit/s InfiniBand RDMA FusionStorage Application Scenarios Basic Concept and key working Principles DHT (distributed hash table): indicates the hash routing algorithm in FusionStorage. Partition: indicates the data partition for a fixed Hash module in a DHT ring. Key-Value: Disk data is divided into Key-Values. Each Key-Value represents a data block . DHT (distributed hash table): indicates the hash routing algorithm in FusionStorage. Partition: indicates the data partition for a fixed Hash module in a DHT ring. Key-Value: Disk data is divided into Key-Values. Each Key-Value represents a data block. Volume: indicates a consecutive LBA addressing presented to an application. Resource pool: a storage pool consisting of a group of partitions which are mapped to a DHT ring in FusionStorage. Volume: indicates a consecutive Logical Block Addressing (LBA) addressing.\nWhich of the following statements is correct about serial and parallel transmission? A With low speed and low-cost parallel transmission is suitable for short distance transmission. B With fast speed but high-cost parallel transmission is suitable for long distance transmission. C With low speed and low-cost serial transmission is suitable for long distance transmission. D With fast speed but high-cost serial transmission is suitable for short distance transmission. Hi, Answer is A, With low speed and low-cost parallel transmission is suitable for short distance transmission. Dear friend, I think the answer is A. What is serial transmission? In telecommunications and data transmission, the continuous transmission of one bit at a time through a communication channel or a computer bus. Parallel communication, on the other hand, provides multiple bits as a single unit through a network having many similar channels. In serial transmission, eight bits are transmitted at a time, with a start bit and a stop bit. All telecommunication and most computer networks use serial communication. Serial computer buses are becoming more common, even over shorter distances, as the signal integrity and transfer speed of newer serial technologies have begun to outweigh the simplicity of parallel buses. Most communication systems use serial mode. Serial networks can be extended to great distances for less money because fewer physical wires are required. What is parallel transmission? Parallel communication is a means of transmitting multiple binary bits (bits) simultaneously in data transmission.\nDear All, Today we are going to discuss about Dorado V6 software architecture. Huawei Dorado is Premium all-flash storage systems designed for mission-critical workloads, suitable for the industry, scale, or strategy of any business simplifying the first steps toward digital transformation. Excellent performance: FlashLink realizes efficient I/O scheduling, providing high performance and low system latency. Stable and reliable: Innovative RAID algorithms, value-added features, and multilevel reliability solutions ensure 99.9999% reliability and 24/7 stable service system operation. Efficient: Multiple efficiency-improving features, such as heterogeneous virtualization and inline deduplication and compression, protect customers' investments. SAS protocol stack: After an I/O request is initiated from a block device, it reaches SSDs through a SAS link after the encapsulation of two layers (SCSI and SAS protocols). NVMe protocol stack: After an I/O request is initiated from a block device, it reaches SSDs through a SAS link after the encapsulation of only one layer (the NVMe protocol). The NVMe protocol stack is simpler by reducing one layer encapsulation. The protocol encapsulation overhead is cut by a half. Protocol encapsulation consumes CPU resources and increases I/O transmission latency. SAS and NVMe back-end protocols treat write requests differently. For an application program's write request: SAS: The SCSI (SAS back end) protocol completes one write operation after four interactions. NVMe: The NVMe protocol completes one write operation after only two interactions. Therefore, the NVMe protocol is twice as efficient as the SCSI protocol in its processing of write requests.\nRAID 2.0+ divides all SSDs in the storage system into small virtual blocks for unified management, achieving the highest possible efficiency. It is like to build the best highway. I/O priority adjustment ensures that read and write I/Os in the controller and system have the highest priority than other I/Os. In the highway analogy, they would be the ambulances; all the other vehicles give way to them. Cold/Hot data partitioning: Metadata and data are stored separately in the controller and system to accelerate data access. This is just like how highways are divided into lanes allowing different speeds. Sequential write of large blocks enables controllers to detect the SSD data layout, aggregate disparate I/Os into large sequential data blocks, and centrally write them into SSDs. This is like to build a multilane highway instead of a single-lane highway. Infrastructure layer: RAID 2.0+ divides disk space into virtual blocks. Service loads are balanced on these blocks to achieve fast reconstruction. RAID 2.0+ is the foundation for the high performance and robust reliability of Dorado. Data distribution layer: Adjusts data distribution in the storage system and on SSDs. Hot/Cold data partitioning stores hot and cold data separately to minimize the data amount in garbage collection, greatly elevating system reliability and performance. Global wear leveling and Huawei-patented anti-wear leveling intelligently adjust the amount of data written to SSDs, thereby extending the service life of SSDs.\nDear All, Today we are going to discuse about Huawei DeviceManger developed by Huawei. Huawei Device Manager is a robust and reliable data storage management solution offered by Huawei, one of the leading providers of data center storage solutions. The Huawei Device Manager is specifically designed to manage storage resources in data centers, providing centralized storage management, performance optimization, and efficient resource utilization. DeviceManager main window The device manager provides an easy-to-use graphical user interface that makes storage management simple and intuitive, enabling IT administrators to monitor and manage the entire storage environment from a single console. The system supports a wide range of storage resources, including network-attached storage (NAS), storage area networks (SANs), and object storage. The Huawei Device Manager provides a wealth of performance optimization features, including real-time monitoring, automatic resource allocation, and proactive performance tuning. With real-time monitoring, administrators can quickly identify performance bottlenecks and resolve issues before they impact business operations. The automatic resource allocation feature enables the device manager to dynamically allocate resources to ensure optimal performance and efficient resource utilization. The device manager also provides a robust data protection and disaster recovery solution, ensuring that critical data is always protected and available. The system supports real-time data replication, snapshots, and backups, enabling administrators to quickly restore data in the event of a disaster. Functions on DeviceManager Application Type Function LUN Groupsallows you to create and manage LUN groups. LUNsallows you to create and manage LUNs. Hostsallows you to create and manage hosts.\nPort Groupsallows you to create and manage port groups for service access. SmartQoSdynamically allocates storage system resources to meet the performance objectives of applications. SmartMigrationmigrates services on a source LUN transparently to a target LUN without interrupting host services. After the migration, the target LUN can replace the source LUN to carry the services. Protection Groupsallows you to create and manage protection groups as protected objects and enable data protection for them. LUNsallows you to manage data protection for LUNs. HyperCDPquickly and intensively generates a data copy that preserves the state of source data at a certain point in time. Remote Devicesallows you to create remote links among storage systems and transfer data. HyperMetro Domainsallows you to manage and configure HyperMetro domains, enabling the HyperMetro feature of a storage system. Alarms and Eventsallows you to view alarms and logs. Performanceallows you to monitor and analyze device performance in real time. Forecastallows you to view capacity forecast information of a storage system. Report Exportallows you to export a performance monitoring report of a storage system. Task Centerallows you to manage current and historical system tasks. Hardwareallows you to monitor storage devices and their components, and set related parameters. External Storageallows you to add external storage devices and manage external LUNs. Storage Poolsallows you to integrate storage space of disks for unified management. Basic Informationallows you to configure the storage device time and DNS service. Block Storage Serviceallows you to set the iSCSI device name, iSCSI initiator name, and iSNS service.\nHello, everyone! In my first post this year I would like to show you all the HCIA-Storage materials I have, hoping they will help you in your studies. HUAWEI CERTIFIED ICT ASSOCIATE-STORAGE Training and certificating engineers with basic knowledge and product O&M skills in the storage field. The below table summarizes the experience shared by Huawei HCIP-AI experts: No.\nHello, everyone! This post will share with you the definitions and principles of SAS and SATA. SAS is the serial standard of the SCSI bus protocol. SAS uses serial technology to achieve a higher transmission rate and better scalability and is compatible with SATA disks. SAS adopts the point-to-point architecture to achieve a transmission rate of up to 3 Gbit/s, 6Gbit/s, 12 Gbit/s, or higher. The full-duplex mode is supported. The parallel bus has been developed to the peak and the bandwidth limit is reached. Serial buses such as FC, IB, and the Ethernet have the following disadvantages for storage applications: FC: It is expensive and applicable to complex networking and long-distance scenarios. IB: It is expensive, and the networking is complex. ISCSI: The latency is high, and the transmission rate is Low. Provides the serial communication mode to allow multiple data channels to communicate with devices at full speed. Binds multiple narrow ports to form a wide port. Uses expanders to expand interfaces, providing excellent scalability. Works in full-duplex mode. SAS uses expanders to expand interfaces. One SAS domain supports a maximum of 16,384 disk devices. Generally, a SAS cable has four channels, each of which supports 12 Gbit/s bandwidth. SAS devices are connected in the form of a loop (also called a chain). A cable supports 4x 12 Gbit/s bandwidth, which limits the number of disks in the loop. A maximum of 168 disks is supported in a loop.\nHello, everyone! This post will share with you the definitions and principles of CIFS, NFS and NDMP. In 1996, Microsoft renamed SMB to CIFS and added many new functions. Now, CIFS includes SMB1, SMB2, AND SMB3.0. CIFS uses the C/S mode and basic network protocols including TCP/IP and 1PX/SPX. NFS is short for Network File System. The network file sharing protocol is defined by the IETF and widely used in the Linux/Unix environment. NFS works based on the client/server architecture. The servers provide the clients with access to shared file systems. NFS enables clients using different operating systems to share files over a network. Theoretically, RPC data can be transmitted over IP/Ethernet or IP/InfiniBand, as well as RDMA/Ethernet or RDMA/IP/InfiniBand. By July 2015, the OceanStor V5 still does not support NFS over RDMA. NDMP protocol is designed for the data backup system of NAS devices. It enables NAS devices to directly send data to the connected disk devices or the backup servers on the network for backup, without any backup client agent required. There are two networking modes for NDMP 2-way 3-way In 2-way networking, the backup media is directly connected to the NAS storage system instead of the backup server. When a backup server performs a backup operation, the backup server sends the backup command to the NAS storage device through the Ethernet. The NAS storage device directly backs up data to the tape library connected to the backup server.\nHello, everyone! The post will share with you the storage system architecture evolution. Fixed storage system configuration: Limited Fibre Channel interfaces are provided, and the flexibility is poor. Capacity expansion can be implemented only by cascading disk enclosures. Dual-controller active-active redundancy configuration: The single-controller and dual-controller active-passive architecture gradually evolve to the active-active architecture. Flexible hardware component configuration: Fast Ethernet(FE) interface modules are supported, greatly improving flexibility and scalability. The number of ports can be selected as required. Flexible software function configuration: Unified storage that supports both SAN and NAS protocols has become a hotspot. Users can flexibly configure multi-protocol services as required. Bus architecture : Scale-up multi-controller architecture based on the bus; interconnection and upgrade by using more powerful CPUs, interface modules, memory, and protocols. Hi-Star architecture : Switch-based connection of front-end interfaces, back-end disk interfaces, and cache modules, and back-end Fibre Channel connection. Direct-connection architecture : Front-end interfaces and back-end disk interfaces directly connected to cache resources to avoid latency caused by bus and switch connections. Virtual matrix architecture : Scale-out expansion mode, full switching mode, x86 platform, and loose coupling. Improved data reliability : Emergence of snapshots, clones, and data replication(synchronous and asynchronous) technologies. Simplified data management : Storage devices are flexibly managed by centralized management software. Thin provisioning technology resolves the conflict between resource investment and optimal use. Improved space utilization : Tiered storage promotes storage space utilization and Service efficiency. The data deduplication technology reduces maintenance and capacity expansion Costs. Optimized service performance : Performance is optimized according to service types.\nHello, everyone In this post, we can discuss the storage network technologies IP SAN and FC SAN. The storage area network (SAN) is a high-speed network dedicated to storage operations. It is usually independent of the computer local area network (LAN). A SAN connects a host and a storage device and provides a dedicated communication channel for any host and any storage device on the SAN. SAN separates storage devices from servers to share storage resources at the server level. SANs bring channel and network technologies into storage environments, providing a new networked storage solution that meets the requirements of throughput, availability, reliability, scalability, and manageability simultaneously. In brief, IP-SAN (IP storage) uses IP channels instead of Fibre Channel to connect servers and storage devices. In addition to iSCSI, standards such as FCIP and iFCP are being developed. iSCSI is the fastest growing network and has become a powerful representative of IP storage. Like Fibre Channel, IP storage is interchangeable, but unlike Fibre Channel, IP networks are mature and do not have interoperability problems, which is the biggest headache for Fibre Channel SANs. IP has been widely recognized in the IT industry, and there are many network management software and service products available. Logical ports are created based on bond ports, VLAN ports, or Ethernet ports. The logical ports are Virtual ports that carry host services. A unique IP address is allocated to each logical port for carrying its services. No. Description 1 Indicates that multiple Ethernet ports are bonded to form a bond port.\n2 Indicates that an Ethernet port is added to multiple VLANs. 3 Indicates that a bond port is added to multiple VLANs. 4 Indicates that a bond port is used to create multiple logical ports. 5 Indicates that a VLAN port is used to create multiple logical ports. 6 Indicates that an Ethernet port is used to create multiple logical ports. VLAN is a technology that logically divides a physical LAN into multiple broadcast domains. Ethernet ports or bond ports in a storage System can be added to multiple independent VLANS. You can configure different services in different VLANS to ensure the security and reliability of service data. IP address failover indicates that a logical IP address fails over from a faulty port to an available port. In this way, services are Switched from the faulty port to the available port without interruption. The faulty port can take over Services back after being recovered. During the IP address failover services are Switched from the faulty port to the available port, ensuring service continuity and improving reliability of paths for accessing file systems. This process is transparent to Users. The essence is a Service SWitchover between ports. The ports can be Ethernet ports, bond ports, or VLAN ports. To improve reliability of paths for accessing file systems, you can create logical ports based on Ethernet_ports. When the Ethernet port that corresponds to a logical port fails, the System will: Locate an available Ethernet port of the same type. Delete the logical port from the faulty Ethernet port.\nCreate the same logical port on the available Ethernet port to carry services. Ensure service continuity. To improve reliability of paths for accessing file systems, you can bond multiple Ethernet ports to form a bond port. When the Ethernet ports that are used to create the bond port fails, the system will: Locate an available port. Delete the logical port created on the faulty port. Create a logical port with the same IP address on the available port. Switch services to the available port. After the faulty port recovers, it can take over services again. You can create VLANs to isolate different services. When an Ethernet port on a VLAN fails, the system will: Locate an available port of the same type. Delete the logical port from the faulty port. Create the same logical port on the available port. Switch services to the available port. Generally, a SAN consists of a disk array (RAID) and a Fibre Channel (Fibre Channel). (The SAN is also called FC-SAN to distinguish it from the IP SAN.) SANs communicate with servers and clients through SCSI commands rather than TCP/IP, and data processing is at the block level. SAN can also be defined as data storage-centric. It adopts a scalable network topology and provides multi-channel data exchange between any nodes in the SAN through direct connection of optical channels with high transmission rates. In addition, data storage management is centralized in a relatively independent storage area network.\nHello, everyone! This post will share with you the definitions and principles of RDMA and IB. RDMA is short for Remote Direct Memory Access, which is a method of transferring data in a buffer between application software on two servers over a network. Low latency High throughput Low CPU and OS resource occupancy RDMA Bearer Network The IB technology is specifically designed for server connections, and is widely used for communication between servers (for example, replication and distributed working), between a server and a storage device (for example, SAN and DAS), and between a server and a network (for example, LAN, WAN, and the Internet). Standard-protocol-based High bandwidth and low latency RDMA Transmission uninstallation IB defines a series of devices for system communication, including channel adapters, switches, and routers. The IB protocol consists of five layers Application layer, Transmission layer, Network layer, Link layer and Physical layer. Application layer Transmission layer: Sends, receives, and reassembles data packet segments. Network layer: Provides addressing and routing. Link layer: Provides data packet design and point-to-point connection for local subsystems. Physical layer: Determines the connection rate. The lB supports the following basic operation types: Send : The data (payload) in the local buffer is transmitted to the buffer allocated by the peer end. The buffer allocated by the server is invisible to the client. RDMA write : Write the payload of the local buffer to the corresponding storage space of the peer end through the address negotiated with the peer end.\nHello, everyone! This post describes features, positioning, and typical application scenarios of Huawei intelligent storage products, including Huawei OcenStor all-flash storage, Huawei OceanStor hybrid flash storage, Huawei OceanStor distributed storage, and Huawei edge data storage(FusionCube). Note: Huawei OceanStor Dorado series is used as an example. Note: Huawei OceanStor Dorado(2U) is used as an example. Table 1 Storage system software architecture Software Function The dedicated operating system OceanStor OS manages storage system hardware and runs storage service software. The basic function software provides basic data storage and access functions. The value-added software provides advanced functions such as backup, disaster recovery, and performance tuning. The management software provides the management utilities to the storage system. Configures and maintains the storage system. The software includes SmartKit and eService. Enables the application server to communicate and cooperate with the storage system over a SAN. The software includes OceanStor BCManager and UltraPath. Enterprise-class hybrid flash storage products provide stable, reliable, converged, and efficient data services. Meets requirements of various services and applications such as OLTP/OLAP databases, high-performance computing, digital media, Internet operation, centralized storage, backup, disaster recovery and data migration. Provides a wide range of efficient and flexible backup and disaster recovery solutions to ensure business continuity and data security.\nStorage device (2U) For example, Huawei OceanStor hybrid flash storage 5110 V5: Storage device (2U) For example, Huawei OceanStor hybrid storage 5600/5800 V5 kunpeng: Storage device(2U) For example, Huawei OceanStor hybrid flash storage 5300 V5 kunpeng: Storage device(4U), for example, Huawei OceanStor hybrid flash storage 6800/18800 V5 Kunpeng: The distributed technology creates a large-scale storage resource pool that provides services for upper-layer applications and clients through standard service interfaces. Supports large-scale expansion and elastic EC data redundancy protection, improving disk space utilization. One storage system Supports block, file, HDFS, and object storage services. The solution with separated storage and computing resources ensures stable latency and fast response to mission-critical services. Note: Huawei OceanStor 100D is used as an example in the figure. Huawei OceanStor 100D is used as an example. Four-in-one storage service, enabling one storage system to support four types of storage services. lt is an edge data storage infrastructure based on the converged architecture. It is mainly used in hybrid load scenarios, such as databases desktop clouds, containers, and virtualization. The Kunpeng ecosystem is supported to flexibly meet the elastic configuration requirements of computing, storage, and I/O resources and meet the requirements of multiple types of application load modes with an IT infrastructure at the edge. lt is an edge IT infrastructure solution with integrated design and is delivered as an integrated cabinet. The solution is mainly used in edge data centers and edge application scenarios of vertical industries. The Kunpeng ecosystem is supported.\nHello, everyone! This post will share with you the backup solution. In information technology and data management, a backup refers to a copy of data in a file system or database for swiftly and promptly recovering the valid data and normal operation of the system when a disaster or misoperation occurs. Nature: Stores a copy of data elsewhere. Purpose: Recovers data when an accident occurs. Gartner: More than 30% of users use backup copies for development and testing, and more than 20% of users adopt cloud-based disaster recovery. Backup DR Recovers missing data. Recovers data from a recent point in time. Recovers damaged data. Directly takes over services. Recovers historical data. Recovers missing data. Directly takes over services. Recovers damaged data. : Supported Not supported Backup focuses on data recoverability, while DR is focused on application continuity. Backup is the basis of DR. Backup Archive Recovers data and system. Retains data for a long term. Meets the SLA(RPO and RTO). Meets legal requirements and efficiency requirements. Retains source data at the original location. Only retains data copies and deletes source data. Provides data retention control. Provides object retention control. : Supported Not supported Backup is for data recovery and archiving for legal compliance. Item Description Application Local backup for small data centers Benefits Integrated backup server and backup storage, greatly reducing device acquisition and maintenance costs. Broad compatibility for various operating systems, databases, and virtualization environments. Comprehensive protection for virtual physical and cloud environments. P2P, P2V, V2V, and V2P migration scenarios. Deduplication and compression.\nWeb Ul for easy management, operation, and maintenance. Highlights Huawei's unique two-level cloud backup solution can store backup data in the local data center in the backup resource pool of the cloud data center or in the S3 space of the public cloud. Huawei VBSs can back up data to the backup resource pool of the remote cloud data center and or in the S3 space of the public cloudwhich can be used to build the two-level e-Government cloud. Architecture Comparison Architecture Strength Weakness LAN-Base The backup system and the application system are separated, conserving the hardware resources of application servers during backup. Additional backup servers increase hardware costs. Backup agents adversely affect the performance of application Servers. Backup data is transmitted over the LAN, which adversely affects network performance. Backup services must be separately maintained, complicating management and maintenance operations. Users must be quite competent in processing services. LAN-Free Backup data Is transmitted using no LAN resources, significantly improving backup performance while maintaining high network performance. Backup agents adversely affect the performance of application servers. The costs are high. Devices must meet certain requirements. Server-Free Backup data is transmitted using no LAN resources, and network performance remains unaffected. Host services remain nearly unaffected. Backup performance is excellent. The costs are high. Devices must meet strict requirements. NDMP Backup and recovery duration is shortened. The throughput depends on the Speed of Storage devices rather than the processing capability of the server-. Therefore, the system performance is greatly improved.\nThe steps to change or reset the BIOS password can vary depending on the computer or motherboard model, but here are some general guidelines: To change the BIOS password: Turn on your computer and enter the BIOS setup menu by pressing the key that appears on the screen during startup (usually F2, F10, or Delete). Find the Security or Password section and select the option to change the BIOS password. Enter the current password, if prompted, and then enter the new password twice to confirm it. Save the changes and exit the BIOS setup menu. To reset the BIOS password: Turn off your computer and unplug it from the power source. Open the computer case and locate the CMOS battery on the motherboard. Remove the CMOS battery for a few minutes and then reinsert it. Close the computer case, plug the computer back in, and turn it on. Enter the BIOS setup menu and set a new password or disable the password entirely. Note that some computers or motherboards may have a jumper or switch on the motherboard that can be used to reset the BIOS password. Consult the user manual or manufacturer's website for specific instructions on how to reset the BIOS password for your particular model. Dear friend, Set or modify the BIOS password using the BIOS: 1. Access the BIOS. 2. Press or to switch to the Security screen. 3. Select Set Supervisor Password , press Enter , enter the original password, and set or modify the administrator password for login. 4.\nHello, everyone! This post will share with you the concept and importance of the DR system, and the advantages and disadvantages of common DR solutions. Three risks: data loss, data damage, and service interruption. The loss caused by service interruption per hour is millions of dollars. Regulatory compliance: financial compliance, security isolation, geo-redundant solution, and high service continuity IT O&M: System disaster recovery simplifies IT O&M work and avoids the impact of major events. Costly investment High capital expenditure (CAPEX) High purchase costs of infrastructure such as servers, storage devices, and software. High basic construction costs on facilities such as equipment rooms. High OPEX Professional O&M support (implementation, training, and onsite support). Long-term costs on resources such as water and electricity. Cumbersome management Multiple devices are not centrally managed. Independent storage media, servers, and network management pages, complex workflows, and low efficiency Complicated capacity expansion. The capacity is insufficient and needs to be expanded. The rollout period is long. Limited DR capability Poor security and DR capabilities. Data cannot be backed up out of the data center, and infrastructure faults may cause extreme situations. Poor agility Capabilities such as disaster recovery and data sharing are restricted by physical locations of data. Applications and data cannot be separated. Agile applications and better DR features cannot be built. High availability (HA) ensures that applications can still be accessed when a single component of the local system is faulty, no matter whether the fault is a service software fault, physical facility fault, or IT software/hardware fault.\nThe best HA is that users using the data center service are completely unaware of a machine that breaks down in the data center. However, if a server in a data center breaks down, it takes some time for services running on the server to failover. As a result, customers will be aware of the failure. The key indicator of HA is availability. Its calculation formula is [1 (Downtime)/(Downtime + Runtime)]. The following nines are used to represent availability: 4 nines: 99.99% = 0.01% x 365 x 24 x 60 = 52.56 minutes/year 5 nines: 99.999% = 0.001% x 365 = 5.265 minutes/year 6 nines: 99.9999% = 0.0001% x 365 = 31 seconds/year For HA, shared storage is usually used. In this case, RPO = 0. In addition, the active/active cluster HA mode is used to ensure that RTO is almost 0. If the active/passive HA mode is used, RTO needs to be reduced to the minimum. A disaster is an unexpected event (caused by human errors or natural factors) that results in severe faults or breakdown of the system in one data center. In this case, services may be interrupted or become unacceptable. If the system unavailability reaches a certain level at a specific time, the system must be switched to the standby site. Disaster recovery (DR) refers to the capability of recovering data, applications, and services in data centers at different locations when the production center is damaged by a disaster.\nIn the DR mode, a redundant site is established in addition to the production site. If the production site is damaged due to a disaster, the redundant site can take over services from the production site to ensure service continuity. To achieve higher availability, customers even establish multiple redundant sites. They are interrelated and complementary to each other. They overlap with each other and have significant differences. Dimension HA DR Scenario HA refers to a local HA system. When one or more applications are running on multiple servers, ensure that the running applications are not interrupted when any server is faulty. The applications and system can be quickly switched to other servers. DR refers to a remote (intra-city or remote) HA system. It is used to recover data, applications, and services when a disaster occurs. HA is used to ensure high availability of services. Storage Generally, HA uses shared storage. Therefore, data will not be lost (RPO = 0) and the switchover duration, that is, RTO, is considered. Data replication is used for remote disaster recovery data. Based on different data replication technologies (synchronous and asynchronous), data loss often causes RPO to be greater than 0. However, remote application switchover usually takes a longer time. In this case, RTO is greater than 0. Fault Load switchover between servers in the cluster caused by a single faulty component. Service switchover between data centers caused by large-scale faults. Network HA is used in LAN. DR is used in WAN.\nCloud HA is a mechanism that ensures service continuity in a cloud environment. DR is a mechanism that ensures service continuity among multiple cloud environments Objective HA is used to ensure high availability of services. DR ensures data reliability and service availability. Backup : Backup is a process of copying all or part of data sets from an application host's disks or a storage array to other storage media in a data center. Backup is a method of DR. DR : A DR system consists of two or more sets of IT systems that are geographically far from each other. These IT systems provide the same functions, and monitor the health status of each other. In the event of an accident (such as a fire or an earthquake), applications on a broken-down system can be switched to other systems to ensure business continuity. Recovery Point Objective (RPO) indicates the maximum amount of data that can be lost when a disaster occurs. Recovery Time Object (RTO) indicates the time for system recovery. The smaller the RPO and RTO, the higher the system availability, and the larger the investment required by users. Level Definition RTO TCO Data level Builds a remote DR center to back up data remotely, which prevents data loss or corruption in the event of a disaster. The remote DR center is considered as a remote data backup center. Data-level DR cannot prevent service interruption if a disaster occurs.\nOnce a device or data center fails, a service failover starts, ensuring data integrity and service continuity, as well as DR of zero RPO and RTO. The solution also supports the tri-mirror function, enhancing data reliability. In addition, this solution can protect snapshots of protected objects for rolling back data, preventing data loss and data corruption caused by viruses. Application scenarios: Service loads need to be shared across data centers without interrupting services. Partial or all services (including network, storage, and host services) in a center fail and need to be restored. Links between two data centers of a DR system is of high speed and low latency. This solution establishes a same-city DR center and a remote DR center. It uses the remote DR center to recover services if the production center fails due to a disaster or misoperation, ensuring business continuity. This solution belongs to Huawei Business Continuity and Disaster Recovery Solution. In the Disaster Recovery Data Center Solution (Geo-Redundant Mode), three data centers coexist. The continuity of core services can be ensured when two data centers are damaged, remarkably improving availability of the DR solution. The production center, same-city DR center, and remote DR center are the three data centers in the solution. Production center Provides external services. Same-city DR center locates at a place dozens of kilometers away from the production center. Fibre Channel network-based direct connection is recommended. Synchronous or Asynchronous replication is implemented and level-1 DR protection is provided in the data center.\nHello, friend! This chapter describes two methods of managing storage systems: OceanStor DeviceManager and CLI, as well as management content and related operations. Storage management allows users to use management tools to query, set, manage, and maintain storage systems. DeviceManager is storage management software designed by Huawei for a single storage system. It can help you easily configure, manage, and maintain storage devices. Main software functions include storage resource allocation, user management, data protection feature management, device performance monitoring, and alarm management. You must add port number 8088 after the IP address of the management network port. Otherwise, the login fails. Format: https://xxx.xxx.xxx.xxx:8088 Note: The GUI may vary slightly depending on the product version and model. The actual GUI prevails. No. Name Description 1 Function pane Displays available functions related to the current operation. 2 Navigation bar Lists all functional modules of the storage system. 3 Alarm and task statistics area The alarm statistics area displays the number of alarms by severity and helps users learn about the running status of the storage system. The task statistics area displays all the tasks executed by users. You can check whether the tasks are executed successfully. 4 Device management area In the device management area, you can view and modify device information, and power off or restart devices. 5 Logout and language area The logout and language area provide buttons of logout and language. DeviceManager supports two languages: English and simplified Chinese.\nTo prevent misoperations from compromising the storage system stability and service data security, the storage system defines user levels and roles to determine user permission and scope of permission. CLI allows you to manage and maintain the storage system. Configuration commands are entered on the keyboard and compiled and executed by programs. The command output is displayed in text or graphics on the CLI. Terminal software is required for logging in to the CLI. PuTTY is used as an example. Enter the user name and password as prompted. The system asks you to change the password upon the first login. Change the password immediately to ensure system security. The following information is displayed when the login is successful: You are required to follow the format conventions when you use the CLI commands. Typical command formats are: First field : operation that you want to perform, for example, change (modify) and show (query). Second field : object of an operation, for example, storage_pool (storage pool) and host (host). Third field (available only in some commands): object attribute, for example, relocation_speed (migration rate). Other fields : other parameters required. For example: change user keeps unchanged. user_name = , mandatory; For level = and action = , one of them can be selected. For parameter level= ?, level= remains unchanged. The value of must be an optional value, for example, level=admin . Correct command example: Format Description Boldface The keywords of a command are in boldface.\nItalic The arguments of a command line, which will be replaced by actual values, are in italics [ ] Items in square brackets [ ] are optional. { x | y | ... } Indicates that one option is selected from two or more options. { x | y | ... } Indicates that one or no option is selected from two or more options. { x | y | ... } * Indicates that multiple options are selected from two or more options. At least one option must be selected, and at most all options can be selected { x | y | ... } * Indicates that multiple options are selected or none is selected from two or more options. On the CLI, you can press Tab or the space bar to use the command completion function. The difference between the two keys is as follows: The space key is used to supplement only the current field, whereas the Tab key is used to supplement all possible values. When all the fields required by the command are entered and the conditions for running the command are met, the system prompts that the command can be run after you press Tab . In this case, you can press Enter to run the command. Press Ctrl+A to view the optional values of certain parameters in certain commands. Generally, these values need to be obtained from the system. You can turn pages on the context-sensitive help page. Enter a question mark (?)\nto query the basic instruction of CLI operations and detailed description of command parameters. After entering the first field of the command and a space, enter a question mark (?). You can query all available next fields and the detailed description of each field. Exit the context-sensitive help page. Purpose : Redundant information is deleted, and valid content is displayed as required. How to use : After entering the complete query command, enter |and press Tab or the space bar. Related Commands filterColumn column filtering command. filterRow row filtering command. show xxx|filterColumn{exclude|include}columnList=? Parameter Description Value exclude Column fields available for filtering that do not need to be displayed. By typing show xxx|filterColumn exclude and pressing the Tab or space key, you can see all of the applicable column fields. include Column fields available for filtering that need to be displayed. By typing show xxx|filterColumn include and pressing the Tab or space key, you can see all of the applicable column fields. columnList= Column fields that are available for a filtering. To query multiple column fields, separate them by commas (,) for this parameter. logicOp=? and: Multiple columns that meet the condition are displayed. or: Any column that meets the condition is displayed. predict=? not: The logicOp is not. equal_to: a value equal to value=? greater_than: a value greater than value=? greater_equal: a value equal to or greater than value=? less_than: a value less than value=? less_equal: a value less than or equal to value=? match: regular expression matching value=?\nHello, everyone! This post will share with you some storage basic management operations, like managing licenses, configuring basic storage services, collecting storage system information, manage alarms and events... Introduction : A protocol used to synchronize the system time of a computer to the Coordinated Universal Time (UTC). A server that supports the NTP protocol is called an NTP server. Function : When an alarm is generated, users can accurately locate the time when the alarm is generated based on alarm logs. Setting mode : Set the time manually. Synchronize the client time. Set automatic NTP synchronization. Managing the device time on DeviceManage r Managing the device time on the CLI The change ntp_server config command is used to automatically synchronize the storage system time with the NTP server time. The show system ntp command is used to query NTP settings. The show ntp status command is used to query the NTP status. The show ntp_server general command is used to query the settings of the time synchronization function. License file : Permission credentials for using various value-added features (such as snapshot, remote replication, clone, and SmartQoS). Precautions : During routine device management, you need to check whether the license file is available. Based on the status of the imported or activated license, different license operations are displayed on the License Management page: Import License , Activate License , and Update License . For an activated license file, DeviceManager provides two control modes: Running time-based control: displays the expiration time of the license.\nHello, everyone! As the cost of storage devices decreases, large-capacity storage devices have been used by more and more enterprises to store data generated by enterprise service application systems and IT systems, such as emails, documents, service data, and data backup. Therefore, effective management of storage devices is critical to the continuity and stability of enterprise services. O&M is essentially the operation and maintenance of networks, servers, and services in each phase of their lifecycles to achieve a consistent and acceptable status in terms of cost, stability, and efficiency. Technical layer : Streamline the O&M lifecycle of each product and identify the key measures of each task. Process layer (ITIL process management framework): change, event, and problem management. To restore services as soon as possible To minimize the impact of emergencies on service running To ensure that the service quality and availability meet the SLA requirements Emergency Any event that causes or may cause service interruption or service quality deterioration Hardware faults, software faults, and service request interruption Classification and online support Priority (based on the impact and urgency) Investigation and diagnosis Solution and recovery End Responsibilities, monitoring, tracking, and communication Task Locate the root cause of the problem and take measures to eliminate known errors. Minimize the number of incidents caused by IT infrastructure errors and minimize the negative impact of problems. Prevent the recurrence of emergencies related to errors. Problem: obtained from multiple emergencies with the same symptom or a major incident and indicates that an error with unknown causes exists.\nKnown errors: The root cause of a problem has been successfully located and a solution has been found. Problem control Known error control Proactive problem management Trend analysis Review of major issues Ensure that all changes are effectively controlled and handled through standardized means and processes, and that approved changes are implemented with minimum risks, high efficiency, and high cost-effectiveness. Change: An action that causes the status of one or more IT infrastructure CIs to change. Standard change (approved in advance) Request for Change (RFC) Change Schedule (FSC) Change Advisory Board (CAB) Receive, record, approve, plan, test, implement, and review change requests. Provide the IT infrastructure change report. Modify CMDB. The input information includes: Change request Data information provided by the configuration management database, especially information about the impact of changes Change implementation schedule Capability database provided by capability management and budget information provided by the financial management process The output information includes: Updated change implementation schedule Signals that trigger the start of configuration management and release management Agenda, minutes, and action items of CAB Change management report Measure the value of all IT assets and configuration items used in organizations and services. Provide accurate information about IT infrastructure configuration for other service management processes. Support the operation of incident management, problem management, change management, and release management Verify the correctness of the configuration records related to the IT infrastructure and correct the detected errors. Identify and define configuration items. Plan, define, and manage the configuration management database. Periodically verify the accuracy and integrity of CMDB.\nHello, everyone! An HDD consists of platters, an actuator arm, read/write heads, a spindle, a port, and control circuits. Disk capacity = Number of cylinders x Number of heads x Number of sectors x 512 bytes. The unit is MB or GB. The disk capacity is determined by the capacity of a single platter and the number of platters . Because the processing speed of a CPU is much faster than that of a disk, the CPU must wait until the disk completes a read/write operation before issuing a new command. To solve this problem, a cache is added to the disk to improve the read/write speed. The average access time is determined by: Average seek time Average latency time The date transfer rate is determined by: Internal transfer rate External transfer rate/Interface transfer rate IOPS Input/Output operations per second (IOPS) is a key indicator to measure disk performance. IOPS is calculated by the seek time, rotation latency, and data transmission time. Transmission bandwidth (throughput) Indicates the amount of data that is successfully transmitted in a unit time, that is, the speed at which data streams are transmitted. For example, if it takes 10s to write 10,000 files of 1 KB size, the transmission bandwidth is only 1 MB/s; if it takes 0.1s to write a 10 MB file, the transmission bandwidth is 100 MB/s. For example, the methods for transmitting numbers 1 to 8 are as follows: Multiple lines are connected between two ends, and one number is transmitted on each line.\nHello, everyone! This post will share with you the SSD. Compared to HDDs, SSDs have absolute advantages in terms of performance, reliability, power consumption, and portability. SSDs have been widely used in various industries. Uses NAND flash to save data, providing a faster speed than HDDs. Has no mechanical structure inside, so it consumes less power, dissipates less heat, and generates less noise. Its service life is determined by the number of program/erase (P/E) cycles. An SSD consists of a control unit and a storage unit (mainly flash memory chips). Control unit: SSD controller, host interface, and DRAM Storage unit: NAND flash Internal storage units in NAND flash include: LUNs, planes, blocks, pages, and cells Operations on the NAND flash include erase, program, and read. NAND flash is a non-volatile medium. A block must be erased before new data is written to it. A program/erase (P/E) cycle is the process of erasing a block and then writing it again. SLC 1. Supports 50,000 to 100,000 P/E cycles, providing the best reliability. 2. The storage capacity is small. 3. The cost is the highest. MLC 1. Supports about 3,000 P/E cycles. 2. The speed is slower than that of SLC. 3. The storage capacity is relatively large. 4. The price is relatively low. TLC 1. Provides higher data density and supports only several hundred to 1,000 P/E cycles. 2. The reliability and performance are low. 3. Generally used in personal devices due to the cost advantage, but cannot meet the requirements of enterprise products. QLC 1.\nwhat a virtual SAN is? Dear friend, A virtual storage area network (VSAN) is a logical partition in a physical storage area network (SAN). VSANs enable traffic to be segregated within specific parts of a storage area network, so that if a problem occurs in one logical partition, it can be handled with minimal disruption to the rest of the network. Using multiple isolated VSANs also makes the storage system easier to configure and scale out. Users can be added or migrated without changing the physical layout. Virtual SAN Appliances allow unused storage capacity on virtual servers to be pooled and accessed by virtual servers as needed. Virtual SAN appliances are typically downloaded as software programs that run on virtual machines, but some storage hardware vendors are beginning to integrate virtual SAN appliances into their firmware. Depending on the vendor, a virtual SAN appliance can also be referred to as a software-defined storage (SDS) appliance, or simply a virtual storage appliance. Virtual SAN acronyms are spelled differently by different vendors. When spelled in all capital letters, the acronyms are typically associated with Cisco Systems and are used in conjunction with zoning, which divides the physical SAN into isolated subnets. When spelled as a lowercase v, the acronym is usually associated with VMware and Hyper-V features that allow available hard drive storage to be pooled from among cluster hosts. SAN Architecture A physical storage area network consists of a fabric layer, a host layer, a SAN switch, and a storage layer. Non-disruptive data migration.\nHard Drive Data Recovery. This is extremely helpful for IT & Non-IT users. My first article of this new year as well. Introductions : Services offered for data recovery from damaged hard drives are referred to as hard drive data recovery services. The hard disk can be broken, burned, or damaged by even minor shocks. This primarily occurs as a result of the hard drive's relatively delicate and small size being handled carelessly. The following factors might cause the hard disk to distort and the drive to become inaccessible in logical damage cases: The hard disk data recovery services platforms support us to consider the following situations: There are several indicators that notify us when a physical drive fails. There are a lot of indicators that warn us of a physical hard disk drive failure, including the drive not spinning, an overheated drive, a grinding noise, and a drive that displays \"Not initialized\" on the window screen. How to reduce the risk of hard disk drive failure: Importance of the Hard drive data recovery services: The hard disk data recovery services swiftly and safely restore the data from the hard drive by retrieving, repairing, and replacing lost, corrupted, deleted, or damaged data. It saves the user time and effort while recovering inaccessible files. The more expensive and time-consuming route is hardware data recovery, which requires technicians or engineers to disassemble the hard drive in order to retrieve the data.\nDear All, Today we are going to study Software RAID vs Hardware RAID Software RAID vs Hardware RAID RAID stands for Redundant Array of cost-effective Disks. it's the simplest way to virtualize multiple, hard disc drives into one or additional arrays to enhance performance, capability and availability. The RAID is deployed either on a special controller (hardware RAID), or by Operating systems software system driver (software RAID). Hardware RAID Advantages: Hardware RAID is devoted processingsystem, via controllers or RAID cards to manage the RAID configuration severally from the operating system. The RAID controller doesn't take processingpower remove from the disks it manages. Thus, extra space and speed will be used to read and write data.It will work on any operating system. substitution failing disk is easy simply plug it out and place in a very new one. Disadvantages: As hardware RAID needs extra controller hardware, the costvalue is more than softwareRAID. If your RAID controller fails, you have got to seek out a compatible one to replaceto let start working RAID system and perform the manner you set it up. Software RAID Advantages: Unlike hardware RAID, software RAID uses the process power of the operating system within which the RAID disks are located and installed the costvalue is lower as a result of no extra hardware RAID controller is needed. It conjointly permits users to reconfigure arrays while not being restricted by the hardware RAID controller. Disadvantages: Software RAID tends to be slower than hardware RAID.\nDear All, Today we are going to discuss about BCManager eReplicaiton Use cases. DR Data Center Solution (Active-Active Mode) Solution overview An active-active DR solution based on HyperMetro An active-active + local protection DR solution based on HyperMetro eReplication values Application awareness Automatic sensing of VM resources Simplified management Visualized topologies End-to-end monitoring DR Data Center Solution (Active-Passive Mode) Solution overview A DR data center solution based on synchronous or asynchronous storage array replication eReplication values Application awareness Automatic sensing of VM resources VM consistency Simplified management Visualized topologies Policy-driven protection One-click switchover End-to-end monitoring DR testing Recoverability verification One-click testing 3DC DR Solution Solution overview A 3DC DR solution based on array replication in cascaded and parallel networks A 3DC DR solution based on active-active SAN and asynchronous array replication eReplication values Application awareness Automatic sensing of VM resources VM consistency Simplified management Visualized topologies Policy-driven protection One-click switchover End-to-end monitoring DR testing Recoverability verification One-click testing Dashboard Displays whether the RPO of protected groups meets the requirements Displays the estimated switchover time for DR tests Displays the data status of recovery plan tests. Displays whether the objects are protected properly. Displays the latest top 10 alarms and informs users for processing. Execution historical records of protected groups Unified Resource Management Creates a site. Discovers storage devices Discovers the vCenter server Discovers the remote eReplication server Unified Resource Management Basic site information and protected groups' RPO compliance List of protected groups. You can view the status of the protected groups at a site.\nRemote eReplication server and site resource list Policy-based Automatic Protection Set basic information about the protected group, including its name and description Select the VMs that you want to protect. Automatically detects the selected networking mode of VMs, and matches a proper policy template for users. You can then set related time and consistency policies. You can create the corresponding recovery plan when creating a protected group. After a protected group is created, the system protects the VMs based on the specified DR policy. If an automatic time policy is set, the system periodically protects the VMs based on the DR policy. An alarm is generated automatically if the execution fails. You can view protection history and history protection status, and report whether RPO can be met. An alarm is generated automatically if the execution fails. Unified DR Plan Management Select a protected group and create a recovery plan. All recovery plans are centrally managed. If you select to create a recovery plan when a protected group is created simultaneously, ignore this step. You can configure whether to enable recovering protected objects and the startup sequence. Set the recovery network for protected objects Procedure defining based on different processes (planned migration, fault recovery, test, clearing, and reprotection) is supported. You can add or delete certain procedures for recovery in different scenarios. One-Key DR Testing and Clearing Select the testing host/cluster and network. You can close non-important VMs on the testing host/cluster to release resources. The testing procedure does not affect services at the production site.\nDear All, Today we are going to learn about HyperMetro HyperMetro is Huawei's active-active storage solution that enables two storage systems to process services simultaneously, establishing a mutual backup relationship between them. If one storage system malfunctions, the other one will automatically take over services without data loss or interruption. With HyperMetro being deployed, you do not need to worry about your storage systems' inability to automatically switch over services between them and will enjoy rock-solid reliability, enhanced service continuity, and higher storage resource utilization. Huawei's active-active solution supports both single-data center (DC) and cross-DC deployments. HyperMetro is also called the active-active feature. It enables two data centers to establish a mutual backup relationship. Both data centers should be in the running status. If a device fault occurs in one data center or the entire data center fails, services are automatically switched to the other one. Hyper Metro Networking. Single-DC deployment In this mode, the active-active storage systems are deployed in two equipment rooms in the same campus. Hosts are deployed in a cluster and communicate with storage systems through a switched fabric (Fibre Channel or IP). Dual-write mirroring channels are deployed on the storage systems to ensure continuous operation of active-active services . Cross-DC deployment In this mode, the active-active storage systems are deployed in two DCs in the same city or in two cities located close. The distance between the two DCs is within 300 km. Both of the DCs can handle service requests concurrently, thereby accelerating service response and improving resource utilization.\nFinancial institutions are using real-time, big data analytics to make faster business choices and communicate with clients more effectively, similar to how many other businesses are supercharging their capabilities through digital transformation. Huawei's OceanStor Dorado All-Flash Storage is the most cost-effective storage available today to provide lightning-fast performance to these endeavors. OceanStor Dorado has been adopted by more than 100 large financial institutions worldwide, including eight of the world's top 20 banks, thanks to its world-leading response times 21 million Input/Output Operations Per Second (IOPS), making it the world's fastest all-flash storage according to the Storage Performance Council test model and unsurpassed reliability. Transaction capabilities are doubled when using OceanStor Dorado All-Flash storage, easily coping with surging access needs. Peak-hour access is no longer a problem thanks to the increased capacity. 2x online service scale; key applications running in a stable state 24 hours a day, 7 days a week. Figure 1:All flash Storage OceanDrodo 5000/6000 V6 Today I am going to explain about LUN planning of Huawei All Flash Storage, after deciding or buying the Huawei All flash Storage, you need to plan lots of things like Basic Storage Service Planning, storage pool planning, File System Planning and many others but today we are specifically talking about the LUN planning.\nFollowing are the steps involved in this planning, Figure2: LUN planning step by step Here is a case study been taken as an example The Huawei OceanStor Dorado NVMe-based All-Flash Array (AFA) storage solution deployment at a banks new data center to replicate the one in the other data center, the bank needed a next-generation storage system. The bank sought an agile, flexible, and reliable open storage system that could store massive volumes of data seamlessly in order to harness the benefits of its digital underlying infrastructure. Lets take a look for the series of steps involved in detail. Type planning: So, the first step in the list is of planning for the type two types under consideration are Common LUNs and protocol endpoint (PE) LUNs. PE LUNs are used in VMware ESXi software-defined storage for virtual volumes (VVols). LUN Quantity and Capacity Planning Based on service requirements, determine the number and capacity of LUNs. In the case of the above Data center Replication scenario, the number of LUNs will be large so will be their Capacity. But considering the actual situation of the scenario sometimes you may need a greater number of LUNs with respect to capacity for specific data transfer requirements This is the thin LUN's maximum capacity. This value must not be exceeded for each thin LUN's total dynamic storage resource allocation. The LUN's maximum capacity must not exceed the system's specifications.\nApplication type planning: Another most important step is the decision about the application, based on the service I/O paradigm, choose an application type. LUN performance may be impacted if the application type set on the LUN does not match that of the service. The predefined application type determines the size of the application request and if it is required. Preset application types are provided for typical applications. In block service scenarios, possible options are Default, Oracle_OLAP Oracle_OLTP Oracle_OLAP&OLTP SQL_Server_OLAP SQL_Server_OLTP SQL_Server_OLAP&OLTP SAP_HANA, Vmware_VDI Hyper-V_VDI FusionAccess_VDI After you've assigned an application type to a LUN, you can't change it in subsequent actions. Compression and deduplication are enabled (if there is a license). If none of the preset application types suits your I/O model, you can create one using the new LUN workload type general command. Mapping Planning Select the hosts to which LUNs are mapped based on service requirements LUNs and hosts have one-to-one or many-to-one mapping connections. Figure 3: Host Configuration The storage system assigns a host LUN ID to a LUN that is mapped to a host. If you choose Automatic, the system will automatically assign each LUN a unique host LUN ID. Starting with the Start ID, the system automatically assigns each LUN a unique host LUN ID.\nPolicy planning: Policy for the LUN needs to be defined in the beginning so here are three important policy planning and decisions to be made To read data, the cache prefetches it in order to improve the hit ratio and limit the amount of read I/Os delivered to discs, lowering latency and increasing speed. Resident, Default, and Recycle are the three cache read policies available. Write-back and write-through policies are two types of cache write policies. Write-back: As soon as each write I/O arrives at the cache, a write success notification is sent to the host. Before writing to drives, the cache sorts and aggregates data. Write-through: Each write request is transmitted straight to the disc where it will be stored. The write performance is influenced by important disc factors such disc type, rotational speed, and seek time. Write-through has a higher level of reliability than write-back. By running create LUN on the CLI, you can set the cache write policy when creating a LUN. When you establish a LUN with OceanStor Device Manager, the default cache write policy is write-back, and you can't modify it. A storage system prefetches data from discs to the cache while reading data from a LUN, using one of four prefetch policies: non-prefetch, intelligent prefetch, constant prefetch, and variable prefetch . Excessive or insufficient prefetch might occur if a prefetch policy is set incorrectly.\nHow does Random Access Memory (RAM) work in our Computers Random access memory (RAM) is the best-known form of computer memory. This is what allows your computer to surf the internet and then quickly switch to loading an application or editing a document. RAM is considered \"random access\" because you can access any memory cell directly if you know the row and column that intersect at that cell. RAM is basically your computer's short-term memory. Similar to a microprocessor, a memory chip is an integrated circuit (IC) made of millions of transistors and capacitors. In the most common form of computer memory, dynamic random access memory (DRAM), a transistor and a capacitor are paired to create a memory cell , which represents a single bit of data. The capacitor holds the bit of information a 0 or a 1 (see How Bits and Bytes Work for information on bits). The transistor acts as a switch that lets the control circuitry on the memory chip read the capacitor or change its state. A capacitor is like a small bucket that can store electrons. To store a 1 in the memory cell, the bucket is filled with electrons. To store a 0, it is emptied. The problem with the capacitor's bucket is that it has a leak. In a matter of a few milliseconds a full bucket becomes empty. Therefore, for dynamic memory to work, either the CPU or the memory controller has to come along and recharge all of the capacitors holding a 1 before they discharge.\nRedis (REmote DIctionary Server) is a powerful NoSQL key-value data storage, cache, and message broker. Redis has quick read/write operations, diverse data types, and an advanced memory structure. It's perfect for building fast, scalable web apps. Its keys are strings, hashes, lists, sets, sorted sets, bitmaps, and hyperlog logs, making it a data structure server. Redis' memory-based read and write operations are quick. Data can also be written to disk or memory. Redis caches data in memory. Twitter, GitHub, Instagram, Pinterest, and Redis, a popular key-value database. Many firms are hiring Redis engineers for database administrator and other roles as Redis usage grows. Redis measures amplitude. Redis can be used to store your website sessions to implement \"sticky sessions\" across many servers, which means you can keep a user's login even if he connects to a different server of the same website, like Facebook: although their load balancer tries to keep the user assigned to the same server on each access, if that's not possible, the user won't lose his session as the session store is shared across the different instances. Redis has uses beyond session storage. Publisher/Subscriber technologies let you broadcast messages to all servers that listen to a channel. Redis stores random data that multiple servers need to access quickly in a message or job queue, where a master posts jobs and workers process them. Redis offers numerous value kinds and structures, hence it has many uses. Redis is more powerful than just a caching system for WordPress websites.\nWhich enterprise feature are shared by multiple protocols in Oceanstor Pacific? File, object, and Hadoop distributed file system (HDFS) services are all commonly used by organizations in different phases of their data pipelines for HPDA scenarios like autonomous driving, precision medicine, and smart manufacturing. Traditionally, these services have either been provided by separate storage platformswhere multiple copies of data are requiredor by using gateways in front of a central storage platform. Both of these scenarios are suboptimal; making multiple copies consumes time, increases complexity, and wastes storage space, while using a NAS or object gateway in front of a block storage array will compromise performance. In contrast, the multi-protocol capability of OceanStor Pacific allows one copy of data to be shared using multiple protocols. OceanStor Pacific supports NFS, CIFS, HDFS, and S3 protocols. This is designed to improve analytical efficiency because data written using one protocol can be read over multiple protocols without data migration while preserving protocol semantics and providing consistent performance. ESG analyzed OceanStor Pacific in a multi-protocol test environment to validate semantic integrity, performance, and advanced functionality like snapshots, quotas, QoS, object storage versioning, and object versioning. Multi-protocol testing began with the creation of four files in a shared directoryone each using a CIFS, NFS, HDFS, and S3 client. When each file was created, the MD5 hash was recorded for verification purposes using the client that created it. The files were then examined with all four clients.\nWe've entered the digital age. Moore's Law associated with a single chip is slowing down, and traditional computing can no longer meet new requirements. Computing is entering the golden age of architecture innovation. The world is calling for diversified computing capabilities that have the following characteristics: Heterogeneous computing: Computing will evolve from general-purpose CPUs to parallel and distributed computing with ARM, NPU, and GPU processors. Collaborative computing: 5G will enable more data to be distributed at the edge and on devices. Computing capabilities will follow the footsteps of data and implement cloud-edge-device collaboration. Brute force computing: AI computing power is growing rapidly. It is estimated that AI will generate more than 80% of data center computing power by 2025. Pervasive computing: Computing power will be ubiquitous, including edge-device-cloud and embedded systems. At HUAWEI CONNECT in September 2019, we announced the Kunpeng + Centerm dual-engine computing strategy and our commitment to providing the world's most powerful computing power. With its strategy of open hardware, open source software, and partner enablement, Huawei has made substantial progress in promoting the development of the computing industry ecosystem. Open hardware: Huawei focuses on producing high-quality chips, boards, and motherboards, enabling partners to develop self-branded integrated computing products. Eleven partners have launched server and PC products based on Kunpeng motherboards, including Huanghe, Changhong Tiangong, Unishy, Tongfang PC, and Xiangjiang. The products are now on the market. Open source software: The openEuler open source community went live in December 2019.\nHello everyone! This post enquires about a storage pool rebalancing after expansion. Please see below for details. I have a plan to expand a storage pool on a Dorado 6000 V6 ( 6.1.0.SPH25 ). Currently, the installed disks are 58 and I would like to add 58 more on it. The usage of the storage pool is about 202TB of 338TB . My question is how long does it take for the system to rebalance the data accross the disks after the expansion is complete? Also, is there any impact when rebalancing the running to the storage service? I'd really appreciate if anyone could give me some advice. Thank you! Dear Amsidhi, Wait for the disk to automatically power on. It takes about 15 to 30 seconds for a disk to power on. There is no impact when rebalancing running to the storage service. View indicators on the disk to verify the installation. If the disk is running properly: The disk's Running indicator is steady green. The disk's Alarm/Location indicator is off. If the disk is not running properly: The disk's Running indicator is off. The disk's Alarm/Location indicator is steady red. Here is . Thanks. Dear Amsidhi, Wait for the disk to automatically power on. It takes about 15 to 30 seconds for a disk to power on. There is no impact when rebalancing running to the storage service. View indicators on the disk to verify the installation. If the disk is running properly: The disk's Running indicator is steady green.\nHello everyone, This post is talking about iSCSI Qualified Name (IQN) and how to configure it in Linux. A worldwide unique name for identifying the node. iSCSI uses the iSCSI Qualified Name (IQN) and Extended Unique Identifier (EUI) . In an iSCSI network, each iSCSI element that uses the network has a unique iSCSI name and is assigned an address for access. Each iSCSI element, whether an initiator or target, is identified by a unique iSCSI Qualified Name (IQN). The IQN is a logical name that is not linked to an IP address. By default, Red Hat generates unique iSCSI names for your iSCSI initiators, for example, iqn.1994-05.com.redhat:876ee1a1014. Usually, you do not have to change the default value, but if you do, make sure that the new iSCSI name you enter is worldwide unique. An iSCSI initiator name must comply with the following format: A sample IQN format is iqn.yyyy-mm.naming-authority: unique name yyyy-mm is the year and month when the naming authority was established. naming-authority is usually reverse syntax of the Internet domain name of the naming authority. Unique name is any name you want to use, for example, the name of your host. The naming authority must make sure that any names assigned following the colon are unique. An iSCSI initiator name contains only: Special characters: hyphens (-), periods (. ), and colons (:) Lower-case letters, for example, a to z Digits, for example, 0 to 9 An iSCSI initiator name can contain a maximum of 223 characters.\nDear All, Today we are going to learn about SAN Synchronous/Asynchronous Replication SAN Synchronous Replication As shown above the deployment setup. The target RPO is 0, and the RTO is within minutes. Only SAN-based DR replication supports synchronous replication. It is recommended that the distance be less than 100 km. RD provides DR management functions, including topology, DR test, drill, and DR. To manage applications and recover DR applications, you need to install the Agent on the server. The RD management network needs to communicate with hosts and storage devices. FC/iSCSI links are supported. FC links are recommended for synchronous replication. SAN Synchronous Replication Principles The synchronization procedure is as follows: The production storage receives a write request from the host. HyperReplication logs only the address information. It does not record data content. The data from the write request is written to both the active and standby LUNs. If a LUN is in the write-back state, data will be written to the cache. HyperReplication waits for the data write results from the active and standby LUNs. If writing to both LUNs is successful, the system deletes the log. If writing to either LUN fails, the system retains the log and replicates the data again in the next synchronization. The system returns the write result of the source LUN to the host. Splitting: In split mode, write requests from production hosts go only to the active LUN, and the difference between the active and standby LUNs is recorded by the differential log.\nFirst of all the main thing that I want is to fix this issue without any data erase or format.. I just want to unlock and fix this bug I think the main problem is that there is a bug in the system that make the phone can't recognize that I entered the password so the \"screen lock app\" locked everything in my phone, I can't even make a backup or reach my files even if I connect it to my pc it still not showing my files and HiSuite can't recognize it because the phone is acting like I didn't enter the password while it's already open but it's a bug My phone is: Huawei P20 Lite / Huawei Nova 3e The cause of this issue is: My phone was always filling up the storage without downloading anything or doing any update!\nI don't know why and always when I free up space.. my phone take alot of mega bytes or sometimes giga bytes again (especially when I put it on charger mostly it took alot of mega bytes for nothing) so someday I saw it filling up the space again so I let it be full and then restarted my phone to return some of the space it took before (this method works fine with me for two years), then when I did it this time it give me back a 300-500 mega bytes but the performance of my phone was very bad and it's fine all I have to do is to restart again as usual.. I restart it again and I got this issue after restart.. since that day my apps isn't working.. I tried to delete apps from settings and I have free up more than 1 GB but it still the same nothing changed!\nDelay is latency. Low latency improves telecoms system UX, while high latency does the opposite. Data packet latency quantifies network data transfer time. Requests should be quick. Tracking a packet's circuit completion time measures network latency. Network latency slows website loading, interrupts live video and audio feeds, and makes the app unusable. Latency might hurt user experience depending on the scenario. Geographic distance causes latency issues. Data travels farther in dispersed IP networks, which can delay or even crash an application. Edge computing is useful in autonomous driving to minimize sensing and response time. A simple latency test is a ping. It's a network diagnostic tool for testing server and device connection. ICMP echo request packets are delivered to destination servers to ping them. Application-specific latency testing. Traceroute, MTR, and Ping are available to network managers for this. Ping commands check if a host machine is online. MTR tracks device delays and transit time by combining Ping and Traceroute. Latency can be measured with a stopwatch or with elaborate equipment and computer commands. Storage latency is the time it takes a storage array to complete a read request or acknowledge a database write. Storage latency has decreased due to media innovations. Storage latencies were measured in milliseconds before flash SSDs. Flash drives sub-1 ms latencies were a major advance. Read and write latencies are decreased with NVMe drives. For reads and writes, this reference architecture's testing goal was 1 ms or less. Physical reads from storage are random, small-block I/O operations for OLTP applications.\nA tree is a hierarchical data structure that consists of nodes connected by edges. It is a non-linear data structure, with a root node at the top and child nodes below it. Each node in a tree can have one or more child nodes, and these child nodes can have their own child nodes, and so on. The tree data structure is often used to represent hierarchical relationships, such as the structure of a file system or the organization of a company. It is a useful way to organize and store data in a way that is easily searchable and navigable. There are several types of tree data structures, including: Binary tree: A tree in which each node has at most two children. Binary search tree: A binary tree in which the value of each node is greater than the values of all nodes in its left subtree and less than the values of all nodes in its right subtree. Balanced tree: A tree in which the height of the left and right subtrees of each node differs by at most one. Red-black tree: A type of balanced binary search tree in which each node is colored either red or black. N-ary tree: A tree in which each node can have more than two children. Trie: A tree-like data structure used for storing and searching for words or strings. Heap: A tree-based data structure in which the parent node is always larger (or smaller) than its children.\nWhat is Huawei OpenStack? OpenStack is an open-source cloud operating system, or an open-source cloud computing management platform. OpenStack manages underlying resources (computing, storage, and network networking) (or IaaS) and provides services for upper-layer applications. (Bare Metal, VM, Containers) or users to use. Dear friend, OpenStack is an open source cloud platform management project. It is not simply software, but is a combination of several major components that together provide open source software for the construction and management of public clouds, private clouds, and hybrid clouds. Now, tens of thousands of individuals and more than 200 enterprises from more than 100 countries have participated in OpenStack development, including NASA, Huawei, Google, HP, Intel, IBM, and Microsoft. These organizations and individuals use OpenStack as a common front-end of infrastructure as a service (IaaS) resources. The OpenStack project is primarily to simplify cloud deployment and provide good scalability for the cloud. OpenStack or its evolved versions are widely used in various scenarios, including self-built private clouds, public clouds, leased private clouds, and hybrid clouds. Users include Cisco, Intel, IBM, Huawei, and Seagate. OpenStack supports virtual machine (VM) software or containers such as KVM, Xen, Lvc, and Docker. OpenStack manages data center resources and simplifies resource allocation. It manages three types of resources: Computing resources: OpenStack can plan and manage a large number of VMs, allowing enterprises or service providers to provide computing resources as required. Developers can access computing resources through APIs to create cloud applications, while administrators and users can access these resources through web pages.\nHi All, Storage : Dorado 3000 V6 (nvme). please help, how i can get below information? Device model : OceanStor Dorado 3000 V6 Max Read Throughput : Max Read IOPS : Max Write Throughput : Max Write IOPS : Read Latency : Write Latency : Interfaces for ISCSI : RDMA 100G thank you. HDD. The SSD has the read bandwidth of 240 MB/s, write bandwidth of 215 MB/s, read latency less than 100 microseconds, 50,000 read IOPS, and 10,000 write IOPS. HDD vendors are facing a huge threat. Currently, the access performance of SSDs has been improved by 100-fold compared with that of HDDs. For NVMe SSDs, the access performance is 10,000 times higher than that of HDDs. While the latency of storage media has been greatly reduced, the ratio of network latency to the total latency has rocketed from less than 5% to about 65%. That is to say, in more than half of the time, storage media is idle, waiting for the network communication. How to reduce network latency is the key to improving input/output operations per second (IOPS). Disk IOPS and Transmission Bandwidth IOPS is calculated using the seek time, rotation latency, and data transmission time. Seek time: The shorter the seek time, the faster the I/O. The current average seek time is 3 to 15 ms. Rotation latency: It refers to the time required for the platter to rotate the sector of the target data to the position below the h ead. The rotation latency depends on the rotation speed.\nGenerally, the latency is half of the time required for the platter to rotate a full circle. For example, the average rotation latency of a 7200 rpm disk is about 4.17 ms (60 x 1000/7200/2), and the average rotation latency of a 15000 rpm disk is about 2 ms. Data transmission time: It is the time required for transmitting the requested data and can be calculated by dividing the data size by the data transfer rate. For example, the data transfer rate o f IDE/ATA disks can reach 133 MB/s and that of SATA II disks can reach 300 MB/s. Random I/Os require the head to change tracks frequently. The data transmission time is much shorter than the time for track changes. In this case, data transmission time can be ignored. Theoretically, the maximum IOPS of a disk can be calculated using the following formula: IOPS = 1000 ms/(Seek time + Rotation latency). The data transmission time is ignored. For example, if the average seek time is 3 ms, the theoretical maximum IOPS for 7200 rpm, 10k rpm, and 15k rpm disks is 140, 167, and 200, respectively. Advantages of NVMe: Low latency: Data is n ot read from registers when commands are executed, resulting in a low I/O latency. High bandwidth: PCIe X4 can provide up to 4 Gbit/s throughput for a single drive. High IOPS: NVMe increases the maximum queue depth from 32 to 64,000. The IOPS of SSDs is al so greatly improved.\nLow power consumption: The automatic switchover between power consumption modes and dynamic power management greatly reduce power consumption. Wide driver applicability: The driver applicability problem between different PCIe SSDs is s olved. Huawei OceanStor Dorado all-flash storage systems use NVMe-oF to implement SSD resource sharing, and provide 32 Gbit/s FC-NVMe and NVMe over 100 Gbit/s RDMA networking designs. In this way, the same network protocol is used for front-end network connection, back-end disk enclosure connection, and scale-out controller interconnection. RDMA uses related hardware and network technologies to enable NICs of servers to directly read memory, achieving high bandwidth, low latency, and low resource consumption. However, the RDMA-dedicated IB network architecture is incompatible with a live network, resulting in high costs. RoCE effectively solves this problem. RoCE is a network protocol that uses the Ethernet to carry RDMA. There are two versions of RoCE. RoCEv1 is a link layer protocol and cannot be used in different broadcast domains. RoCEv2 is a network layer protocol and can implement routing functions. RDMA Protocol RDMA is short for Remote Direct Memory Access, a method of transferring data in a buffer between application software on two servers over a network. Comparison between traditional mode and RDMA mode: Compared with the internal bus I/O of traditional DMA, RDMA uses direct buffer transmission between the application software of two end points over a network. Compared with traditional network transmission, RDMA does not require operating systems or protocol stacks.\nStorage protocol: A protocol used to write data to a storage system or storage medium or read data from a storage system or storage medium. Storage services can be classified into file storage, object storage, and block storage based on the access mode of the access interface and the location of the file system. Each type of service provides services through various protocols. The following sections describe the protocols involved in file storage and block storage. File storage is also called Network Attached Storage (NAS) . Different operating systems use different sharing protocols. Network File System (NFS) : a sharing protocol used for UNIX and Linux operating systems. Common Internet File System (SMB)/CIFS : a sharing protocol used in the Windows operating system. Apple Filing Protocol (AFP) : a sharing protocol used in the Mac OS. Active Directory (AD) : provides functions such as desktop configuration and permission management to manage multiple devices in a centralized manner. Generally, AD is used to manage Windows hosts and is installed on Windows servers. New Technology LAN Manager (NTLM) : an identity authentication mode. Kerberos : An identity authentication mode that is more secure than NTLM. Lightweight Directory Access Protocol (LDAP) is a directory access protocol used to provide directory information during sharing. It provides signature and encrypted transmission. Network Information Service (NIS) : manages accounts and passwords of multiple Linux hosts in a centralized manner. NIS is installed on a Linux server and is similar to an AD domain.\nBlock storage, also known as storage area network (SAN) , provides LUNs created on storage for hosts, such as creating VMs and databases. BUS: refers to various physical channels that carry protocol transmission. PCIe: an internal bus protocol used for data exchange between the CPU and its peripherals AT Attachment (ATA) : a specific interface standard for Integrated Drive Electronics (IDE) hard disks. Paralle ATA(PATA) : parallel version of the ATA. Serial ATA (SATA) : The serial version of ATA, which is superior to PATA. Small Computer System Interface (SCSI) is a storage unit interface protocol specially designed for small computer systems. SCSI-1 defines the physical interfaces, transport protocols, and standard instruction sets for hard disks, tapes, and other storage devices. SCSI-3 is a set of protocols that use only instruction sets and access models, and no physical interfaces. Initiator (INI) and target (TGT) are required and distinguished by WWN. This is better than SATA. Serial Attached SCSI (SAS) is a physical interface protocol that is compatible with SATA ports and has a short transmission distance. SAS disks cannot be connected to the SATA backplane, but SATA disks can be connected to the SAS backplane. NL-SAS : SAS disk interface and SATA disk body. The performance of NL-SAS disks is lower than that of SAS disks. Fibre Channel (FC ): Hard disks seldom use this type of interface and are generally used as transmission media interfaces. Non-Volatile Memory express(NVMe) : a new interface protocol. The traditional SAS protocol cannot meet the performance requirements of SSDs.\nIt is initially used for SSDs with PCIe interfaces. The I/O path is shortened (RDMA) and there is no complex scheduling layering. External Serial ATA (eSATA): different from SATA ports, SATAe (SATA Express) : Like SATA ports, improves the transfer rate of SSDs. mSATA (mini SATA) : Mini PCIe is used to transmit SATA signals. mSATA is used for solid state disks and is applicable to small storage scenarios. M.2 : Replaces mSATA and uses the Mini PCIE connector for SSDs. It is suitable for small storage applications. Fibre Channel: A protocol used to transmit data based on optical fibers. It has long transmission distance, good performance, and stable latency, but has high costs. It replaces SCSI. FCIP: FC packets are encapsulated into IP packets to extend the transmission distance over the IP network. The upper layer of the two endpoints on the IP network does not know how the lower layer transmits the packets. Therefore, a tunnel is established. iFCP: a gateway-to-gateway protocol. The gateway can process FC packets and provide congestion control and error check. FCoE: maps Fibre Channel to Ethernet, inserts Fibre Channel information into Ethernet packets, and transmits SAN data over Ethernet, allowing LAN and FC SAN data to be transmitted over a single communication cable. Fewer applications. iSCSI: long transmission distance, low cost, multiple memory copies, and poor performance. Using Ethernet to transmit SCSI packets Remote Direct Memory Access (RDMA) is a technology that directly accesses the memory of another computer from the memory of one computer.\nIt allows the NIC to directly access the computer's memory and supports zero-copy network communication without passing through the CPU. There are three types: InfiniBand (IB), RoCE, and iWARP. InfiniBand (IB) : RDMA-based private network, IB network + IB network layer + IB transport layer RoCE v1 : Ethernet-based RDMA technology, Ethernet+IB network layer+IB transport layer RoCE v2 : Ethernet-based RDMA technology, Ethernet+UDP/IP+IB transport layer iWARP : Ethernet-based RDMA technology, Ethernet+TCP/IP+iWARP transport layer protocol iSER: iWARP, RoCE v1, and RoCE v2 SRP: pure IB network transmission SCSI Serial Attached SCSI (SAS): short transmission distance and difficult to expand. SAS is usually used to connect back-end disks without front-end connections. NVMe over fabric (NVMe oF) is a network-based expansion of disks, which supports remote access and enables larger networking. It will replace the traditional SCSI protocol in the future. NVMe over RoCE: It is the most important protocol in NVMe oF and runs the NVMe protocol based on lossless IB networks. Hard Disk Classification Based on Media Solid-state drive (SSD): consists of the control chip, cache chip, and flash chip. Hard disk drive (HDD): consists of disks, heads, rotating shafts, and motors. Digital video disc (DVD) is a medium for storing data. Taped: A thin paper or plastic tape used for magnetic recording of data. HTTP: You can select an HTTP share in the NAS, that is, you can access the file system through a web page. FTP: You can select the FTP protocol on the NAS to access the file system through the FTP software.\nLarge enterprises often have a significant amount of data that needs to be stored, managed, and protected. This can include a variety of types of data, such as financial records, customer information, intellectual property, and operational data. A storage system is necessary to store this data in a centralized location, making it easier to access, manage, and protect. There are several storage solutions that could potentially be suitable for large scale enterprises requiring high availability across multiple zones. Some options to consider include: Cloud storage solutions such as Huawei Cloud Storage, Amazon S3, Google Cloud Storage, and Microsoft Azure Storage provide highly available and scalable storage for large enterprises. These solutions replicate data across multiple availability zones to ensure high availability and durability. Distributed file systems such as Cloud Filestore, Ceph, and Gluster can provide high availability and scalability across multiple zones or regions. These systems replicate data across multiple servers or storage nodes to ensure data is always available and can handle large amounts of data. Object storage systems such as Huawei Object Storage Service (OBS), Amazon S3, Google Cloud Storage, and Microsoft Azure Blob Storage are highly scalable and can store large amounts of data across multiple availability zones. These systems are well suited for storing unstructured data, such as images, videos, and backups. Huawei storage solutions are products and services offered by Huawei that are designed to help businesses and organizations store, manage, and protect their data.\nThese solutions can include hardware, such as storage arrays and servers, as well as software and services, such as data backup and recovery, data migration, and data protection. There are a number of important features that are commonly found in Huawei storage solutions: Performance Huawei storage solutions are designed to offer high performance, with fast data access and low latency. This is important for applications that require fast access to large amounts of data, such as online transaction processing, virtual desktop infrastructure, and online gaming. Scalability Huawei storage solutions are designed to be scalable, allowing them to grow and adapt to changing business needs. This is important for organizations that expect to experience growth or changes in the volume of data they need to store. Reliability Huawei storage solutions are designed to be reliable, with features such as redundant components and data protection to ensure that data is available and protected in the event of hardware or software failures. Data protection: Huawei storage solutions offer a range of data protection features, such as data backup, replication, and snapshotting, to ensure that data is safe and can be recovered in the event of data loss or corruption. Ease of use and management: Huawei storage solutions are designed to be easy to use and manage, with intuitive user interfaces and management tools that make it easy for administrators to set up, configure, and maintain the storage systems.\nCost-effectiveness: Huawei storage solutions are designed to offer a good balance of performance, reliability, and cost, making them an affordable option for businesses of all sizes. Huawei is a leading provider of storage solutions for businesses and organizations of all sizes. Some popular Huawei storage solutions include: Huawei OceanStor: This is a line of enterprise storage systems that offers a range of capabilities, including high-performance data access, data migration, and data protection. It is suitable for use in a variety of environments, including cloud, big data, and traditional IT environments. Huawei HyperMetro This is a storage solution that combines local storage and cloud storage to provide high availability and disaster recovery for mission-critical applications. It is designed to be easy to use and manage, and can be deployed on-premises or in the cloud. Huawei Cloud Storage This is a cloud-based storage solution that offers scalable and secure storage for businesses of all sizes. It is suitable for use in a variety of scenarios, including file storage, backup and recovery, and data archiving. Huawei FusionStorage: This is a distributed storage system that is designed for use in large-scale environments, such as cloud computing and big data. It offers high performance, reliability, and scalability, and is suitable for use in both public and private cloud environments. Image Source : Huawei OceanStor Dorado This is a line of all-flash storage systems that are designed for high-performance applications, such as online transaction processing, virtual desktop infrastructure, and online gaming.\nSmartTier can improve the overall performance of storage systems and reduce user costs. Costs include hardware and software procurement, space, energy and management expenses. It migrates highly active busy data to higher-performance storage media (such as SSD drives) through data migration, migrating idle data with low activity to storage media with higher capacity and lower capacity costs (such as NL-SAS hard drive) to provide shorter response times for higher data, higher IOPS (Input / Output Operations Per Second), and improved storage system performance. SmartTier's statistics, analysis and migration activities are based on SmartTier's implementation strategy and data performance requirements. During the statistics, analysis, and migration activities, there is no impact on existing business continuity and data availability. Intelligent data classification features can automatically match the data of different degrees of activity and different characteristics of the storage media to match, improve storage system performance and reduce user costs. Application scenario Oracle database business, of which 50% is cold data (ie purchase 50% SSD, purchase 50% NL-SAS) A. Storage, B. Storage pool, C. LUNs When the remaining capacity in the storage pool is less than minimum, the data migration activity in the storage pool will stop. Where minimum represents the smaller value of both the total capacity of the storage pool * 10% and 105GB. 1. In the power off state, SSD disks that do not store data cannot last longer than 12 months. SSD disks that have stored data cannot last longer than 3 months. Otherwise, data loss or SSD disk failure may occur. 2.\nThe maximum length of the mechanical hard disks life that has been opened and is in an off-state cannot exceed 6 months. When the maximum permissible time is exceeded, it may cause data loss or mechanical hard disk failure. The maximum permutation time is based on the hard disk drive storage time specifications provided by the manufacturer of the mechanical hard disk. You can view the specifications in the corresponding manual of the mechanical hard disk manufacturer. 3. storage pools must have SSD and SAS disks. Does not affect. 1. SmartTier takes up only a fraction of the memory resources of the storage system. 2. As with any other performance characteristics, SmartTier also increases the overhead of the storage system, which occupies the CPU resource of the storage system. But this overhead will only affect the application when the CPU is a real bottleneck. 3. In addition, when the hard disk is the bottleneck of the storage system, this overhead is very small, because most of the I/O time is used for hard disk read and write, higher CPU utilization and I/O will not bring further delay. 4. Since SmartTier is dynamically matching different active data and different characteristics of the storage medium, the specific calculation of its performance overhead is unrealistic. The impact of this overhead on critical application performance is greater than the overall performance of the storage system. 5. These results show that SmartTier has minimal impact on the overall performance of the storage system. Huawei Support and the Oceanstor v3 documents.\nA fingerprint scanner works by capturing an image of the ridges and valleys on the surface of a person's finger. These patterns, also known as minutiae, are unique to each individual and are used to identify them. There are two main types of fingerprint scanners: optical scanners and capacitive scanners. Optical scanners use a light source and a charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) sensor to capture an image of the fingerprint. The sensor converts the light received into an electrical signal, which is then used to create a digital image of the fingerprint. Capacitive scanners, on the other hand, work by sensing the electrical charge on a person's finger. When a finger is placed on the scanner, it creates a small electrical charge that is different for each person. The scanner captures this charge and uses it to create a digital image of the fingerprint. Once the fingerprint image is captured, the scanner uses a software algorithm to analyze the image and extract the unique characteristics of the fingerprint, such as the position and direction of the ridges and valleys. These characteristics are then used to create a template of the fingerprint, which can be compared to a database of enrolled fingerprints to identify the person. Fingerprint recognition is considered a biometric method of identification, as it uses a person's physical characteristics to identify them. It is considered more secure than traditional methods such as passwords and PINs, as fingerprints are unique to each person and cannot be easily replicated.\nGeneral steps you should take to start developing Android programs: Get familiar with the basics of programming: Before you start developing for Android, it's important to have a solid understanding of programming concepts such as variables, data types, loops, and control structures. Install the Android development environment: To start developing for Android, you will need to install the Android Studio development environment, the official IDE for Android development. Android Studio can be downloaded from the official website. Learn the basics of the Android SDK: The Android SDK (Software Development Kit) is a collection of tools and libraries that you will use to develop Android apps. Learn about the different components of the SDK and how to use them. Learn the basics of the Android programming language: Android apps are typically written in Java, but you can also use other languages like Kotlin. It's important to learn the basics of the Android programming language and the concepts of object-oriented programming. Start building simple apps: Once you have a basic understanding of the development environment and the Android programming language, start building simple apps like a calculator or a to-do list. Learn about the different Android components: Android apps are made up of different components like activities, services, and broadcast receivers. Learn about the different types of components and how to use them. Learn about the Android user interface: Learn about the different types of views and layouts available in Android and how to use them to create a user interface for your app.\nOracle EBS clone and migration are two different processes that are used to move data and configurations from one instance of Oracle E-Business Suite (EBS) to another. A clone is a copy of an existing EBS instance. It creates an exact replica of the source instance, including all data, configurations, and customizations. Cloning is typically used for creating test or development environments, or for creating multiple instances of the same application for different departments or divisions within an organization. Migration , on the other hand, is the process of moving data and configurations from one instance of EBS to another, but with some differences. Migration is usually used when upgrading to a newer version of EBS, or when moving to a different platform, such as moving from an on-premises EBS instance to a cloud-based instance. The migration process generally involves more planning and testing than a clone, as it often involves changes to the data and configurations, such as changes to the database schema. In summary, EBS clone is a process of creating an exact replica of an existing EBS instance while migration is the process of moving data and configurations from one instance of EBS to another, but with some differences. Examples how Oracle EBS clone and migration might be used in real-life scenarios: Clone: A company wants to create a test environment to simulate their production environment. They decide to use the clone option to create an exact replica of their production EBS instance.\nThere are several different data structures that can be used to organize and store data in a computer program. Some of the most common data structures include: Arrays: A collection of elements stored in contiguous memory locations. They can be used to store a fixed-size sequential collection of elements of the same type. Linked Lists: A collection of elements called nodes, where each node points to the next node in the list. They can be used to store a dynamic collection of elements and can be used to implement other data structures such as stacks and queues. Stacks: A last-in, first-out (LIFO) data structure. Elements are added and removed from the top of the stack. Queues: A first-in, first-out (FIFO) data structure. Elements are added to the back of the queue and removed from the front. Trees: A hierarchical data structure where each node has zero or more child nodes. There are different types of trees such as binary tree, AVL tree, B-Tree, etc. Graphs: A non-linear data structure that consists of a finite set of vertices(or nodes) and set of edges connecting these vertices. There are different types of graphs such as directed and undirected, weighted and unweighted. Hash tables: A data structure that uses a hash function to map keys to their corresponding values. It can be used to implement efficient searching and insertion operations. Heaps: A special kind of tree-based data structure in which the tree is a complete binary tree.\nDear all, This post is about login problems withDeviceManager for different browsers. DeviceManager is an integrated storage management software developed by Huawei. DeviceManager has been loaded to the storage system before delivery. On any maintenance terminal connected to the storage system, use a browser to access the IP address of the management network port on the controller of the storage system and use a local or domain user name to log in to the DeviceManager management page. 1. Log in to DeviceManager and configure and manage basic storage services on the graphical user interface (GUI). 2. DeviceManager supports only the TLS protocol (including TLS 1.2 and TLS 1.3). The handshake protocol is responsible for negotiating the cryptographic algorithm and shared key between the client and server, including certificate authentication. It is the most complex part of the four protocols. The password specification change protocol is responsible for signalling to the communicating object how the password is changed. The warning protocol is responsible for communicating the error to the other party when it occurs. Application Data Protocol A protocol responsible for communicating application data carried by TLS to communicating objects. The TLS recording protocol is responsible for message compression, encryption, and data authentication. 3. By default, the DeviceManager allows 32 users to log in at the same time. 4. If the current login user does not perform any operation within the session timeout period ( 30 minutes by default and can be modified), the system automatically logs out and the user needs to log in again. 5.\nIf an account does not log in to the storage system for a specified period ( 60 days by default and can be modified), the account is locked and can be unlocked only by the super administrator. 6. By default, the storage system supports only secure OpenSSL suites. Users fail to log in to DeviceManager using IPv6 on a browser earlier than Firefox 24.0. Cannot add the destination address to an exception or trust list. Do not use Firefox of a version earlier than 24.0. You are advised to use the Chrome browser or use an IPv4 address to log in. An error occurs when you log in to DeviceManager using Internet Explorer 10. When a browser uses HTTPS (HTTP+SSL) to access DeviceManager, an error may occur in the certificate authentication chain of Internet Explorer 10. This is an inherent problem of Internet Explorer 10. The page is blank or cannot be clicked. Press F12 to open the console of Internet Explorer 10. The following message is displayed: script7002: XMLHttpRequest: 0x2f7d Network error. Solution Currently, the browser accesses DeviceManager through HTTPS (HTTP+SSL). The browser and DeviceManager server negotiate a symmetric key to encrypt and decrypt data. If the browser key is damaged, the transmitted data cannot be decrypted on the server. As a result, the browser cannot obtain correct data. The proxy or other program causes the symmetric key information negotiated on the current tab page of the browser to be corrupted. Solution a. Open a new tab of the browser. b.\nHello everyone, This case is about the customer using the Vulnerability Assessment Tooldetected using a weak cipher on port 30002. The customer's VA tool detected that quorum cipher suite is using a weak cipher. This is due to the storage default cipher suite on 6.1.0 version using 0, 1, and 2 which are considered weak, hence in order to fix the VA alarm, we need to add cipher suite 3 & 4 to the storage default storage cipher. Also change the quorum current cipher to 3 & 4, so that the quorum will be able to communicate with storage. Storage Version: 6.1.0SPH15 Quorum Version: 6.1.2 The customer VA tool detected a weak cipher on port 30002. Weak Cipher found on the port 30002 when using cipher suite 0,1,2. hence we provided the below solution to change the default cipher to 3 and 4. after the change VA tool did not flag port 30002. Quorum Usable Cipher Suite on port 30002 is as below 1. Execute show cipher_info to identify the default cipher list and current cipher in the quorum server. Current cipher is the cipher suite that is used for communication between quorum and storage. In storage version 6.1.0SPH15 Default usable cipher suites: cipher_name=AES256+RSA+SHA256, cipher_name=AES256-GCM-SHA384, cipher_name=AES128-GCM-SHA256 All cipher suites: cipher_name=AES256+RSA+SHA256, cipher_name=AES256-GCM-SHA384, cipher_name=AES128-GCM-SHA256, cipher_name=ECDHE-RSA-AES128-GCM-SHA256, cipher_name=ECDHE-RSA-AES256-GCM-SHA384 2. Execute change quorum_server general server_id=0 cipher_name=AES256+RSA+SHA256,AES256-GCM-SHA384,AES128-GCM-SHA256,ECDHE-RSA-AES128-GCM-SHA256,ECDHE-RSA-AES256-GCM-SHA384 to add all the cipher suites to usable cipher suites. Note: Connectivity from quorum to storage will go offline and back online in a minute. 3. Execute show quorum_server general to list the cipher suite.\nHistorically, organizations had centralized computers (mainframe) and information storage devices (tape reels and disk packs) in their data center. The evolution of open systems and the affordability and ease of deployment that they offer made it possible for business units/departments to have their own servers and storage. In earlier implementations of open systems, the storage was typically internal to the server. The proliferation of departmental servers in an enterprise resulted in unprotected, unmanaged, fragmented islands of information and increased operating cost. Originally, there were very limited policies and processes for managing these servers and the data created. To overcome these challenges, storage technology evolved from non-intelligent internal storage to intelligent networked storage. Highlights of this technology evolution include: Redundant Array of Independent Disks (RAID) : This technology was developed to address the cost, performance, and availability requirements of data. It continues to evolve today and is used in all storage architectures such as DAS, SAN, and so on. Direct-attached storage (DAS) : This type of storage connects directly to a server (host) or a group of servers in a cluster. Storage can be either internal or external to the server. External DAS alleviated the challenges of limited internal storage capacity. Storage area network (SAN) : This is a dedicated, high-performance Fibre Channel (FC) network to facilitate block-level communication between servers and storage. Storage is partitioned and assigned to a server for accessing its data. SAN offers scalability, availability, performance, and cost benefits compared to DAS.\n1. Create a backup of the current Oracle HCM installation: Log in to the source machine as the Oracle user. Run the following command to create a full backup of the current Oracle HCM installation, including all data, configurations, and schemas: command expdp system/password directory=data_pump_dir dumpfile=hcm_backup.dmp logfile=expdp.log full=y This command will create a backup file called \"hcm_backup.dmp\" in the directory \"data_pump_dir\" 2. Prepare the new machine: Install a compatible version of the Oracle Database software on the new machine. Configure the new machine with the necessary system settings, such as network settings, storage settings, and memory settings. Install any additional software that is required for the Oracle HCM components you are migrating. 3. Configure the new machine: Create a new database on the new machine using the Oracle Database Configuration Assistant. Configure the new machine to match the settings of the old machine, such as the network settings, database settings, and any other specific configuration settings for the Oracle HCM components you are migrating. 4. Restore the backup on the new machine: Log in to the new machine as the Oracle user. Run the following command to restore the backup of the current Oracle HCM installation, including all data, configurations, and schemas: Copy code impdp system/password directory=data_pump_dir dumpfile=hcm_backup.dmp logfile=impdp.log full=y 5. Update any scripts or automation: Update any scripts or automation that depend on the location of the Oracle HCM installation to point to the new machine. 6.\nTest the new installation: Test the new installation to ensure that all components are working properly and that all data has been migrated correctly. Perform a thorough testing of the Oracle HCM components, including functional, integration and performance testing. 7. Update user access and permissions: Update user access and permissions to ensure that users can access the new Oracle HCM installation. 8. Go live: Once you are satisfied with the new installation, you can make it live and retire the old machine. Communicate with the users and stakeholders before and after the migration process. Note: The above example is for demonstration purposes only, and the actual steps may vary depending on the specific components and version of Oracle HCM that you are migrating. It's always recommended to have a detailed plan and to test the migration process in a test environment before doing it in production. It's also important to have a disaster recovery plan in case of any issues. is it easy to do this kind of migrations Migrating Oracle HCM components or modules from one machine to another can be a complex and time-consuming process, it's not always easy to do. It requires a good understanding of the Oracle HCM components, the version of Oracle HCM being used, and the underlying infrastructure.\nHello, everyone! I need help configuring ESXi NMP multipathing with dorado v3 & HyperMetro. I've configured hosts to use third-party multipath with switchover mode 2. I want to know whether the NMP multipath is configured properly with dorado v3 in HyperMetro. There are only 2 active paths (active I/O) 1 per HBA always to the same controller. There should have 4 active paths. Thanks in advance! Hello, friend! Based on the configuration file Local Preferred HyperMetroWorking Mode is configured and SATP rule is configured as per the recommendation from the guide (VMW_PSP_RR). Based on the working Principles of OS Native MultipathingSoftware, when HyperMetro works in the locally preferred mode, the host multipathing software defines the paths to the owning controller on the local storage array as AO paths and others as ANO paths, therefore this should be the reason why you only see 2 only 2 active paths, 1 per HBA to the same controller. For more details, see . For V3 this is not possible, because this is the working mechanism and the LUN has an owning controller, in this case, the principle is as follows: When ALUA works,the host multipathing software classifies the physical paths to disks as ActiveOptimized (AO) and Active Non-optimized (AN) paths. The host preferentially delivers services to the storage system via the AO paths. An AO path is the optimal I/O access path between the host and the owning controller of a LUN. An AN path is the suboptimal I/O access path between the host and a non-owning controller.\nSCSI Protocol Computers communicate with storage systems through buses. The bus is a path through which data is transferred from the source device to the target device. To put it simple, the high-speed cache of the controller functions as the source device and transfers data to target disks, which serve as the target devices. The controller sends a signal to the bus processor requesting to use the bus. After the request is accepted, the controller's highspeed cache sends data. During this process, the bus is occupied by the controller and other devices connected to the same bus cannot use it. However, the bus processor can interrupt the data transfer at any time and allow other devices to use the bus for operations of a higher priority. A computer has numerous buses, which are like high-speed channels used for transferring information and power from one place to another. For example, the universal serial bus (USB) port is used to connect an MP3 player or digital camera to a computer. The USB port is competent to the data transfer and charging of portable electronic devices that store pictures and music. However, the USB bus is incapable of supporting computers, servers, and many other devices. In this case, SCSI buses are applicable. SCSI, short for Small Computer System Interface, is an interface used to connect between hosts and peripheral devices including disk drives, tape drives, CD-ROM drives, and scanners. Data operations are implemented by SCSI controllers.\nConvergence of Fibre Channel and TCP Ethernet technologies and Fibre Channel technologies are both developing fast. Therefore, it is inevitable that IP SAN and FC SAN that are complementary coexist for a long time. The following protocols use a TCP/IP network to carry FC channels: Internet Fibre Channel Protocol (iFCP) is a gateway-to-gateway protocol that provides Fibre Channel communication services for optical devices on TCP/IP networks. iFCP delivers congestion control, error detection, and recovery functions through TCP. The purpose of iFCP is to enable current Fibre Channel devices to interconnect and network at the line rate over an IP network. The frame address conversion method defined in this protocol allows Fibre Channel storage devices to be added to the IP-based network through transparent gateways. Fibre Channel over Ethernet (FCoE) transmits Fibre Channel signals over an Ethernet, so that Fibre Channel data can be transmitted at the backbone layer of a 10 Gbit/s Ethernet using the Fibre Channel protocol. iFCP Protocol iFCP is a gateway-to-gateway protocol that provides Fiber Channel communication services for Fibre Channel devices on an TCP/IP network to implement end-to-end IP connection. Fibre Channel storage devices, HBAs, and switches can directly connect to iFCP gateways. iFCP provides traffic control, error detection, and error recovery through TCP. It enables Fibre Channel devices to interconnect and network at the line rate over an IP network. The frame address conversion method defined in the iFCP protocol allows Fibre Channel storage devices to be added to the TCP/IP-based network through transparent gateways.\nDear All, Today we are going to learn about SmartThin read and write process. SmartThin Read Process The SmartThin read process is as follows: After receiving a read request from a host, a thin LUN queries the mapping table for the thin LUN and the storage pool to check whether an actual storage space has been allocated to the thin LUN by the storage pool. If the storage pool allocates an actual storage space to the thin LUN, SmartThin uses direct-on-time to read data from the actual storage space and returns the data to the host. If the storage pool does not allocate an actual storage space to the thin LUN, no data is written, SmartThin returns all zeros to the host. Direct-on-time When capacity-on-write is used, the relationship between the actual storage area and logical storage area of data is not calculated using a fixed formula but determined by random mappings based on the capacity-on-write principle. Therefore, when a thin LUN is read or written, the relationship between the actual storage area and logical storage area must be redirected based on a mapping table. A mapping table is used to record the mappings between an actual storage area and a logical storage area. A mapping table is dynamically updated during writes and is queried during reads. Therefore, direct-on-time is classified into read direct-on-time and write direct-on-time.\nSmartThin Write Process The SmartThin write process is as follows: Upon receiving a write request from a host, a thin LUN queries the mapping table between the thin LUN and the storage pool to check whether an actual storage space has been allocated to the thin LUN by the storage pool. If the storage pool has allocated an actual storage space to the thin LUN, data is written to the corresponding area in the storage pool (based on direct-on-time). If the write request asks for releasing space, the system releases the space and returns a response to the host indicating a successful data write. If the storage pool does not allocate actual storage space to the thin LUN, SmartThin uses capacity-on-write to allocate storage from the pool, uses direct-on-time to build a relationship between the actual storage space and the logical storage space, and writes data to the actual storage space. If the write request asks for space to be released, a write success acknowledgement is directly returned to the host. Capacity-on-write Upon receiving a write request from a host, a thin LUN uses direct-on-time to check whether there is physical storage allocated to the logical storage provided for the request. If there is none, a space allocation task is triggered, and the grain size (minimum granularity) is 64 KB. Then data is written to the newly allocated physical storage. Scenarios SmartThin can help core system services that require high service continuity, such as banking transaction systems, expand system capacity online without interrupting ongoing services.\nVideo coding is the science of reducing video size or bit rate without affecting the quality of human perception. Reducing the size of a file is called compression, and the video is compressed using a well-defined and documented set of mathematical tools and algorithms called video codecs. A compressed video is generally a format (called bitstream) that can be understood only by the software that decodes the bitstream. For example, video encoded using the H.264/AVC video codec cannot be decoded by the HEVC codec and vice versa. Once the video is encoded, the quality of the video can be judged objectively or subjectively. Objective measurements include PSNR, SSIM, and VMAF, which are software that uses mathematics to determine video quality. Read more about using PSNR, VMAF, and SSIM here, and see easyVMAF, a convenient tool for VMAF calculations. Subjective measurements such as MOS involve strong human factors, in which a group of people rate the video on a scale of 0 - 5 to indicate its quality. This is also known as Goldeneye viewing. You may hear people alternately using the terms video encoding and video transcoding. While this is usually not opposed, there are nuances between the two. Video encoding generally refers to the process of compressing original, uncompressed video. While video transcoding refers to re-encoding a compressed file, it involves the additional step of decoding incoming video before encoding it. However, don't confuse both processes encode the video at the last step.\nTypically, transcoders have many functions, such as Decode different container formats (mp4 and ts). Different video codecs are used to decode the bitstream, such as H.264/AVC, HEVC, AV1, VP9, etc. Change the resolution of the video to produce a different resolution output (critical for ABR streaming) In addition to transcoding, there are two terms that are rarely used in the industry. Conversion is the act of changing the video bit rate. Transmuxing is the behavior of changing the format of a container (for example, mp4 to avi or ts). With an introduction to video encoding and transcoding, let's now look at the most important rate-distortion tradeoffs in video encoding in the next section. To understand the trade-off between bit rate (or video size) and video quality, it is important to understand how video compression works. You don't have to delve into the quality bit rate trade-offs, but there are a few things you need to know. Video compression algorithm has two important things (in H.264/AVC, HEVC, VP9, AV1, etc.) -- Converts video from the \"pixel domain\" to the \"frequency domain\" using the Discrete Cosine Transform (DCT). If you don't know how DCT works, check out our explanation for 5-year-olds. A technique called quantization is used to discard much of the frequency domain data (called \"coefficients\") while ensuring that such data loss is not perceived by the human eye. Essentially, when you compress the video, you discard some information while ensuring that the video quality is not compromised.\nDear All, Today we are going to learn about disaster Recovery and High availability. Disaster Recovery A disaster is an unexpected event (caused by nature or human error) that results in severe faults or breakdown of the system in a data center. In such an event, services may be interrupted or perform at an unacceptable level. If the system unavailability reaches a certain level at a specific time, the system must be switched to the standby site. Disaster recovery (DR) refers to the capability of recovering data, applications, and services in data centers at different locations when the production center is affected by a disaster. In the DR mode, a redundant site is established in addition to the production site. If the production site is damaged due to a disaster, the redundant site can take over services from the production site to ensure service continuity. To achieve higher availability, customers even establish multiple redundant sites. High availability High availability (HA) ensures that applications can still be accessed when a single component of the local system is faulty, no matter whether the fault is a service software fault, physical facility fault, or IT software/hardware fault. In HA, users using a data center's services would be unaware of a machine breaking down. However, if a server in a data center breaks down, it takes some time for services running on the server to fail, but customers will be aware of the failure. The key indicator of HA is availability. Its calculation formula is [1 (Downtime)/(Downtime + Runtime)].\nDear All, Today we are going to discuss about DR Requirements and Challenges. DR Requirements Three risks: data loss, data damage, and service interruption. The loss caused by service interruption can reach millions of dollars per hour. Regulatory compliance: financial compliance, security isolation, geo-redundant solution, and high service continuity IT O&M: System disaster recovery simplifies IT O&M work and negates the impact of major events DR Challenges Costly investment High capital expenditure (CAPEX) High purchase costs for infrastructure, such as servers, storage devices, and software High basic construction costs for facilities such as equipment rooms. High OPEX Professional O&M support (implementation, training, and onsite support) Long-term costs on resources such as water and electricity. Cumbersome management Multiple devices are not centrally managed. Independent storage media, servers, and network management pages, complex workflows, and low efficiency Complicated capacity expansion The capacity is insufficient and needs to be expanded. The rollout period is long. Limited DR capability Poor security and DR capabilities Data cannot be backed up outside of the data center, and infrastructure faults may cause extreme problems. Poor agility Capabilities such as disaster recovery and data sharing are restricted by physical locations of data. Applications and data cannot be separated. Agile applications and better DR features cannot be built. Diversified applications and inconvenient management: An increasing number of service systems are running in enterprise IT systems, and more and more applications require DR protection as a key service. Common applications include Oracle, DB2, SQL Server, and Exchange.\nDear All, Today we are going to learn about Types of DR. Active-Passive DR Solution DR management visualization: Deployment of DR management software and one-click commissioning One-click DR drills and switchover and assistance for customized script tools, enabling one-click recovery of the backup service system Mature and efficient DR services: One-stop analysis, design, delivery, and drills for DR systems Support for reusing legacy non-Huawei devices to perform DR solutions, saving money for customers Active-Active DR Solution Gateway-free and efficient active-active DR Reliable service-level architecture, ensuring zero data loss upon DC-level faults and 24/7 services. No virtualization gateway on the active-active storage layer, reducing failure points and simplifying implementation and commissioning Geo-Redundant DR Solution Short DR construction period and low delivery risks DR construction period reduced by 30% from ten months to seven months Cooperation of multiple vendors for efficient management, reducing project delivery periods Effective evaluation and analysis of multiple services and applications to ensure rapid DR system construction Ensuring effective verification of DR design and reducing project implementation risks Visualized remote DR management One-click visualized deployment and commissioning of DR management software Devices in the production center, same-city DR, and remote DR centers are centrally managed and monitored, simplifying device maintenance. The visualized management supports one-click DR testing and switchover and enables customers to customize scripts to recover the standby service system by one click, simplifying the management and maintenance of the DR system.\nHello all, This post introduces the networking modes of the Business Continuity and Disaster Recovery Solution (Geo-Redundant Mode). SAN HyperReplication Cascading Network Figure 1 illustrates this networking mode. Figure 1 SAN HyperReplication cascading network Table 1 lists the models and versions of storage systems supported by production center A, intra-city DR center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems. Table 1 Supported storage models and versions and interworking requirement (in the SAN HyperReplication cascading network scenario) Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems Production center A OceanStor V3 series V300R002 and later OceanStor V5 series OceanStor Dorado V3 series V300R001C21 and later Remote replication (synchronous and asynchronous) can be configured between any two of the supported storage systems. Intra-city DR center B Remote DR center C SAN HyperReplication Parallel Network Figure 2 illustrates this networking mode. Figure 2 SAN HyperReplication parallel network Table 2 lists the models and versions of storage systems supported by production center A, intra-city DR center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems.\nTable 2 Supported storage models and versions and interworking requirement (in the SAN HyperReplication parallel network scenario) Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems Production center A OceanStor V3 series V300R002 and later OceanStor V5 series OceanStor Dorado V3 series V300R001C21 and later Remote replication (synchronous and asynchronous) can be configured between any two of the supported storage systems. Intra-city DR center B Remote DR center C SAN HyperMetro + SAN HyperReplication In this networking mode, two combinations are supported: SAN HyperMetro + SAN synchronous replication and SAN HyperMetro + SAN asynchronous replication. See Figure 3. Figure 3 SAN HyperMetro + SAN HyperReplication If site A is configured as the preferred site, two networking schemes are supported in each combination: Cascading network: A (preferred)-B-C Parallel network: C-A (preferred)-B Table 3 lists the models and versions of storage systems supported by data center A, data center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems. Table 3 Supported storage models and versions and interworking requirement (in the SAN HyperMetro + SAN HyperReplication scenario) Solution Category Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems SAN HyperMetro + SAN HyperReplication/S Data center A OceanStor V3 series V300R006C00 and later OceanStor V5 series OceanStor Dorado V3 series V300R001C30 and later The storage systems at both sites in the SAN HyperMetro solution must meet the active-active deployment requirements.\nIf the active asynchronous remote replication link between A and C fails, the system automatically switches replication services to the standby asynchronous remote replication link between B and C. Table 4 lists the models and versions of storage systems supported by data center A, data center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems. Table 4 Supported storage models and versions and interworking requirement (in the SAN DR Star scenario) Solution Category Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems HyperMetro + HyperReplication/A Data center A OceanStor V3 series V300R006C30 and later OceanStor V5 series V500R007C20 and later OceanStor Dorado V3 series V300R002 and later The storage systems at both sites in the SAN HyperMetro solution must meet the active-active deployment requirements. As long as the storage system in remote DR center C meets the storage model and version requirement, it can establish a synchronous remote replication relationship with the storage system in data center B. Data center B Remote DR center C HyperReplication/S + HyperReplication/A Data center A OceanStor V3 series V300R006C30 and later OceanStor V5 series V500R007C20 and later OceanStor Dorado V3 series V300R002 and later Remote replication (synchronous and asynchronous) can be configured between any two of the supported storage systems. Data center B Remote DR center C NAS HyperMetro + NAS HyperReplication Figure 5 illustrates this networking mode.\nFigure 5 NAS HyperMetro + NAS HyperReplication If site A is configured as the preferred site, two networking schemes are supported: Cascading network: A (preferred)-B-C Parallel network: C-A (preferred)-B Table 5 lists the models and versions of storage systems supported by data center A, data center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems. Table 5 Supported storage models and versions and interworking requirement (in the NAS HyperMetro + NAS HyperReplication/A scenario) Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems Data center A OceanStor V3 series V300R006C10 and later OceanStor V5 series The storage systems at both sites in the NAS HyperMetro solution must meet the active-active deployment requirements. As long as the storage system in remote DR center C meets the storage model and version requirement, it can establish a NAS asynchronous remote replication relationship with the storage system in data center B. Data center B Remote DR center C NAS HyperMetro + HyperVault Figure 6 illustrates this networking mode. Figure 6 NAS HyperMetro + HyperVault If site A is configured as the preferred site, two networking schemes are supported: Cascading network: A (preferred)-B-C Parallel network: C-A (preferred)-B Table 6 lists the models and versions of storage systems supported by data center A, data center B, and remote DR center C in this solution and describes the requirement for interworking between storage systems.\nTable 6 Supported storage models and versions and interworking requirement (in the NAS HyperMetro + HyperVault scenario) Data Center Location Storage Model and Version Requirement for Interworking Between Storage Systems Data center A OceanStor V3 series V300R006C10 and later OceanStor V5 series The storage systems at both sites in the NAS HyperMetro solution must meet the active-active deployment requirements. As long as the storage system in remote DR center C meets the storage model and version requirement, it can establish a HyperVault relationship with the storage system in data center B. Data center B Remote DR center C Different networking modes apply to different scenarios. Table 7 lists the characteristics of different networking modes. Table 7 Characteristics of different networking modes Network Type Advantage Disadvantage SAN HyperReplication cascading network The cascading network has minor impact on the performance of the production center. When a regional disaster occurs, the value of the system recovery point objective (RPO) is large if the entire intra-city disaster recovery center is damaged. (The specific value depends on the asynchronous replication period.) SAN HyperReplication parallel network When a regional disaster occurs, the parallel network effectively avoids the drawbacks of the cascading network. The parallel network has high requirements for the performance of the production center. SAN HyperMetro + SAN HyperReplication In this solution, two data centers provide service simultaneously. In addition, this solution can protect services against regional disasters, whereas the active-active data center solution cannot. The performance requirement is high for data centers A and B.\n1. Powering On and Off the Storage System 2. Power On and Off interface module 3. Restarting the Storage Device 4. Emergency power-on and power-off 5.Powering on the Storage System (Remotely on the CLI) Method 1 After all devices are installed, power on them and check that they are working correctly. Prerequisites Ensure that all devices are properly installed and device installation check is completed. Otherwise, power on them after these devices and hardware are correctly installed. Context NOTICE Before powering on a storage system, ensure that all expansion cables have been properly connected. If expansion cable connections are adjusted after a storage system is powered on, the storage system may malfunction. During the power-on, do not remove or insert optical fibers, network cables, coffer disks, or interface modules to avoid system data loss. When powering on a storage system, do not wear an ESD wrist strap to avoid electric shock. NOTE The correct power-on sequence is as follows: 1. Turn on the external power switches corresponding to all the devices. 2. Press the power button on each controller enclosure. 3. Power on switches (if any switch is deployed but not powered on). 4. Power on application servers. Procedure 1. Connect the external power supply (PDB or PDU), as shown in Figure 1-1 . a. In scenarios where PDB is used, check labels on power cables and match power cables with power switches in the cabinet PDB.\nFor example, if the label on the AC power cable connected to a controller enclosure displays PowerBox Output B_8, it indicates that the power cable of the controller enclosure corresponds to power switch SW8 on module Output B. You can turn on that power switch to power on the controller enclosure. b. Connect devices in the cabinet with external power supplies in the following sequence: disk enclosure, controller enclosure, switch (in the SAN network), and application server. Figure 1-1 Connecting external power supplies 2. Press the power button on each controller enclosure. NOTE You only need to press the power button once. If the power indicator of the controller enclosure is blinking green, the storage system is being started. Do not hold down the power button. If the power button is held for 5 seconds, the system is powered off. The power-on process takes 15 to 30 minutes. After the controller enclosure is powered on, other disk enclosures connected will be automatically powered on. After system power-on, the disk initialization process automatically starts. The time required for the process is dependent on the quantity of the disks to be initialized. Figure 1-2 Power button on 2 U controller enclosure Figure 1-3 Power button on 4 U controller enclosure Follow-up Procedure After the storage device has been powered on, verify that the indicator of each hardware component is in a normal state by referring to Checking the Running Status of the Storage Device.\nMethod 2 Prerequisites Ensure that all devices are properly installed and device installation check is completed. Otherwise, power on them after these devices and hardware are correctly installed. Context NOTICE Before powering on a new 2 U controller enclosure, ensure that its slots 0 to 3 (where coffer disks reside) are fully populated with disks. Before powering on a new 3 U controller enclosure, ensure that it has connected to a configuration disk enclosure on which slots 0 to 3 (where coffer disks reside) are fully populated with disks. Ensure that the configuration disk enclosure is a new device that never connects to any controller enclosure. Before powering on a storage system, ensure that all expansion cables have been properly connected. If expansion cable connections are adjusted after a storage system is powered on, the storage system may malfunction. During the power-on, do not remove or insert optical fibers, network cables, coffer disks, or interface modules to avoid system data loss. When powering on a storage system, do not wear an ESD wrist strap to avoid electric shock. If the storage system is off, ensure that the external power supplies are also turned off. After turning on the external power switches, press the power button on each controller enclosure within 40 minutes. NOTE The correct power-on sequence is as follows: 1. Turn on the external power switches corresponding to all the devices. 2. Press the power button on each controller enclosure. 3.\nPress the power button on the OceanStor Dorado NAS unit (applicable to V300R002C10 and later). 4. Power on switches (if any switch is deployed but not powered on). 5. Power on application servers. Procedure 1. Connect the external power supply (PDB or PDU), as shown in Figure 2-1 . a. In scenarios where PDB is used, check labels on power cables and match power cables with power switches in the cabinet PDB. For example, if the label on the AC power cable connected to a controller enclosure displays PowerBox Output B_8, it indicates that the power cable of the controller enclosure corresponds to power switch SW8 on module Output B. You can turn on that power switch to power on the controller enclosure. b. Connect devices in the cabinet with external power supplies in the following sequence: disk enclosure, controller enclosure, switch (in the SAN network), and application server. Figure 2-1 Connecting external power supplies 2. Press the power button on each controller enclosure. NOTICE You only need to press the power button once. If the power indicator of the controller enclosure is blinking green, the storage system is being started. Do not hold down the power button. If the power button is held for 5 seconds, the system is powered off. The power-on process takes 15 to 30 minutes. After the controller enclosure is powered on, other disk enclosures connected will be automatically powered on. After system power-on, the disk initialization process automatically starts.\nThe time required for the process is dependent on the quantity of the disks to be initialized. 3. For V300R002C10 and later, press the power button on the OceanStor Dorado NAS unit if it is used on the network. Follow-up Procedure NOTE The following figures and indicator descriptions are only used to help you determine whether a storage system is successfully powered on. For details about indicators, see \"Hardware Architecture\" in the OceanStor Dorado5000 V3 and Dorado6000 V3 V300R002 Product Description. Figure 2-2 , Figure 2-3 , Figure 2-4 , and Figure 2-5 show the indicator status displayed when a storage system is successfully powered on.\nFigure 2-2 Indicators on a 2 U controller enclosure (front view) 1 Running indicator on a disk module (steady green) 2 Location/Alarm indicator on a disk module (off) 3 Location indicator on a controller enclosure (off) 4 Alarm indicator on a controller enclosure (off) 5 Power indicator/power button on a controller enclosure (steady green) - - Figure 2-3 Indicators on a 3 U controller enclosure (front view) 1 Running/Alarm indicator on a BBU (steady green) 2 Alarm indicator on a controller enclosure (off) 3 Location indicator on a controller enclosure (off) 4 Power indicator on a controller (steady green) 5 Running/Alarm indicator on a fan module (steady green) 6 Alarm indicator on a controller (off) 7 Power indicator/power button on a controller enclosure (steady green) - - Figure 2-4 Indicators on a 2 U disk enclosure (front view) 1 Running indicator on a disk module (steady green) 2 Location/Alarm indicator on a disk module (off) 3 Location indicator on a disk enclosure (off) 4 Alarm indicator on a disk enclosure (off) 5 Power indicator on a disk enclosure (steady green) - - Figure 2-5 Indicators on the front panel of the OceanStor Dorado NAS unit (applicable to V300R002C10 and later) 1 Running indicator on a disk module (steady green) 2 Location/Alarm indicator on a disk module (off) 3 Location indicator on a controller enclosure (off) 4 Alarm indicator on a controller enclosure (off) 5 Power indicator/power button on a controller enclosure (steady green) - - Check whether the following requirements are met.\nIf yes, the storage system is successfully powered on and is running properly. The Power indicators on the OceanStor Dorado NAS units (applicable to V300R002C10 and later), controllers, controller enclosures, and disk enclosures are steady green. The Alarm indicators on the OceanStor Dorado NAS units (applicable to V300R002C10 and later), controllers, controller enclosures, and disk enclosures are off. The Running indicators on coffer disks are steady green, and the Alarm/Location indicators are off. Disks with coffer disk labels are coffer disks. Before services are configured, the Running indicators on the first three coffer disks are blinking green, and the Alarm/Location indicators are off. If any Alarm indicator is on, the device is faulty and you need to rectify the fault. For details about how to rectify faults, see OceanStor Dorado V3 Series V300R002 Troubleshooting of the storage system. If a power indicator is not steady green, contact Huawei technical support engineers. Respect the correct power-off sequence when you power off the storage device especially for replacing cabinet subracks or removing power link failures. Prerequisites No service is running on the storage device. Context Typically, a storage system is powered off in the following sequence: Stop host services, power off the controller enclosure, and then disconnect the external power supply. For V300R002C10 and later versions, if an OceanStor Dorado NAS unit is used, the storage system should be powered off in the following sequence: Stop host services, power off the OceanStor Dorado NAS unit, power off the controller enclosure, and then disconnect the external power supply.\nScenario 1: None of the Controller Enclosures in the Cluster Are Powered On If none of the controller enclosures in the cluster are powered on, you can use the BMC system's external management IP address to log in to the BMC system and run the power-on command to power on all controller enclosures in the cluster. The implementation process is as follows: 1. Use the IP address of the management network port to log in to the BMC system. NOTE The way to log in to the BMC system is the same as the way to log in to the CLI of the storage system through the management network port. If the storage system is not powered on, you will be directly logged in to the BMC system after making an attempt to log in to the CLI of the storage system through the management network port. 2. Run the power-on command to power on all controller enclosures in the cluster. Scenario 2: Some Controller Enclosures in the Cluster Are Not Powered On If some controller enclosures in the cluster are not powered on, you can log in to the storage system using the IP address of the management network port and run the power-on command to power on the controller enclosures that are not powered on in the cluster. The implementation process is as follows: 1. Use the IP address of the management network port to log in to the CLI of the storage system. 2.\nRun the power-on command to power on the controller enclosure that is not powered on in the cluster. This section describes the networking rules and networking diagrams for powering on a storage system remotely. Networking Rules If a storage system has only one controller enclosure, ensure that: The power cables of the controller enclosure are correctly connected. The maintenance terminal is connected to any management network port on the controller enclosure. If a storage system has multiple controller enclosures, ensure that: The power cables of all controller enclosures are correctly connected. The maintenance terminal is connected to any management network port on any controller enclosure. The scale-out cables of all controller enclosures are correctly connected. The maintenance network ports on all controller enclosures are correctly connected according to the standard networking mode. Networking Diagrams The cable connections of a single controller enclosure are simple. This section only describes the networking mode in the multi-controller-enclosure scenario. In the following figures, purple cables are management network cables, and orange and blue cables are scale-out cables. To use the remote power-on function, connect network cables (green cables in the following figures) to maintenance ports on controller enclosures. NOTE If green cables are not connected, after the power-on command is executed, only the controller enclosure where the currently logged-in controller resides can be powered on. Other controller enclosures cannot be powered on.\nFigure 1 OceanStor Dorado 3000 four-controller networking diagram Figure 2 OceanStor Dorado 5000 and Dorado 6000 four-controller networking diagram Figure 3 OceanStor Dorado 8000 and Dorado 18000 eight-controller networking diagram This section describes how to remotely power on the storage system when none of the controller enclosures in the cluster are powered on. Prerequisites Cables have been connected according to the standard networking mode. The power cables of all controller enclosures in the cluster have been connected. None of the controller enclosures in the cluster are powered on. This operation should not apply if some controllers in a controller enclosure are not powered on. If the password of the storage system super administrator admin is not initialized, the BMC administrator cannot access the BMC system through the serial port and cannot use the remote power-on function. Procedure 1. Log in to the BMC system using the management IP address as the BMC administrator. NOTE For OceanStor Dorado 6.1.0 and earlier versions, you cannot log in to the storage system using an IPv6 address to run a command to remotely power on the storage system. This step is the same as the way to log in to the CLI through the IP address of the management network port. If the storage system is not powered on, you will be directly logged in to the BMC system after making an attempt to log in to the CLI of the storage system through the management network port.\nDear All, Today we are going to learn about Disaster Recovery Technologies Disaster Recovery Technologies Host-based DR technology: Data replication software is installed on hosts in the production center and the DR center. Remote switchover software can also be installed at the host layer to form a complete application-level DR solution. This data replication mode has low costs, mainly in software procurement, and is compatible with servers and storage devices of different brands. It is suitable for users with complex hardware composition. The software occupies a large number of host and network resources. Host-Based DR can support the Following. Host Layer DR Technology - Application Level Host Layer DR Technology - Database Level Host Layer DR Technology - Logical Volume Level We will discuss it in upcoming articles in more details. Network-based DR technology: A storage gateway is added to the storage area network (SAN) between the front-end application servers and back-end storage systems. The storage gateway establishes a mirroring relationship between two volumes on different storage devices. When data is written to the primary volume, the gateway also writes the data to the backup volume. When the primary storage device experiences a fault, services are switched to the secondary storage device and the backup volume is used to ensure non-disruptive data services. A smart switch is added to the storage area network (SAN) between the front-end application servers and back-end storage systems.\nDifferences between traditional enterprise applications and cloud computing applications. 2015-2020 Cloud applications will gradually become the mainstream. What are the key factors driving the growth of the vertical market software market? Demand for vertical market software is expected to rise as the use of vertical market shares promotes better organization and more efficient use of resources. In addition, there is less competition in the vertical market software market as participating designers and developers rely heavily on specific players and customers to purchase their products. Vertical market software market players offer advantages in product features, data analytics, costs, and sales and marketing. In terms of functionality, vertical software products have the ability to modify their solutions to better meet the needs of various industries. Sales of vertical market software are expected to grow as they enable the creation of tailored, industry-specific tools, as well as more user-friendly UI/UX. This personalization is particularly important in providing data analytics solutions. Vertical market software participants benefit from the ability to create highly customized data analytics use cases. In addition, vertical market software reduces customers' overall technology costs in terms of cost. Finance industry : financial private cloud + hybrid cloud solution M&E industry : cloud platforms that support distributed deployment and mass data processing Telecom operators : NFV/SDN transformation NFV/SDN: Transform the CT network with IT technologies Directly migrate non-core traditional enterprise applications to the cloud computing platform without any modification Enterprise-level core applications are not suitable for virtualization and cloudification.\nHyperSnap: The RTO and RPO of common data backup are long and services are interrupted. HyperClone: An independent copy needs to be quickly obtained from the production environment for testing, analysis, or DR drills. HyperLock: Some data needs to be protected against tampering. HyperSnap: saves storage space. Data of multiple snapshots is independent of each other. HyperClone: Quick deployment saves storage space, does not interrupt services, ensures the read and write performance of source data, and generates multiple copies at the same point in time. HyperLock: Data can be written once and read multiple times, preventing data from being tampered with and protecting important data from being archived. HyperSnap: data protection and simulation test. HyperClone: application testing and data analysis, DR drill, and IT O&M. HyperLock: implements read-only protection for archived data. HyperSnap: Redirect on write (ROW). When data is added or modified, the new data is stored in a new area. You only need to modify the file metadata and the pointer of the new data block. After data is deleted, the original data is stored in the snapshot space. HyperClone: ROW. The parent file system, file system snapshots, and clone file systems all share a single data source. A clone file system is also a snapshot, but it is readable. Modifying a clone file system is equivalent to modifying a snapshot. During splitting, a copy of the data shared by the clone file system and the parent file system is made.\nSmartVirtualization is a heterogeneous virtualization feature developed by Huawei. When a local storage system (OceanStor Dorado V6 series storage system) is connected to another type of Huawei storage system or a third-party storage system, this feature enables the local storage system to use and manage storage resources of the peer storage system as local storage resources despite of the different software and hardware architectures. SmartVirtualization resolves the incompatibility issues among different storage systems, so users can manage heterogeneous storage systems and use storage resources from both legacy and new storage systems, protecting customer investments.Service Data Migration Between Storage Systems As services grow continuously, more storage is required for storing increasing data. The legacy storage system cannot provide satisfactory data storage capacity and performance. In this case, you can purchase a storage system that provides a larger capacity and better performance to replace the legacy storage system. As software and hardware of the legacy and new storage systems are different, the services may be interrupted and data may be lost during data migration. SmartVirtualization can mask the differences between storage systems to map an external LUN of the legacy storage system to the new storage system (presented as an eDevLUN on the new storage system). Figure5 Migrating data from a legacy storage system to a new storage system After a legacy storage system is replaced with a new storage system, some data in the new storage system is rarely accessed, which is called cold data.\nOn the morning of December 20th, when doing the replacement for the DIMM140 memory module of controller 0B, controller 0A is removed by mistake. After controllers 0A and 0B are inserted back into the controller enclosure, controller 0A cannot be powered on and the disk domain fault alarm is generated. As a result, services on the storage device are interrupted. 1. Storage Event Analysis According to the storage event records, at around 10:25 controller 0B start to offline. Few minutes later, controller 0A is removed by mistake. For the 6800F V5 storage system, its consisted of 4 controllers, usually, controllers 0A and 0B are mirrored to each other, and controllers 0C and 0D are mirrored to each other. Since controller 0A and controller 0B are offline at the same time, engine 0 (controller 0A and controller 0B) fault. Controller 0A is inserted back at around 10:30, and the controller 0B memory module was replaced successfully at around 11:00, but the storage internal process didnt finish success after both controllers were back, and the disk domain is faulty when the controllers powered on. 2. Storage Log Analysis we queried the storage process and found that the controller 0A still not finish the power-on process. In the storage log, we can find the controller 0A power-on process failure is due to I/O error when the storage system tried to recover the system object of the controller 0A. The I/O error is because the service image of this engine is faulty.\nHello, everyone! Can you share the suggested software maintenance checklist? What are the routine maintenance items? Thanks in advance! Hello, friend! This list of maintenance items and frequencies helps system administrators check the device environment and device status. If a fault occurs, it will be detected and rectified in a timely manner, ensuring that the storage systems continue running normally. First maintenance items Maintenance Item Operation Checking the installation of SmartKit tools On the maintenance terminal, check whether SmartKit tools have been installed, including: Collect Device Archives Information Collection Disk Health Analysis Inspection Patch Tool NOTE: If SmartKit has not been installed, log in to , search SmartKit , and download the installation package and operation guide of the corresponding version. Follow instructions in the operation guide to install tools. Checking the installation and configuration of eService On the maintenance terminal, check whether eService has been installed and an appropriate alarm policy has been configured. NOTE: If eService has not been installed, log in to , search eService , and download the installation package and operation guide of the corresponding version. Follow instructions in the operation guide to install tools. Checking the alarm policy configuration On DeviceManager, check whether the alarm policy has been configured. Email notification SM notification System notification Alarm dump Trap IP address management USM user management Alarm masking Syslog notification NOTE: If it has not been configured, see section Configuring Alarm Handling Policies of the of the corresponding product model.\nLDAP (Lightweight Directory Access Protocol) is a software protocol that enables anyone to locate information about companies, individuals, and other network resources such as files and devices, whether on the Internet or an intranet. LDAP is a \"lightweight\" (less code) version of Directory Access Protocol (DAP), which is a component of the network directory services standard X.500. A directory informs the user of the network location of a resource. On TCP/IP networks (including the internet), the domain name system (DNS) is the directory system used to map domain names to their respective network addresses (a unique location on the network). However, the user may be unfamiliar with the domain. LDAP enables a user to look for a person without knowing their location (although additional information will help with the search). Commonly, LDAP is used to provide a central location for authentication storing usernames and passwords. With a plugin, LDAP can subsequently be utilized in many applications or services to validate users. LDAP can be used to validate Docker, Jenkins, Kubernetes, Open VPN, and Linux Samba server identities and passwords, as examples. System administrators can also use LDAP single sign-on to control access to an LDAP database. Additionally, LDAP can be used to add operations to a directory server database, authenticate or bind sessions, delete LDAP entries, search and compare entries using different commands, edit existing entries, extend entries, abandon requests, or unbind operations. LDAP is accessed by most workers multiple times each day, often by the hundreds.\nEven when the methods to fulfill a query are numerous and complex, that person may not even realize the connection has happened. Common components of an LDAP query include: Linked session. The LDAP port is the means by which the user communicates with the server. Request. The user communicates with the server by submitting a query. In response, the LDAP protocol searches the directory, retrieves the requested data, and sends it to the client. Completion. The client drops the LDAP connection. Although it appears to be a straightforward search, a lot of code is actually behind the scenes making this feature work. The search's maximum size, the maximum amount of time the server can spend processing it, the maximum number of searchable variables, and many other factors are all up to the developers. It's possible for an employee to use LDAP searches at each of their temporary workplaces. However, LDAP settings can significantly alter how searches are conducted and how the LDAP operates. The LDAP needs to verify the user's identity before performing any kind of search. There are two strategies that can be used to accomplish that task: Simple. Users can access the server once they have entered the correct username and password. Uncomplicated Authentication and Protection Layer (SASL). Before a user is allowed to join, authentication must be completed by a secondary service like Kerberos. This is a viable choice for businesses that value top-notch safety.\nDear All, Today we are going to learn about PCIe, NVMe and SAS vs NVMe. Why do we choose PCIe? PCIe is used to obtain significantly improved system throughput, scalability, and flexibility at lower production costs, which are almost impossible to achieve using the traditional bus-based interconnection. High-performance and high-bandwidth serial interconnection standard: PCIe PCIe is future-oriented, and higher throughputs can be achieved in the future. PCIe is providing increasing throughput using the latest technologies, and the transition from PCI to PCIe can be simplified by guaranteeing compatibility with PCI software using layered protocols and drives. The PCIe protocol has the following features: Point-to-point connection High reliability Tree networking Full duplex Frame-structure-based transmission PCIe Protocol Structure PCIe device layers include the physical layer, data link layer, transaction layer, and application layer. Physical layer Data link layer Transaction layer Application layer The physical layer in a PCIe bus architecture determines the physical features of the bus. In future, the performance of a PCIe bus can be further improved by increasing the speed or changing the encoding or decoding mode. Such changes only affect the physical layer, facilitating upgrades. Data link layer The data link layer ensures the correctness and reliability of data packets transmitted over a PCIe bus. It checks whether the data packet encapsulation is complete and correct, adds the sequence number and CRC code to the data, and uses the ack/nack handshake protocol for error detection and correction.\nTransaction layer The processing layer receives read and write requests from the software layer or creates a request encapsulation packet and transmits it to the data link layer. This type of packet is called a transaction layer packet (TLP). The TLP receives data link layer packets (DLLP) from the link layer, associates the DLLP with a related software request, and transmits it to the software layer for processing. NVMe NVMe is short for Non-Volatile Memory Express. The NVMe standard is oriented to PCIe SSDs. Direct connection from the native PCIe channel to the CPU can avoid the latency caused by communication between the external controller (PCH) of the SATA and SAS interface and the CPU. PCIe is an interface form and a bus standard. NVMe is a standard interface protocol customized for PCIe SSDs. In terms of the entire storage process, NVMe not only serves as a logical protocol port, but also as an instruction standard and a specified protocol. The low latency and parallelism of PCIe channels and the parallelism of contemporary processors, platforms, and applications can be used to greatly improve the read and write performance of SSDs with controllable costs. They can also reduce the latency caused by the Advanced Host Controller Interface (AHCI) and ensure enhanced performance of SSDs in the SATA era NVMe Protocol Stack In terms of the transmission path, I/Os of a SAS all-flash array are transmitted from the front-end server to the CPU through the FC/IP front-end interface protocol of a storage device.\nThey are then transmitted to a SAS chip, a SAS expander, and finally a SAS SSD through PCIe links and switches. The Huawei NVMe-based all-flash storage system supports end-to-end NVMe. Data I/Os are transmitted from a front-end server to the CPU through a storage device's FC-NVMe/NVMe Over RDMA front-end interface protocol. Back-end data is transmitted directly to NVMe-based SSDs through 100 Gbit/s RDMA. The CPU of the NVMe-based all-flash storage system appears to communicate directly with NVMe SSDs via a shorter transmission path, providing higher transmission efficiency and a lower transmission latency. In terms of software protocol parsing, SAS- and NVMe-based all-flash storage systems differ greatly in protocol interaction for data writes. If the SAS back-end SCSI protocol is used, four protocol interactions are required for a complete data write operation. Huawei NVMe-based all-flash storage systems require only two protocol interactions, making them twice as efficient as SAS-based all-flash storage systems in terms of processing write requests. Advantages of NVMe: Low latency: Data is not read from registers when commands are executed, resulting in a low I/O latency. High bandwidth: PCIe X4 can provide up to 4 Gbit/s throughput for a single drive. High IOPS: NVMe increases the maximum queue depth from 32 to 64,000. The IOPS of SSDs is also greatly improved. Low power consumption: The automatic switchover between power consumption modes and dynamic power management greatly reduce power consumption. Wide driver applicability: The driver applicability problem between different PCIe SSDs is solved.\nFailed to create a HyperMetro pair. A HyperMetro pair fails to be created, and the following message is displayed: The type combination of the local and remote LUNs is not supported.Suggestion: Use an allowed member LUN type combination, such as thick LUN and thick LUN, thin LUN and thin LUN, eDevLUN and eDevLUN, and eDevLUN and thick LUN. If a HyperMetro pair is created based on two existing LUNs, the LUN types and capacities must be the same. If one device is a converged storage device and the other is a Dorado storage device, the LUN type of the converged storage device can be thick or thin, but the LUN type of the Dorado storage device can only be the thin. If the LUN type of the converged storage device is thick LUN, and HyperMetro is configured for the and Dorado devices, the LUN types are inconsistent. As a result, the HyperMetro pair fails to be created. Therefore, you are advised to select devices with the same product model and patch version when configuring active-active services. Supplement: When adding a HyperMetro pair to an existing HyperMetro consistency group, ensure that the consistency group is in the suspended state. When creating a LUN, ensure that the types and capacities of the two LUNs are the same. That is, select SmartThin when creating a LUN on the converged storage device. Failed to delete the HyperMetro consistency group. The following message is displayed: A HyperMetro consistency group cannot be deleted locally or forcibly started when the link is up.\nSuggestion: Perform this operation when the link is down. When deleting a HyperMetro consistency group, we had selected \"Only delete the configuration information about the local device if the HyperMetro consistency group fails or the remote device is disconnected\", which is to ensure that the HyperMetro consistency group can be deleted from a single device when an exception occurs. If the HyperMetro consistency group is running properly and needs to be deleted, do not select \"Only delete the configuration information about the local device if the HyperMetro consistency group fails or the remote device is disconnected\". If a HyperMetro consistency group is in the normal state and needs to be deleted, do not select this option \"Only delete the configuration information about the local device if the HyperMetro consistency group fails or the remote device is disconnected\". Failed to delete a HyperMetro pair. The following message is displayed: Description: The operating status of the HyperMetro does not support deletion. Suggestion: Delete the HyperMetro when it is in the Pause, Force Start, or To Be Synchronized state. When a HyperMetro pair is running properly, LUNs in the pair are in the dual-write state. If you delete a pair directly, data will be lost. Therefore, you cannot delete a pair that is running properly. Before deleting a HyperMetro pair, set the status of the pair to Paused (usually services at non-preferred sites are suspended). Failed to delete a HyperMetro pair. The following message is displayed: The connection between the local and remote devices is normal.\nSuggestion: Select \"Only delete the configuration information about the local device if the HyperMetro pair fails or the remote device is disconnected\" when the HyperMetro pair is not in Invalid state, or try again when the local device is disconnected from the remote device. The cause is similar to that of the failure to delete a HyperMetro consistency group. When deleting a HyperMetro pair in normal health status, you cannot select this option: Delete configurations of the local device (applicable only to scenarios where the HyperMetro pair fails or the remote device is disconnected) Failed to delete a HyperMetro pair. The following message is displayed: A member LUN of the HyperMetro in the No Access state has been mapped. Suggestion: Remove the mapping of the member LUN and try again. The HyperMetro pair is suspended, but the LUNs at both ends can be identified by the host. To ensure data consistency of the HyperMetro LUNs, you need to remove the mappings of the LUNs that do not have services when deleting the HyperMetro pair. Map the LUNs that do not have services and retain only the mappings of the LUNs at the preferred site. Then, the HyperMetro pair is successfully deleted. Failed to create a HyperMetro domain. The following message is displayed: The local and remote storage devices are of different performance and specifications. Suggestion: Check the models of the local and remote storage devices. lf they are of different performance and specifications, replace the devices and then create the HyperMetro domain.\nDear Members, Its not a new thing to use ICT solutions to provide digital health services, prevent disease, and improve quality of life. The only question is when, not if, digital healthcare will transform conventional software, hardware, and services. So, what is digital healthcare? You have probably heard of, seen, or even worn wearable medical devices think heart rate monitors, exercise apps, sweat meters (for diabetes), and oximeters (for oxygen in the blood). Wearables are just one example under the umbrella of digital healthcare. According to Food and Drug Administration (FDA), digital healthcare covers mobile health (mHealth), health information technology (IT), wearable devices, telehealth and telemedicine, and personalized medicine. Digital technologies are reshaping the healthcare landscape, providing huge potential to enable accurate diagnosis and treatments, and enhance the delivery of health care for individuals anywhere, and every time. Digital technology facilitates personalized patient care for medical organizations, helping construct interconnected systems and processes for both patients and medical staff at greater efficiency and accuracy. In recent years, the global pandemic has put healthcare under the spotlight, and further fueled speculation about the digital transformation of the healthcare sector. At the peak of the pandemic, the number of patients in hospital every day usually surpassed the maximum treatment capacity of each hospital. This not only overwhelmed most departments, but was hindered by traditional ICT infrastructure, which is incapable of fast and accurate diagnosis. As we see the light at the end of the tunnel, many in the healthcare industry believe it is time for digital transformation.\nBig data refers to a process that is used when traditional data mining and handling techniques cannot uncover the insights and meaning of the underlying data. Data that is unstructured or time sensitive or simply very large cannot be processed by relational database engines. This type of data requires a different processing approach called big data, which uses massive parallelism on readily-available hardware. Quite simply, big data reflects the changing world we live in. The more things change, the more the changes are captured and recorded as data. Take weather as an example. For a weather forecaster, the amount of data collected around the world about local conditions is substantial. Logically, it would make sense that local environments dictate regional effects and regional effects dictate global effects, but it could well be the other way around. One way or another, this weather data reflects the attributes of big data, where real-time processing is needed for a massive amount of data, and where the large number of inputs can be machine generated, personal observations or outside forces like sun spots. Processing information like this illustrates why big data has become so important: Most data collected now is unstructured and requires different storage and processing tthan that found in traditional relational databases. Available computational power is sky-rocketing, meaning there are more opportunities to process big data. The Internet has democratized data, steadily increasing the data available while also producing more and more raw data. Data in its raw form has no value.\nHello all, This post is about theHuawei all-flash storage solutions Dorado V6. Data has become an important factor in keeping an organization agile and competitive. However, traditional ways of storing data, to meet both technical and compliance needs, are failing. Two examples of how organizations have been able to modernise their storage environment demonstrate just how much of a competitive advantage investment in this area can offer. The core of intelligent management; the scheduling, mining, and analysis of data, help applications deliver personalised experience and companies implement precision control over the design, production and logistics process to reduce management costs. Companies from a variety of industries, across Europe and Asia, choose to go with the Huawei OceanStor Dorado All-Flash Storage Series, to help them make storage the driving force for their enterprise. The architecture consists of the following three layers: Front-end access layer Hosts access the storage system through front-end interconnect modules (FIMs), implementing service failover between controllers within seconds, non-disruptive upgrade, and active-active even distribution of host I/Os. Controller interconnect layer Controllers within a controller enclosure are fully interconnected. In scale-out scenarios, controllers in all controller enclosures are also fully interconnected, constituting a controller enclosure group with a global cache. Back-end interconnect layer: Controller enclosures share disk enclosures through back-end interconnect I/O modules (BIMs), ensuring that data in disk enclosures can be accessed even when seven out of eight controllers are faulty Here is the The AI-enabled features on Huawei's OceanStor Dorado V6 AFA make it the most cost-effective enterprise storage system.\nHello all, Today we will share a customer story for Huawei All-Flash Storage. \"We were looking for a cost-effective and high-performance all-flash storage solution that was Non-Volatile Memory express (NVMe)-capable in the backend and also able to offer NVMe in the frontend, keeping one eye on future plans. It soon became clear that Huawei, with its new OceanStor Dorado all-flash storage series, was ideally suited to our needs.\"-- Aleksander Baumann Head of Information Technology (IT) Operations at Sympany. For Swiss health insurance provider Sympany billed as \"the refreshingly different insurer\" 2020 was the time to find a new storage solution, given that projected data growth was threatening legacy storage resources. After extrapolating the required capacity, a shortlist of potential successor solutions was drawn up. In the end, the company opted for Huawei's latest OceanStor Dorado all-flash storage solution. While digitalization across industries is leading to explosive data growth, in many instances such data simply has to be stored locally, so that it can be easily retrieved. This is exactly the case for Sympany who, on average, needs to upgrade or increase its storage capacity every five years. With projections for future capacity requirements always factored into the upgrade process, Sympanys data growth has, however, been far, far higher than expected: as a result, in 2020, a new low cost, high performance solution was urgently required.\n\"We were looking for a cost-effective and high-performance all-flash storage solution that was Non-Volatile Memory express (NVMe)-capable in the backend and also able to offer NVMe in the frontend, keeping one eye on future plans,\" explained Aleksander Baumann, Head of Information Technology (IT) Operations at Sympany. Duly, a shortlist of potential solutions and vendors was drawn up: \"It soon became clear that Huawei, with its new OceanStor Dorado all-flash storage series, was ideally suited to our needs,\" Baumann said. For a long time flash storage has been considered expensive and out of the reach and budget of most. However, with its innovative OceanStor Dorado all-flash storage products, Huawei now offers flash storage solutions at extremely competitive prices, paired with performance levels unrivalled in the industry: 20 million Input/Output Operations Per Second (IOPS). In actual fact, when Sympany first looked into Huawei, the latest version of OceanStor Dorado was not yet available on the open market. But a number of customers, including another major Swiss health insurance provider, were already expressing interest in it. \"In such cases, we typically offer a type of trial version that potential customers can test onsite,\" said Marco Kley, an Enterprise Account Manager at Huawei. This makes it possible for enterprises to experience the promised performance in practical tests in a real-world environment. With OceanStor Dorado all-flash storage, Huawei guarantees a compression rate of at least 2.6 to 1, all while offering extremely high reliability, tolerating the failure of up to seven out of eight controllers.\nThis is a confidence boost for enterprises when it comes to overall performance and protecting their investment. \"Huaweis new product delivers everything it promises,\" Baumann summarized, adding that the Proof of Concept (PoC) option where enterprises test the new product on their own site is ideal for eliminating any uncertainties OceanStor Dorado all-flash storage tolerate the failure of up to seven out of eight controllers. Belsoft Infortix a Swiss IT infrastructure service provider and a long-standing Huawei Gold Partner was brought on board at the implementation stage. \"We greatly appreciate the breath of fresh air the Chinese manufacturer is bringing to the market. Its always particularly exciting to be able to help integrate a completely new solution with innovative features,\" Simon Hni, Head of Enterprise Solutions at Belsoft Infortix, said. This Huawei partner installed OceanStor Dorado all-flash storage in Sympanys data center, laid the associated cabling systems, and also carried out basic configuration, with the entire process taking under two days. And once the system was put into operation and the first performance test was carried out, a compression rate of 4.6 to 1 was achieved a welcome surprise for Sympany and almost double the guaranteed rate while still delivering outstanding performance. \"At these compression rates, were gaining even more storage space than expected, and such compression rates are also available when storing real-time services. The system is powerful and we have a comfort zone, given the additional capacity.\nWhat is a SAN? A storage area network (SAN) is a dedicated high-speed network or subnetwork that interconnects and presents shared pools of storage devices to multiple servers. Storage accessibility and availability are crucial issues for business computing. For many enterprise applications, traditional direct-attached disk installations inside of individual servers might be an easy and affordable choice. However, the disksand the crucial data they holdare connected to the physical server through a specialized interface, such SAS. Modern workplace computing frequently necessitates a far higher degree of control, management, and adaptability. The storage area network developed as a result of these requirements (SAN). SANs are used to provide access to a pool of shared storage to multiple computers and servers. Storage appears to computers on the network as direct-attached storage. A SAN transfers the burden of storage management from individual servers to a single location where it can be accessed, controlled, and safeguarded. Because traffic doesn't have to compete for LAN bandwidth with servers and workloads, connecting storage to servers across a network other than the conventional LAN optimizes storage traffic performance. SAN is an acronym for storage area network. SANs consolidate storage in a single storage area separated from the local area network (LAN). Computers and devices connected to the SAN have access to storage devices like tape libraries and disk arrays available on the SAN servers as though it is local storage. The components of SAN include cabling, host bus adapters, and SAN switches attached to storage arrays and servers.\nSANs use block-based storage and high-speed architecture to connect servers to logical disk units (LUNs), a range of block storage from a pool of shared storage, and appear to the server as a logical disk. A SAN comprises three distinct layers: host, fabric, and storage. The host layer is made up of the servers connected to the SAN, which perform business tasks like databases that need access to storage. SAN hosts communicate with a server's operating system via host bus adapters (HBAs), unique network adapters designed specifically for SAN connectivity. This enables a task to use the operating system to send storage commands and data to the SAN and its storage resources. The network fabric that connects SAN hosts and storage is made up of cables and networking hardware, which together make up the fabric layer. SAN switches, gateways, routers, and protocol bridges can all be considered SAN networking equipment. The fabric layer provides numerous alternate channels from hosts to storage throughout the fabric, increasing redundancy in comparison to a standard network. This means that the SAN can utilize a different path for communication if there is a disturbance on one path. Numerous storage devices, most often hard disk drives (HDDs), but also including SSDs, CD, DVD, and tape drives, are included in the storage layer. RAID groups can be created from storage devices within a SAN to increase storage capacity and boost reliability. Multiple protocols are supported by SAN technologies, enabling communication between operating systems, layers, and applications.\nHello all, This post shows some FAQs About Active-Active SAN for Oceanstor V3. 1. How to handle a link disconnection event and how long does it take? When a link is disconnected, HyperMetro performs arbitration to prevent dual-active nodes. If local arbitration is performed, the preferred site becomes the active site and the non-preferred site becomes the standby site. If the quorum server is used, the preferred site submits the arbitration request first, and then the non-preferred site submits the arbitration request. The time difference is 5s. After receiving a link disconnection event, the device submits the event for arbitration. After obtaining the arbitration result, the device splits the event and provides services at one end. Generally, it takes 10s to complete the task. 2. If a storage device is powered off, how long does it take to reset to 0? If the power-off mode is fast, the power-off mode is the same as the I/O error processing mode. The power-off mode is immediately transferred. The power failure is about 13-14s. 3. What will I do if HyperMetro receives an I/O error when it is in the normal state? If a dual-write I/O error occurs, the I/O is negotiated and arbitrated (not submitted to the quorum server). If data fails to be written to site A, site A becomes the primary and site B becomes the primary. 4. How do I perform arbitration when the links between the quorum server and storage arrays are disconnected?\nIf the link between sites A and B is normal, the arbitration mode is changed from server mode to local mode through negotiation. The arbitration mode of HyperMetro is changed to static arbitration. The preferred site is directly arbitrated successfully. 5. What is the status of the failed arbitration end? How to recover the arbitration end? If the arbitration fails, the status of the peer end changes to To-be-synchronized or suspended, and the access is forbidden. The write protection mode is set. You can restore the data synchronously. 6. What are the application scenarios and restrictions of forcible startup? If both the primary and secondary sites are in the to-be-synchronized or suspended state, that is, dual-slave sites are available, forcibly start the preferred site, change the HyperMetro status to waiting for synchronization and readable, and run the synchronization command to synchronize data to the non-preferred site. In HyperMetro dual-slave mode, the HyperMetro management link is disconnected because it cannot be negotiated. In this case, you can forcibly start one storage array and ensure that the other storage array has no available path to the host (the physical link is disconnected or removed). It is not allowed to forcibly start both sides at the same time. Do not forcibly start the secondary end when the replication link is disconnected. 7. Why Is the Performance Unbalanced Between Two Sites or Two Controllers in SAN Active-Active Mode? The performance of the two storage arrays is inconsistent.\nIn balancing mode, one is the primary site of the DLM lock, and the other is the secondary site of the DLM lock. Services at the secondary site need to send messages to the primary site when the DLM lock is added. Therefore, the performance of the secondary site is worse than that of the primary site. The performance of the two controllers may also be unbalanced because DLM locks affect performance, and DLM locks may not be evenly distributed on the controllers. 8. What are the multipathing software versions required for SAN HyperMetro? Third-party multipathing software is supported only after the ALUA protocol is supported in V300R003C20 and later versions. In earlier versions, only self-developed multipathing software is supported. 9. What is the primary/secondary relationship of HyperMetro? Can a switchover be performed? HyperMetro has a primary/secondary relationship. You can run the diagnose command to check the relationship. When creating the primary end, you can perform the following operations to switch the primary/secondary end: Pause the secondary end, and then stop the primary end from receiving services. Synchronize again. 10. Full Prefetch Principles and Application Scenarios of HyperMetro During HyperMetro prefetching, the DLM lock prefetches the entire lock permission to the local host. All local controller locks are hit, eliminating the need for inter-array and inter-controller messages. Therefore, performance may be improved. This scenario applies only to the scenario where services are dropped on one side in the preferred site mode. 11. Does the mapping need to be removed when a HyperMetro pair is deleted?\nWhen a HyperMetro pair is deleted, the mapping between the secondary LUN and the primary LUN needs to be deleted. The reason is that the LUN can be read and written again after the HyperMetro pair is deleted. However, the host still uses the same LUN as the primary LUN. As a result, data on the LUN is incomplete. 12. Check whether the active node of the DLM lock is the same as that of the active node of the HyperMetro pair. How to confirm the primary/secondary DLM The primary/secondary of the SAN HyperMetro is the same as that of the DLM lock. When a HyperMetro is created, two zones are created, and the primary ends of the two zones are distributed to two sites. When a primary/secondary switchover occurs, a new DLM lock is created based on the zone ID of the current primary site to implement the switchover of the primary site. 13 What Version Does SAN HyperMetro Support Third-Party Multipathing Software? V300R003C20SPC200 and later versions support this function. 14. In which version of SAN HyperMetro, host links are checked during arbitration upon link disconnection? V3R6C00SPC100 and later versions support this function. When a link is disconnected, the system checks whether the host link is connected during site arbitration. The FC link can be detected 15 seconds after the link is disconnected. iSCSI links cannot be guaranteed, and host link disconnection may not be detected.\nGood day Community! This post talks about analyzing the cause of a slow performance of Dorado V6. Please see more details below. The performance has slow in recent days and the cause is analyzed. 11:00 15:00 13th , the storage bandwidth changed due to the changes in the read and write I/O size on the host. However, the storage CPU usage is less than 40% , the disk usage is less than 30% and the average latency of the storage controller is less than 800 s . Therefore, 03:00 09:00 15th , the storage bandwidth changes due to the read and write I/O size on the host. However, the CPU usage of the storage device is less than 50% , the disk usage is less than 35% and the average latency of the storage controller is less than 650 s . Therefore, there is the storage bandwidth changes due to the changes in the read and write I/O size on the host. However, the CPU usage of the storage device is less than 50%, the disk usage is less than 35%, and the average latency of the storage controller is less than 480 s By the way, the collected logs show that a large number of bit errors exist on the FC port CTE0.A.IOM3.P0,CTE0.B.IOM3.P0 .\nHello, everyone! How can I configure UltraPath to take over multiple sets of active-active storage? Can you help me? Thanks in advance! Hello, friend! You can set the working mode of HyperMetro storage using a storage system and virtual LUN. If the working mode is set using storage systems, the setting takes effect globally for the storage system. If the working mode is set using a virtual LUN, the setting takes effect only for the specified virtual LUN. If there are multiple sets of active-active storage devices, you are advised to select the active-active storage where the number of LUNs will be increased and set the HyperMetro working mode based on the storage system. After the number of LUNs is increased, you do not need to set the HyperMetro working mode again. Other active-active storage systems work in active-active mode based on virtual LUNs. If the number of LUNs changes, you need to set the active-active working mode again. The following uses Dorado V3 and V6 as an example to describe how to take over active-active storage. Assume that the number of LUNs on Dorado V6 will be increased. Array 0 and Array 1 are Dorado V3 storage devices, and Array 2 and Array 3 are Dorado V6 storage devices. For an OceanStor V6 HyperMetro storage system, set its HyperMetro working mode to preferred array mode and set the storage system whose ID is 2 as the preferred array.\nHello all, This article describes network management, SNMP-based network management architecture, SNMP protocol, and basic concepts and operations of MIB. With the growing scale of the network and the variety of devices in the network, how to effectively manage the increasingly complex network so as to provide high-quality network services has become the biggest challenge for network management. Network management has become an important part of the entire network solution. Network management usually consists of 4 elements: Managed node: The device that needs to be managed. Agent : Software or hardware that tracks the status of managed devices. Manager : A device that communicates with agents in different managed nodes and displays the status of these agents. Network Management Protocol: The protocol used by network management workstations and agents to exchange information. At present, the most widely used network management protocol in TCP/IP network is Simple Network Management Protocol (SNMP). This diagram shows how SNMP operates through the manager-agent model. There are 4 main components in the SNMP-based network management architecture: Network Management Station (NMS) The NMS is usually a stand-alone device that runs network management applications. Network management applications can at least provide a human-computer interface, through which network administrators complete most of the network management work. SNMP agent (Agent) Agent is a software module residing on the managed device. It is mainly responsible for receiving and processing request packets from NMS, forms response packets, and returns them to NMS; in some emergency situations, it will actively send trap packets to notify NMS.\nSNMP protocol The SNMP protocol belongs to the application layer protocol of the TCP/IP network and is used to exchange management information between the NMS and the managed devices. Management Information Base (MIB) MIB is a collection of managed objects, which is the bridge between NMS and Agent for communication, and enables standard connection between network management software and equipment. Each Agent maintains such a MIB library, and the NMS can read or set the values of objects in the MIB library. The interrelationship of several main components and the way they communicate with each other are described as follows: The NMS communicates with the agent of the device through the SNMP protocol, and completes the reading and modification operations of the MIB, thereby realizing the monitoring and management of the network device. SNMP is the carrier of communication between NMS and Agent, and completes information exchange through its Protocol Data Unit (PDU). SNMP is not responsible for the actual transmission of data, and the task of data exchange is accomplished through transport layer protocols such as UDP. Agent is an agent process on the device. Its main work includes communicating with the NMS, maintaining the MIB library in the device, and managing and monitoring each module in the device. MIB stores the information of each module in the device. The monitoring and maintenance of the device is completed by reading and writing the MIB information.\nHello everyone! This post talks about troubleshooting the failure of the Watchdog Status item in the O9000 inspection report. The failure of the Watchdog Status item in the O9000 inspection report. Versions earlier than 7.X : The /opt/huawei/deploy/script/localexec.py process on the node is suspended. As a result, the ps -ef |grep /opt/huawei/deploy/bin/daemon|grep -v'grep '| grep -v'localexec' command fails to be executed asynchronously through the process. As a result, no command output is displayed. As a result, the node incorrectly reports the /opt/huawei/deploy/bin/daemon is not running message. The /opt/huawei/deploy/script/localexec.py process on the node is suspended. As a result, the ps -ef |grep /opt/huawei/deploy/bin/daemon|grep -v'grep '| grep -v'localexec' command fails to be executed asynchronously through the process. As a result, no command output is displayed. As a result, the node incorrectly reports the /opt/huawei/deploy/bin/daemon is not running message. 1. Run the ps -ef |grep localexec.py |grep -v grep command on the faulty node to check the ID of the /opt/huawei/deploy/script/localexec.py process: 2. Run the kill -9 19731 command to kill the process: 3. Run the ps -ef |grep localexec.py |grep -v grep command to check whether the process exists on the device. If no command output is displayed, the process is killed. Go to step 4 . 4.Run the python /opt/huawei/deploy/script/localexec.py command to start the /opt/huawei/deploy/script/localexec.py process: 5. Run the ps -ef |grep localexec.py |grep -v grep command to check whether the /opt/huawei/deploy/script/localexec.py process exists on the device. If the command output is displayed, the process is started. You can perform the inspection again.\nHello everyone In this post, you can learn: 1. Powering On and Off Hardware 2. Powering On and Off the System 3. Power-On and Power-Off 4. Powering Off a Node 5. Restarting a Node Scenarios After all hardware devices are installed, you need to power on the entire system for trial running and check whether the hardware devices are successfully installed. Workflow Figure 1 Hardware power-on process Procedure 1. Turn on the power switches on the PDC. NOTE The voltage of the AC input power supply to the cabinet is 200 V to 240 V to prevent damage on devices and ensure the safety of the installation personnel. Before powering on hardware, ensure that uplink ports are not connected to the customer's switch devices. Power on basic cabinets and then power on extension cabinets. Turn off switches on the PDUs before turning on the switches that control power supply to all cabinets in the PDC. a. Trained personnel of the customer turn on the power switches on the PDC for all cabinets. b. Obtain the information on the PDU monitor to check if the PDU output voltage stays within 200 V to 240 V. NOTE If the PDU output voltage is not within 200 V to 240 V, immediately contact the customer and Huawei technical support engineers. Do not perform the subsequent procedure. 2. Turn on the power switches of PDUs one by one in the cabinet. NOTE When the PDUs are powered on, the server is powered on. 3.\nTurn on the power switches of devices. 4. Check the status of devices. a. Check the status of fan modules to ensure that all fan modules run at full speed and then down to even speed, and the sound of fan modules is normal. b. Check the indicator status on panels to ensure the proper operating of devices. Scenarios Power off unnecessary hardware based on system status: If no service is running in the system, perform the procedure provided in this topic to power off hardware. If services are running in the system, power off hardware by following instructions in Power Control. Workflow Figure 2 Hardware power-off process Procedure 1. (Optional) Turn off the power switches of devices. 2. Turn off the power switches of PDUs one by one in the cabinet. NOTE When the PDUs are powered off, the server is powered off. 3. Turn off the power switches on the PDC. NOTE Trained personnel of the customer turn off the power switches on the PDC for all cabinets. Scenarios Power on all FusionCube components in sequence during holidays or normal supply of mains electricity. This ensures device security and normal service running. NOTICE Before powering on the system, ensure that the operating environment in the equipment room meets the secure power-on requirements. Procedure 1. (Optional) Power on devices outside the cabinet. Turn on the power switches of the devices outside the cabinet. 2. Power on the cabinet. a. Power on the power distribution units (PDUs) of the cabinet. b.\nc. Restart the DBN to mount the storage disks to the system. reboot d. Check whether service volumes are properly mounted. /usr/bin/lsscsi NOTE If an error message is displayed, rectify the fault according to the error information. e. Repeat 6.a to 6.d on all other DBNs to check whether service volumes are properly mounted. Check Before Power-Off Procedure 1. Check Distributed Storage system status. For details, see Check Before Power-Off. 2. Stop database services. For details, see the related database guide. NOTE Ensure that the upper layer applications are stopped before you stop database services. 3. The system can be powered off by one click or manually. Power off the system in one-click mode. For details, see Powering Off the System by One Click. NOTE If the one-click power-off function is unavailable due to a FusionCube system fault, you can also manually power off the system. For details, see Powering Off the System Manually. Manually power off the server. For details, see Powering Off the System Manually. Powering Off the System by One Click NOTICE This operation automatically powers off nodes in sequence. During the power-off process, no operation can be performed on the system. Scenarios This function allows you to power off the system in one click. After you click the Power Off button on the Power Off System page, all nodes will be powered off in sequence based on the actual deployment modes and scenarios.\nAfter running the dswareTool command of Distributed Storage, you need to enter the user name and password for authentication. Default user name: admin Default password: user-defined public password On the page to change the host password displayed during system initialization, change the password of user admin. If the password has been changed, use the new password. d. Enter y as prompted. e. Check whether the Distributed Storage service is stopped. shdswareTool.sh--opqueryDSwareServiceStatus The Distributed Storage service is stopped if the following information is displayed: 3. Stop FusionCube Vision. NOTICE Do not perform other operations on FusionCube Vision during power-off. a. Use PuTTY to log in to FusionCube Vision. Host name: FusionCube Vision IP address NOTE If the MCNA is deployed in single-node mode, enter the MCNA management IP address. If the MCNA is deployed in active/standby mode, enter the management IP address of the standby MCNA. Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su-root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. c. Power off FusionCube Vision.\nIf no, go to 2.e. e. If no, power off the node as required. Impact If only one MCNA is deployed, this operation will cause the management page to be inaccessible. If MCNAs are deployed in active/standby mode, this operation will interrupt the connection to the management page for five minutes. Prerequisites The pre-check is complete. If VBS is deployed on the node to be powered off and the node is accessed using the SCSI protocol, services on the volumes mapped from the VBS are stopped. Otherwise, the service data cache deployed on the node may be lost. For example, if a file system is mounted to a node, you need to unmount the file system before powering off the node. Procedure 1. Check the active/standby relationship of FusionCube Vision deployed on the MCNAs. NOTICE Perform this step only when MCNAs are deployed in active/standby mode. a. Use PuTTY to log in to FusionCube Vision. Host name: MCNA management IP address Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su-root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password.\nc. Check the name of the current host (for example, FCV01). hostname d. Check the active/standby relationship of FusionCube Vision. sh/opt/dfv/oam/oam-u/ha/ha/module/hacom/script/status_ha.sh e. Check whether the current FusionCube Vision node is the active FusionCube Vision node. NOTE If the value of HAActive is active, the current node is the active FusionCube Vision node. If the value of HAActive is standby, the current node is the standby FusionCube Vision node. If yes, go to 1.f. If no, go to 2. f. Perform a FusionCube Vision active/standby switchover. /opt/dfv/oam/oam-u/ha/ha/module/hacom/tools/ha_client_tool--ip=localhost--port=61591--switchover--name=product 2. Power off the node. a. Use PuTTY to log in to the MCNA. Host name: MCNA management IP address Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su-root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. c. Power off the node. shutdown -h now Impact This operation will interrupt services running on the DBN. You are advised to migrate or suspend services running on the node before powering it off. Prerequisites The pre-check is complete.\nIf VBS is deployed on the node to be powered off and the node is accessed using the SCSI protocol, services on the volumes mapped from the VBS are stopped. Otherwise, the service data cache deployed on the node may be lost. For example, if a file system is mounted to a node, you need to unmount the file system before powering off the node. Procedure 1. Check whether the database services on the node are stopped. If yes, go to 2. If no, stop the database services on the DBN. 2. Power off the node. a. Use PuTTY to log in to the DBN. Host name: DBN management IP address Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su-root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. c. Power off the node. shutdown -h now Impact Powering off multiple SNAs at the same time will result in storage pool exceptions and thus only one SNA can be powered off at a time. Prerequisites The pre-check is complete.\nCheck whether abnormal processes exist. If yes, rectify related faults according to the handling suggestions. If no, go to 2.e. e. If no, restart the node based on requirements. Impact If only one MCNA is deployed, this operation will cause the management page to be inaccessible. If MCNAs are deployed in active/standby mode, this operation will interrupt the connection to the management page for five minutes. Prerequisites The pre-check is complete. Procedure 1. Check the active/standby relationship of FusionCube Vision deployed on the MCNAs. NOTICE Perform this step only when MCNAs are deployed in active/standby mode. a. Use PuTTY to log in to FusionCube Vision. Host name: MCNA management IP address Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su - root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. c. Check the name of the current host (for example, FCV01). hostname d. Check the active/standby relationship of FusionCube Vision. sh /opt/dfv/oam/oam-u/ha/ha/module/hacom/script/status_ha.sh e. Check whether the current FusionCube Vision node is the active FusionCube Vision node.\nNOTE If the value of HAActive is active, the current node is the active FusionCube Vision node. If the value of HAActive is standby, the current node is the standby FusionCube Vision node. If yes, go to 1.f. If no, go to 3. f. Perform a FusionCube Vision active/standby switchover. /opt/dfv/oam/oam-u/ha/ha/module/hacom/tools/ha_client_tool --ip=localhost --port=61591 --switchover --name=product g. Wait for 5 minutes and check the FusionCube Vision active/standby relationship to ensure that the switchover is successful. sh /opt/dfv/oam/oam-u/ha/ha/module/hacom/script/status_ha.sh 2. Restart the node. a. Use PuTTY to log in to the MCNA. Host name: MCNA management IP address Default user name: manageromm Default password: User-defined public password b. Switch to user root. i. Run the following command: su - root c. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. d. Restart the node. reboot Impact This operation will interrupt services running on the DBN. You are advised to migrate or suspend services running on the node before restarting it. Prerequisites The pre-check is complete. Procedure 1. Check whether the database services on the node are stopped. If yes, go to 2. If no, stop the database services on the DBN. 2. Restart the node. a. Use PuTTY to log in to the DBN.\nHost name: DBN management IP address Default user name: manageromm Default password: user-defined public password NOTE On the page to change the host password displayed during system initialization, change the password of user manageromm. If the password has been changed, use the new password. b. Switch to user root. i. Run the following command: su - root ii. Enter the password of user root as prompted. The default password is the user-defined public password. NOTE On the page to change the host password displayed during system initialization, change the password of user root. If the password has been changed, use the new password. c. Restart the node. reboot Impact If the restart takes a long time, the SNA may be removed from the storage pool, which affects services. For this reason, you are advised to suspend services before this operation or perform this operation in off-peak hours. Restarting multiple SNAs at the same time will result in storage pool exceptions and thus only one SNA can be restarted at a time. Prerequisites The pre-check is complete. Procedure 1. On Distributed Storage, set the node to enter the maintenance mode. NOTE If the restart takes a long time (more than 75 minutes), you need to click Exit Maintenance Mode at intervals ( 75 minutes) and then click Set Maintenance Mode. Otherwise, the maintenance mode of the node will time out, and the node will be removed from the storage pool. If the node is in the maintenance mode, skip this step. a.\nDear team, The storage pool capacity alarm is generated. The thin LUN has snapshots. The remaining space of the storage pool is less than the pre-reserved space. How to calculate the space? When the space reaches the value, the LUN changes to write through. Thanks. Dear Axe, The write-through and write-protection thresholds are set for the storage device. The calculation formula is as follows: 1. Write protection is triggered when the minimum values of the following three are reached: 5% of the pool space 20 GB (Thin LUN + Thin LUN with snapshot and RM) x 1 GB 2. The write-through threshold is four times the write-protection threshold. The pre-reserved space is four times of the reserved space. If the remaining space of the StoragePool is less than the pre-reserved space of the StoragePool, the write mode is changed to write through. Reserved space definition: (a) Reserved space (currently defined as 20GB) (b) Each thinLun, thinFS, snapshot, and thickLun with asynchronous remote replication in the storage pool provides 1 GB of reserved space, and the sum of the total reserved space (c) Watermark of the total capacity of the StoragePool (currently 5%) Reserved space is defined as the minimum value of the above three items. Internal definition. The reserved space capacity is the minimum value selected from the calculation results of the preceding three rules. If the remaining space of the StoragePool is smaller than the pre-reserved space of the StoragePool, the write through mode is changed.\nWhat are the features of HyperVault and in what equipment can we find them? HyperVault has the following features: Time-saving local backup and recovery: A storage system can generate a local snapshot within several seconds to obtain a consistent copy of the source file system, and roll back the snapshot to quickly recover data to that at the desired point in time. Incremental backup for changed data: In remote backup mode, full backup at an initial time and permanent incremental backup save bandwidth. Flexible and reliable data backup strategy: HyperVault supports self-defined backup policies and threshold for the number of copies. A copy of invalid backup data will not affect follow-up backup tasks. The following products are supported by HyperVault: HyperVault uses snapshot and remote replication technologies to back up and recover data. It applies to both local and remote backup and recovery. Based on file systems, HyperVault enables data backup and recovery within a storage system and between different storage systems. Data backup involves local backup and remote backup. Using file system snapshot or remote replication technology, HyperVault backs up data to a specific point in time on the source storage system or on the backup storage system based on a data policy. specified backup. Data recovery involves local recovery and remote recovery. With file system snapshot rollback or remote replication technology, HyperVault specifies a local backup snapshot of a file system to roll back or specifies a remote snapshot of the backup storage system for recovery.\nHyperSnap allows users to quickly obtain a virtual, consistent, point-in-time copy of a source LUN without interrupting services running on the source LUN. The copy can be readable and writable immediately after being created. Reading from and writing to the copy do not affect the source LUN's data and performance. Therefore, HyperSnap can help users with online backup, data analysis, application testing, and a list of other scenarios. OceanStor Dorado or The new-gen OceanStor hybrid flash storage support HyperCDP schedules to meet customers' backup requirements. HyperCDP objects cannot be mapped to hosts directly. To read data from a HyperCDP object, you can create a duplicate for it and map the duplicate to the host. OceanStor Dorado or The new-gen OceanStor hybrid flash storage supports HyperClone, which allows the system to create a complete physical copy of the source LUN's data on the target LUN. The source and target LUNs that form a clone pair must have the same capacity. The target LUN can either be empty or have existing data. If you do not specify a target LUN, the system automatically creates one. If the target LUN has data, the data will be overwritten by the source LUN during synchronization. After the clone pair is created, you can synchronize data. During the synchronization, the target LUN can be read and written immediately. The source LUN of HyperClone can be a writable snapshot, but the target LUN cannot be a writable snapshot.\nHello all, Huawei FusionStorage is an intelligent distributed storage product that supports large-scale scale-out. FusionStorage organizes local storage resources of general-purpose servers using storage system software and provides object storage and Hadoop Distributed File System (HDFS) storage for upper-layer applications on demand, meeting the requirements of complex service loads for large capacity, elastic scalability, and high reliability. FusionStorage is widely adopted in various industries and scenarios such as finance, data centers (DCs), and carriers. As an emerging storage type and service form, object storage is a cloud storage service that provides massive, secure, reliable, and cost-effective data storage capabilities and stores unstructured data, such as documents, images, as well as audio and video files. Object storage brings all the benefits of existing storage services, including the fast disk access of block storage and distributed sharing of file storage. HDFS storage is distributed big data storage provided by Huawei. It is fully compatible with native HDFS semantics and delivers enterprise-grade reliability. Object storage and HDFS storage of FusionStorage provide enterprises and individual users with cost-effective and reliable storage options, and have the following advantages: Elastic scalability FusionStorage leverages a fully symmetric architecture and supports flexible and seamless expansion. Broad compatibility FusionStorage can be integrated into solutions and supports various upper-layer service applications. High security FusionStorage leverages keys, permission control, and refined access policies to jointly ensure secure data access, and supports HTTPS- and SSL-based data transmission and anti-leeching. Solid reliability FusionStorage supports multi-region high-reliability networking and protects data using erasure coding (EC).\nProduct Line Product Family Product Model OceanStor products: OceanStor 2200 V3/2600 V3/2800 V3/5300 V3/5500 V3/5600 V3/5800 V3/6800 V3/6900 V3/18500 V3/18800 V3 OceanStor 2800 V5/2810 V5/5110 V5/5110F V5/5300 V5/5300F/V55310F V5/ 5300 V5 Kunpeng/5500 V5/5500 V5 Elite/5500F V5/5500 V5 Kunpeng/5600 V5/5600F V5/5600 V5 Kunpeng/5600F V5 Kunpeng/5800 V5/5800F V5/5800 V5 Kunpeng/5800F V5 Kunpeng/6800 V5/6800F V5/18500 V5/18500F V5/18800 V5/18800F V5 OceanStor Dorado products: OceanStor Dorado 3000 V3/Dorado 5000 V3/Dorado 6000 V3/Dorado 18000 V3 Scale-out storage products: FusionStorage FusionStorage OBS Release Date Severity Urgency Versions Involved OceanStor products: V500R007C20 V500R007C50 V300R006C30 V300R006C60 V500R007C60SPC100 OceanStor Dorado products: V300R002C00 to V300R002C20 Scale-out storage products: FusionStorage OBS 7.0.0 to 7.0.1 FusionStorage 8.0.0 FusionStorage 8.0.0.1 Note: The issue described in this warning also occurs for storage devices that are upgraded from the preceding risky versions to later versions. Devices Involved Product Model Application Scope Operation Category Category ID Operation Requirements Contacts Service Contact R&D Contact Keywords: CA certificate of Call Home authentication is about to expire, CA certificate Abstract: The CA certificate used by the Call Home service is about to expire. Therefore, an alarm indicating that the CA certificate of Call Home authentication is about to expire is generated on the storage device 30 days in advance. If the issue is not handled in time, an alarm indicating that the CA certificate of Call Home authentication has expired will be generated, then the storage device cannot connect to the eService server. As a result, the Call Home service becomes invalid, but storage services will not be affected.\n[Issue Description] Trigger Conditions: The storage device version was or is a risky version. Symptom: An alarm indicating that the CA certificate of Call Home authentication is about to expire is generated on the storage device. The alarm ID is 0xF50000007. Take flash storage as an example: If the issue is not handled according to Solutions before the certificate expires, an alarm indicating that the CA certificate of Call Home authentication has expired will be generated on the storage device. The alarm ID is 0xF50000008. Take flash storage as an example: Methods for viewing the certificate expiration time: For checking the expiration time of the CA certificate of Call Home authentication on DeviceManager, you are advised to implement the solution six months before the certificate expires. For details, see . [Certificate Expiration Description] A built-in CA certificate of Call Home authentication is configured on the storage device for eService server identity authentication. The validity period of the CA certificate is determined by its vendor. An alarm indicating that the CA certificate of Call Home authentication is to about to expire is generated on the storage device 30 days in advance. [Impacts and Risks] Impact 1: For a storage device that is not configured with the Call Home function, an alarm indicating that the CA certificate of Call Home authentication is about to expire is generated on the storage device 30 days in advance.\nIf the issue is not handled according to Solutions before the certificate expires, an alarm indicating that the CA certificate of Call Home authentication has expired will be generated. The two alarms do not affect services. Impact 2: For a storage device configured with the Call Home function, an alarm indicating that the CA certificate of Call Home authentication is about to expire is generated on the storage device 30 days in advance. If the issue is not handled according to Solutions before the certificate expires, an alarm indicating that the CA certificate of Call Home authentication has expired will be generated. As a result, the storage device cannot connect to the eService server, causing the Call Home service to be invalid. Huawei technical support center cannot receive device alarms and perform remote inspection and log collection, but services are not affected. The following table lists expiration dates of CA certificates of Call Home authentication in different storage versions. Product Series Version Certificate Expiration Date Remarks V300R006C30 to V300R006C50SPH105 V300R006C60 V500R007C20 to V500R007C30SPH105 V500R007C60SPC100 V500R007C30SPH106 and later patches V300R002C00 to V300R002C10SPH104 V300R002C20 8.0.0 8.0.0.1 [Solutions] The following table lists the solutions for different scenarios. Version Scenario Whether Call Home Service Configured Solution V300R006C30 to V300R006C50SPC100 V500R007C20 to V500R007C30SPC100 Dorado V300R002C00 to V300R002C10SPC100 FusionStorage OBS 7.0.0 FusionStorage OBS 7.0.1 FusionStorage 8.0.0 FusionStorage 8.0.0.1 V300R006C60 V500R007C50 V5R7C60SPC100 Dorado V300R002C20 Solution 1: Delete the certificate that is about to expire. (After the CA certificate is deleted, the Call Home service cannot be used.)\nAn error is reported when LUNs are deleted from OceanStor Dorado 5600 V6 snapshots. 1. 2022 - 09 - 22 17:18:05: Deactivate the snapshot whose ID is 16, and the snapshots whose IDs are 17 and 18 are not deactivated. 2. At 2022 - 09-22 17:18:54, the consistency snapshot fails to be re-created, as shown in the following figure. 3. Cause of the snapshot rebuilding failure: During the rebuilding of the consistency snapshot storage, the snapshot is deactivated and then created again. During the deactivation, the internal processing of the storage is abnormal. If the first snapshot (generally, the snapshot with the smallest ID) in the array does not exist, The system considers that all snapshots in the same batch are deactivated and skips deleting snapshots from the consistency group. For example, snapshots whose IDs are 17 and 18 are not deactivated, resulting in residual snapshots. During snapshot re-creation, if the snapshot is activated, the snapshot name remains unchanged before and after the snapshot re-creation. In addition, the system indexes the snapshot records based on the snapshot name. When snapshots whose IDs are 17 and 18 are recreated, the snapshot index already exists. As a result, the system considers that the snapshot fails to be created because of a conflict. In addition, the system sets the record of the current consistency snapshot to being rebuilt. As a result, the subsequent deletion fails. When the consistency snapshot is being rebuilt, the activation status of snapshots in the consistency group is inconsistent.\nFew Most common Types of RAM The following are some common types of RAM: SRAM : Static random access memory uses multiple transistors, typically four to six, for each memory cell but doesn't have a capacitor in each cell. It is used primarily for cache. DRAM : Dynamic random access memory has memory cells with a paired transistor and capacitor requiring constant refreshing. FPM DRAM : Fast page mode dynamic random access memory was the original form of DRAM. It waits through the entire process of locating a bit of data by column and row and then reading the bit before it starts on the next bit. Maximum transfer rate to L2 cache is approximately 176 Mbps. EDO DRAM : Extended data-out dynamic random access memory does not wait for all of the processing of the first bit before continuing to the next one. As soon as the address of the first bit is located, EDO DRAM begins looking for the next bit. It is about 5-20 percent faster than FPM DRAM. Maximum transfer rate to L2 cache is approximately 264 Mbps. SDRAM : Synchronous dynamic random access memory takes advantage of the burst mode concept to greatly improve performance. It does this by staying on the row containing the requested bit and moving rapidly through the columns, reading each bit as it goes. The idea is that most of the time the data needed by the CPU will be in sequence.\nSDRAM is about 5 percent faster than EDO RAM and has a transfer rate of 0.8-1.3 megatransfers per second (MT/s). It was developed in 1988. DDR SDRAM : This is the next generation of SDRAM. Double data rate synchronous dynamic RAM is just like SDRAM except that is has higher bandwidth, meaning greater speed. Its transfer rate is 2.1-3.2 MT/s. DDR was released in 2000 and has advanced three subsequent generations. DDR2 (2003) has a transfer rate of 4.2-6.4 MT/s and DDR3 (2007) transfers data at 8.5-14.9 MT/s. The most recent generation in widespread use is DDR4, launched in 2014. Its transfer rate is 17-21.3 MT/s. These standards are set by the Joint Electron Device Engineering Council (JEDEC), an organization made up of electronics companies. JEDEC released its specification for DDR5 in July 2020. RAM manufacturer Micron believes the new standard will increase performance by 87 percent when compared with a DDR4 module. RDRAM : Rambus dynamic random access memory is a radical departure from the previous DRAM architecture. Designed by Rambus, RDRAM uses a Rambus in-line memory module (RIMM) , which is similar in size and pin configuration to a standard DIMM. What makes RDRAM so different is its use of a special high-speed data bus called the Rambus channel. RDRAM memory chips work in parallel to achieve a data rate of 800 MHz, or 1,600 Mbps or higher. Since they operate at such high speeds, they generate much more heat than other types of chips.\nTo help dissipate the excess heat Rambus chips are fitted with a heat spreader, which looks like a long thin wafer. Just like there are smaller versions of DIMMs, there are also SO-RIMMs, designed for notebook computers. Credit Card Memory : Credit card memory is a proprietary self-contained DRAM memory module that plugs into a special slot for use in notebook computers. PCMCIA Memory Card : Another self-contained DRAM module for notebooks, cards of this type are not proprietary and should work with any notebook computer whose system bus matches the memory card's configuration. They are rarely used nowadays. CMOS RAM : CMOS RAM is a term for the small amount of memory used by your computer and some other devices to remember things like hard disk settings. This memory uses a small battery to provide it with the power it needs to maintain the memory contents. VRAM : VideoRAM, also known as multiport dynamic random access memory (MPDRAM), is a type of RAM used specifically for video adapters or 3-D accelerators. The \"multiport\" part comes from the fact that VRAM normally has two independent access ports instead of one, allowing the CPU and graphics processor to access the RAM simultaneously. Located on the graphics card, VRAM comes in a variety of formats, many of which are proprietary. The amount of VRAM is a determining factor in the resolution and color depth of the display. VRAM is also used to hold graphics-specific information such as 3-D geometry data and texture maps.\nSupercomputing is a key technology that can transform society and open the door to the next stage of human technological evolution. Many countries are prioritizing supercomputing research and development, including the United States, Japan, the European Union, Russia and China. For example, among the top 500 most powerful supercomputers released in June 2021, China has 188 entries, with Tianhe-2 ranked 7th. Previously, Tianhe No. 2, Tianhe No. 3, Shenwei Taihu Light ranked the top 10 times in a row. However, new technologies such as cloud computing, big data, artificial intelligence and blockchain have taken attention away from supercomputing. In addition, the application ecosystem is limited and the machine-hour supercomputing service mode is not diversified enough. Therefore, China's supercomputing field still has great room for development. Seeing its broader value in socio-economic progress, more and more provinces and cities in China are building supercomputing centers and deploying next-generation supercomputing systems. China currently operates 10 state-level supercomputing centers in major cities such as Tianjin, Shenzhen, Guangzhou and Xi'an, and many more are planned. To transform supercomputing centers from computing service providers to integrated data value providers, China is prioritizing five supercomputing trends: diversified computing, all-optical networks, data intensification, containerized applications, and converged architectures. Diversified computing is becoming mainstream. Traditional high-performance computing (HPC) systems use CPUs for double-precision floating-point computing, while emerging supercomputing systems use CPUs, GPUs, and FPGAs for more powerful parallel computing. Today, Chinese industry is stepping up the development and deployment of self-developed microprocessors and accelerators to improve the efficiency of heterogeneous computing and hybrid applications.\nThe optical switching technology is becoming mature, and the network is becoming all-optical. Converged Ethernet-based Remote Direct Memory Access (RoCE) and lossless data technologies enable the supercomputing center's compute, storage, and management networks to be integrated into a single box. An all-optical supercomputing internet between supercomputing centers has been proposed to facilitate resource sharing. Data is becoming more and more intensified. Traditional supercomputing applications such as weather forecasting, energy exploration and satellite remote sensing will generate more and more data as accuracy improves. In addition, over 80% of emerging supercomputing applications (including self-driving cars, genetic testing and brain science) Generate PB-level data. Larger data volumes, more data types and concurrent tasks, and higher reliability requirements impose higher requirements on supercomputing storage to provide higher bandwidth, IOPS, reliability, and support for massive concurrent access. Containerized applications. The containerization technology encapsulates the supercomputing running environment and decouples supercomputing applications from underlying hardware, making it easier for non-professional users to use supercomputing. Containerization technology is currently open source, making ecosystem development more viable. Supercomputing architectures are converging. Consistent with the first four trends, supercomputing will employ a heterogeneous, multi-state composite architecture to converge siloed resources, data, and applications. This means a unified, converged heterogeneous system in which CPUs, GPUs, and other specialized computing power systems are scheduled on a unified service scheduling platform. Supercomputing applications will be managed on a unified application platform and data assets carried by a unified data base.\nBreaks data silos, ensures that data does not need to be migrated when the data base remains unchanged, optimizes the TCO, and improves the ROI. Of the five trends, data intensification is the most significant. Traditionally, supercomputing has been used primarily to solve computing problems. The customer takes the data on the hard drive to the supercomputing center and copies the results back to the hard drive, leaving no long-term stored data at the center. However, the development of supercomputing has brought changes and new challenges. First, the amount of data involved in computing has increased dramatically. Applications such as weather forecasting and satellite remote sensing, for example, have improved accuracy and doubled the volume of data. More types of data, both structured and unstructured, involve computing. For example, image data are used directly for calculations in applications such as brain science and electronic cryomicroscopy (CryoEM). Second, the computing capability has been greatly improved. Currently, few single tasks consume all of the computing power in a cluster, so in most cases multiple tasks run simultaneously. The Shanghai Jiao Tong University 100 PFLOPS High Performance Computing Center can run nearly 50 concurrent tasks, some of which require high bandwidth and others require high I/O performance. Therefore, more balanced storage capabilities are required. Third, greater reliability is needed. When traditional supercomputing is applied to research projects, users can tolerate iterative operations before reliable results are obtained. However, today's supercomputing is mainly used in production systems that require high reliability of results and processes.\nTherefore, the storage system needs to be very reliable. Fourth, supercomputing centers and data centers need to converge. In recent years, supercomputing centers are exploring more diverse services such as AI computing, big data analytics, virtualization and disaster recovery. In the process, the center found data mobility to be the biggest issue, as storage is split between supercomputing files, virtualized blocks, machine learning objects, and big data HDFS. Moving storage data is the biggest challenge facing supercomputing centers. These developments represent both challenges and opportunities for the data storage industry and are key to the transformation of supercomputing from compute-intensive to data-intensive. Data-intensive supercomputing is a data-centric, high-performance data analytics platform with the analytics capabilities of traditional supercomputing, big data analytics, and artificial intelligence. It supports end-to-end scientific computing services through application-driven unified data sources. It also provides diverse computing capabilities for research and operations, and leverages accumulated data knowledge to provide a high level of data value services. Data-intensive supercomputing transforms computing centers into centers that provide diverse computing services. Ultimately, diversified computing convergence and a unified storage foundation for massive data will enable high-performance data analytics, driving supercomputing from the era of computing services to the era of data value (Figure 2). Data-intensive supercomputing delivers: Research : An architecture that incorporates HPC, artificial intelligence, and big data technologies is an interdisciplinary innovation driven by data-intensive research. It facilitates the evolution of research from the third paradigm (computational science) to the fourth paradigm (data science).\nBusiness : Unify the data foundation, converge, efficient, secure, and low-carbon, reduce the lifecycle management costs of massive structured and unstructured data, and improve the data utilization efficiency of converged applications such as scientific computing, big data, and artificial intelligence. Industry : China-made high-performance data analytics (HPDA) software, parallel file systems, data storage, and data management systems drive the development of China's supercomputing storage industry and application technology ecosystem. Figure 1: Supercomputing Enters Data Value Era Data-intensive supercomputing is widely used in scientific research, manufacturing, and commerce. For example, in gene sequencing applications, the DNBSEQ-T7 sequencer manufactured by Huawei University generates 4.5 terabytes of data every 24 hours. At full capacity, 1.7 petabytes of data are totaled per year. Analyzing biological information typically creates intermediate files and results approximately five times the amount of raw data. West China Hospital uses data-intensive supercomputing to improve gene sequencing efficiency, reducing the time required for a single gene sequencing task from 3 hours to minutes. Autonomous applications are highly complex and involve more than 10 steps of data import, preprocessing, training, simulation, and result analysis. Each requires different protocols, such as object, NAS, and HDFS. Data silos are also a major problem because it takes twice as long to replicate data as it takes to process and analyze it. Automaker Geely Volvo uses data-intensive supercomputing, single data base, multi-protocol interoperability, adapts to the entire service process, and improves data analysis efficiency while reducing data storage costs.\nHello, everyone! Is there any possibility to use own certificate for the SMB3 Encryption? I can't find any hint in the documentation. Storage: OceanStor Dorado 8000 V6. Thanks in advance! Hello, friend! No, Currently not supported! There are two communications when using SMB protocol. The 1st one is between the Windows client and storage, this part is to setup SMB protocol session, the version of the SMB protocol used depends on the negotiation between the Windows client and storage. For this part, no certificate is required. The 2nd one is between storage and AD domain, this part is for storage to join AD domain, SMB 2 is preferred, then SMB1. For this part, no certificate is required. A little confused with your requirement, if you already used the storage device of the other vendors, and you changed the certificate, please provide more details about how it is working. On storage, only when using LDAPS, the certificate is mandatorily required. For more details, connect . Hope this helps! Hello, friend! No, Currently not supported! There are two communications when using SMB protocol. The 1st one is between the Windows client and storage, this part is to setup SMB protocol session, the version of the SMB protocol used depends on the negotiation between the Windows client and storage. For this part, no certificate is required. The 2nd one is between storage and AD domain, this part is for storage to join AD domain, SMB 2 is preferred, then SMB1. For this part, no certificate is required.\nAfter the replication link disconnected and reconnected, remote replication took a very long time to complete synchronization. eService detected a remote device disconnection alarmfor OceanStor Dorado 8000 V6, and the replication links were restored after 3 minutes. But the asynchronous remote replication pairs took a very long time to complete the initial synchronization (usually, it should be complete in 30 minutes). The remote replication ports were flapping on Device Manager, fault - normal - fault. Alarm like below: 1. From the storage event log, all the replication links were disconnected at the same time, then restored almost at the same time. It means the replication links disconnected because of the external SAN network issue. 2. Check the kernel message log, after the replication link connected is restored, there were massive IO times out on replication links, and then the ABTS commands were issued to abort the IOs. However, the ABTS command also times out, triggering the replication link reset frequently, that's why replication link ports were flapping on Device Manager. 3. Check the storage of internal FTDS data, controller 0B of the primary site had very high latency on remote data write/send start at 2 pm on 25th Oct. In the meanwhile, the secondary storage has a very low local write delay. It means the high remote write delay is caused by the replication link/network. As below graph, the maximum delay reached 10 seconds, which can easily trigger an IO timeout. Also, the high latency issue on the replication link makes the data synchronization really slow.\nDear team, This post enquires about a SmartKit port configuration on a firewall. Please see more details displayed below. We would like to install SmartKit version 22.0.0.10 and some tools such as historical performance, expansion, patchtool and upgrade array. Which ports are required to be open on the firewall side? Also, is the connection one-way or two-way? Thanks in advance for your assistance! Dear user, Function Permission Level Protocol and Port Description Unified Storage Deployment Super administrator SSH: 22 - Disk Deployment Commissioning N/A TLV: 8080 SSH:22 - Default Management IP Address Modification Tool Administrator SSH: 22 - Patch Tool Super administrator SSH: 22 TLV: 8080 REST:8088 - Cross Version Upgrade Super administrator SSH: 22 TLV: 8080 - Expansion Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - Upgrade Super administrator SSH: 22 SFTP: 22 TLV: 8080 REST: 8088 - Historical Performance Monitoring Super administrator SFTP: 22 TLV: 8080 REST: 8088 - CRU Replacement Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - FRU Replacement Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - Log Analysis N/A - No applicable if no device is connected. Upgrade Evaluation Tool Super administrator SSH: 22 TLV: 8080 - Information Collection Read-only user SSH: 22 REST: 8088 Some inspection items require the permission of an administrator or super administrator. Inspection Read-only user SSH: 22 TLV: 19001 REST: 8088 Some inspection items require the permission of an administrator or super administrator.\nDisk Firmware Upgrade Super administrator SSH: 22 REST: 8088 - Disk Health Analysis N/A - No applicable if no device is connected. SmartKit Frame N/A - No applicable if no device is connected. OEM Tool Super administrator SSH: 22 - CLIDK Super administrator SSH: 22 - Storage Solution Deployment Super administrator SSH: 22 - Storage Maintenance Tool OEM Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - Huawei Data Reduction Estimation Tool N/A - No applicable if no device is connected. InfoGrab Super administrator SSH: 22 JDBC: 1521, 1433, 3306, 5000, or 50000 Some collection items require the root permission. Thanks. Dear user, Function Permission Level Protocol and Port Description Unified Storage Deployment Super administrator SSH: 22 - Disk Deployment Commissioning N/A TLV: 8080 SSH:22 - Default Management IP Address Modification Tool Administrator SSH: 22 - Patch Tool Super administrator SSH: 22 TLV: 8080 REST:8088 - Cross Version Upgrade Super administrator SSH: 22 TLV: 8080 - Expansion Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - Upgrade Super administrator SSH: 22 SFTP: 22 TLV: 8080 REST: 8088 - Historical Performance Monitoring Super administrator SFTP: 22 TLV: 8080 REST: 8088 - CRU Replacement Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - FRU Replacement Super administrator SSH: 22 TLV: 8080/19001 REST: 8088 - Log Analysis N/A - No applicable if no device is connected. Upgrade Evaluation Tool Super administrator SSH: 22 TLV: 8080 - Information Collection Read-only user SSH: 22 REST: 8088 Some inspection items require the permission of an administrator or super administrator.\noning. The fabric operating systems within the fabric switches implement software zoning. The name server and the Fiber Channel Protocol are nearly typically used to perform software zoning. When a port sends a request to a name server, the name server only responds with information about the ports in the same zone as the requesting port. Hardware does not impose a soft zone, or software zone. As a result, if a frame is delivered (addressed) to a port for which it was not intended, it is delivered to that port. Hard zones are in contrast to this type of zoning. The members of a zone can be defined using their WWNs when utilizing software zoning: Node WWN Port WWN You can usually establish symbolic names for zone members and zones themselves using zoning software. It's typically easier to work with a device's symbolic name or aliases rather than the WWN address. Only the amount of memory in the fabric switch limits the number of possible members in a zone. A member might be a part of more than one zone. The fabric can have numerous sets of zones, but only one set can be active at any given moment. You can switch to another zone configuration at any moment without having to turn off the switch. You don't have to worry about physical connections to the switch while using software zoning.\nStorages are one of the devices that can be used to store information. In the following, we will examine the types of information storage methods and technologies used in them. Types of information storage Introducing Tape Storage Tool Information storage terms on the computer What is Block Storage? What is Block Level Storage? Advantages of Block Level Storage system What is File Level Storage? What is Cold Storage? What is Tiered storage? What are Flash Ready and Flash Storage? Disadvantages of Flash Storage It is a system for storing inactive data that is commonly used for data storage purposes for backup, archiving and disaster recovery. High-performance core storage is generally expensive and not suitable for storing inactive data. What is important in cold storage is low cost, high capacity and high ability to store data for a long time or Data Durability. Data retrieval and Response Time in such systems are slower than systems or devices for active data. LTO Tape and HDDs are suitable for cold storage. One of the technologies used in the field of storage and storage is Tiered Storage. In this method, the data is stored in different storage devices and media, but the choice of media depends on the number of data references, ie based on the required performance, availability and recovery, the decision is made for the location of the data.\nIn this method, the most commonly used data is stored in a store with the highest level of efficiency, and data that is rarely used is stored in cheaper and weaker storage. Cache, RAM, Hard Drive, or Tape may be the right place. Tiered storage, which acts like a hierarchical storage management, not only increases speed and efficiency when referring to data, but also manages data life cycle (Information Life-cycle Management) to prevent data corruption and the sudden loss and destruction of information. Today, Tiered storage infrastructure encompasses a range of infrastructures: from Two-Tiered architecture, which includes SCSI and Fiber Channel Attached Disk, to much more complex infrastructures, which include 5 or 6 layers or Tiered. Flash Ready storage is a device or system that supports Flash technology. Flash technology enables the electrical programming and cleaning of data, and for nearly half a century, it remains the most widely used technology in storage systems and equipment. Instead of storing data on hard drives with flash drives, Flash Storage is placed on solid state memory chips called Flash, the ones we know by USB. At the enterprise level, Flash Storage refers to the use of Solid State drives, or SSDs, that are used to store large amounts of data or files, such as the All Flash Array. Flash storage can be a replacement for HDDs and any other storage media. Recent advances in flash production allow you to store more data per unit per rack than HDDs, while input / output operations are faster.\nHello all, I will introduce you the software and network architecture of OceanStor 9000 V5. Figure 1 the software architecture of OceanStor 9000 V5. Table 1 describes the major software modules shown in Figure 1. Table 1 Software modules Type Description Distributed file system layer Protocol and Value-added Service (PVS) The PVS responds to connection requests initiated by external NFS, CIFS, and FTP applications. In addition, the PVS provides value-added features such as snapshot, storage tiering, and remote replication. Client agent (CA) and Metadata Service (MDS) The CA implements semantic analysis of application protocols such as NFS, CIFS, and FTP and sends the analysis results to underlying modules. The MDS manages file system metadata and acts as the cache for reading and writing file system metadata. The OceanStor 9000 V5 network architecture contains the management network, front-end service network, and back-end storage network. The OceanStor 9000 V5 network architecture consists of the following: A front-end service network that connects OceanStor 9000 V5 to the user's network A back-end storage network that interconnects OceanStor 9000 V5 nodes A management network that connects OceanStor 9000 V5 to the user's maintenance network OceanStor 9000 V5 supports multiple networking modes, including 25GE, 10GE, InfiniBand (IB), and GE networking to meet different network requirements. Typical Network This section provides an example network for operations per second (OPS)-intensive applications. In this example, OceanStor 9000 V5 adopts a 10GE network where all nodes provide IPMI ports or Mgmt ports and are managed and maintained through management switches. Figure 2 shows the 10GE network.\nDear all, This post describes the background, definition, and benefits of the NDMP feature. As digital information becomes the trend, users attach more importance to data and are more demanding on data security. Data loss due to hardware faults and misoperations must be prevented. Data backup is a means to retain data and use the retained data to recover data lost after a system damage or other faults. Data backup prevents data loss or corruption caused by unexpected accidents, ensuring data security. To address the urgent need of data backup, a large number of backup software products and solutions appear on the market. However, a storage system is hardly compatible with all the backup software products and solutions, and data backup using the Common Internet File System (CIFS) or Network File System (NFS) protocol consumes a large amount of network and server resources. To address these problems, a data backup and recovery technology based on Network Data Management Protocol (NDMP) emerges. Network Data Management Protocol (NDMP) is an open protocol used for enterprise-class data management. This protocol defines a network-based control mechanism to control data transmission between Huawei storage system and tape libraries during backup and recovery. Also, NDMP allows storage systems to directly transmit data to tape libraries or backup servers over a network, without the need of backup clients or consumption of network and server resources. NDMP has five versions, ranging from V1 to V5. Each version is optimized from its predecessor to provide more functions and enable easier backup.\nDear Members, Instant access to services is now key to survival for every organization operating in any industry, but perhaps nowhere more so than in healthcare. Here, service delays can have a devastating, life-changing effect. And as demand for healthcare services rises around the world, especially since the start of the pandemic, the pressure placed on hospital Information Technology (IT) infrastructure is only increasing. So it's imperative that institutions renew their efforts to ensure that their systems are efficient, responsive, and most important always on. As the primary provider of healthcare services in West Occitania, a part of France's southernmost administrative region (excepting the island of Corsica), Toulouse University Hospital (TUH) is no stranger to these challenges. Receiving 550,000 emergency calls, 276,000 hospitalizations, and caring for over 820,000 patients each year, it needs its IT systems to deliver and to deliver fast. But with admissions growing by 30% each year, the hospitals legacy systems were struggling and beginning to strain. Racing Against Time: When Every Second Counts TUH has been a leading national healthcare institution for over 18 years, earning its position as one of France's top four Centres Hospitaliers Universitaires (CHU): hospitals associated with a university. As such, it operates across locations in the north and south of the city it's named for, Toulouse, employing over 3000 doctors and medical students , along with a further 11,600 hospital staff, to perform a range of healthcare services, from daily patient care and long-term cancer treatment to medical research.\nToday we are going to explain some differences between NVMe and SATA SSDs, first, we will see what is an SSD. SSD refers to a solid-state drive, a type of computer storage that is noticeably quicker than a hard drive. One of the main advantages of using an SSD over a traditional hard drive is the speed with which data can be read and written. Even the slowest solid-state drive (SSD) is three times as quick as a hard drive. The difference will be most apparent when you first power on your computer, launch a program, or switch between tasks. SSDs are popular drives that are now standard equipment for most computers, including Apple's Macs. SSDs, unlike hard disk drives (HDDs), employ persistent flash memory to store data rather than disks, motors, and read/write heads. SSDs have grown so popular because they outperform HDDs in terms of performance and power consumption. the Non-Volatile Memory Express (NVMe) standard was initially created to address the various problems with the SATA interface and communication protocols. Using the PCIe bus, rather than the slower SATA bus, may allow storage devices to achieve previously unattainable levels of throughput with NVMe technology. Keep in mind that SATA-based SSDs can only have one queue, and that queues can only be 32 commands deep, while NVM Express (NVMe) enables up to 65535 command queues, each of which can have up to 65536 commands, and PCIe 4.0 (the current version) enables up to 32 lanes and, in principle, can transport data at up to 64,000MB/s.\nThere is great promise for storage devices based on NVMe technology due to its improved efficiency, performance, and interoperability with a wide range of platforms. The widespread adoption of this technology in the corporate sector is anticipated. SATA SSDs appear like laptop hard drives, flat and rectangular like.They're the entry-level SSD option and have a standard hard disk interface. But a computer with a SATA solid-state drive (SSD) has three to four times the bandwidth (how much data it can read/write simultaneously) of a computer with a hard drive. SATA SSDs are also more available and cheaper than NVMe-PCIe ones and are still great for common applications.. Form Factors of SSDs: SSDs are also available in a variety of form factors. This pertains to the SSD's physical size, shape, and connector: The most frequent size is 2.5-inch. They are the same physical size and shape as mechanical hard drives, and they fit in the same space. M.2 - M.2 SSDs are slim memory sticks that fit into an M.2 slot or socket on the motherboard (see our diagram and explanation of motherboard parts for more info). Some M.2 slots accept only SATA SSDs, while others accept both SATA SSDs and PCIe-NVMe SSDs. Add-in card (AIC): These are only found on desktop computers. They connect to a motherboard's PCIe x4 or x16 slot, which is usually reserved for GPUs or RAID controllers. AIC SSDs, such as the Intel Optane 900p, are among the fastest SSDs, thanks in part to their huge surface area.\nUltraPath can meet the requirements on high reliability and superb performance of storage networks. This section introduces the background, purpose, and benefits of UltraPath. As modern technology develops, the requirements laid upon the security and stability of storage networks increase during the implementation of IT infrastructure. A fault in the storage network may cause the entire network to break down. Such a fault is called a single point of failure in the storage network. To avoid this, a highly reliable storage network not only includes redundant devices and components, but also adopts the interconnection of redundant links, as shown in Figure Multi-path networking. The redundancy design can improve the reliability as well as the performance of the entire storage network. The multipathing technology helps achieve this goal. Figure1 Multi-path networking The multipathing technology is realized through multipathing software. If a path fails or cannot meet the performance requirement, multipathing software automatically and transparently transfers I/Os to other available paths to ensure that I/Os are transmitted effectively and reliably. As shown in Figure UltraPath handling path faults, multipathing software can handle many faults such as HBA faults, link faults, and controller faults. Figure1 UltraPath handling path faults The multipathing software (normally called Multi-Path I/O, MPIO) built-in host operating systems only provides basic failover and load balancing functions, failing to meet the high reliability requirements.\nHard disk drives (HDDs) are poor in performance. To solve the problem, caches and tiers are developed, which can accelerate performance to a certain extent. However, traditional caches and tiers are limited. Cache alone cannot separate cold and hot data throughout its lifecycle. Moreover, policies and periods must be configured for tiers based on experience, which poses great challenges to ease of use and performance acceleration. In the all-flash storage era, the traditional storage software stack designed for HDDs is too heavy. Architecture innovation is required to overcome system bottlenecks and give full play to the performance of flash media. To this end, the native all-flash technology comes into being. This technology combines redirect-on-write (ROW) with more flexible metadata index design and disk-controller collaboration to achieve data reduction, ultimate performance, and stable latency of all-flash storage system. When it comes to hybrid flash storage, it is the most important design objective to optimize the overall performance (that is, how to use solid-state drives (SSDs) to accelerate performance and HDDs to provide capacity), so that the hybrid flash storage system can obtain the performance similar to that of an all-flash storage system. Therefore, OceanStor new-gen hybrid flash storage combines the advantages of HDDs and all-flash storage and uses the innovative SmartAcceleration feature to further improve the performance and efficiency of storage systems. The core of SmartAcceleration is the dynamic adaptive data layout (DADL) architecture.\nIn case of a disk fault, if the number of available disks is no less than the number of member disks in a RAID group, data is restored to new chunks (CKs) during common reconstruction. After the reconstruction, the number of member columns in the RAID group remains unchanged as well as the RAID redundancy level. In the new-gen OceanStor hybrid flash storage, a RAID group has N+M member columns (N indicates data columns and M indicates parity columns). When the system has faulty disks, common reconstruction is triggered if the number of normal member disks in the disk domain is still greater than or equal to N+M. During the reconstruction, the system uses idle CKs to replace the faulty ones in the chunk group (CKG) and restores data to the new CKs. The RAID level remains N+M. As shown in the following figure, D0, D1, D2, P, and Q form a CKG. If disk 2 fails, a new CK D2_new on disk 5 is used to replace D2 on disk 2. In this way, D0, D1, D2_new, P, and Q form a new CKG and the system restores the data of D2 to D2_new. In this scenario, a common reconstruction can keep the number of member disks in a RAID group and the RAID redundancy level unchanged.\nIf the number of available disks in a disk domain is less than the number of member disks in a RAID group due to consecutive disk faults or disk replacement, no new member disks are available to form a new CKG for common reconstruction. As a result, the original RAID redundancy level cannot be retained, and the system enters the degraded mode, reducing data reliability and increasing system risks. The new-gen OceanStor hybrid flash storage automatically triggers the dynamic RAID reconstruction in such a case. If the total number of available disks in a disk domain is less than the number of RAID member disks, the system retains the number of parity columns (M) and reduces the number of data columns (N) during the dynamic reconstruction, which is different from the common reconstruction. After the reconstruction, the RAID redundancy level is unchanged. In the following figure, there are six disks (4+2). If disk 2 fails, data D2 in CKG0 is written to the new CKG1 as new data (D2') and the RAID level is 3+2. D0, D1, and D3 form a new 3+2 CKG0 with new parity columns P' and Q'. After the reconstruction, the number of member disks in the RAID group decreases, but the RAID redundancy level remains unchanged. After the faulty disks are replaced and the total number of available disks in the disk domain meets the requirement of RAID member disks, the number of data columns automatically restores to the normal value.\nThe distributed file systems of the new-gen OceanStor hybrid flash storage apply to file sharing scenarios with coexisting mass volumes of small and large files. Data in each directory is evenly distributed to each controller for load balancing. The same controller processes the I/Os of a directory and its files to eliminate forwarding across controllers and improve performance for directory traversal, attribute traversal, and batch attribute configuration. When large files are written to a storage pool, RAID 2.0+ globally distributes their data blocks to all disks in the storage pool for improved write bandwidth. The new-gen OceanStor hybrid flash storage supports two directory balancing policies for NAS: Local Allocation: The controller whose IP address is mounted to a host preferentially processes the directory access request of that host to optimize performance and latency. The system dynamically adjusts the distribution ratios of remote controllers in the background based on the data capacity, file quantity, and load of each controller. When one controller exceeds the others in one of these parameters and the difference reaches a threshold, the system automatically increases the distribution ratios of remote controllers to universally balance controller loads. This is the default distribution mode of the new-gen OceanStor hybrid flash storage, and it can achieve the optimal performance when each IP address of each controller is evenly mounted to the hosts. Local allocation is best for scenarios with requirements for high performance and low latency, such as OA file sharing, check images, and EDA back-end simulation.\nThe audit log feature records device operations in logs and provides the log audit function to manage device access security and analyze service patterns and trends. The new-gen OceanStor hybrid flash storage provides audit logs for management operations and NAS service operations. Service access audit is to log the NAS service access operations on NAS clients. An independent audit log file system is used to record the logs. This function can be separately configured for each vStore for isolation and supports the guarantee and non-guarantee modes. Specific audit objects can be configured. For details about the audit objects, see the feature guide. After the audit log function is enabled for a vStore, operation logs must be recorded for operations to be audited before the operations are performed. In guarantee mode, if an operation fails to be recorded in the operation log, the operation will not be performed, which may cause service interruption. In non-guarantee mode, the operation can still be performed but the operation log is incomplete. The system uses the guarantee mode by default, and users can modify the mode online. The audit log file system stores logs in evt or XML format and can be shared and accessed by the audit log server in real time. Management operation audit is to log device configuration and management operations performed by device administrators. Only configuration modifications are recorded (query operations are not recorded). The log file is stored in the system and does not support online access.\nMasking of Redundant LUNs In a redundant storage network, an application server with no multipathing software detects a LUN on each path. Therefore, a LUN mapped through multiple paths is mistaken for two or more different LUNs. Redundant LUNs exist because each path reports a LUN directly to the application server. The dual-link direct-connection network shown in the left side of Figure Masking the redundant LUN is an example. As shown in the figure, the storage system maps one LUN to the application server. Since two paths exist between the application server and the storage system and no multipathing software is installed, the application server simultaneously detects two LUNs, LUN0 and LUN1, indicating that a redundant LUN exists. The two detected LUNs actually are the same LUN from the storage system. Due to the identification errors of the application server, different applications on the application server repeatedly write different data to the same location of the LUN, resulting in data corruption. To resolve this problem, the application server must identify which is the real and available LUN. Figure1 Masking the redundant LUN As UltraPath is able to acquire configuration information of the storage system, it clearly knows which LUN has been mapped to the application server. As shown in the right side of Figure Masking the redundant LUN, UltraPath installed on the application server masks redundant LUNs on the operating system driver layer to provide the application server with only one available LUN, the virtual LUN.\nIn this case, the application server only needs to deliver data read and write operations to UltraPath that masks the redundant LUNs, and properly writes data into LUNs without damaging other data. Optimum Path Selection To ensure service continuity and stability, a storage system is generally equipped with two or more controllers to implement redundancy parts. Each LUN in a storage system has its owning controller, and no other controllers can operate on the LUN, preventing data corruption due to possible controller conflicts. If an application server wants to access a LUN through non-owning controllers, this access request is still redirected to the owning controller. Therefore, the highest I/O speed occurs when application servers access the target LUN directly through the owning controller. In a multipath environment, the owning controller of a LUN on the application server that corresponds to the LUN on the storage array is called the prior controller of the LUN on the application server. Therefore, the highest I/O speed occurs when an application server with UltraPath inside accesses the LUN on the storage system through the prior controller (owning controller). The path to the prior controller is the optimum path. As UltraPath is able to acquire owning controller information, it can automatically select one or more optimum paths for data streams to achieve the highest I/O speed. As shown in Figure Optimum path selection by UltraPath, the owning controller (prior controller) is controller A, and UltraPath selects the path to controller A as the optimum path.\nFigure1 Optimum path selection by UltraPath Failover and Failback Failover When a path fails, UltraPath fails over its services to another functional path. Figure UltraPath failover shows the failover process. Figure1 UltraPath failover An application on the application server sends an I/O request to the virtual LUNs displayed on UltraPath. UltraPath designate Path0 to transfer this I/O request. A fault on Path0 prevents this I/O from being sent to the storage system. The I/O is returned to UltraPath. UltraPath designate Path1 to transfer this I/O request. Path1 is normal. The I/O request is sent to the storage system successfully. A message indicating the I/O request is sent successfully is sent to UltraPath. UltraPath sends the message to the application server. Failback UltraPath automatically delivers I/Os to the first path again after the path recovers from the fault. There are two methods to recover a path: For a hot-swappable system (for example, Windows), the SCSI device will be deleted if the link between an application and a storage array is down. After the link is recovered, a SCSI device will be created. UltraPath can immediately sense the path recovery. For a non-hot-swappable system (for example, AIX or earlier versions of Linux), UltraPath periodically tests and detects the path recovery. I/O Load Balancing UltraPath provides load balancing within a controller and across controllers, as shown in Figure Two I/O load balance modes. Figure1 Two I/O load balance modes For load balancing within a controller, I/Os poll among all the paths of the controller.\nData is written into the dynamic random-access memory (DRAM) first. After data is mirrored to other controllers, a response is returned to the host. After data is accumulated to a certain amount, it is flushed from the DRAM to disks. During the flushing, the intelligent engine distributes the data into different layers. Specifically, modified data in small I/Os and hot data are distributed to the performance layer, and other data to the capacity layer. Data distributed to the performance and capacity layers is aggregated to sequential large I/Os based on ROW and then written to persistent storage media. When the capacity of the performance layer reaches a certain watermark, the data that becomes cold is migrated to the capacity layer as recommended by the algorithm. The data to be read is first searched in the DRAM read cache. If the data is found, it is returned; if the data is not found, the next step is performed. The data to be read is searched in the read cache of the performance layer. If the data is found, it is returned; if the data is not found, the next step is performed. The data to be read is searched in the storage pool. According to the index, the data is returned from the tier of the performance layer or from the capacity layer. If the requested data is not found in the DRAM read cache, a copy will be stored in the DRAM read cache after the data is returned to the host.\nArray encryption uses the built-in encryption engine of the controller processor to implement encryption and decryption. The independent built-in encryption engine leverages the encryption algorithm of Arm hardware to offload encryption workloads. The encryption and decryption algorithms of the storage system are offloaded to the hardware for execution, without involving software. When Huawei OceanStor Doradonext-gen OceanStor hybrid flash storage system encrypts data, the block management subsystem generates a data encryption key (DEK) on each disk, and applies for an authentication key (AK) from the key manager. The AK is used to encrypt the DEK. After service I/Os are delivered, encryption and decryption are offloaded to the built-in encryption engine for execution. The encryption engine supports the AES-256-XTS and SM4-128-XTS (only for the Chinese mainland) algorithms. The algorithm used by the key manager must match that used by the encryption engine. Figure Built-in encryption engine shows the example topology of configuring an Internal Key Manager. Figure1 Built-in encryption engine Data encryption: After the built-in encryption engine is enabled, the block device management subsystem uses the built-in encryption engine to encrypt and decrypt data with the DEK during data writes and reads. When the storage system receives a write request, the built-in encryption engine encrypts the plaintext data, and the block device management subsystem then writes the encrypted data into storage media. When the storage system receives a read request, the block device management subsystem reads the encrypted data, which is then decrypted by the built-in encryption engine into plaintext.\nSmartAcceleration uses a new and unified cold and hot data sensing algorithm that drives the data flow policies in the cache and tier of the performance layer. The new algorithm uses several key technologies, which are described as follows: Different from the traditional hot data statistics algorithm that uses a fixed data structure, the hot data statistics algorithm of the OceanStor new-gen hybrid flash storage uses a multi-granularity learning-oriented statistics structure. It can adaptively adjust to the data structure based on the model changes and better adapt to the conversion of large and small I/O models and the change of sequential random models, maximizing efficiency in various service scenarios. Traditional cold and hot data sensing algorithms usually consider only historical statistics. The OceanStor new-gen hybrid flash storage uses multi-dimensional feature analysis to identify global hot data. In addition to traditional historical statistics, it also incorporates the time series forecasting algorithm and neural network algorithm of machine learning. In scenarios where data changes between hot and cold, time series forecasting and historical statistics are combined to determine whether data is stored in the performance layer or capacity layer. The cold and hot data sensing technology that integrates multi-dimensional features ensures that all hot data can be effectively identified and accelerated. In addition to identifying micro I/O characteristics, the new cold and hot data sensing algorithm optimizes and allocates system resources based on the global workload.\nController enclosures and disk enclosures containing disks use the patented parallel backplane design to achieve high disk integration. 2 U 36-slot palm-sized NVMe disk enclosures, which are developed using the Huawei's patented horizontal backplane heat dissipation technology and disk encapsulation design, are used to allow a disk enclosure to house more disks per unit volume. Such a design makes the product industry-leading in energy efficiency (IOPS/W) by providing customers with more capacity in the same occupied space and delivering higher performance with the same power consumption. The number of slots for customized palm-sized NVMe SSDs in a single enclosure increases from 24/25 to 36, which is a 44% improvement. Traditional disk enclosures adopt 2 U height with 24/25 slots provided. In this case, take the scenario where 100 disks are to be configured as an example. At least 8 U height must be prepared for a traditional product with more than four controller enclosures and eight power modules required. OceanStor Dorado only requires 6 U space with six power modules and three enclosures containing palm-sized NVMe SSDs and configured. In addition, slot space for capacity expansion is reserved for customers. The height is 25% to 40% lower than the industry level. In terms of power topology architecture, centralized power supply is more efficient than distributed power supply. The former's power modules only consume 1/3 to 1/4 energy of that of the power modules used in the industry, therefore eliminating waste and improving efficiency. As less space is occupied, the cost on renting equipment rooms is reduced.\nThe SSD performance layer is the key for a hybrid flash storage system to achieve an optimal data layout. The performance layer accelerates the hot application data identified by the intelligent engine and metadata generated by the ROW architecture to provide all-flash-like capabilities, such as high performance, balanced distribution, value-added and lossless capability, and data reduction. By default, SmartAcceleration uses SSDs to create a unified performance layer. Hybrid-flash or all-flash storage pools can be created based on the performance layer. When creating a hybrid-flash storage pool, select the required HDDs first. Based on the number and capacity of the selected HDDs, the system automatically recommends a performance layer capacity quota (actually the SSD space) for each hybrid-flash storage pool. When creating an all-flash storage pool, you do not need to select HDDs. Directly enter the capacity requirements of the all-flash storage pool. The requirements for the number of disks in the performance layer of each product model are as follows: SSDs 4 HDDs 8 SSDs 8 HDDs 8 SSDs 6 HDDs 8 SSDs 8 HDDs 8 Compared with traditional tier or cache configuration, the performance layer: Can be shared by multiple pools, improving utilization and flexibility. Is distributed by the system to each pool for intelligent acceleration based on the recommended quota by default, and can also be flexibly increased or decreased. Converges tiers and caches and is automatically configured within the system to achieve the optimal overall efficiency. Can be exclusively used by vStores if necessary.\nIn addition, multiple isolated performance layers can be manually created. The capacity of both the performance and capacity layers can be expanded independently. To ensure consistent performance, it is recommended that the performance and capacity layers be expanded correspondingly based on the original ratio. By default, the system recommends the performance layer quota to be allocated to each storage pool. You can create storage pools based on the default recommendations. After storage pools are created, the system automatically allocates the caches and tiers based on the performance layer quota allocated to each storage pool. When the performance layer quota is small (for example, the ratio of SSD space to HDD space is less than or equal to 3%), only cache acceleration is supported. When the quota is large, both cache and tier acceleration are supported. The cold and hot data sensing algorithm identifies the hot data in reads and writes based on the characteristics of service models. Hot read data is preferentially placed in the cache, and hot data involved in both reads and writes is preferentially placed in the tier. Cold data is directly distributed to the HDD capacity layer. When hot data becomes cold as time goes by, data in the cache will be automatically evicted and that in the tier will be migrated to the HDD capacity layer. The cache and tier accelerate hot data with different characteristics in different scenarios as follows: The hot read data that may be stable or variable is usually accelerated by the cache.\nHard disk drives (HDDs) are poor in performance. To solve the problem, caches and tiers are developed, which can accelerate performance to a certain extent. However, traditional caches and tiers are limited. Cache alone cannot separate cold and hot data throughout its lifecycle. Moreover, policies and periods must be configured for tiers based on experience, which poses great challenges to ease of use and performance acceleration. In the all-flash storage era, the traditional storage software stack designed for HDDs is too heavy. Architecture innovation is required to overcome system bottlenecks and give full play to the performance of flash media. To this end, the native all-flash technology comes into being. This technology combines redirect-on-write (ROW) with more flexible metadata index design and disk-controller collaboration to achieve data reduction, ultimate performance, and stable latency of all-flash storage system. When it comes to hybrid flash storage, it is the most important design objective to optimize the overall performance (that is, how to use solid-state drives (SSDs) to accelerate performance and HDDs to provide capacity), so that the hybrid flash storage system can obtain the performance similar to that of an all-flash storage system. Therefore, OceanStor new-gen hybrid flash storage combines the advantages of HDDs and all-flash storage and uses the innovative SmartAcceleration feature to further improve the performance and efficiency of storage systems. The core of SmartAcceleration is the dynamic adaptive data layout (DADL) architecture.\nVMware VM backup uses four methods to access virtual disk data in datastores: SAN, HotAdd, NBD, and NBDSSL. These methods are called VMware transport modes. Before configuring a backup job, you need to understand VMware transport modes so that you can select a proper transport mode and determine the deployment mode of ProtectAgent. During backup, the system selects a transport mode based on the priorities of SAN, HotAdd, NBD, and NBDSSL in descending order. If virtual disks need to be stored in the FC SAN or iSCSI SAN for the VMware virtualization platform, select the SAN transport mode for backup. In this mode, the ProtectAgent host must be deployed in the storage area network (SAN) at the production site. The ProtectAgent host directly reads SAN storage data for backup and restore, without the need to transmit any data through ESXi or LAN. Therefore, the data transmission efficiency is high and Ethernet resources are not occupied. The requirements for using the SAN transport mode are as follows: ProtectAgent must run on a physical server. The physical host where ProtectAgent resides must be able to access the datastore where the VM disks to be backed up reside. The LUN corresponding to the datastore must be mapped to the physical host. Figure1 Deployment of the ProtectAgent host When running on a VMware VM, ProtectAgent creates a linked clone for the target VM and mounts the VMDK file of the linked clone to the ProtectAgent host using the HotAdd function.\nThen, ProtectAgent can read the target virtual disk copy to back up the target VM. This mode is called the HotAdd transport mode. The restrictions on using the HotAdd transport mode are as follows: ProtectAgent must run on a VMware VM. The VM running ProtectAgent must be able to access the datastore where the target virtual disk resides. The VMFS block size and version of the datastore where the VM running ProtectAgent resides must be the same as those of the datastore where the target VM resides. Only VMs with SCSI or SATA disks can be backed up. When using the HotAdd transport mode for backup and restore, the VM running ProtectAgent must be able to connect to TCP port 902 on the ESX/ESXi host. Figure1 Deployment of the ProtectAgent host NBD/NBDSSL is a LAN-based transport mode. The ESXi host reads data from storage and then transmits the data to the ProtectAgent host through the LAN. When data is transmitted in NBD mode, the data is not encrypted. When data is transmitted in NBDSSL mode, the data is encrypted using SSL. By default, OceanProtect A8000 uses NBDSSL for encrypted transmission. The following uses the NBDSSL transport mode as an example. The advantages of the NBDSSL transport mode are as follows: The ESXi host where the target VM resides can use any storage, including the local storage, SAN storage, and NAS storage. If the ESXi host and the ProtectAgent host are located on a private network, you can use a data transmission mode without encryption.\nInternet customers' platforms usually run services such as videos, images, user logs, databases, AI analysis, and big data. In terms of infrastructure as a service(IaaS), self-developed or universal server architecture (using local disks) and distributed parallel file systems are used to provide high-performance block/NAS pools, large-capacity HDFS pools, and backup- and archiving-specific OBS pools. Cloud vendors provide IaaS, including elastic block storage (EBS), scalable file service (SFS), and object storage service (OBS). In scenarios requiring high performance, such as AI analysis, data processing, and EBS, all-flash storage is used together with self-developed or Ceph distributed storage software to deliver a high performance of flash pool. Distributed storage mainly adopts three-copy redundancy. As storage, network, and computing power develop, traditional server architectures gradually expose imbalances in resource utilization. Vendors such as Amazon, FaceBook, and NVIDIA, has focused on heterogeneous computing and led the trend in data center architecture evolution. Chinese vendors, such as Baidu and Alibaba, have started research on heterogeneous computing and applied infrastructure decoupling architectures. In traditional cloud architectures, data centers are CPU-centric. As Moore's Law slows down, resource utilization of data centers gradually decreases. With the sharp increase of data volume due to digital construction, data centers will become data-centric. Technologies such as architecture decoupling, resource pooling, and resource sharing can improve resource utilization, reliability, and scalability and reduce costs. Large-sized enterprises have strong R&D capabilities and can develop their own hardware and software. Small-sized enterprises have limited service space. Therefore, OceanStor Micor is designed for medium-sized enterprises that have the following requirements: 1.\nDual-link direct connection is the simplest and most inexpensive storage network connection, as shown in Figure UltraPath in dual-link direct connection. Figure1 UltraPath in dual-link direct connection The application server uses optical fibers to connect different storage controllers for redundancy. In this networking mode, the path between the application server and the LUN's owning controller is the optimum one while other paths stand by. In normal cases, UltraPath selects the optimum path for data transfer. If the optimum path is down, another standby path is used. After the optimum path recovers, it takes over data transfer again. The dual-link single-switch interconnection adds one switch on the basis of dual-link direct connection, improving data access and forwarding capabilities, as shown in Figure UltraPath in dual-link single-switch interconnection. Figure1 UltraPath in dual-link single-switch interconnection In Figure UltraPath in dual-link single-switch interconnection, there are four paths between the application server and the storage system. In this networking mode, the two paths between the application server and the LUN's owning controller are optimum while other two paths stand by. A switch expands host ports to improve access capability of the storage system. Besides, a switch extends the supported distance of transfer so that a remote application server can connect to the storage system through the switch. As only one switch is available in this networking mode, it may encounter a single point of failure. To prevent the failure, you can adopt Dual-Link Dual-Switch Interconnection, as shown in Figure UltraPath in dual-link dual-switch interconnection.\nThe new-gen OceanStor hybrid flash storage uses RDMA for networking between controllers and between smart disk enclosures and controller enclosures. Data is directly transmitted between controllers over RDMA links without forwarding. Data is remotely transferred over RDMA links by interface modules without intervention by the CPUs on either side. This greatly improves data transfer efficiency and reduces the access latency. Generally, the data transmission process has two steps: The transmit end sends a control message to the receive end. The receive end prepares memory resources and tells the transmit end to send data. The transmit end sends data to the receive end. With RDMA, the receive end prepares memory resources in advance and the transmit end sends the control message and data together. This reduces one interaction between the transmit and receive ends for a lower latency. The RDMA technology provides higher reliability and lower communication latency than PCIe links. The picture as above compares the I/O interaction processes over PCIe and RDMA links. Data transfer involves I/O request delivery, data transfer to the peer end, data reception at the peer end, data verification, and acknowledgement. In the PCIe communication model, which is bidirectional communication, after data has been transferred from controller A to controller B, the CPU of controller A must notify controller B of data arrival through the control flow to trigger an interrupt on controller B. Then controller B invokes interrupt processing, checks the data, and returns a response.\nThe high-end devices of new-gen OceanStor hybrid flash storage support front-end interconnect I/O modules (FIMs). Each FIM connects to four controllers through four PCIe buses. Hosts can connect to any port on an FIM to access four controllers at the same time. On the high-end devices of new-gen OceanStor hybrid flash storage: The Fibre Channel interface modules are FIMs. Controller faults are transparent to hosts and no link alarm is reported. Upgrade over a single Fibre Channel or FC-NVMe link is supported. The IP SAN does not support FIMs. Link alarms are reported in the event of a controller fault. Upgrade over a single iSCSI or NVMe over RoCE link is supported. NAS networking (NFS or CIFS) does not support FIMs. Link alarms are reported in the event of a controller fault. Upgrade over a single NAS link is supported. An FIM intelligently identifies host I/Os and distributes the I/Os based on specific rules. In this way, host I/Os are sent directly to the most appropriate controller without pretreatment of the controllers, preventing forwarding between controllers. Because each port of FIMs has internal connections to all of the four controllers, host I/Os can be delivered to the most appropriate controller even if there is only one link between the host and the storage system. Moreover, services are not affected in the event of a controller failure. FIMs simplify the system networking. Without FIMs, at least four connections are required between a host and a four-controller system.\nHyperDetect is a data protection technology deployed in storage system containers. It provides ransomware detection capabilities, including ransomware file interception, real-time ransomware detection, and intelligent ransomware detection. Figure1 OceanStor Dorado HyperDetect (for File) Principles The system has set common detection models, in which known ransomware file name extensions are preconfigured. You can either import detection models to update the ransomware file name extensions or create file name extension filtering rules to add the ransomware file name extensions to be intercepted. You can select vStores and update file name extension filtering rules based on service requirements. The system checks whether the file name extensions of the files to be operated in all file systems of the vStore match the filtering rules. If they match, the system intercepts the ransomware files. Abnormal I/O behaviors refer to the I/O behaviors during ransomware attack. Real-time ransomware detection combines quick filtering of abnormal I/O behaviors and deep inspection of file corruption to detect whether data is attacked by ransomware. 1.Quick filtering of abnormal I/O behaviors Abnormal I/O behaviors are identified by analyzing I/Os of storage systems, and deep inspection is performed on the files that involve abnormal I/O behaviors. 2.Deep inspection of file corruption Basic characteristics of file contents are detected to identify corrupted files. For files without abnormal basic characteristics, the system performs deep inspection on the file based on the machine learning algorithm to check for corruption.\nIntegrated circuit (IC) design depends on IT technologies and services, including electronics design automation (EDA) tools, and infrastructure such as compute, storage, and network resources. As the chip scale and design complexity increase, the chip size decreases, and the EDA tools develop, traditional IT architectures are facing problems such as unavailable storage resource sharing, management, and use; poor reliability when there is no enterprise-level RAID; and lack of protection features, such as snapshots and all-in-one backup. OceanStor 2910This product, used as the IC infrastructure of small- and medium-sized enterprises, owns the following features: Integration of compute, storage, and network resources: OceanStor 2910This product deeply integrates compute, storage, and virtualization. A single device provides complete data center capabilities. OceanStor 2910This productworks with a simplified intelligent O&M management system, greatly improving IC design efficiency. Enterprise-level NAS features: OceanStor 2910This product supports hundreds of millions of files, concurrent access by multiple protocols, automatic tiering of hot and cold data, quota management, multi-tenant, and QoS. Highly reliable architecture: The dual-controller and full-redundancy system architecture and enterprise-level RAID ensure the security and reliability of high-value EDA data, and easy-to-use, efficient, and cost-effective services. Figure1 Integrated IT infrastructure for small- and medium-sized IC enterprises Currently, files in enterprises are managed based on services. Files of different services are separated. File sharing is implemented through USB flash drives or emails, which is inefficient and insecure. Server disks and removable disks are used to store data, resulting in low reliability. There are no anti-virus, anti-tamper, and disaster recovery solutions.\nWrite back: A caching technique in which the completion of a write request is signaled as soon as the data is in cache, and actual writing to non-volatile media occurs at a later time. Write protection: Data writing to storage systems is prohibited. In normal cases, the default write mode of a namespace is write back. However, the write mode changes to write protection if any of the following faults occurs. Table1 Namespace write protection situations and recommended actions Impact The write mode of all service objects on the controller enclosure changes to write protection. Recommended action Check whether BBUs are properly connected. Check whether the BBUs break down. Replace the faulty BBUs if any. If the power of the BBUs is insufficient, wait until the BBUs are fully charged. Impact The write mode of all service objects on the controller enclosure changes to write protection. Recommended action Check whether the built-in coffer disks of the controllers are faulty. Replace the faulty coffer disks if any. Impact The write mode of all service objects on the controller enclosure changes to write protection after the specified period of time. Recommended action Replace the faulty controller at off-peak hours within the write back hold time. If a spare part is unavailable during the write back hold time, you can extend the hold time properly after assessing risks to prevent write protection from adversely affecting services. Impact The namespaces in the storage pool change to write protection mode. Recommended action Contact Huawei engineers.\nData reconstruction is not possible if the number of available member disks in a disk domain with conventional RAID is less than the number of member disks in a RAID group due to continuous disk faults or disk replacement. Guaranteeing user data redundancy is impossible without reconstruction. The storage system overcomes this problem with dynamic RAID reconstruction by CK. If the total number of available disks in a storage pool is less than the number of RAID member disks, the system retains the number of parity columns ( ) and reduces the number of data columns ( ) during reconstruction. After the reconstruction, the number of member disks in the RAID group decreases, but the RAID redundancy level remains unchanged. After the faulty disk is replaced, the system increases the number of data columns to based on the number of available disks in the storage pool. + mode is used to write new data and gradually rebalance data from the fault period. Example A storage pool consists of 13 disks with RAID 6 and a hot spare policy of Low (1 disk) . The stripe length of CKGs is + = 10+2. RAID 2.0+ block virtualization uses CKs from 12 disks (Disk 0 to Disk 11) to form a CKG using the stripe structure with as 10 and as 2.\nHuawei OceanStor Micro is a new series of storage products designed for massive Internet/Cloud data storage. It can be used for the next-generation data center pooling architecture with composable disaggregated infrastructure (CDI). OceanStor Micro separates storage from compute and provides high-performance, high-reliability, and cost-effective storage services, improving resource utilization and reducing operation and maintenance (O&M) costs. OceanStor Micro is a new series of storage products designed for massive Internet/Cloud data storage. It can be used for the next-generation data center pooling architecture with composable disaggregated infrastructure (CDI). OceanStor Micro separates storage from compute and provides high-performance, high-reliability, and cost-effective storage services, improving resource utilization and reducing operation and maintenance (O&M) costs. It typically applies to Lustre scenario and Ceph scenario. OceanStor Micro can be integrated with the open-source parallel file system Lustre to provide high bandwidth and high IOPS for high-performance computing (HPC) through the HPC Performance Enablement license. Figure Lustre scenario shows the application scenario. An OceanStor Micro storage device connects to one or more metadata servers (MDSs) or object storage servers (OSSs). Generally, one OceanStor Micro storage device and two OSSs are considered as a building block for scale-out. OceanStor Micro leverages RAID 2.0+ block virtualization technology to tolerate simultaneous failure of three disks and implement fast reconstruction. In addition, it uses the NVMe over Fabrics protocol to improve performance. Therefore, OceanStor Micro ensures high-reliability and high-performance storage, meeting Lustre's requirements on bandwidth and IOPS. Figure1 Lustre scenario OceanStor Micro can interconnect with the distributed storage system Ceph.\nOceanStor Micro 1500 uses a 2 U controller enclosure with two controllers in an active-active structure and 36 NVMe disk slots. The NVMe disk adopts a customized physical form, which allows a 36-slot enclosure to house 40% more NVMe disks than 2.5-inch disks. All FRUs are redundant and can be replaced online. Front-end interface modules include 2-port 100 Gbit/s NVMe over RoCE interface modules, 4-port 25 Gbit/s NVMe over RoCE interface modules, and 4-port 16 Gbit/s and 32 Gbit/s Fibre Channel interface modules. For OceanStor Micro 1500, the two controllers in an enclosure are interconnected through RDMA mirror channels. Each controller has two GE management/maintenance ports and one serial port. The following figure shows the front and rear views of OceanStor Micro 1500. Figure1 Front view of OceanStor Micro 1500 Figure1 Rear view of OceanStor Micro 1500 OceanStor Micro 1500 integrates disks and controllers and uses a symmetric active-active architecture. Two controllers work in load balancing mode in normal situations and take over services in fault scenarios. The two controllers are interconnected by RDMA mirror channels. The following figure shows the logical architecture. Figure1 Logical architecture of OceanStor Micro 1500 Improved Interface Module Density To increase the number of interface modules per system, the interface modules use compact connectors to reduce the module thickness to 18.5 mm, allowing a 2 U controller enclosure of OceanStor Micro 1500 to house up to 12 interface modules. OceanStor Micro is a new series of storage products designed for massive Internet/Cloud data storage.\nHuawei OceanStor Micro is a new series of storage products designed for massive Internet/Cloud data storage. It can be used for the next-generation data center pooling architecture with composable disaggregated infrastructure (CDI). OceanStor Micro separates storage from compute and provides high-performance, high-reliability, and cost-effective storage services, improving resource utilization and reducing operation and maintenance (O&M) costs. OceanStor Micro has the following highlights: Robust reliability, ensuring service continuity Failover transparent to applications: The dual-controller architecture enables services to be switched over in seconds if a controller is faulty, without service interruption. Host links are not impacted. Global resource balancing: The end-to-end active-active architecture balances resources globally. Intra-enclosure large-ratio erasure coding (EC) algorithm: tolerates the failure of at most three disks. Superb performance Based on Huawei-developed chips and FlashLink, OceanStor Micro provides high IOPS and bandwidth, meeting performance requirements in cloud, Internet, and high-performance computing (HPC) scenarios. Optimal cost-effectiveness OceanStor Micro enclosures provide hardware-based data reduction capability, improving effective capacity and reducing TCO while meeting performance requirements. This document describes and highlights the unique advantages of OceanStor Micro in terms of the product positioning, hardware and software architecture, and features. The OceanStor Micro series products include: OceanStor Micro 1300 OceanStor Micro 1500 For details about the product models and specifications, see the product specifications list. In the past few years, the explosive growth of data and mining of data values have led to the innovation of IT systems, especially storage devices. The main storage media changes from hard disk drives (HDDs) to solid-state drives (SSDs).\nThe main storage protocol changes from SAS to NVMe. As NVMe over Fabrics (NVMe-oF) gradually matures, the network latency can be reduced to 50 s. The preceding factors facilitate the decoupling of storage and compute in data centers. Vendors, such as Western Digital and Foxconn, have released high-speed Ethernet Bunch Of Flash (EBOF) enclosures based on NVMe over RoCE. Startup companies, such as Fungible and VAST Data, have launched end-to-end solutions based on NVMe over RoCE EBOF enclosures with data processing units (DPUs). Traditional vendors, such as NVIDIA and Intel, have also released their own diskless solutions based on DPUs or infrastructure processing units (IPUs). Data centers with a storage-compute decoupling architecture have become a trend. Therefore, Huawei launches OceanStor Micro series products. Robust reliability Mainstream NVMe servers do not support hardware-based RAID. As a result, disk faults affect upper-layer services. The OceanStor Micro EBOF enclosure supports RAID 5, RAID 6, and RAID TP with a maximum of 23+2 stripe verification, ensuring that disk faults do not affect upper-layer services. The technology reduces the number of data copies for upper layer applications from three copies to two copies, improving space utilization. In addition, it uses the built-in data reduction capability to reduce customer investment. Superb performance Two modes are provided: pass-through and pooling. In pooling mode, the system supports large-ratio EC and data reduction, achieving 1 million read IOPS and 40 GB/s read bandwidth. In pass-through mode, the system directly maps disks to hosts, achieving over 5 million read IOPS.\nThis section describes the deployment modes and highlights of NAS HyperMetro in active-active mode. The NAS HyperMetro solution in active-active mode uses the HyperMetro feature of OceanStor Dorado. HyperMetro is Huawei's active-active storage solution that enables two storage systems to process services simultaneously, establishing a mutual backup relationship between them. In the event of a device fault or DC failure, the other functioning DC automatically takes over services. This ensures robust reliability, enhanced service continuity, and high storage resource utilization. Single-DC deployment In this mode, the active-active storage systems are deployed in two equipment rooms in the same campus. Hosts communicate with storage systems through a switched fabric (IP switches for NAS file systems). HyperMetro replication links are deployed between the storage systems to ensure continuous operation of active-active services. Cross-DC deployment In this mode, the active-active storage systems are deployed in two DCs in the same city or in two cities located in close proximity. The distance between the two DCs is within 300 km. Both of the DCs can handle service requests concurrently, thereby accelerating service response and improving resource utilization. If one DC fails, its services are automatically switched to the other DC. In cross-DC deployment scenarios involving long-distance transmission, dense wavelength division multiplexing (DWDM) devices must be used to ensure a short transmission latency. In addition, HyperMetro replication links must be deployed between the active-active storage systems for data synchronization.\nWhat is a Database? A database is a collection of data typically stored electronically in a computer system and controlled by a database management system (DBMS). The data, the DBMS, and the applications associated with them are referred to as a database system (or just database). The words database management system and database are often used interchangeably, but technically theyre not the same. To distinguish them consider the case of a social media app that stores different information about its users such as messages, photos, comments, etc. The database stores this big collection of data, but thats pretty much what it does. If you want to edit, update, or delete data, you need a DBMS that does the talking for you. Some of the most popular DBMS are Oracle, MySQL, SQL Server, and PostgreSQL. Image by author made on Canva Again, people often refer to both the DBMS and DB simply as database but now you know how this actually works. Another way to think of a database is as a big spreadsheet with many rows and columns. Thats a good comparison, but a database goes beyond that. Both the database and spreadsheet are good for storing information, but they mainly differ in the following aspects: How the data is stored and manipulated: Databases allow complex data manipulation, while spreadsheets aren't meant for users who need much data manipulation.\nIntelligent Multi-core Technology The new-gen OceanStor hybrid flash storage uses high-performance processors. The controller contains more CPUs and cores than any other controller in the industry. For the symmetric multiprocessor (SMP), the biggest challenge is that the system performance can grow linearly as the number of CPUs increases. The SMP system has the following two key problems: The more CPUs, the more overhead of communication between CPUs, and the more memory access across CPUs. The more the number of cores, the more likely that conflicts may be caused by program mutual exclusion, and the longer time for conflict handling. The new-gen OceanStor hybrid flash storage uses the intelligent multi-core technology to allow performance to increase linearly with the number of CPUs. The key optimization technologies for the key problems are as follows: The CPU grouping and distribution technologies are used among CPUs. Each I/O is scheduled within one CPU during the process of being delivered to the controllers by the multipathing software and arriving at the back-end disk enclosures. In addition, memory is allocated on the current memory channel, minimizing the communication overhead between CPUs. CPUs are grouped based on service attributes. The front-end, back-end, and node interconnection networks are scheduled in separate CPU core groups. The I/O stack for processing a task is scheduled only within the CPU core group, which effectively controls the conflict impact and processing overhead, improves the scheduling efficiency, and accelerates the processing of the I/O stack.\nThis sharing describes how to add disks to the disk drawer of an enclosure. Step 1 : Insert disks. Wear an ESD wrist strap, ESD gloves, and ESD clothes. Take the disks out of their ESD bags. Insert disks into empty disk trays in sequence. Push the release buttons on the disk drawer in the arrow directions to eject the ejector levers. Hold the ejector levers and pull out the disk drawer as far as it will go. Locate the disk module to be replaced, and pull the spring plate upwards to open the cover of the disk tray. Place the replacement disk module into the disk tray in the disk drawer, and close the cover of the disk tray. Pull the unlocking latches on both sides of the disk drawer, and horizontally insert the disk drawer into the controller enclosure. Push the disk drawer into the controller enclosure until it cannot be pushed further, lower the ejector levers, and wait until the disks are automatically powered on. Step 2 : View indicators on the disk drawer and determine whether the disk is successfully inserted. If the disk is running properly: The Active indicator is steady green. The Fault indicator is off. If the disk is not running properly: The Active indicator is off. The Fault indicator is steady yellow. If the Active indicator is off whereas the Fault indicator is steady yellow, the disk may be inserted incorrectly. In this case, remove the disk, wait approximately 1 minute, and reinsert the disk.\nThe new-gen OceanStor hybrid flash storage uses a multi-dimensional intelligent global cold and hot data sensing and data collaboration algorithm, and provides a unified performance layer to sense cold and hot data in all scenarios, all ranges, and all models, maximizing the performance of the hybrid flash storage system. Different from the traditional hot data statistics algorithm that uses a fixed data structure, the hot data statistics algorithm of the new-gen OceanStor hybrid flash storage uses a multi-granularity learning-oriented statistics structure. It can adaptively adjust to the data structure based on the model changes and better adapt to the conversion between large and small I/O models and the change between sequential and random models, maximizing efficiency in various service scenarios. Traditional cold and hot data sensing algorithms usually consider only historical statistics. The new-gen OceanStor hybrid flash storage uses multi-dimensional feature analysis to identify global hot data. In addition to traditional historical statistics, it also incorporates the time series forecasting algorithm of machine learning. In scenarios where data changes between hot and cold states, the time series forecasting and historical statistics are combined to determine whether data is stored in the performance layer or capacity layer. The cold and hot data sensing technology that integrates multi-dimensional features ensures that all hot data can be effectively identified and accelerated. Different from the traditional data prefetching that considers only sequential flows, the new-gen OceanStor hybrid flash storage uses a prefetch policy that supports interval sequential flows and I/O-associated flows as well as traditional sequential flows.\nChallenges to Traditional Data Centers As the core infrastructure of the digital economy, data centers (DCs) are undergoing digital transformation with the rapid development of new generation information technologies. In the digital transformation of enterprise services, traditional DCs have exposed many disadvantages, such as independent resources, difficult expansion, and complex operation and maintenance (O&M). Traditional DCs are faced with the following challenges: High investment cost In traditional solutions, compute, storage, and network resources are separated, and separate storage devices and switches are required, resulting in high investment cost. Sluggish service rollout The traditional service rollout process includes hardware model selection, procurement from multiple vendors, split delivery, installation, and joint commissioning, complicating and slowing down the rollout. Complex O&M Storage, compute, and network devices provided by different vendors are independently configured and managed, requiring a lot of maintenance manpower. Simplified Management Integrates storage, switching, and compute resources without network installation and design. Supports one-click initialization without system installation, achieving quick service rollout. Provides a unified management GUI and supports automatic fault locating, simplifying routine O&M. Supports hierarchical management and remote application provisioning. High Reliability The active-active storage architecture provides a reliable data storage platform. The diskless compute design eliminates the need for system disks and RAID controller cards, enabling quick recovery from OS faults. The full hardware redundancy design ensures that services are not affected if a single fan or a single expansion module is faulty and services can be quickly and automatically recovered if a single node is faulty.\nOceanStor 2910 is a new IT infrastructure platform based on hyper-converged technologies. It integrates compute, storage, and network resources into one device and implements visualized IT resource management on a unified management platform to provide compute, storage, and network services for users. OceanStor 2910 is an IT infrastructure platform oriented for enterprise information integration. It implements in-depth vertical integration and optimization of hardware and software, rapid deployment, and unified management, improving the operation efficiency of core services and reducing the overall investment cost. Typical Applications Integrated IT Infrastructure for Small- and Medium-sized IC Enterprises Integrated circuit (IC) design depends on IT technologies and services, including electronics design automation (EDA) tools, and infrastructure such as compute, storage, and network resources. As the chip scale and design complexity increase, the chip size decreases, and the EDA tools develop, traditional IT architectures are facing problems such as unavailable storage resource sharing, management, and use; poor reliability when there is no enterprise-level RAID; and lack of protection features, such as snapshots and all-in-one backup. Enterprise Office File Management Currently, files in enterprises are managed based on services. Files of different services are separated. File sharing is implemented through USB flash drives or emails, which is inefficient and insecure. Server disks and removable disks are used to store data, resulting in low reliability. There are no anti-virus, anti-tamper, and disaster recovery solutions. In combination with file management software, implements file sharing collaboration and file search. OceanStor 2910 provides enterprise-level storage features to realize file backup, data encryption, and file antivirus.\nWhat is the meaning of Clock rate in CPU? What is CPU Clock rate? The term is also known as \" clock rate \", \" computer frequency\" and \"CPU frequency\" . The unit of measurement of this index in the computer is GHz (abbreviated GHz), which represents billions of pulses per second. The clock rate of personal computers is a characteristic of the performance as well as the speed of data processing by the CPU. The higher the value, the better the system performance. This parameter can be considered as a suitable factor for CPU selection if other indicators are the same; However, considering a combination of indicators such as clock speed, number of instructions per cycle processed by the CPU (number of instructions per hour / hour, or IPC for short), as well as whether the CPU is multi-core, is entirely up to the decision maker. It is more suitable for determining the overall performance of the computer processor. Therefore, to compare the performance of two processors, it is better not to compare their processing speed or operating frequency, but instead use the amount of power and workload that the processor is able to complete in each of its work cycles. Therefore, doing more work in fewer cycles is better than doing less work in more cycles. On the other hand, the lower the processing speed of the processor information and the higher the power and volume of analytical information, the more efficient the processor will be.\nThis sharing describes the networking rules between a controller enclosure and disk enclosures and between disk enclosures. Introduction to Disk Enclosures A controller enclosure supports 2 U disk enclosures and 4 U disk enclosures, the two types of disk enclosures comply with the same networking rules. 2 U disk enclosure: 4 U disk enclosure: Networking Rules Before network planning, note the following: Connect a controller enclosure to a maximum of four disk enclosures. Connect storage node A of a controller enclosure to expansion module A of a disk enclosure and storage node B of the controller enclosure to expansion module B of the disk enclosure. Connect the P3 port on the storage node of the controller enclosure to the P0 port on the expansion module of the disk enclosure. Connect the P1 port on the expansion module of the disk enclosure to the P0 port on the expansion module of a lower-level disk enclosure. Networking Diagram To ensure network reliability, connect disk enclosures in a loop in forward redundancy mode so that two independent links are set up between the disk enclosures for mutual backup. The methods for connecting to 2 U and 4 U disk enclosures are the same. The following uses 2 U disk enclosures as an example to describe the network connections between a controller enclosure and different numbers of disk enclosures.\nIf alarm information fails to be reported, users cannot view it on the management console. Alarm information fails to be reported and is unavailable on the management console. Possible cause 1: The CM process is abnormal. Possible cause 2: Communication is abnormal between the management console and the CM process. Possible cause 1: The CM fails. a. Use PuTTY to log in to the node where the FusionStorage management IP address resides as the omuser user. Run the su command and enter root password to switch to the root user. b. Run ps -ef | grep snas_cm to check whether the CM process is normal (if there are records, the CM process is normal; otherwise, the CM process is abnormal). -If yes, go to cause 2 . -If no, go to c . c. Restart the CM. -Run ps -ef | grep cm to check the CM process number. The CM process number includes snas_cm. -Run kill -9 to end the CM process. The daemon will automatically start the CM. d. Restart the management console. Run cd / opt / deviceManager / bin to go to the / bin directory.Then, run sh restart.sh . e. Check whether alarm information can be reported. - If yes, no further operations are required. - If no, go to cause 2 . Possible cause 2: Communication is abnormal between the management console and the CM process. a. Log in to the node. b. Run cd / var / ISM / deviceManager / logs to go to the directory.\nHuawei has been restricted in several fields due to the U.S. ban. However, the company never gave up and always came out with extraordinary solutions. The latest input reveals that Huawei is using OceanStor storage chips for its ARM processors-based hardware. According to the information, Huawei has recently broadcast the benefits of the OceanStor Dorado V6 storage chips at the Palais des Congres event in Paris. The company reports that these chipsets facilitate file, block, and object access at a much lower price than the U.S. industries. So far, the Chinese manufacturer has been using the x86 processors from Intel and AMD. However, the U.S. prohibitions forced the company to pull out its hands from various foreign sectors. As a result, the tech maker started searching for new ways that can make it independent. Consequently, OceanStor chipsets have proved a perfect component for Huawei ARM processors hardware. Unlike the traditional x86 processors, these chips dont consume unnecessary energy and are 30% more efficient in energy savings. Alongside this, OceanStor Dorado V6 is available in numerous variants for every kind of product. It ranges from entry-level to superior performance grade and multi-petabyte capacity. Further, it also supports 32-bit and 62-bit drives and controllers. The director of Huawei Storage products Ludovice Nicoleau said that the company offered the storage chipset in the year 2002. At that time, these chipsets were mainly designed for the telcos systems. Thereafter, the company gradually shifted them to operating systems. As mentioned, Huawei has been relying upon the x86 processors for standard storage.\nHello, everyone! Today, I would like to share on the topic of logical unit number masking. The terminology logical unit number (LUN) was first used to describe the object that runs I/Os on a SCSI target. A single LUN is usual for a single SCSI device, but other devices, like as tape libraries, may contain many LUNs. The array makes virtual disks available to the servers with storage arrays. LUNs are used to identify these virtual disks. The very same storage device or LUN can be seen by many hosts. This capacity has the potential to be a concern, both in terms of practicality and security. LUN masking is another way to protect storage devices from hosts that seek to take over resources that have already been assigned. With LUNs, every storage device makes its resources available to the hosts. Every partition in the storage server, for instance, has its own LUN. If the host server needs access to the storage, it must first seek access to the storage device's LUN. LUN masking is used to limit who has accessibility to the LUNs. Access requests from various hosts are accepted or rejected by the storage device. With the storage device control application, the user can designate which hosts can access which LUN. When a host accesses a specific LUN, the storage device looks up that LUN in its access list. The storage device also controls whether or not the LUN may be accessed.\nHello, everyone! Today, I would like to share on the benefits and drawbacks of edge storage. The cost of installing storage on edge devices increases. The reasoning is that keeping (and processing) data domestically is less expensive or time-consuming than sending it to a datacenter or the cloud. The most important reason driving storage to the edge is the price of bandwidth. According to industry experts, bandwidth costs 4 times as much per gb as storage, based on Cloud Platforms pricing. Costs are saved by removing superfluous data before it is posted. The difficulties of reaching the edge with traditional networking technologies, as well as the stability and latency constraints that can plague networks over long distances, are the other factors. Latency and bandwidth are two major challenges or should I say motivators for any edge technology. That is, the connection's speed and capacity back to the core. If you're attempting to reduce the amount of data you send while reacting quicker, you'll eventually need to do more at the edge. The problem is that edge storage is more difficult to manage and sustain due to its position. Increasing capacity will necessitate a hardware replacement, and offloading data will rely on such low-bandwidth connections to distant places. Despite this, providers and customers are seeking for ways to increase edge storage capacity, notably by dumping data across the network or physically. In comparison to cloud storage, edge storage does not scale as efficiently. To prevent the system from running out of resources, accurate capacity management is required.\nThis doc describes how to replace a disk drawer. 1. Wear an ESD wrist strap and a pair of protective gloves. 2. Take the replacement disk drawer out of its ESD bag and place it on an ESD table. 3. Push the release sliders on the disk drawer in the arrow directions to eject the ejector levers. 4. Hold the ejector levers and pull out the disk drawer as far as it will go. 5. Remove all disk modules from the disk drawer and install them in the same positions in the replacement disk drawer. 6. Lift both sides of the crimping frame and remove it. 7. Open the cable tie and pull out the cable connectors connected to the disk drawer. 9. Loosen the screws on the cable manager counterclockwise and lift the cable manager. 10. Pull the unlocking latches on both sides of the disk drawer, pull the disk drawer out of the guide rails horizontally, and place the disk drawer on the ESD table. 11. Push the replacement disk drawer into the guide rails until it cannot move forward. 12. Align the screws on the cable manager and tighten the screws clockwise. 13. Connect the cables in the cable manager to the cable ports in the disk drawer, and then close the cable tie. 14. Pull the unlocking latches on both sides of the disk drawer, and horizontally insert the disk drawer into the controller enclosure. 15.\nHello, everyone! Do you know how to calculate RAID column Can you describe it simply? Hello, friend! For disk redundancy, the number of RAID columns (N+M) is calculated as follows: Note: In 6.1.2 and later versions, RAID 5 cannot be configured on DeviceManager. To configure RAID 5, run the create storage_pool name=? disk_list=? raid_level=RAID5 max_raid_member_number=? command to create a RAID 5 storage pool. The optional parameter max_raid_member_number specifies the maximum number of member disks in a RAID group. For details, refer to the command reference specific to your product model and version. RAID 6 or RAID-TP with higher reliability is recommended. For OceanStor Dorado 3000 V6, the value of max_raid_member_number can be 15 or 25. If the value of max_raid_member_number is not specified, the maximum number of member disks in a RAID group is 15 by default. If the value of max_raid_member_number is 15 or not specified: Number of RAID columns = Min (Number of member disks in the storage pool Number of reserved columns, 15) If the value of max_raid_member_number is 25: Number of RAID columns = Min (Number of member disks in the storage pool Number of reserved columns, 25) For OceanStor Dorado 5000 V6/OceanStor Dorado 6000 V6/OceanStor Dorado 8000 V6/OceanStor Dorado 18000 V6, the value of max_raid_member_number can be 12 or 25. If the value of max_raid_member_number is not specified, the maximum number of member disks in a RAID group is 12 by default.\nIf the value of max_raid_member_number is 12 or not specified: Number of RAID columns = Min (Number of member disks in the storage pool Number of reserved columns, 12) If the value of max_raid_member_number is 25: Number of RAID columns = Min (Number of member disks in the storage pool Number of reserved columns, 25) Number of RAID columns = Min (Number of member disks in a storage pool Number of reserved columns, 25) Wherein, Number of reserved columns = Max (1, Number of hot spare disks) Note: The methods for calculating the number of RAID columns in the following three scenarios are slightly different: On OceanStor Dorado 5000 V6/OceanStor Dorado 6000 V6, a storage pool spans over multiple controller enclosures and contains disks from smart disk enclosures. On OceanStor Dorado 8000 V6/OceanStor Dorado 18000 V6, eight controllers are configured without back-end full interconnection, and a storage pool spans over multiple controller enclosures. A storage pool with a single controller enclosure is expanded to span over multiple controller enclosures. Number of member disks in a storage pool in the preceding formulas refers to the number of disks owned by a single controller enclosure for a storage pool.\nMultimedia Introduction to SmartDedupe and SmartCompression for Block SmartDedupe Step 1: The storage system divides newly-written data into blocks. The Application Request Size set on the LUN is the block size. The storage system uses a similar fingerprint algorithm to calculate the SFPs and GFPs of the new data blocks. The storage system writes the data blocks to disks and records data blocks' fingerprint and location information in the opportunity table. Step 2: The storage system periodically checks whether SFPs exist in the opportunity table. If yes, go to 2. If no, continue the periodic check. The storage system checks whether similar data blocks are the same based on byte-by-byte comparison. If yes, the storage system considers the new data block redundant and deletes it. Then, the storage system points the fingerprint and storage location of the new data block to that of the existing one in the fingerprint table. If no, the storage system performs combining compression on the new data block, records its fingerprint in the fingerprint table, updates the fingerprint to the metadata of the data block, and reclaims the storage space of the data block. The storage systems support inline compression. If SmartCompression is enabled for a LUN when it is created, the storage system will compress all the data written to the LUN. Multimedia Introduction to SmartDedupe and SmartCompression for File Step 1: The storage system divides newly-written data into blocks. The Application Request Size set on the file system is the block size.\ni. definition of the snapshot used capacity shows in this picture is the Snapshots exclusively occupy capacity. ii. Lets watch the snapshot and source file system list below, the difference between the source file system and the snapshot 1 is data A and B(Correspondence to the data E and G on source file system). and the data difference with snapshot 2 is B(Correspondence to the data G on source file system), then the total data protection capacity is A + B. In other words, the snapshot 1 protect A and B, and snapshot 2 protect B(since the data E C D has no changed) iii. for the snapshot used capacity, the difference between snapshot1 and snapshot 2 is A. Then the data only protect by the snapshot 1 is A, which shows the snapshot used capacity(Snapshots exclusively occupy capacity) is the size of data A. And the data only protected by snapshot2 is nothing(snapshot 2 protect B, however, snapshot 1 protect A and B), thus the size of the snapshot used capacity will be 0 for snapshot2. If u sum the snapshot used capacity(Snapshots exclusively occupy capacity) of all the snapshot, it will not equal to the total capacity for data protection because the data B(which is the data that protect by both snap1 and snap2) is not Include in the snapshot used capacity(Snapshots exclusively occupy capacity). iv. the example shows above is for only 2 snapshot in a shot time.\nAfter a node running the DeviceManager (a DeviceManager node for short) fails, no normal DeviceManager node automatically takes over the services on the faulty node, disabling users from accessing the DeviceManager. When you log in to DeviceManager, a message is displayed stating \"The communication is abnormal or the system is busy. Try again later.\" After a DeviceManager node fails, no normal DeviceManager node automatically takes over the services on the faulty node, and the DeviceManager page is inaccessible, as shown in figure below. Possible cause 1: The management IP address is switching to another node due to a node restart or network exception. Possible cause 2: The DeviceManager software fails. Possible cause 3: The Cluster Management (CM) fails. Possible cause 1: The management IP address is switching to another node due to a node restart or network exception. Wait about 1 minute until the management IP address finishes switching to another node and then try to log in again. If yes, no further operations are required. If no, go to . Possible cause 2: The DeviceManager software fails. a. Use PuTTY to log in as user omuser to the FusionStorage management IP address. Run the su command and input the password of user root as prompted to switch to user root .DeviceManager b. Run ps -ef | grep ibase to check whether the DeviceManager software process is normal (if there are records, the process is normal; otherwise, the process is abnormal). - If yes, go to . - If no, go to .\nHello everyone In this post, you can learn: 1. Remotely Powering On or Off a Storage Node on the iBMC Management Page 2. Powering On or Off a Storage Node by Pressing the Power Button on the Node 3. Powering Off a Node Using the Operating System of the Node 4. Powering On a Cluster 5. Powering Off a Cluster on DeviceManager Perform operations on the iBMC management page to power on or off a single storage node. Procedure 1. Log in to the iBMC management page of the node. 2. On the menu bar, choose Power . On the Power Control page, perform power-on and power-off operations as required. You can also choose Information Overview in Information on the home page, and then select Power On or Forced Power Off in Virtual Buttons as required. This mode is displayed on the home page after login. NOTICE Ensure that no service risk exists before you power off, forcibly power off, forcibly restart, forcibly power off, or power on the server. Forcibly powering off the server may damage the program or data that is not saved. Exercise caution when performing this operation based on actual operating systems. For details about the power-on and power-off operations, see the online help in the upper right corner of the iBMC management page. 3. In the displayed dialog box, click OK . After the operation is complete, a message is displayed on the Power Control page, indicating that the operation is successful.\nYou can press the power button on a device to power on or off a node. Procedure Power on a node by pressing the power button. NOTICE Turn on the power buttons of the devices one by one to prevent damage to the power supply devices caused by excessive transient current. Press the power button on the front panel to power on the node. For details about the positions of the power buttons on each storage node, see the front panel of the storage node in . Power off the node by pressing the power button or removing the power plug of the node. During maintenance, powering off a single node by removing the power plug of the node may cause data reliability risks. Contact technical support before performing this operation. Hold down the power button for 6 seconds to power off the storage node. If the time is shorter than 6 seconds, the node will not be powered off. Log in to the operating system of a node and run related commands to power off the storage node. Prerequisites If VBS is deployed on the node to be powered off and the node is accessed using the SCSI protocol, stop services on the volumes mapped from the VBS and then power off the node. Otherwise, the service data cache deployed on the node may be lost. For example, if a file system is mounted to a node, you need to unmount the file system before powering off the node. Procedure 1.\nLog in to a storage node. You can log in to a storage node through the physical KVM in the equipment room. For details, see . For details about how to remotely log in to a storage node using PuTTY, see . 2. You can run the following commands to power on or off the node: Run the reboot command to restart the node. Run the poweroff command to power off the node. Follow-up Procedure You can press the power button on a storage node or remotely power on the node on the iBMC management page. This operation enables you to power on a cluster. Context Services running on the storage system have been stopped. Procedure 1. Power on nodes in the cluster by referring to Remotely Powering On or Off a Storage Node on the iBMC Management Page or Powering On or Off a Storage Node by Pressing the Power Button on the Node. 2. Repeat 1 to power on all the other nodes in the cluster. 3. After all the nodes in the cluster are powered on, power on the cluster by referring to . On DeviceManager, you can power off all nodes in a cluster. Procedure 1. Log in to DeviceManager. 2. Click in the upper right corner and choose Power Off Cluster . 3. Carefully read information in the Danger dialog box and select I have read and understand the consequences associated with performing this operation .\nThe storage system uses the ROW technique to implement HyperSnap . The detailed implementation is as follows: Creating a snapshot As soon as a user creates a snapshot, the snapshot is immediately activated and the system generates a consistent data copy of the source volume. In addition, the system copies the source volume's pointer to the snapshot. The snapshot's pointer points to the locations that store the source volume data. That is, the snapshot data is the same as the source volume data. shows the snapshot data distribution. Figure 1 Snapshot data distribution (initial creation) The generated snapshot volume is a read-only snapshot. Users can access the snapshot using the SCSI or iSCSI interface. Writing data to the source volume When an application server attempts to write data to the source volume that has a snapshot, the storage system uses ROW to save the new data (D) to a new location in a storage pool and directs the source volume's pointer to the new location. The snapshot's pointer still points to the locations that store the original data (C). In this way, the snapshot preserves the source volume data at the snapshot creation time. shows the snapshot data distribution. Figure 2 Snapshot data distribution (after new data is written to the source volume) In addition to quickly generating point-in-time consistent copies of source volumes, HyperSnap is able to quickly recover source volumes.\nIf data on a source volume suffers mis-deletion, corruption, or virus attacks, you can perform a snapshot rollback to restore the source volume to the data state preserved at the point in time when the snapshot was created, thereby minimizing data loss. illustrates how to perform a snapshot rollback. Figure 3 Snapshot rollback In addition to creating read-only snapshot copies for volumes, the storage system allows you to create clones for snapshots. Such clones are writable mirrors. If a source volume needs to be tested or analyzed, which results in data changes, you can create a snapshot for the production data and create a clone for the snapshot. Then, you can use the clone to serve test and analysis purposes, thereby preventing the production data from being affected. As shown in , a clone shares the source snapshot data. Figure 4 Clone data distribution (initial creation) In the event of writing data to a clone volume, the system allocates new storage space to the data, as shown in . Figure 5 Clone data distribution (after writing data) To use copies for quick data backup and recovery and to improve storage space utilization, the storage system allows you to create a linked clone, that is, create a snapshot for a clone and then create another clone for that snapshot, as shown in .\n[Issue] There is an alarm about bit errors on one port of storage. 2019-06-07 15:09:36 0xF01080015 Fault Major Unrecovered None FC host port (controller enclosure CTE0, -- controller A, ) has too many bit errors. The system performance may be affected. [Principle] As shown in the image above, the storage port detects the bit errors, which must be due to the poor quality of the hardware link. Its possible causes are among the SFPs of storage/switch/host, cables. [Analysis and Suggestions] Step1. Collect storage logs. Step2. Open the file(\\log_controller_0_MAIN\\Config\\config.txt). and then search the key words port ID, like .H2. and ensure if you find the information of right port. Please dont replace the SFP directly just judging by the health status. Because it cannt be usually solved by this way. Step3. We need to pay more attention to the Value of RxPowerReal and TxPowerReal. Situation1: the TxPowerReal value of fault port is lower than 300uW. Please do not hesitate to replace the SFP of storage directly. Situation2: the RxPowerReal value of fault port is lower than 300uW. The root cause must be among the SFP of storage, the SFP of switch, the cable between storage and switch. Solutions: If the fault port is connected to switches from Huawei, please collect switch logs and contact R&D engineers. If switch is not from Huawei, we need a crossing test.\nExcessively used MySQL Storage Engines There are many popular MySQL Storage Engines used and they are fine for the user's goals but here few of them will be described below. MyISAM was the MySQL default storage engine prior to version 5.5.1, and it still remains a popular choice due to its simplicity and speed. Most specialists consider it to be the best option for beginners who only start mastering MySQL. Unfortunately, one of the primary factors ensuring simplicity is the absence of foreign keys support. As a result, your work wont involve complicated configurations, but your options are limited. Also, this engine requires less disk space and thus is suitable in cases of limited disk space. It provides extreme speed with the SELECT and INSERT statements, which is a valuable advantage, but it can be slow when dealing with the DELETE and UPDATE statements, as it does not support transactions with rollbacks and table-level locking. If you consider MyISAM for your project, note that it applies best to the data warehousing applications or web apps that wont use transactions. Though MyISAM is still used by many applications, it was not surprising that it stopped being the default one and was replaced by InnoDB. If you work on applications based on MySQL now, InnoDB will most likely be your storage engine. It ensures all options that a database would require, and is the most popular choice for the absolute majority of developers. InnoDB supports transactions and foreign keys constraints.\nThus, it can check the INSERT, UPDATE, and DELETE statements consistency much better. It is less speedy than MyISAM, but it is less vulnerable to crushes either. Besides, it offers such an essential advantage as row-level locking, ensuring a more efficient multi-user performance. In addition, in the case with multiple storage engines configured, it would be better to disable the unused storage engines to improve server performance. For example, you have ten configured storage engines but use only one of them. In this case, nine storage engines stand idle and only consume server resources with bringing no benefit. Therefore, MyISAM and InnoDB are the most widely used storage engines for MySQL. However, this RDBMS supports more than a dozen of them. Its worth paying attention to one more engine that is called Federated. Although not default, Federated is a well-known storage engine for MySQL. Its peculiarity is that it allows accessing the data from a remote MySQL database. At the same time, it does not need replication or cluster technologies. The key is the local Federated table. When a query addresses that table, it applies automatically to the remote federated table. The data arent stored locally. This technique is excellent when developers need to work with several databases in MySQL and remote MySQL servers. Remote servers store the data, while the local server only points to them with the connection string. The efficiency of this action does not depend on the storage engine type on the remote server. However, Federated has a dangerous drawback.\nRemote replication: The requirements for storage system stability are increasing. Disasters may cause irrecoverable damage to the production system. Therefore, data access continuity, recoverability, and high availability must be ensured. HyperMetro: Traditional DR services cannot be automatically switched over. Remote replication: implements remote backup and recovery, provides continuous service support, and implements disaster recovery. HyperMetro: achieves high-level data reliability and service continuity, and improves resource utilization of storage systems. Synchronous remote replication: B ased on initial synchronization and dual-write. Asynchronous remote replication: It is implemented based on initial synchronization and snapshots. The primary and secondary snapshots start at the beginning of a replication period and stop at the end of the period. HyperMetro: Data consistency between two storage devices is implemented based on initial synchronization and dual write, and automatic service switchover is implemented based on the quorum server. Synchronous remote replication: Manual synchronization is required after a pair is created. In normal cases, write I/Os are delivered only to the primary LUN. Data consistency between the two LUNs is ensured through dual-write. Asynchronous remote replication: After a pair is created, data is automatically synchronized. In normal cases, write I/Os are delivered only to the primary LUN. Data is periodically incrementally replicated to ensure data consistency between the two LUNs. HyperMetro: After a pair is created to synchronize data, dual-write is used to ensure data consistency between the two ends. APs in active-active mode deliver services at the preferred site, and active-active mode deliver services at the two sites at the same time.\nRemote replication: You need to manually set the secondary LUN to the readable and writable state and map the secondary LUN to the host. HyperMetro: Automatic switchover based on quorum servers. Remote replication: After the link recovers, the primary/secondary switchover and re-synchronization (one synchronous remote replication and multiple asynchronous remote replication) are performed to ensure that data on the primary LUN is recovered. HyperMetro: The latest data is synchronized from one node to the other node. Synchronous remote replication: applies to scenarios where the primary and secondary sites are close to each other, and intra-city DR backup is implemented. Asynchronous remote replication: applies to scenarios where the primary site is far away from the secondary site or the network bandwidth is limited. HyperMetro: applies to core services of users and requires that services are not affected after a storage fault occurs. Remote replication: Synchronous replication or asynchronous replication can be configured based on performance requirements. Asynchronous remote replication implements link compression. HyperMetro: ensures real-time data consistency between two storage systems. If one storage system is faulty, services are not interrupted. Remote replication: Services need to be manually started. Manual synchronization is required after the remote relationship is established. During the primary/secondary switchover, ensure that the secondary LUN is readable and writable (after all, services must be carried immediately after the switchover). Storage products from other vendors are not supported. Synchronous replication cannot exceed 300 km. HyperMetro: A long latency may occur during dual-write. The two sites cannot be too far away from each other.\nDear Members, Since , various measures are put in place to protect against them, including: Prevention before attack : Vulnerabilities are quickly identified and rectified, and ransomware is analyzed and quickly identified. Interception during attack : Known ransomware is accurately detected and removed, and unknown attacks are identified immediately. Tracing after attack : Paths are analyzed for timely blockage, and features are saved to libraries for future prevention. Many security vendors would advise enterprises to enhance their security awareness, and to periodically back up their important data to minimize exposure to risks. However, despite having so many measures in place, storage is an absolutely essential part of ransomware protection. Ransomware is hard to prevent and hard to fend off Ransomware is different from common computer viruses. If common viruses were hoaxes, ransomware would be a well-planned conspiracy. Behind targeted ransomware is a profit-oriented criminal who wont stop until they reach their goal. Ransomware has the following features: Many different camouflage methods: The camouflaged malware can gain access to the system through storage media, phishing emails, website Trojans, social networks, malicious insiders, and zero-day vulnerabilities (where security vulnerabilities that have not been resolved yet), and it is not possible to guarantee successful interception. Prolonged latency: Ransomware is evolving and becoming increasingly complex. Attackers aiming for big returns have been known to invest a lot of time and money into researching and carefully planning their attacks for weeks or even months to maximize their chances of success.\nWhy most Businesses prefer MySQL over other Databases? PROVISION OF COMPREHENSIVE SUPPORT FOR BUSINESSES: 24/7 UPTIME: RELIABLE CLOUD MANAGEMENT AND SECURITY: Cloud security and management has been a big issue since cloud computing started gaining prominence. Without robust security, your business data might be compromised or your employees might become venerable to cybercriminals. MySQL will help to improve your business data security. Since professionals experienced in MySQL dont work full-time, it is expected that after the end of the contract, they will hand over the project that they were working on to your employees. While the consultant will work on the project, it is expected that your employees will queue behind the consultants to learn their skill and from their experience. For instance, if the project your business is embarking on is about building a cloud management system where you can store your business data, you can actually hire a competent IT consultant to help you complete this critical project; otherwise, your in-house engineers and information technology specialist may not be able to achieve a complete job if the job is left for them alone to do. IMPROVED BUSINESS PERFORMANCE: Another benefit of using MySQL is that it guarantees improved business performance and productivity. MySQL has a storage engine framework that users can use to facilitate their administration of their database without any performance error. With MySQL, you will enjoy on-demand scalability. This will enable you to facilitate the deeper management of your database, and also track every data management and changes in your business data.\nDue to the volatility and limited size of a computers main memory, most computers have additional memory devices called mass storage (or secondary storage) systems, including magnetic disks, CDs, DVDs, magnetic tapes, flash drives, and solid-state drives. A major disadvantage of magnetic and optical mass storage systems is that they typically require mechanical motion and therefore require significantly more time to store and retrieve data than a machines main memory, where all activities are performed electronically. Moreover, storage systems with moving parts are more prone to mechanical failures than solid state systems. While flash drives and solid-state disks do not require moving parts, other electronic considerations can limit their speed or longevity relative to main memory. For years, magnetic technology has dominated the mass storage arena. The most common example in use today is the magnetic disk or hard disk drive (HDD) , in which a thin, spinning disk with magnetic coating is used to hold data. Read/write heads are placed above and/or below the disk so that as the disk spins, each head traverses a circle, called a track. By repositioning the read/write heads, different concentric tracks can be accessed. In many cases, a disk storage system consists of several disks mounted on a common spindle, one on top of the other, with enough space for the read/ write heads to slip between the platters. The bandwidth of a system is a measure of bit ratethe amount of data (measured in bits) that can be sent in a fixed amount of time.\nThe latency of a system is the time elapsed between the transmission and the receipt of a request. Another class of mass storage systems applies optical technology. An example is the compact disk (CD) . These disks are 12 centimeters (approximately 5 inches) in diameter and consist of reflective material covered with a clear protective coating. Information is recorded on them by creating variations in their reflective surfaces. This information can then be retrieved by means of a laser that detects irregularities on the reflective surface of the CD as it spins. A common property of mass storage systems based on magnetic or optic technology is that physical motion, such as spinning disks, moving read/write heads, and aiming laser beams, is required to store and retrieve data. This means that data storage and retrieval is slow compared to the speed of electronic circuitry. Flash memory technology has the potential of alleviating this drawback. In a flash memory system, bits are stored by sending electronic signals directly to the storage medium where they cause electrons to be trapped in tiny chambers of silicon dioxide, thus altering the characteristics of small electronic circuits. Flash memory devices called flash drives , with capacities of hundreds of GBs, are available for general mass storage applications. These units are packaged in ever smaller plastic cases with a removable cap on one end to protect the units electrical connector when the drive is offline.\nHello everybody! This post is about a solution to the failure of upgrading the hard disk firmware. Please see more details below. SmartKit The SFTP user name or password is incorrect SmartKit 3 99% SmartKit 2. Check whether the hard disk firmware upgrade package meets the version requirements. If the upgrade package is incorrect, replace the upgrade package and upgrade the hard disk firmware again. 3. View the event.txt file in the storage log and search for the keyword ' disk firmware '. If there is no record, the disk upgrade command fails to be executed. In this case, check other files. 4. cli.log upgrade disk cmd result: nok ,cmd: upgrade disk ip=IP.X user=****** password=****** IP.X IP SmartKit SmartKit 5. Check the recent message_XXXXXXXX_mem in the storage logs. Check whether the following information exists near the time when the disk upgrade command is executed on the storage device: Connection from IP.X port PORT.Y on IP.Y port 22 [ERROR][User(username: PORT.X) IP Address is not permited. ][AA][aa_pam_auth.c:AA_pamLocalAuthHandle,254] [ERROR][Auth (username) by aa finished, result(1077949071). ][AA][aa_pam_auth.c:AA_pamLocalUserAuth,405] [ERROR][User(username) pam auth failed. ][AA][aa_pam_auth.c:AA_procRecvMsgFromSocket,1514] [INFO][Recv user(username) auth result(1077949071) from AA. ][PAM_AA][pam_om.c:send_to_AA,182] [INFO][AA auth failed, ret(1). ][PAM_AA][pam_om.c:pam_sm_authenticate,433] Failed password for username from IP.X port PORT.X ssh2 IP.X indicates the IP address of the FTP or SFTP server; IP.Y indicates the IP address of a controller, while PORT.X indicates a random port number. If PORT.X and PORT.Y are the same, the storage device uses different ports to log in to itself and considers itself as an FTP or SFTP server.\nThe information indicates that the authorization of fails: 6. Check the event.txt file again and check whether the message \" The user (user name username) failed to login from the source (IP.X). The error message is 0x4040328f \" exists near the time when the disk upgrade command is executed. 0x4040328f indicates that the user IP address is not in the trustlist and is not allowed to log in. The login IP address is not in the storage trustlist when the command for upgrading the hard disk firmware is executed. 7. Check the event.txt file again and search for ' username:IP.X succeeded in adding security rule whitelist(IP address IP.Y) '. In the preceding information, username indicates the administrator account name, IP.X indicates the login IP address of the administrator, while IP.Y indicates the IP address added to the trustlist. The preceding information indicates that IP address Y (generally the IP address of the host where the upgrade is performed) is added to the trustlist some time before the hard disk firmware upgrade: 8. View the advanced O&M command ' upgrade disk ip=? user=? password=? path=? upgrade_mode=? [disk_id=?] [port=?] [protocol=?] ' in the storage product documentation. In the command, ip indicates the IP address of the FTP or SFTP server.\nDear All, Today we are going to learn about DR VS Backup and Levels of DR. Differences Between DR and Backup Backup: Backup is the process of copying all or parts of data sets from an application host's disks or a storage array to other storage media in a data center. A backup is a method of DR. DR: A DR system consists of two or more sets of IT systems that are geographically far from each other. These IT systems provide the same functions and monitor each other's health status. In the event of an accident (such as a fire or an earthquake), applications on a broken-down system can be switched to other systems to ensure business continuity. Generally, DR indicates the backup of data or application systems across equipment rooms, whereas backup refers to local data or system backup. A DR and backup solution combine local backup and remote data replication to provide comprehensive data protection Generally, backups are implemented using backup software while DR is implemented using replication or mirroring software. Their differences are as follows: Data is in a different format after being processed by backup software and is available only after being recovered. However, replication or mirroring software does not change data formats but mounts data to hosts directly. Backups offer a longer data protection period than replication or mirroring.\nA service paradigm known as \"cloud storage\" involves sending and storing data on remote storage systems, where it is then maintained, managed, backed up, and made accessible to users across a network, most often the internet. Users often pay a monthly, per-consumption fee for the storage of their cloud data. A virtualized storage system with accessible interfaces, almost instant elasticity and scalability, multi-tenancy, and metered resources forms the foundation of cloud storage. Data that is kept in the cloud is organized into logical pools and spread out over various commodity storage systems that are either on-site or housed in a data center run by a third-party cloud provider. Data that has been moved to the cloud is maintained and managed by cloud service providers. The capacity of storage services in the cloud can be increased or decreased as required. Businesses that use cloud storage do away with the need to purchase, operate, and maintain internal storage infrastructure. The cost of storage per gigabyte has been drastically reduced thanks to cloud storage, however depending on how it is utilized, operating costs have been imposed by cloud storage providers. Types of Cloud Storage There are three main cloud storage options, based on different access models: public, private and hybrid. . These storage services provide a multi-tenant storage environment that is most suited for unstructured data on a subscription basis. Data is stored in the service provider's data centers with storage data spread across multiple regions or continents.\nCustomers generally pay on a per-use basis, similar to the utility payment model. In many cases, there are also transaction charges based on frequency and the volume of data being accessed. This market sector is dominated by the following services: Amazon Simple Storage Service (S3); Amazon Glacier for deep archival or cold storage; Google Cloud Storage; Google Cloud Storage Nearline for cold data; and Microsoft Azure. Private cloud . A private cloud storage service is an in-house storage resource deployed as a dedicated environment protected behind a firewall. Internally hosted private cloud storage implementations emulate some of the features of commercial public cloud services, providing easy access and allocation of storage resources for business users, as well as object storage protocols. Private clouds are appropriate for users who need customization and more control over their data or who have stringent data security or regulatory requirements. . This cloud storage option is a mix of private cloud storage and third-party public cloud storage services, with a layer of orchestration management to operationally integrate the two platforms. How does cloud storage work? Large data centers are maintained by cloud service providers in numerous places all over the world. Customers who buy cloud storage from a vendor hand over control of the majority of the data storage-related issues to the vendor, including security, capacity, storage servers and computing resources, data availability and network delivery.\nThrough conventional storage protocols or application programming interfaces (APIs), customer applications can access the cloud data that is being stored there, or they can move their own data to the cloud. How cloud storage works varies depending on the type of storage used. The three main types are lock storage, file storage object storage : divides large volumes of data into smaller units called blocks. Each block is associated with a unique identifier and placed on one of the system's storage drives. Block storage is fast, efficient and provides the low latency required by applications such as databases and high-performance workloads. File storage organizes data in a hierarchical system of files and folders; it is commonly used with personal computer storage drives and network-attached storage (NAS). Data in a file storage system is stored in files, and the files are stored in folders. Directories and subdirectories are used to organize the folders and locate files and data. A file storage-based cloud can make data access and retrieval easier, with this hierarchical format being familiar to users and required by some applications. stores data as objects, which consist of three components: data stored in a file, metadata associated with the data file and a unique identifier. Using the RESTful API, an object storage protocol stores a file and its associated metadata as a single object and assigns it an identification (ID) number. To retrieve content, the user presents the ID to the system and the content is assembled with all its metadata, authentication and security.\nThe OceanStor Pacific Series is a suite of data storage infrastructure services that provide optimal, cost-effective, and reliable performance for services that need to store information on the cloud in large volumes. Services such as artificial intelligence, HPC (High performance computing), video sharing sites, etc. are the users of these services. In this service, existing service, operational and structural restrictions have been broken and new standards in this field have been defined. Improving the level of coordination in the internal performance of servers, the use of the next generation of flexible algorithms and a large set of dedicated hardware, are the factors to achieve this powerful and superior performance. With the new standard that this service provides to large companies in need of cloud services, all their potential capabilities will be realized. In today's smart world, it is inevitable and vital. Today, the ability to store data on a large scale has become vital for large, multinational corporations to help break through the analog structure and enter Prepare for the digital age. Large companies face the problem of lack of storage space and the inability to manage huge amounts of information. OceanStor Pacific Series service by Huawei, seeing these problems and to solve them, has been set up to provide optimal, economical and sustainable services by creating a new standard in this field and in the very near future will become a trusted option in storage services. Be information.\nThe Huawei OceanStor service, which is the basic version of this service, is currently active in more than 150 countries and serves more than 12,000 customers in various fields of work such as mobile services, finance, government, energy, health, transportation and various manufacturers. Offers. This service is an ideal choice for storing and processing information, the three main features of which are follows: Optimal performance: In this service, a new structure has been created in which files are transferred to the server without interruption. This function optimizes the information transfer process and does not lose any part of it, a problem that occurs in most traditional ports used in this type of service. The strengthening of Huawei's investment and the increasing efforts of specialists have led to the acquisition of the most advanced technologies in the world for this company. In this system, unlike the old models, information no longer needs to be transferred in multiple memory structures. This feature has increased optimization in the data storage process by up to 25% and reduced the space required for data storage by 20%. Cost-effectiveness: In the new structure of OceanStor Pacific Series, structural constraints have been broken by using the innovative and secure vNode mode alongside the use of next-generation elastic EC Algorithm technology. With this method, up to 93% of hard disk space can be used and about 40% more than the average provided in the whole industry. Depending on their importance, data is stored on SSDs, HHDs, or BlueRay disks.\nTo improve the reliability of the storage network and avoid single points of failure, multiple physical paths are required between the storage device and the host. To shield redundant devices from the same block device that are identified through different links, a piece of software is required to aggregate physical block devices from multiple paths into one virtual block device. For host applications, the multipath software has the function of failover and load balancing, so the host applications only need to deliver I/Os to virtual disks. Different multipath software has different functions. You need to select the software based on the actual application scenario. Blocking cause: When a path is faulty, the HBA attempts to reconnect for a period of time. During this period, I/Os stay on the HBA and do not return to UltraPath immediately. Therefore, I/Os are blocked for a period of time during the path switchover. To set the load balancing mode, run the following command(ALUA): set workingmode={0|1} [array_id=ID | vlun_id={ID | ID1,ID2... | ID1-ID2}] 1: Intra-controller load balancing. All I/O sending LUNs are on the working controller. 0: Load balancing among controllers. All I/Os are sent to the working and non-working controllers of the LUN. Set the working controller switchover policy of a LUN: If all paths from the host to the controller to which the LUN belongs are disconnected, switch to another controller to deliver I/Os.\nset luntrespass={on | off} [array_id=ID | vlun_id={ID | ID1,ID2... | ID1-ID2}] Run the following command to set the load balancing routing algorithm: set loadbalancemode={round-robin | min-queue-depth | min-task} [array_id=ID | vlun_id={ID | ID1,ID2... | ID1-ID2}] round-robin: polling; min-queue-depth: minimum queue depth; min-task: minimum task. Set the HyperMetro working mode. set hypermetro workingmode={priority | balance} primary_array_id=ID [vlun_id={ID | ID1,ID2... | ID1-ID2}] Setting the HyperMetro Working Mode of a Specified Virtual LUN Has a Higher Priority than Setting the HyperMetro Working Mode of a Storage System Storage configuration: 1. When creating an initiator, select Use third-party multipathing. 2. When adding or modifying the initiator attribute Use third-party multipathing, if the LUN has been mapped to the host, restart the host instead of restarting the multipathing service (because multipath is a user-mode process). 3. The switching mode and special mode type depend on the storage version. 4. The path type is used to determine whether the active-active working mode is balanced or unbalanced. 5. The switchover modes of all initiators of the storage system added to the same host must be the same. Otherwise, host services may be interrupted. 6. After the initiator mode is configured on the disk array, you need to restart the host for the new configuration to take effect. You do not need to restart the host when mapping LUNs for the first time. set hypermetroworkingmode=balance primary_array_id=ID set hypermetroworkingmode=priority primary_array_id=ID Basic idea: Paths from hosts to storage devices are classified into AO and AN paths. I/Os are preferentially delivered from the AO path.\nActive Optimized (AO): 1. Prerequisites: The path on the working controller is connected. 2. In other conditions, multipath needs to be configured as the preferred path for the initiator. Active Non-optimized (AN): indicates a non-AO trail. ALUA handover policy: When one AO path fails to provide access, host I/Os are delivered to the other AO path. When all AO paths to the working controller are down and cannot provide access, host I/Os are delivered from the AN path to the non-working controller. Supplement: 1. For RHEL8 and later versions, the status of multipath -ll may not be updated after a path fault occurs due to kernel parameter changes. You are advised to add detect_checker no to device. 2. Third-party UltraPath cannot detect whether the HyperMetro scenario is used. You need to configure the path type and set the HyperMetro mode on the storage device. 3. Huawei UltraPath uses ALUA by default. Third-party UltraPath can use ALUA or non-ALUA, that is, multibus. Limitations: 1. The storage system does not support cross-engine switchover. As a result, the host performance may be poor after LUN switchover, and I/Os may be zero. 2. ALUA cannot identify the LUN downtime status. If LUN downtime occurs in HyperMetro, you need to find an error and ask the host to switch paths for retry. If the LUN fails to be found, HyperMetro does not support third-party multipathing. In this process, I/Os will be zero, services will be interrupted, or the link status will be abnormal. 3.\nFile Storage File storage provides file-based, client-side access over the TCP/IP protocol. In file storage, data is transferred via file I/Os in the local area network (LAN). A file I/O is a high-level request for accessing a specific file. For example, a client can access a file by specifying the file name, location, or other attributes. The NAS system records the locations of files on disks and converts the client's file I/Os to block I/Os to obtain data. File storage is a commonly used type of storage for desktop users. When you open and close a document on your computer, you used the file system. Clients can access file systems on the file storage for file upload and download. Protocols used for file sharing between clients and storage include CIFS (SMB) and NFS. In addition to file sharing, file storage also provides file management functions, such as reliability maintenance and file access control. Although there are differences in managing file storage and local files, file storage is basically a directory to users. One can use file storage almost the same as using local files. Because NAS access requires the conversion of file system format, it is not suitable for applications using blocks, especially database applications that require raw devices. File Storage has the following advantages: Comprehensive information access: Local directories and files can be accessed by users on other computers over LAN. Multiple end users can collaborate with each other based on same files, such as project documents and source code.\nAlthough SaaS services like Microsoft 365 and Google Workspace offer relatively little in the way of backup functionality, they do have some backup features that can be used to support application availability or give short-term protection against user error. Simply put, they don't offer backup for the sake of company continuity. That implies that erased data is only temporarily retained and that recovering lost data might be a nightmare. They do, however, offer native archiving options to aid in meeting regulatory and legal obligations. Since archiving and backup involve comparable processes, such as creating and keeping copies of production data, many IT professionals mistakenly believe that archiving and backup are interchangeable. The similarities stop there, though. Let's highlight the differences between backup and archiving to understand their distinct functions better. Backup refers to the process of making copies of data or data files to use in the event the original data or data files are lost or destroyed. In actuality, not all SaaS backup solutions are created equal. Native backup features can't be used to restore data in its original state from a specific point in time because they are primarily intended for application availability. Admins and end users can explore their backups for specific bits of data using a third-party SaaS backup solution like Spanning, which also offers numerous granular recovery options. Data archiving is the process of retaining data for long-term storage. Even though the data might not be in use, it can be brought into use and can be stored for future purposes.\nA method for eDiscovery and a legal hold in the event of litigation is provided by data archiving platforms like Google Vault and Microsoft Exchange Online Archive, which assist in meeting information retention obligations for things like financial transactions, agreements, and personnel records. Compliance with governmental or industry rules frequently imposes a condition that must be met. In the event that your data is lost or damaged in any other manner, a backup is a copy of your current and active data that can be utilized for operational recoveries. A backup's primary function is to enable data restoration to a previous state. On the other hand, archives are meant to be utilized as storage facilities for information that must be retained for a long time but isn't necessary crucial for operational activities. Data that must be archived in order to comply with regulations, for instance, may not always be acceptable for backup. A backup is only a copy of the most recent and relevant data that is kept on your servers. The original files remain in the same location and are unaffected when a backup copy of your data is made. Despite the fact that an archive is also a copy of your data, the archiving process moves the data from the main storage location of your company to more affordable and long-term storage places. The attributes valued in a backup and an archive differ because they serve distinct purposes. Speed is a key component with a backup.\nHello everyone, Have a nice day! Clusters of Innovation (COI) describe groups of interconnected companies and associated institutions in a particular field. Together, they support and encourage the continued generation and co-creation of business and technology innovations within the cluster. The poster child of innovation clusters is California's Silicon Valley, but innovative startup and research clusters are emerging rapidly in various global locations, including London, Israel, Japan, and Singapore. These clusters indicate a technological leapfrogging to come. They can help economies transform by building higher value and more productive economic sectors. Clusters typically build on several factors to create success. Early-stage clusters are often best defined by their closeness to leading educational institutions, such as Stanford University in the US, Tokyo University in Japan, Cambridge University in the UK, and NUS in Singapore. Science parks and research triangles co-founded by universities abound in the developed world, for example, in London, Paris, Berlin, and Singapore. Another common theme is support for the cluster from much larger multinationals such as IBM, Microsoft, Google, Huawei, Baidu, Infosys, and Mitsubishi. They can bring international innovation requirements and high-quality business skills to local partners and startups, supporting and nurturing their collective growth over time. As clusters evolve, an additional requirement for their survival is flexible startup financing from Venture Capital funds, banks, or governments. But perhaps even more important than any of these technical factors is a social one the development of strong interpersonal networks of people willing to interact and share ideas.\nDear All, Today we are going to learn about Host Layer DR Types Host Layer DR Technology - Application Level Application-level DR technology uses application software to implement remote data replication and synchronization. When the production center fails, the application software system in the DR center recovers and takes over services from the production center. Working principle: The application software is connected to two remote databases. The service processing data is stored in the databases of both the active and standby centers. Advantages and disadvantages: Supports wide area networks (WANs). No independent hardware or software is required. Data is logically replicated to avoid spreading human errors. The consistency check needs to be performed periodically. Backup data in the backup center cannot be quickly restored to the active center. Major modifications need to be made to the application program. Host Layer DR Technology - Database Level Database-level DR technology is designed for specific databases. Generally, typical databases have the database-level DR function, like Oracle Data Guard and DB2 HADR. Database-level DR is implemented by transmitting database logs and replaying them at the DR site. The database-level DR technology supports a smooth switchover Working principles: Configure the active and standby database servers. When a transaction operation is performed on the active database, the log file is simultaneously sent to the standby database. The standby database then replays the received log file to ensure data consistency with the active database.\nProblem Description: 1. Storage:OceanStor Dorado 5000 V6 6.1.2 2. Customer fail to copy the file from EMC NAS to Huawei NAS. 3. Failed to copy some Data from EMC NAS to Huawei NAS 4. Windows OS The file name you specified is not valid or too long. Specify a different file name Problem Analysis: 1. Firstly, follow the alarm information to troubleshoot the issue. Consider that the File name and the file path is too long to copy. The source path cannot contains more than 256 characters on Windows Scenario. 2. Check the source path, it is obviously that the source path is short enough. 3. Check the fault file name, all of the faulty file has AOF character, consider that the AOF may cause the name fault issue. Try to rename the file name as test1, it still fail to be copied. 4. Run the command dir /r through cmd in the current path. Found that there are one more name for the fault file. It is the ADS flow file. 5. Check the ADS flow file name, we can found a character in the Suffix, it should be the character that cause the failure to copy. 6. collect the log with smartkit, after check the log in the issue timing, we can find some result of fail to copy due to the file name, such as , the must be the character , which confirm the root cause.\nMany customers encounter a plenty of problems when configure SNMP clients connecting to storage, although there are various types of SNMP clients, but all communications are based on standard SNMP protocol, including the SNMP proxy on storage and SNMP client, this article can guide you troubleshooting almost most of the problems besides software defects. Troubleshooting steps 1. Check the running status of SNMP processes on both controllers. a. Login CLI of storage, any controller is ok. b. Run command \" change user_mode current_mode user_mode=developer \" to enter developer mode. c. Run command \" show snmp status \" to show the status of SNMP process on both controllers. If the column \"Status\" shows \"Running\", skip this section and go to step 2. If the column \"Status\" shows not \"Running\", go ahead. d. Run command \" change snmp status status=start\" to start the process. e. Wait for two minutes. f. Run command \" show snmp status \" again, if the process is still not in running state, collect storage logs and contact TAC engineers for further investigation. 2. Check the listening port of SNMP process on storage. Run command \" show snmp port\" to check the listening port of SNMP process on storage. Above figure shows the listening port 161, then check the port settings on SNMP client, make sure they are the same, otherwise SNMP clients can't connect to storage. Or you can change the listening port on storage as below figure. 3. For SNMPv1/v2, check all settings is fine. a.\nExecute command \" show snmp version \" If SNMPv1/v2 switch is off, execute command \"change snmp version v1v2c_switch=On\" to enable SNMPv1/v2c on storage. b. Execute command \"change snmp community read_community=****** write_community=******\" to change the write/read community, make sure the settings on SNMP clients and storage are the same. 4. Check whether IP address of SNMP client is locked by storage due to too high frequently connection. Check the event log of storage and search with ID 0x200F00310014. If yes, the account is locked because the number of incorrect password attempts exceeds six. Run these CLI commands shown in the preceding figure to shorten safe strategy lock time and increase the number of retry times, then reset a simple password and try again. The snmpwalk and snmpget commands are available on both Zabbix and eService clients. Run the tcpdump command on the server that sends SNMP requests to capture packets, as follows: tcpdump host 1.1.1.1 and port 161 -w test.cap The preceding command indicates the management IP address of the storage array. Port 161 is the default port. Write the captured packets to the test.cap file. If you do not write the packets to the file, run the tcpdump host 1.1.1.1 and port 161 command. The following information is displayed: The information in the red box is the community name. As shown in the preceding figure, Zabbix represents the IP address and port ID of the Zabbix server, and 8.46.75.116 represents the IP address of the storage array.\nCustomer paused HyperMetro pair between two storages and shutdown the first storage for power maintenance. But since the shutdown, some of the LUNs which unavailable for VMware ESXi hosts. 1. From storage configuration data, LUN11 and LUN11.remote belong to HyperMetro pair 0x4fe8d8c1102000c, LUN12 and LUN12.remote belong to HyperMetro pair 0x4fe8d8c11020011. Lun Id: 225 Lun Name: LUN11 Volume ID: 160 Userpool Id: 1 Type: Thin User Lun Capacity: 21474836480(Sectors) Allocated Capacity: 4493343264(Sectors) Protection Capacity: 21736302368(Sectors) Sector Size: 512(B) Health status: Normal Running status: Online WWN: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Owning Controller: 0A Work Controller: 0A Cache Partition Id: -- Smart Cache Partition ID: -- Snapshot IDs: Clone IDs: -- HyperCopy IDs: -- LUN Copy IDs: -- HyperMetro ID(s): 0x4fe8d8c1102000c Lun Id: 242 Lun Name: SRV_LUN12 Volume ID: 177 Userpool Id: 1 Type: Thin User Lun Capacity: 21474836480(Sectors) Allocated Capacity: 9402649904(Sectors) Protection Capacity: 6681397664(Sectors) Sector Size: 512(B) Health status: Normal Running status: Online WWN: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Owning Controller: 0B Work Controller: 0B Cache Partition Id: -- Smart Cache Partition ID: -- Snapshot IDs: Clone IDs: -- HyperCopy IDs: -- LUN Copy IDs: -- HyperMetro ID(s): 0x4fe8d8c11020011 2. As the configuration data of storage array A, the preferred/priority site is A for the HyperMetro pairs.\nHyperMetro Information ------------------------------ HyperMetro Pair Information [11]------------------------------ Pair ID:0x4fe8d8c1102000c Remote Pair ID:0xc Pair Status: Normal Pair Health Status:Normal Arb Status: Unknown Is Priority: Yes Consistency Group ID:0xffffffffffffffff Domain ID:0x7c1cf1899d420100 DS Type:Lun DS Size:0x500000000 Work Mode: AA Recovery Policy: Auto Isolation Switch: Close Isolation Threshold(ms):1000 Sync Speed Level: Asap Start Time:2022-06-24/01:29:35 UTC+08:00 End Time:2022-06-24/01:30:11 UTC+08:00 Local DS Information------------------- Local DS ID:225 Local Array WWN:0xxxxxxxxxxxx Local DS Status:Consistent Read/Write:Read and Write Remote DS Information------------------- Remote DS ID:166 Remote Array WWN:0xxxxxxxxxxxx Remote DS Status:Consistent Read/Write:Read and Write HyperMetro Information ------------------------------ HyperMetro Pair Information [15] ------------------------------ Pair ID:0x4fe8d8c11020011 Remote Pair ID:0x11 Pair Status: Normal Pair Health Status: Normal Arb Status: Unknown Is Priority: Yes Consistency Group ID:0xffffffffffffffff Domain ID:0x7c1cf1899d420100 DS Type: Lun DS Size:0x500000000 Work Mode: AA Recovery Policy: Auto Isolation Switch: Close Isolation Threshold(ms):1000 Sync Speed Level: Asap Start Time:2022-06-24/01:29:35 UTC+08:00 End Time:2022-06-24/01:30:12 UTC+08:00 Local DS Information------------------- Local DS ID:242 Local Array WWN:0xxxxxxxxxxxx Local DS Status: Consistent Read/Write: Read and Write Remote DS Information------------------- Remote DS ID:184 Remote Array WWN:0xxxxxxxxxxx Remote DS Status: Consistent Read/Write: Read and Write 3. From event log in storage B, the two HyperMetro pairs stopped service at Preferred storage side. It means, service stopped at storage and switched over to storage B. 2022-06-23 20:18:54 DST 0x200F3C01001E Event Informational -- None User adm_Tvan:xx.xx.xx.xx succeeded in suspending HyperMetro (HyperMetro ID 4fe8d8c1102000c, local LUN ID 166, remote LUN ID 225, HyperMetro Domain ID 7c1cf1899d420100, Site with services stopped Preferred).\nExternal LUN & eDevLUN External LUN External LUN is a LUN in a heterogeneous storage system, which is displayed as an external LUN on the DeviceManager. eDevLUN In the storage pool of a local storage system, the mapped external LUNs are reorganized as raw storage devices based on a certain data organization form. A raw device is called an eDevLUN. The physical space occupied by an eDevLUN in the local storage system is merely the storage space needed by the metadata. The service data is still stored on the heterogeneous storage system. Application servers can use eDevLUNs to access data on external LUNs in the heterogeneous storage system, and the SmartMigration feature can be configured for the eDevLUNs. Relationship Between an eDevLUN and an External LUN An eDevLUN consists of data and metadata. A mapping relationship is established between data and metadata. The physical space needed by data is provided by the external LUN from the heterogeneous storage system. Data does not occupy the capacity of the local storage system. Metadata is used to manage storage locations of data on an eDevLUN. The space used to store metadata comes from the metadata space in the storage pool created in the local storage system. Metadata occupies merely a small amount of space. Therefore, eDevLUNs occupy a small amount of space in the local storage system. (If no value- added feature is configured for eDevLUNs, each eDevLUN occupies only dozens of KBs in the storage pool created in the local storage system.)\nBackground: Due to different feature of FC and IP protocol, when they are as replication links, they have different relaibility and latency on the links, to guarantee storage running stably and balancedly, for V6 storages, only one type of replication link can be added for remote device on DeviceManager. For example, if FC replication link has already been added, IP replication links cannot be added. If IP replication link has already been added, FC replication links cannot be added. if production service is already migrated to storage environment, but due to change plan of infrastucture, you have to change the replication links from FC to IP without service interruption, this article can guide you to achieve the aim smoothly. Solution Idea: If the network is successfully setup, after the ports planed for replication links are added to replication port group on both storages, the replicaiton links can be established automatically, the result is the two types of replication links exist at the same time. To achieve the goal, below are the main steps. 1. Keep FC links. 2. Add IP links, 3. Check the confirm two types of replication links exist normally in the meantime. 4. Remove all FC links. The all key steps can be done in CLI. Procedure: 1. Create logical ports planned for replication links on two storages. Create a logical port: 2. Login DeviceManager of storage, split the all involved remote replicaiton pairs or consistency group, pause all HyperMetro pairs or consistency group.\nThere're two storages connecting to the same SMTP server for alarm notification. The configuration on OceanProtect X8000 is like below The SMTP configuration on OceanStor 2600 V5 is like below: But only OceanProtect X8000 can test through the SMTP function. 1. From the event log, the SMTP notification failed with error code 0x4000ca16 173309 2022-09-15 11:11:34 DST 0x200F00310065 Event Informational -- None xxx:xx.xx.xx.86 failed to test email server (server address xxxxxxxxx). Error code: 0x4000ca16. 2. Based on Error Code Reference guide, the error code 0x4000ca16 means the network between storage and SMTP server is not reachable, or SMTP configuration incorrect. 3. From controller kernel log, the SMTP server in fact responded error code 550. Based on the standard definition, it means \"A \"550 Authentication is required for relay\" error indicates that your email server requires SMTP authentication in order to send outgoing mail, but the email client used to send email has not been authenticated with your username and password.\" 4. Check the SMTP server authentication settings, it allows Anonymous access, and requires TLS encryption for basic authentication, which means, the configuration is correct. 5. Check the SMTP server whitelist of IP address, it's like below: There's a misunderstanding on whitelist settings of SMTP server. There are two groups of \"computers\" in whitelist, first group is xx.xx.17.170 with network mask xx.xx.17.172; the second group is xx.xx.17.175 with network mask xx.xx.17.177.\nAfter doing the joined of the controller to the domain the following alarms appeared on the array. On the CLI command line output, only two DCs was discovered by the array. 1. From the event log, controller 0C can't connect AD and DNS server 31078 2022-09-28 17:33:54 DST 0xF4045002C Fault Major Unrecovered None The DNS service of tenant (ID 0, name --) on controller (ID 2) is unavailable. 31065 2022-09-28 17:22:14 DST 0xF4045002B Fault Major Unrecovered None The AD domain service of vStore (ID 0, name --) on controller (ID 2) is unavailable. 2. From controller 0C kernel log, it failed to bind AD and DNS server by IP address xx.xx.xx.xx. it means the network is not reachable. [2022-09-28 17:21:59] [1114204.310352][15000028c05e1][ERR][bind(ip:xx.xx.xx.xx) return(-1) failed. errcode:-5. [54986]][PROTO_CMR_COMM][bind_src,238][cmr_auth] [2022-09-28 17:21:59] [1114204.310405][15000028c05e1][ERR][bind src ip failed! DstIp:xx.xx.xx.xx, SrcIp:xx.xx.xx.xx, port 389. errcode:-5. [54986]][PROTO_CMR_COMM][detect_tcp_is_alive_with_src,198][cmr_auth] [2022-09-28 17:21:59] [1114204.310583][15000029205e6][ERR][TenantId( 0), cldap ping DC(xxxxxxxx) all IP failed. errcode:-1. [54986]][PROTO_CMR_AUTH][ping_all_ip,213][cmr_auth] 3. From storage configuration, the issue logic IP address belong controller 0A.\nIn this case, there's some free Grains inside filesystem.\nAs an example, below filesystem statistics Fs Id: XX Fs Name: XXXXXX Fs Description: Userpool Id: XX Type: Thin File system Capacity: 21474836480(KB) Allocated Capacity: 11425601426(KB) Min FS Capacity: 8913292662(KB) Block Size: 16384(B) Application Scenario: User Defined Health Status: Normal Running Status: Online Owner Controller: 0A Work Controller: 0A Home Pair Id: 0 Cache Partition Id: -- Capacity Threshold: 90 Initial Distribute Policy: Automatic Relocation Policy: None Fs Io Priority: Low Snapshot Reserve Percent: 20 Snapshot Reserve Capacity(sector): 8589934592 Snapshot Auto Del Switch: No Snapshot Used Capacity(sector): 124725228 Timing Snapshot Max Number: 550 Timing Snapshot Schedule ID: 12 Snapshot Dir Visable: Yes Snapshot Timing Switch: Yes Snapshot Rollback Mode: No Snapshot Background Freeing Capacity: -- Atime Switch: Yes Read-only Switch: No Is Clone Fs: No Checksum Switch: Yes Compression Switch: Yes Compression Method: Fast Dedup Switch: Yes Dedup Check Switch: No Dedup Intelligent Switch: Yes Dedup Running Stat: Yes Is Thick Dedup Compression Enabled: No Dedup Metadata Sample Ratio: 1 WORM Type: Normal WORM AutoLock Switch: -- WORM AutoDelete Switch: -- WORM MinProtectPeriod: -- WORM MaxProtectPeriod: -- WORM DefProtectPeriod: -- WORM AutoLockTime: -- WORM WriteCheck Switch: -- WORM ClockTime: -- WORM ExpiredTime: -- Data Transfer Policy: 0 Quota Switch: 0 Free Capacity: 10049235040(KB) Is Mapped : No Dedup saved capacity: 24300636(KB) Compress saved capacity: 262138712(KB) Total saved capacity: 286439348(KB) Dedup saved capacity ratio: 0 Compress saved capacity ratio:3 Total saved capacity ratio: 3 Smart Cache Partition ID: -- Smart Cache Cached Size: 0 (B) Smart Cache Hit Rage(%): 0 Infact Size In Disk: 8913292662 (KB) Fs inode total count: 2000000000 Fs inode used count: 14185977 Traverse Dir Adapter: No Isolate Damaged Object: Yes Space Self Adjustment Mode: Off Auto Size Enable: Yes Auto Reduction Threshold Percent(%): 50 Auto Expansion Threshold Percent(%): 85 Minimum Auto Size: 21474836480(KB) Maximum Auto Size: 25769803776(KB) Auto Size Increment: 1048576(KB) Space Recycle Mode: Autosize First Vstore ID: 11 Fs Group Id: -- Parent Fs Id: -- Children Clone Fs Number: 0 Split Speed: -- Split Status: -- Split Progress(%): -- Allocated Pool Quota: 7049576448(KB) Alternate Data Streams Enable: Yes Support File System Tier: Yes SSD Capacity Upper Limit: -- Used SSD Capacity: 0(KB) Fs Prefetch Policy: Intelligent, Prefetch Value: 4096KB Long File Name Enable: Yes Security Style: NTFS Meta Rsv Percent: 0 Background Dedup Enabled: No Background Compression Enabled: No Unix Permissions: 777 While checking the filesystem capacity, need to pay attention to data protection capacity and data reduction saved capacity.\nTitle: Analysis on Long Backup Task Delay. Keywords: OceanStor Dorado 5000 V6, high latency Product: OceanStor 5000 V6 Fault Type: Other Application scenario: troubleshooting Language: English [Problem Description] The storage device is used for backup services, but the customer stops most backup tasks, but the latency of a few tasks is still long. [ ] 1.Obtain storage logs and performance logs, according to log and performance data, the performance issue should related to two causes:The first one is one known issue which fixed in the latest patch 6.1.0.SPH22, this issue will lead to CPU core usage running in high value, and storage enter overload state lead to storage response time increased. 2.The second part is the service pressure, the storage is consist of 4 controllers with 2 engines, there are two disk domains created, but the two disk domains are using the independent disks from single engine, StoragePool001-1 only using disks from engine 0 and StoragePool001-2 is using disks from engine 1. And the service pressure and capacity usage are totally different between the two disk domains/storage pools, for engine 0, the service load is not high, and storage pool capacity usage only 21%. But the engine 1, service load is quite high and the capacity usage is already 90%. (the performance will also be affected if storage pool capacity usage is too high). 3.High CPU load is a known issue and can be resolved by installing the latest patch. 4.The service load is unbalanced. The load of cluster 1 is very high, causing performance deterioration.\nTitle: One of the paths from the host to the storage device is broken. Keywords: OceanStor 6800 V5, multipath, bit error Product: OceanStor 6800 V5 Fault Type: Other Application scenario: troubleshooting Language: English [Problem Description] One of the paths from the host to the storage device is broken. [ ] 1. Collect storage logs using Smartkit . 2. Collect SNS switch logs using supportshow cmd. 3. Check the storage log, and find the LUN IO timeout error many times; 4. Analyzed the logs and found that the BadCrc error persisted on the link. The source was RPort 0x10700 on the host side. 5. Checked the SFP port 7 in the FC switch log. It was found that the RX and TX power of port 7 was normal. 6. Multiple ports report the BadCrc error at the same time. According to the following networking model, the fault occurs only when the link between the host and the switch is abnormal. After the user replaced the cable between the host and the switch, the fault was rectified. [Root Cause] The link between the host and port 7 of the SAN switch is abnormal. As a result, the BadCrc error is continuously generated. As a result, the I/O is abnormal. [Solution] 1. Replace the link between the HBA card 100000109bcbd052 of TRJ-Venus-Db3 and SAN switch port 7, including the optical module and optical fiber. 2. After the replacement is complete, collect storage logs and check whether the fault is rectified. 3.\nBackground: Sometimes, customers don't really know the feature of their service, like IO model, deduplication ratio etc.. the configured application type exactly does match the service, it makes storage waste its resource, then customers need to change the application type online. Solution: Create new LUNs with the required application, migrate data from existing LUNs to newly created LUNs. SmartMigration synchronizes and splits service data to migrate all data from the source LUN to the target LUN. During the whole process, service will not be interrupted. Procedure: 1. Create SmartMigration pair Select source LUN Select \"Advanced\" and \"Automatic\" to create a target LUN Application Type selection: check the below notes splitting Modes elect Manual Note: Oracle_OLAP will disable the deduplication function and enable the compression function for the workload. Adopting this workload further improves performance, but it will reduce the data reduction rate If you want to enable the deduplication and compression functions with grain size 32KB, you need manually create a workload. Command: create workload_type general name=* io_size=* compression_enabled=* dedup_enabled=* Then in the application Type area, you could select the new workload created Complete creation Start synchronization, after synchronization is completed, the running status changes to normal. 2. Consistency splitting SmartMigration Split the source LUN pair mapped to the same host, which has been synchronized, and whose running status is normal. Perform split operation. Split complete, running status is migrated\". The source LUN workload has been modified and the grain size is 32 KB. 3.\nProblem description: the chart is generated by restAPI enquiries note: no alarm on the storage side. Problem analysis: The memory on storage is divided into two parts, one part is for OS(management plane), and the other part is for service(I/O plane). the memory usage you see is the OS part. Free memory is the amount of memory that is currently not used for anything. This number should be small because memory that is not used is simply wasted. Available memory is the amount of memory that is available for allocation to a new process or to existing processes. Currently, it equals (total memory -free memory)/total memory, but it is incorrect, it should be (total memory-available memory)/total memory. In future versions, this part will be improved. And why does the free memory keep decreasing? In the OS, To improve the efficiency of memory access, when the free memory is sufficient, the OS does not reclaim the released memory to free memory but will leave it as cached memory, buffer memory, and so on, so the free memory will be less and less. The OS attempts to release the memory to free memory only when the free memory is very lower than a certain threshold or when new memory resources are insufficient. Meanwhile, once the free memory is lower than 200MB, the storage system will proactively release the memory. In conclusion. 1. The way for calculating the memory usage is incorrect, this part will be improved in the future. 2.\nProblem description: In storage of 6.1.3 version, customer used MMC to do the below operations, but got failure. 1. created CIFS share failed. 2. change share permission of CIFS share failed. Problem analysis: The customer provided the below information. 1. the issue can be reproduced inevitably, customer reproduced the issue and capture the network traffic using wireshark. 2. all storage logs collected on DeviceManager. Start the analysis based on above information. 1. check the network traffic, when create the share, storage responds \"WERR_UNSUPPORTED_TYPE\" which means storage does not support this operation, but this is impossible, for storage of 6.1.3 version, can do anything for the shares by MMC. there may be a special configuration on the filesystem involved. 2. check the config file, the involved filesystem belongs to a vStore which is the member of HyperMetro vStore pair. For the vStore pair, the vStore on one site is master role, the vStore on the other site is slave role, as the rule, vStore settings can't be changed from slave vStore. 3. check the logical port used by MMC to access the CIFS share, the logical port belongs to slave vStore, and shows Linkup in slave vStore. as the rule, this operation is not supported, storage responds are not supported error. Root cause: the logic port used by MMC to access the CIFS share belongs to slave vStore and shows linkup in slave vStore, as the rule, vStore settings can't be changed from slave vStore.\nProblem description: one customer reported a problem that his storage got alarm about IP conflict, below is the detailed alarm information. Problem analysis: for this problem, the customer already provided the storage log collected on DeviceManager. a. check the alarm details carefully, the alarm is about the management port CTE0.B.MGMT. b. in the log files of controller 0B, find the relative log file, the current log file path is \"messages/message_euler\", and the historical log file path is \"messages/Euler/message_euler_xxx\" (xxx indicates the time point when the file is archived) c. in the log file, search IP address in the alarm, the below log files indicate the MAC of the device has the same IP address in the network. Root cause: although we have got the MAC address of the device has the same IP address in the network, but based on the mechnism of this alarm, storage send arping command to its gateway device, the gateway device respond storage the MAC, the arp mapping is stored on the network device, so there are two possibilities, one is the device in network has the MAC address is set same IP address, the other one is network device keep incorrect arp mappings. Solution: since there are possibilities, suggest to troubleshoot the issue referring to below steps. 1.the first thing is to check whether device has the MAC is set same IP address, if yes, just need change the IP address of the device., if no, go to step 2.\nProblem description: one customer started NDMP backup task failed on backup software. Problem analysis: customer provided storage logs collected on DeviceManager. 1. Based on the backup procedure, when starting the backup process, the backup software need send command to storage, storage will record whether the backup task is started successfully or not. 2. Check the event information of storage preliminarily, at the time point when the backup got failure, storage record below operation log, which indicates storage started the backup task failed. 3. check the config log, find the owner controller of the filesystem involved. a. search \"Fs INFORMATION\" in config log file, here you can find all filesystems created on this storage. b. find the filesystem involved among the filesystems. c. find \"Owner Controller\" item in detailed informaiton of this filesystem, for this case, its owner controller is 0B. 4. check the message log on the controller the involved filesystem belongs to. a. in this case, the filesyste belongs to controller 0B, then find the message file which including the logs generated at the time point when the issue happened. b. check the logs around that time, it shows below error, storage can't find the snapshot path. since the the log shows the detailed path of snapshot, that means, storage already has snapshot license, the snapshot is already created. but the patch can't not be found, maybe it is set invisible on this filesystem.\nProblem Description: 1. Storage: OceanStor Dorado 18000 V6 V600R003C00 6.0.1.SPH15 SPC200 SPH15 2. A great amount of Bit error alarm occur 3. The system performance may be affected. Problem Analysis: 1. Check the Bit error alarm, it mentions the port CTE0.IOM.L11 P1 and CTE0.IOM.H11 P1 2. There is too muchintermittent disconnection with the peer switch in the report time. The link between the host and the peer switch is abnormal. 3. In the log, this means the Local port. And this means the remote port. 4. Thus, this means the link between local to remote port. 5. This means the remote connection. 6. So as the pic shows: The error code is mostly reported in the remote connection (the number of the error code is Hundreds of thousands, while it is hundreds in the poll port) 7. And they are all about link abnormal. 8. Thus, we can give the result that the link abnormal cause the bit error alarm. 9. Moreover, check the port local side. In the error report, it mentions CTE0.IOM.L11 P1 and CTE0.IOM.H11 P1 10. Check the RX/TX power of port IOM.L11 P1 and IOM.H11 P1 in config file, it is normal: 11. So the storage side is normal. Need to check the link between the peer switch and host. Also, check the RX/TX power in the peer switch. 12. For the problem that the alarm occur and disappeared automatically, it is most likely that the link recovered after several seconds, so it disappeared.\nHello, everyone! Today, I would like to share on the workings, types and usage of logical unit number masking. For one or more storage systems, the logical unit could be part of a storage drive, the full storage drive, or all parts of numerous storage drives, such as a harddrive, solid-state drive, or tapes. A LUN can refer to a single drive or partition, several storage drives or partitions, or an entire RAID configuration. In any event, the logical unit is processed as if it were a single device, and the logical unit number is used to identify it. A LUN's capacity limit varies according on the system. In a storage-area network, a LUN is crucial to the management of a block storage array (SAN). Since access and control privileges may be assigned via logical identifiers, using a LUN can ease storage resource utilization. Performance and reliability are influenced by the underlying storage structure and logical unit type. Here are a few observations: Striped LUN with parity: Data and parity information are spread among 3 or more physical disks in a striped LUN with parity. If a physical drive fails, the contents on the remaining drives can be used to rebuild the data. Write performance may be impacted by the parity computation. Striped LUN: Distributes I/O requests across multiple physical disks, significantly improving performance. Concatenated LUN: Combines many logical units or volumes into a single logical unit or volume.\nProblem Description: 1. Storage:OceanStor 2200 V3 V300R006C50 2. The replication work is not fast enough. The remaining time for the remote replication is abnormal 3. The Remote Replication Task Stays in the Synchronization Phase for a Long Time Alarm Information: The remaining time shows it remains 1734 hours for the synchronizing. Problem Analysis: 1. The remote pair ID: 28xxxxxxxxxxc0003 2. In the config file, the Corresponding WWN of this pair ID(28xxxxxxxxxxc0003) is 21xxxxxxxxxx8469c 3. For this WWN, there are four Links with LinkID 0, 1, 256, and 257. 4. In the event log, these four Links kept reporting replication link disconnect events and then recovered. 2022-04-27 06:41:04 0xF0E10001 Fault Major Recovered 2022-04-27 06:41:24 Replication link (link ID 1, local controller 0A, local port CTE0.A.H1, remote controller 0A, remote port CTE0.A.IOM0.P1, remote device name SEL_HANA_OCEAN2200, serial number 2102350WQW9WL9000007) is disconnected. Therefore, the remote device cannot be accessed. 5. That was impossible for 4 interface cards to have such problems at the same time. Therefore, the problem must be on the replication link. It is recommended that the customer diagnose the problem at the point of time about the quality of the replication link. 6. Moreover, for this Pair ID, there is also a speed level that is Low 7. For the speed level low, the transform speed is normally from 0 to 5 MB/s even though the customer has a 20Mbps link. We recommend Customer changes the Speed level from low to medium.\nFusionCompute Xen V100R006C10SPC101 An alarm is generated on the FusionCompute platform :ALM-15.1004006 HA Resource in a Cluster Is Going to Be Insufficient 1. Checking the HA Configuration of the Cluster. This site enabled Tolerate cluster host failures. 2. Check the configuration and quantity of VMs. The Max CPU resources is 10 (Mhz) and max memory resource is 100(GB) 3. Check the host specifications. There are eight hosts in this cluster. The total memory of each host is less than 400 GB. 4. Refer to the following document for calculation. 5. This is because automatic computing is selected. Automatic computing considers all VMs as the maximum specifications (10 vCPUs, 100 GB) of VMs on the live network. This calculation has a large error when only a small number of VMs with large specifications are used. 6. Based on the live network specifications, we can calculate that the resources of at least one host are redundant. 15 VM as 80 GB Memory flavor. Use 4 nodes Memory 2 VM as 90 GB Memory flavor. Use 0.5 nodes Memory 2 VM as 92GB Memory flavor. Use 0.5 nodes Memory 4 VM as 100GB Memory flavor. Use 0.5 nodes Memory 2 VM as 10GB Memory flavor and 2 VM as 5 Memory flavor and 2 VM as 6GB Memory flavor. Combine as 0.5 hosts Remaining resources host =8(total host)-4-0.5-0.5-0.5-0.5=2 Therefore, sufficient resources are available for VM HA after a fault occurs. Automatic computing considers all VMs as the maximum specifications.\nAfter CIFS sharing is enabled on OceanStor 5500 V5 , guest access is enabled. Windows users can access the OceanStor 5500 V5 without entering the user name and password. However, when third-party software accesses the OceanStor 5500 V5, an error message is displayed and the access fails. After the customer enters the CIFSip+ path on the software client, an error message is displayed, or the user cannot access the software. 1. Check storage logs and find errors. [smb2_create_show_error_info,5026][CSD_1] is a normal print. error code(0xc00000ba) NT_STATUS_FILE_IS_A_DIRECTORY indicates that the request initiated by the client is a file, but the file is actually a folder. This error code is returned. This is normal. 2. Check the CIFS configuration on the storage device. The guest mode has been enabled. CIFS user information is configured. Local users, domain users, and EveryOne users are configured. 3. Log in to the client software server. The software is installed on the Windows server. Test the access of the Windows server to the CIFS. It is found that the access is performed by domain users instead of anonymous access. 4. Clear the records of the Windows host on the software server, access the CIFS anonymously, and find that the system cannot access the software. Error 1272 is reported. 5. After guest authentication is not enabled on the Windows server, guest security is enabled on the storage device, and the Windows host cannot access the CIFS share in anonymous mode. The server of the software does not use anonymous access to the CIFS.\nIf Capacity Utilization on OceanStor 9000 V5 7.1.1.SPC1 is normal. Background information: 20TB of data written on daily basis. 4.4PB space used in about 90days. 90days passed and 4505/90 = 50TB (4.4 PB = 4505TB) is written on each on storage. It means 20T application data occupy about 50TB of storage space. The utilization is about 20/50=40% The redundant Ratio is 2+2 for each directory. Customer use 512KB as stripe size for each directory. It stores a huge count of images EC use (8+2) (N+M protection), 8 data, and 2 EC stored together in one stripe Analysis: Huge count of files stored in the current storage with 20 nodes. It will consume more metadata The current inode count is 7641411621 Max Inode Count is 37756179380 UsedCapacity(MB) is 4728394683 Current Average file size The current inode count is 7641411621 4728394683MB/7641411621=0.618MB=618KB Once the Average file size Max Inode Count is 37756179380 4728394683MB/37756179380=0.125MB=125KB stripes are not fully written space consume Current stripe size = 512KB Use the current average file size of 618KB as an example. Use 128KB as stripe size as an example to show how many stripes are not fully written space rough consume 618K files will be split into 4* 128K and 106K blocks. The first four 128K blocks are stored in four disks. The fifth 106K blocks is stored in the fifth disk. It will waste some storage space. The sixth to eighth have no block to store only write zero in it.\n[Problem Description] When the Veeam backup software starts backup, the system reports an incorrect account or password when logging in to the storage system. The system records an event. OceanStor 9000 version is V300R006C00SPC200 [Problem Analysis] The Veeam backup software starts backup. When logging in to the storage system, an incorrect account or password is reported. The system records events, as shown in the screenshot. 1. Analyze nas_klog logs. According to the analysis of the protocol log nas_klog, the NTLM authentication failure records of clients 192. *. *.4 at 11:30:37 are as follows (time records in the log): Log details are as follows: 2. Analyze the cmr_auth log. Analyzed the authentication process log cmr_auth (time recorded in the log). After receiving the authentication request from the client, the storage node forwards the authentication message to the domain controller. The process is as follows: 1) The storage device sends packets to the domain controller (172.*. *.12) for the first time. The packets are rejected by the domain controller because no TCP link is established. 2) After the first failure, the storage device changes the domain controller to the domain controller (172. *. *.12) to send packets. The domain controller rejects the packets because no TCP link is established. 3) After the second failure, the storage device replaces the domain controller with the domain controller (172. *. *.20) to send packets. The domain controller rejects the packets because no TCP link is established.\n4) The storage device fails to send packet requests to the domain controller for three times. Finally, the storage device returns a login failure message to the client. The extended analysis of the cmr_auth log of the authentication process (time records in the three areas of the log) shows that the authentication is successful in most of the time. According to the log analysis, there is a possibility that the access to below domain controller fails. KV-DC01 172.*. *.12 KV-DC02 172. *. *.21 KV-CA 172. *. *.20 KV-112-DC01 172. *. *.12 PP-DC01 172. *. *.12 PP-DC02 172. *. *.21 UN-DC02 192. *. *.196 3. Storage and domain controller TCP connection analysis The netstat filtering result shows that multiple TCP connections are established between cmr_auth and the domain controller. After 15 minutes, the domain controller sends a RESET packet to disconnect the current connection. After the domain controller sends RST packets, the TCP connection between the domain controller and storage device is disconnected. After the TCP connection between the storage and domain controller is disconnected, the session cache is still in place. During the next authentication, messages are sent to the domain controller based on the current session. However, no TCP connection is available, and the message fails to be sent. The login failure persists after three attempts. A login failure message is returned to the Windows client.\nNote: Because the connection between the storage and the domain controller is a TCP short connection, if the domain controller is not accessed for a short time, the domain controller will send the link disconnection packet. The storage will keep the TCP link alive after the domain controller sends the link disconnection packet. When the keepalive time expires, the corresponding tcp link is released. 1) Storage-related logs of disconnection (The following figure shows the link disconnection packets sent by the domain controller.) 2) Keepalive packets sent by the storage device are as follows: 1. LAB V300R006C00SPC200 1) Storage: V300R006C00SPC200 2) Nine DCs are configured. 3) Mount CIFS. When CIFS is mounted, the storage device establishes a TCP connection with the domain controller. 4) Observe the TCP connection. During CIFS AD domain authentication, if multiple DCs exist, multiple TCP connections are established with the domain controller. 5) TCP link disconnection After the link access expires, the domain controller delivers the link disconnection packet and records the following prints in storage logs After all TCP links are disconnected, sessions are reserved. When a Windows client sends packets through sessions, authentication fails because TCP links are not established. 2. LAB V300R006C20SPC300 1 Storage: V300R006C20SPC300 2) Nine DCs are configured . 3) Observe TCP connections. In the multi-DC scenario, only one TCP link is established. (As the optimization of the current version, TCP links will not be established with all DCs. Links will be established with domain controllers only when services accesses.)\nProblem Description: alerts in vcenter Target:s4xxxxx3.test Stateless event alarm Alarm Definition: ([Event alarm expression: LostStorage Connectivity] OR [Event alarm expression: Lost Storage Path Redundancy]OR [Event alarm expression: Degraded Storage Path Redundancy] OR [Event alarmexpression: Lost connection to NFS server]) Event details: Path redundancy to storagedevice naa.6ccbbxxxxxxxxxxxxxxxxxxxx000000b degraded. Path vmhba3:C0:T10:L1 isdown. Affected datastores:DS30. On vcenter on a hostvmhba3 is in unknown status. all paths ate dead. on a storage side this initiator is in offline status. OnSNS5384no connection with this host by WWPN Problem Analysis : Analysis on storeage side : This is the host which has this issue . On 11th Aug , all links from storage to one port of server which wwn is 10xxxxxxxxxxxxfd were disconnected . and does not recover until now . Path link down on storage is caused by RSCN receiving of port which ID is 0x32801 . Analysis on switch side 10xxxxxxxxxxxxfd port connect to internal port 11 of CX220-02 , and goes out through port 3. Large number of error codes appeared on port 3 , especially pcs err . Usually pcs err means that the link quality is not good . Link reset happened on port 3 frequently before. Port 3 of CX220-02 connected to port 40(slot 3/port 8) of switch 140 . Port 40 happened link reset on 11th Aug . And the last link reset caused TGT link down on storage . Root Cause : link quality is not good between port 3 of CX220-02 and port 40 of switch 140 .\nProblem Description: The NAS HyperMetro remote replication link frequently reports degrade alarms, mainly on the replication links of controllers 1A and 1B, from 09:00 to 22:00 every day. The following figure shows the networking on storage side: NAS active-active primary end: 210xxxxxxxxxxxx0003; backend: Dorado V3 array: 210xxxxxxxxxxxx00002 NAS active-active secondary end: 210xxxxxxxxxxx00004; backend: Dorado V3 array: 210xxxxxxxxxxxx00002 Problem Analysis : 1.Link degradation occurs on controllers 1A and 1B of the NAS engine. Check the logs of controller 1A and select one time point. 2. Checked the logs of controller 1A of the NAS engine (210xxxxxxxxxxx0003) at the corresponding time point. It was found that five times of timeout occurred within one minute, causing link degradation. If the value of abnErrNum is 0, the timeout is not caused by obvious link bit errors. timeoutNum abnErrNum Error code 14 returned from the BDM indicates that I/Os cannot be sent to the secondary end due to link timeout. The link includes the replication link between NAS engines and the processing delay of the secondary storage. can not send to remote, result 14 3. View the logs of controller 1A of the secondary NAS engine 2102352LJL9WL280004. You can find that the write timeout of file system 0x16 continuously occurred at the time when the problem occurred. It is suspected that the memory of the file system was exhausted. 0x4000000016 At the same time, you can see that data is deleted from the file system and the wait queue is full, indicating that the deletion is continuous.\n22 free 260224 wait 26 Based on the preceding analysis, it is suspected that a large number of files are deleted from the file system. As a result, disk flushing is slow. As a result, the memory of the file system is used up. As a result, the replication link is degraded due to the timeout of the write from the primary end to the secondary end of the HyperMetro pair. 4. File system 22 on the secondary storage corresponds to file system 13 on the primary storage 5. File system 13 on the primary storage device also works on controller 1A. You can see that file system 13 on the primary storage device continuously receives the truncate operation from the service side. If the file is truncate to 0, the storage device reclaims the file. The file reclamation operations on the primary storage are also synchronized to the peer storage. Therefore, it can be determined that the continuous file deletion operation on the secondary NAS engine is from the primary NAS engine and the source is the continuous truncate operation on the service. Root Cause : Based on the preceding analysis, files are frequently deleted on the service side. This operation reclaims space in the background and synchronizes it to the secondary storage. As a result, the memory of the secondary storage file system is insufficient, affecting the service write latency of the secondary file system. As a result, the replication link is frequently degraded.\nProblem Analysis : Check the performance data of the secondary storage. The controller and hard disk do not reach the bottleneck. Check the FTDS data at the secondary end. It is found that after the secondary end receives the I/O from the primary end, the average I/O data processed by the secondary end is less than 1 ms. Analyze the configuration of the synchronous remote replication pair. It is found that the I/O timeout period of the pair corresponding to the LUN with long latency is set to 10s. We have a known problem. If the remote IO timeout period is less than 15 seconds, the probability of I/O retransmission is high. A smaller value indicates a higher probability of I/O retransmission. Corresponding printing is also found in the log. In summary, the reason for the long write latency on the primary LUN is that the I/O timeout period of the secondary LUN is small. In some scenarios where I/O scheduling is slow, I/Os are frequently retried on the link when synchronous replication is written to the secondary LUN. The write delay of the secondary I/O increases, resulting in performance deterioration. You are advised to change the timeout interval of the synchronous remote replication pair on the primary site from 10s to 15s. Then, observe whether the issue is resolved. Root Cause : the reason for the long write latency on the primary LUN is that the I/O timeout period of the secondary LUN is small.\nCapture packets on the VM network port. The root cause is as follows: According to the packet analysis, some packets sent by the VM are not sent. For example, normal packets are 1, 2, 3, 4, and 5. The VM sends packets 1, 2, and 5, but packets 3 and 4 are not sent. The peer end continues to request packets 3 and 4, and the source end retransmits the packets. When we shared this conclusion to the customer, the customer did not agree with our analysis result. The customer thought that other platforms were normal. Why did this problem occur only on the HCS platform? We started a series of comparison tests. The test results are as follows: Client: VM OS type: Redhat 7.9 VM IP:10.22.70.79 Server: VM OS type: Redhat 7.9 VM IP:10.22.66.3 Test result: A large number of packets are retransmitted. Analyzed packets and found that out-of-order packets were retransmitted. Two packets with the same seq are searched for, one is out-of-order and the other is retransmitted. This packet indicates that the packet is out of order and then retransmitted after the RTO is reached. As a result, the number of Retry increases during the Iperf test. Client: VM OS type: CentOS 7.6 VM IP:10.22.67.75 Server: VM Name: az1-ecs-iperf-test1 VM OS type: CentOS 7.6 VM IP:10.22.66.58 When Iperf3 traffic is sent for CentOS 7.6 VMs, capture packets on the OS VM eth0, host VM virtual NIC, host trunk2, and host eth4/5 ports.\nIt impacts data migration when the free memory of the NOFS is low. The customer reported that the free memory of the nofs.6 memory pool was too small (12 MB). 1. Check the tool log to confirm that the NOFS memory of node2 is insufficient. 2. After the NOFS workaround tool is run, the NOFS6 memory pool enters the active elimination process. During active elimination, cached NOFS metadata needs to be eliminated to SSDC. However, the SSDC cache module reports an error, causing the elimination failure. 3. Analyzed SSDC-related threads. It was found that the thread failed to apply for memory, causing the thread to be suspended. 4. Check the memory pool information related to SSDC. The free area part of the ssdc.1 memory is empty, and all memory is applied for by the Slab cache. The memory of the SSDC module has been exhausted. 5. Traced the SSDC error and found that the SSDC memory was insufficient . 6. A large number of read I/Os were generated on the storage disk before and after the corresponding time point. 7. Check whether the consistency scanning task is performed during the log operation. 8. There is a consistency scanning task during the period, and the task is in the deep scanning task. In-depth scan will issue a large number of read requests to disks, increasing the memory usage of the NOFS module and causing the SSDC cache to be used. 9.\nA large number of 16 KB/32 KB small strip files and directories exist in OceanStor 9000, and large files exist in the corresponding directories (the file size exceeds 1 GB, as shown in the following figure) on the DeviceManager. A large number of 16 KB or 32 KB small strip files and directories exist in the storage system, and large files are stored in the directories. In the media asset scenario, the default strip is 1024 KB. Large files on the live network are stored in the 16 KB strip directory. As a result, the same file size needs to be split into more logical objects. During the period, a deep consistency scan task was performed, causing high disk read pressure. During the scanning, all data needs to be scanned for consistency check. As a result, a large amount of metadata is cached to the SSDC module. As a result, the memory of the SSDC module is exhausted and threads are suspended. The SSDC thread is suspended. As a result, the NOFS memory is released to the SSDC memory. As a result, the remaining NOFS memory is insufficient, affecting read and write services. After a 16 KB strip is configured for a directory (the default strip is 1024 KB in media asset scenarios), the frontend service is added with a storage background scanning task. As a result, the SSDC memory is quickly exhausted, and the NOFS memory cannot be released to the SSDC memory.\nProblem Description : Copy Interruption issue happened Problem Analysis : 1. Analyzed storage performance logs. It was found that the return delay of the DS2 node (internal storage logical ID) was long. 2. According to the map information, the back-end IP address of the DS2 node is xx.xx.xx.6, which belongs to node 3. 3. Analyze the logs and confirm that the free memory of the NOFS node3 at the corresponding time point is insufficient. 4. Further analyze the logs and confirm that the memory of the NOFS fails to be eliminated due to insufficient SSDC memory. Traced the SSDC error and found that the remaining memory was low . 5. A large number of read I/Os are detected on the disk at the corresponding time point. 6. Search for the log time point and find that consistency scanning tasks exist during the log operation records. 7. After a 16 KB strip is set for the directory, foreground services are added with the storage and background scanning task. As a result, a large amount of metadata is cached to the SSDC module, and the memory is quickly consumed. The NOFS memory cannot be released to the lower-level SSDC memory. As a result, the remaining NOFS memory is insufficient, affecting read and write services. Root Cause : After a 16 KB strip is configured for a directory (the default strip is 1024 KB in media asset scenarios), the frontend service is added with a storage background scanning task.\nHello all, Let's see the logical topology comparison between the SCSI protocol and the NVMe protocol. SCSI (Small Computer System Interface) and NVMe (Non-Volatile Memory Express) are both interfaces used for connecting storage devices to a computer or server, but they differ in several ways. Speed : NVMe is significantly faster than SCSI. NVMe is designed to take advantage of the low latency and high bandwidth of NAND-based flash storage, providing read and write speeds that are several times faster than those of SCSI. Protocol : NVMe is a newer protocol designed specifically for flash-based storage devices, while SCSI is a more traditional protocol that has been in use since the 1980s. NVMe is designed to take full advantage of the parallelism and high performance of modern solid-state drives (SSDs). Queue depth : NVMe has a much higher queue depth than SCSI. This means that it can handle a larger number of input/output (I/O) requests at once, making it better suited for applications that require high I/O throughput, such as virtualization, database applications, and data analytics. Compatibility : SCSI is more widely used and has greater compatibility with older systems and devices, while NVMe is a newer technology that may not be compatible with older systems. In summary, while SCSI has been a reliable and widely used interface for storage devices for many years, NVMe is designed specifically for high-performance flash-based storage and offers significantly faster speeds, higher queue depth, and better performance for applications that require high I/O throughput.\nWhen the 5 V power supply for the backplane on the front panel of a 5288 V3 36-slot Nodes become faulty, half of the service disks (numbered 0 to 11 or 12 to 23) on the front-panel go offline. When the 5 V power supply for the backplane on the rear panel of a 5288 V3 36-slot Nodes become faulty, service disks numbered 24 to 35 on the rear plane go offline. Disk slots on the front panel of a 5288 V3 36-slot Nodes Disk slots on the rear panel of a 5288 V3 36-slot Nodes Possible cause 1: 5 V power supply for the backplane is faulty. Step 1 Possible cause 1: 5 V power supply for the backplane is faulty. 1. If the fault symptom is similar to that described in Symptom , replace the faulty components. If a RH2288 V3 12-slot Nodes node is faulty, see section Replacing a Service Disk Backplane of the relevant node type in the to replace the service disk backplane. If a 5288 V3 36-slot Nodes is faulty because the service disks on the front panel are faulty, see section Replacing a Service Disk Backplane of the relevant node type in the to replace the service disk backplane. If a 5288 V3 36-slot Nodes is faulty because the service disks on the rear panel are faulty, see section Replacing a Storage Node of the relevant node type in the to replace the node. 2. Check whether indicators of all faulty service disks are on.\n[Problem Description] The customer reported that the read performance of the storage was slow. The bandwidth for copying a file from the shared directory of OceanStor 9000 to the local disk of the VM was about 40 MB/s, which was not 150 MB/s. On May 19, after N+M was enabled and preread parameters were adjusted, the bandwidth was changed to 100 MB/s. Note: Before N+M is enabled, N copies of data are read internally. Data can be read only after N copies are returned. After N+M is enabled, N+M copies of data are read. [Problem Analysis] 1, The client reads files from the shared directory of the OceanStor 9000 to the local disk. The bandwidth is about 100 MB/s. No timeout log is printed on the storage side. The workload log on May 19 shows that the average read delay of a single flow is 20-50 ms, and the maximum read delay is 500 ms. The delay is normal, and no storage exception is found. The parameters after Report are expressed as follows: I/O Block Size (KB), File Size (KB), and Average Bandwidth (KB/s).Command parameter description: -r 1 m indicates that the I/O size is 1 MB, and s 16g indicates that a single file is 16 GB (more than 4 GB of client memory, avoiding file caching on client). -i 0 indicates that the write and re-write are tested. -C indicates that the test result is directly displayed.\n-I indicates that the cache is not written f Z:\est\est.tmp indicates that the test file is iztest.tmp (the path is the shared directory of 9000). -b E:\emp\est\\iztest02.xls indicates that the test result is saved separately. 3, On the OceanStor 9000 GUI, the storage performance statistics show that the read/write bandwidth exceeds 300 MB/s. Therefore, the test result is not client cache. This indicates that the read/write capability of the OceanStor 9000 storage exceeds 150 MB/s. Therefore, the performance bottleneck is not caused by the OceanStor 9000 storage. 1, The client reads files from the shared directory of the OceanStor 9000 to the local disk. The bandwidth is about 100 MB/s. No timeout log is printed on the storage side. The workload log on May 19 shows that the average read delay of a single flow is 20-50 ms, and the maximum read delay is 500 ms. The delay is normal, and no storage exception is found. 2, To determine whether the local disk is a bottleneck, The test command is as follows: iozone.exe -r 1 m -s 8g -i 0 -C -I -f E:\emp\est\\iztest.tmp -b E:\emp\est\\iztest02.xls Command parameter description: -r 1 m indicates that the I/O size is 1 MB, and s 8g indicates that a single file is 8 GB (more than 4 GB of client system memory, avoid caching on the client). -i 0 indicates that only write and re-write are tested. -C indicates that the test result is directly displayed. -I indicates that the cache is not written.\nProtection Capacity is different: 1099TB VS 158GB Subscribed Capacity is different: 121.199TB VS 105.599TB Line 9: // Product Serial Number: 210***B000001 Line 411: // Storage Pool Protection Capacity: 1194328064(KB) Line 439: // ID: 0 Line 443: // Capacity: 1288490188(KB) Line 446: // Protection Capacity: 1048576(KB) Line 459: // Snapshot ID(s): 6,14,16,17,33,54,67,68,80,94 Line 527: // ID: 1 Line 531: // Capacity: 1288490188(KB) Line 534: // Protection Capacity: 327155712(KB) Line 547: // Snapshot ID(s): 15,18,36,49,60,79,85,96 Line 615: // ID: 2 Line 619: // Capacity: 1288490188(KB) Line 622: // Protection Capacity: 1048576(KB) Line 635: // Snapshot ID(s): 19,30,37,44,57,73,82,103 Line 703: // ID: 3 Line 707: // Capacity: 1288490188(KB) Line 710: // Protection Capacity: 1048576(KB) Line 723: // Snapshot ID(s): 20,35,48,63,75,90,95,106 Line 791: // ID: 4 Line 795: // Capacity: 1288490188(KB) Line 798: // Protection Capacity: 1048576(KB) Line 811: // Snapshot ID(s): 21,32,53,59,72,88,97,104 Line 879: // ID: 5 Line 883: // Capacity: 1288490188(KB) Line 886: // Protection Capacity: 1048576(KB) Line 899: // Snapshot ID(s): 22,40,47,58,69,81,92,109 Line 967: // ID: 8 Line 971: // Capacity: 1073741824(KB) Line 974: // Protection Capacity: 24117248(KB) Line 987: // Snapshot ID(s): 7,23,43,46,62,76,86,100 Line 1055: // ID: 9 Line 1059: // Capacity: 1073741824(KB) Line 1062: // Protection Capacity: 813694976(KB) Line 1075: // Snapshot ID(s): 24,29,39,55,65,70,91,101 Line 1143: // ID: 10 Line 1147: // Capacity: 1073741824(KB) Line 1150: // Protection Capacity: 17825792(KB) Line 1163: // Snapshot ID(s): 25,34,52,56,71,87,99,105 Line 1231: // ID: 11 Line 1235: // Capacity: 1073741824(KB) Line 1238: // Protection Capacity: 2097152(KB) Line 1251: // Snapshot ID(s): 26,31,38,45,64,78,83,93 Line 1319: // ID: 12 Line 1323: // Capacity: 1073741824(KB) Line 1326: // Protection Capacity: 2097152(KB) Line 1339: // Snapshot ID(s): 27,42,51,61,77,84,102,107 Line 1407: // ID: 13 Line 1411: // Capacity: 1073741824(KB) Line 1414: // Protection Capacity: 2097152(KB) Line 1427: // Snapshot ID(s): 28,41,50,66,74,89,98,108 Line 9: // Product Serial Number: 210***B000002 Line 411: // Storage Pool Protection Capacity: 180355072(KB) Line 439: // ID: 0 Line 443: // Capacity: 1288490188(KB) Line 446: // Protection Capacity: 1048576(KB) Line 459: // Snapshot ID(s): 13,28,34,46,67,73,90 Line 527: // ID: 1 Line 531: // Capacity: 1288490188(KB) Line 534: // Protection Capacity: 42991616(KB) Line 547: // Snapshot ID(s): 15,31,41,51,64,77,80 Line 615: // ID: 2 Line 619: // Capacity: 1288490188(KB) Line 622: // Protection Capacity: 1048576(KB) Line 635: // Snapshot ID(s): 23,39,53,65,75,87,95 Line 703: // ID: 3 Line 707: // Capacity: 1288490188(KB) Line 710: // Protection Capacity: 1048576(KB) Line 723: // Snapshot ID(s): 17,29,42,44,61,71,83 Line 791: // ID: 4 Line 795: // Capacity: 1288490188(KB) Line 798: // Protection Capacity: 1048576(KB) Line 811: // Snapshot ID(s): 19,27,32,49,62,69,85 Line 879: // ID: 5 Line 883: // Capacity: 1288490188(KB) Line 886: // Protection Capacity: 1048576(KB) Line 899: // Snapshot ID(s): 21,43,50,66,74,89,94 Line 967: // ID: 6 Line 971: // Capacity: 1073741824(KB) Line 974: // Protection Capacity: 7340032(KB) Line 987: // Snapshot ID(s): 14,25,40,54,59,78,88 Line 1055: // ID: 7 Line 1059: // Capacity: 1073741824(KB) Line 1062: // Protection Capacity: 114294784(KB) Line 1075: // Snapshot ID(s): 12,22,37,48,56,72,84 Line 1143: // ID: 8 Line 1147: // Capacity: 1073741824(KB) Line 1150: // Protection Capacity: 7340032(KB) Line 1163: // Snapshot ID(s): 16,30,33,52,57,68,81 Line 1231: // ID: 9 Line 1235: // Capacity: 1073741824(KB) Line 1238: // Protection Capacity: 1048576(KB) Line 1251: // Snapshot ID(s): 20,38,47,58,76,91,92 Line 1319: // ID: 10 Line 1323: // Capacity: 1073741824(KB) Line 1326: // Protection Capacity: 1048576(KB) Line 1339: // Snapshot ID(s): 18,26,36,55,60,79,86 Line 1407: // ID: 11 Line 1411: // Capacity: 1073741824(KB) Line 1414: // Protection Capacity: 1048576(KB) Line 1427: // Snapshot ID(s): 24,35,45,63,70,82,93 (1) Protection Capacity: it is based on changed data after make snapshot, due to different of the active time and snapshot number.\nAccording to the customer feedback at the NUS CBI site, when the ImageJ software is used to read images in the Samba directory on the Windows 10 client, the images stored in the Samba directory are longer than those stored in the DDN. The cause is that the OceanStor 100D does not support the oplock feature. As a result, each read I/O request of ImageJ is forwarded to the storage side. As a result, the number of I/O requests increases and the network duration increases, resulting in slow image loading. For this problem, we have two sets of solutions. 1. Complete the development of this feature as soon as possible. 2. Provide temporary workarounds. Explore and verify the following solutions: [Solution 1] The customer uses the Linux (ImageJ) + NFS access mode. [Solution 2] The customer uses the local directory as the transit. Before the tool is opened, the corresponding image is copied to the local computer. After the tool is used, the solution has been verified in the lab, and the image reading and storage duration remains unchanged. The customer has rejected this solution. [Solution 3] One or two Linux servers are used as the transit. The Pacific shared directory is mapped to the Linux server through NFS. Then, the NFS server service is enabled on the Linux server and the shared directory is mapped to the Windows server. The verification is infeasible. [Solution 4]: The client is mounted in Windows+NFS+nolock mode.\nA file-sharing service mainly makes sure that customers can properly transfer various files to multiple people at once. An Internet or cloud service provider that hosts a number of storage servers and application-sharing software is typically referred to as a file-sharing service. A file-sharing service functions by utilizing both cloud storage options and application sharing. The user chooses the file to be shared utilizing online files. A file access URL can be used to access the file once it has been uploaded to the storage servers. File-sharing systems track document revisions to guarantee that the file is delivered safely and without being changed or copied unlawfully. Large file transfers are also supported by these platforms, something that email typically cannot do. File sharing can be done using several methods. The most common techniques for file storage, distribution and transmission include the following: Removable storage devices Centralized file hosting server installations on networks World Wide Web-oriented hyperlinked documents Distributed peer-to-peer networks There are countless file sharing varieties, but most of the file sharing systems available fall into one of two categories, these are operating system file sharing or internet file sharing. Most contemporary operating systems come with built-in file sharing features. Windows, as an illustration, facilitates file sharing utilizing server message blocks (SMB). A folder on Microsoft's Resilient File System (ReFS) can be shared by an administrator, making it available via the SMB protocol.\nThere are several types of internet file sharing, which each serve a specific purpose, but the most common include the following: Peer-to-peer (P2P) file sharing is a consumer-level technology in which each participant's PC acts as a client in a much larger file sharing network. When a participant downloads files from this network, the P2P software identifies where the data resides and then facilitates the download process. While there are legitimate uses for P2P networks, they have gained a reputation for being used primarily for the distribution of pirated media. Enterprise file sync and share services have gained popularity for remote work because they enable files to be saved in the cloud and accessed via a desktop or mobile device. If the user updates a file or creates a new file while working offline, that file is automatically synchronized to the main storage repository the next time the user is online. These storage services retain data in a centralized location where the organization can properly secure and back it up. Any data downloaded to or created on an end user's device should be stored in a special encrypted folder (often referred to as a vault) to keep the data from being compromised if a device is lost or stolen. Portal websites , such as Microsoft's SharePoint Online, allow users to share files and folders with co-workers and, in some cases, people residing outside of the organization via a shared link.\nThese portal-based file sharing services provide real-time, collaborative access through a web browser or mobile app, which means users can access files from anywhere, using nearly any device. An administrator creates a folder and gives the necessary users access to it in order to arrange files in a corporate file sharing platform. To do this, you typically need to create one or more groups and then include those groups in the access control list for the folder. As appropriate, the administrator can use this to establish read/write access to apply to specific users or entire groups. When users have access to folders, they can then access files within those directories. Files may be hosted on a physical server or on the cloud, or they may be kept on another computer. Users open the files they have access to on their local desktops by clicking on them. P2P file sharing, in which each computer acts as a client in a wider network, enables access to files via another computer. On the other side, server-based file sharing is a result of the File Transfer Protocol (FTP), where access can be limited depending on the permissions specified by the administrator. Normally, a user can open a stored file, see it, modify it, save their changes, and the file will reopen with those changes applied. The most recent changes to a shared folder or file will be visible to all users who have access to it.\nFusionCube fails to be deployed, and a message is displayed indicating that the management IP address and host name fail to be configured. According to logs, the IPv6 address of the storage node fails to be obtained. After consulting R&D engineers, the instructions for enabling IPv6 on the ESXi host are obtained. After IPv6 is enabled, the installation is successful. FusionCube supports automatic device discovery during system installation, initialization, and capacity expansion. Automatic device discovery is implemented through the Simple Service Discovery Protocol (SSDP) service. The process of automatically discovering SSDP devices during FCB installation is as follows: 1. The SSDP protocol is embedded in the BMC or MM of the server used by FusionCube. After the server is powered on, SSDP messages are automatically broadcast using an IPv6 address. 2. The SSDP server is deployed in FCB to collect the IPv6 addresses of the corresponding devices through SSDP messages in the **IPv6 broadcast domain. 3. Log in to the device using an IPv6 address to obtain detailed information about the device. The process of automatically discovering SSDP devices during initialization and capacity expansion is as follows: 1. After the system is installed, the SSDP client is embedded in the management VM, CVM, and host operating system. The SSDP client automatically broadcasts SSDP messages using IPv6 addresses. 2. An SSDP server is deployed in the FCC to collect the IPv6 addresses of devices in SSDP messages in the IPv6 broadcast domain of the **management plane. 3.\nOn September 12, 2022, the customer reported that a single remote replication pair of the OceanStor 9000 was disconnected abnormally. A pair interruption alarm was frequently reported, and the alarm was cleared 15 minutes later. he direct cause of the pair disconnection is that the Staff_Feed_Logs_02_09_2022.log file does not exist. As a result, file writing fails. Therefore, the pair synchronization is interrupted. 2. According to the snapshot information, the synchronization start time is 2022-09-12 06:44:19. There are two snapshot ranges, 32355 to 32358 and 32358 to 32361. In addition, user snapshots in the cluster coexist. Note: 32355, 32358, and 32361 are snapshot IDs. 3. The snapshot on the primary end shows that the file was created at 06:40:02 on 2022-09-12, indicating that the file was created between snapshots 32358 and 32361, and the corresponding track record is recorded in snapshot 32358. 5. The three track records are parsed into three different operations during remote replication synchronization. Snapshots 32355 and 32358 will be compared with the final consistency snapshot 32361: Track records 1 and 2 resolve to delete the file because the file does not exist in the final snapshot 32361 (old fid: 32146966540). Track record 3 resolves to create the file because the file (new fid: 32147054092) is not present in snapshots 32358 and 32355, but is present in final snapshot 32361. 6.\nIf the system alarm indicator is red on or blinks red, exceptions occur on the system. Symptom Symptom 1: The system alarm indicator on the front panel of the enclosure is red on. Symptom 2: The system alarm indicator on the front panel of the enclosure blinks red. Position of the health status indicator on a 5288 V3 36-slot Nodes Possible Causes Symptom 1: The system alarm indicator on the front panel of the enclosure is red on. Possible cause 1: The air intake and exhaust vents of the enclosure overheat. Possible cause 2: The CPU overheats. Possible cause 3: One or more fans are absent or faulty. Symptom 2: The system alarm indicator on the front panel of the enclosure blinks red. Possible cause: The power supply is absent or faulty. Flowchart for troubleshooting system failures (1) Flowchart for troubleshooting system failures (2) Procedure Symptom 1: The system alarm indicator on the front panel of the enclosure is red on. a. Possible cause 1: The air intake and exhaust vents of the enclosure overheat. i. Check whether the temperature of the equipment room where the device resides exceeds 35C. -If yes, contact the customer to handle. -If no, go to a.ii . ii. Log in to the management console and click Alarm. On the Current Alarms page, check whether there are fan alarms. -If yes, go to Possible cause 3 . -If no, go to . b. Possible cause 2: The CPU overheats.\ni. Log in to the DeviceManager and check whether CPU temperature is too high. -If yes, go to b.ii . -If no, go to Possible cause 3 . ii. Open the cover of the plug-in frame of the main control panel. iii. Take out the air director. iv. Screw the CPU heat radiator and install the air director. v. Connect power cables to power on the node. Check whether the system alarm indicator is off. -If yes, no further operations are required. -If no, go to Possible cause 3 c. Possible cause 3: One or more fans are absent or faulty. i. Log in to the DeviceManager. In the middle of the DeviceManager, click the Device icon, to go to the Device View. ii. Click the device name, and then the device on the right and enlarge or rotate it based on your requirements. Then, click the fan. iii. Check whether the fan works correctly. -If yes, contact technical support engineers. -If no, go to c.iv . iv. Open the cover of the plug-in frame of the main control panel. v. Check whether all of the five fan indicators on the mainboard are green on. -If yes, no further operations are required. -If no, contact technical support engineers. Symptom 2: The system alarm indicator on the front panel of the enclosure blinks red. a. The power supply is absent or faulty. i. Check whether the power running/alarm indicator is red on. -If yes, an alarm is generated on the power supply and needs to be cleared.\nWhat is SAN and what is function of SAN Dear Anabiya, A storage area network (SAN) is a high-speed storage network that is independent from the server network system, and obtains large bandwidth and high transmission efficiency. From the SAN concept to its current development, the SAN experiences ongoing development phases, including storage pool sharing, file system sharing, multi-servers platform connection, sharing of file systems on heterogeneous servers, and block-level sharing. In earlier stages, most SANs use Fibre Channel switches to connect storage systems and application servers, which causes a misunderstanding that a SAN is a Fibre Channel network that can only use the Fibre Channel protocol. Actually, SAN is a dedicated storage network architecture and is irrelevant to protocols and device types. As GE and 10GE networks become popular, SAN is classified as an FC SAN or an IP SAN. Use of SAN Storage Data sharing Centralized deployment of storage systems allows application servers to store and share data in a cost-effective way. Storage sharing Multiple application servers share a storage system. The storage system can be logically divided into multiple parts with each part connected to a specific server for storage sharing. Data backup Data backup uses an independent network, achieving centralized backup for data across heterogeneous servers. Disaster recovery SAN employs multiple mechanisms to achieve automatic, hot data backup, allowing immediate data recovery in the event of a disaster.\nBuilding computer clustered network rendering is a kind of enhancement of rendering technology, can improve the efficiency of virtual reality art creation, but the complex environment formed solely by technology will not make people fit into it. For art creation, works can't just advocate the play of technology alone. It should also be considered that the application to actual artistic creation is the integration of technology and art. Technology often gives people the impression that is rational, indifferent and objective. Art is perceptual, subjective and intimate. Art brings people the enjoyment of beauty, it depicts the subjective beautiful life image for us. Technology can promote the development of society and help people create things that people need with purpose. Art can show the richness and diversity of the world, and can create subjectivity, individuality and emotion in our world. The advantages of technology and art should be exerted together in the field of virtual reality. At present, the realization of most virtual reality art works is based on the rendering platform for output services, which realizes the creation of a virtual environment and the interaction of artistic creation in this situation. In this process, it is necessary to communicate and interact with people whether it is virtual or conjectural. This is also the significance of combining the establishment of computer cluster network rendering with virtual reality art creation.\nAt present, many art majors involve the creation of virtual reality art forms, such as environmental art major, through the design and construction of the virtual environment, the audience can actively feel the designer's thinking and viewing effect; Industrial design art major can intuitively show the overall effect of the work to be modified at any time; Visual communication art major, can intuitively show the visual work presentation effect; Digital media art major, through the interactivity of virtual reality art creation, can further make the works and audience interact, better understand the creator's thinking; Animation art major, can create virtual scenes to further display their work. Virtual reality art is a new form of art, and cluster network rendering technology is an objective existence. Since virtual reality art is to create a virtual environment for the audience to feel and experience, it must use the corresponding technical means as the way of its creation realization. Art and technology are not two separate forms, the fusion of technology and art is very important. When virtual reality creates artworks, we should bring technology into it appropriately. The fusion of art and technology is the most remarkable feature of virtual reality art creation. Cluster network rendering can quickly and perfectly transform static art into dynamic art that the audience can explore, so that the audience can better appreciate and understand the author's artistic thought.\nDear All, Today we are going to learn about Bcmanger eReplication Introduction to eReplication eReplication is a piece of DR management software designed for Huawei DR solutions, With the incorporated technologies such as replication, snapshot, active-active, clone, and FusionSphere host-based replication, eReplication enables visualized, streamlined, easy-to-do management and monitoring operations. Local protection Local high availability Active-passive DR Active-active DR DR (geo-redundant mode) Service Integration Automatically identifies VMs and applications Automatically identifies array configurations of the corresponding application Automatically identifies link configurations between sites Automatically starts applications after a DR switchover Service Integration Template-Based Policy Configuration DR Visualization Automatic Grouping Working principle If vCenter Server is discovered in eReplication, datastore will be grouped based on the following rules: For VMs crossing multiple datastore, the multiple datastore are automatically combined into a group. For LUNs (in one consistency group) that belong to multiple datastore, the multiple datastore are combined into a group. Datastore are grouped based on VMs and the relationship between their datastore. If a VM on a datastore changes or the capacity of a VM needs to be expanded, the groups are automatically updated. Grouping is automatically executed and changes are automatically updated. VM Consistency Huawei OceanStor eReplication provides policy-based automatic protection for VMware. To implement policy-based automatic protection, you must create protected groups for your applications on the OceanStor eReplication Server. After the protected groups are created, OceanStor eReplication will automatically implement protection based on the policy (creating snapshots or executing replication every 30 minutes) that you have configured.\nVM Consistency Protection Consistency protection for VMs is implemented by using vCenter to generate silent snapshots of VMs (when silent snapshots are generated, VMs will be suspended and data in the VM memory will be flushed to disks). After silent snapshots are generated, storage array replication is executed or snapshots are created. After replication is started, eReplication informs vCenter Server of deleting the silent snapshots of VMs. One-Click Testing and Switchover Working principle eReplication enables users to develop and configure different switchover and testing processes based on their applications and protection technologies. A process can be automatically executed through procedures that are loaded, explained, and set by a built-in recovery process engine module. Users do not need to execute complex configuration and operations. Technical characteristics The existing switchover and testing procedures can be compiled to meet different requirements One-Click DR Testing and Environment Clearing Key points Storage snapshots utilized, without affecting storage replication Test results exported, and records cleared Environments restored after DR drill The whole process exerts no impacts on the production environment. Advantages Discovers and solves hidden problems. Checks whether the RPO and RTO requirements can be met. Reduces costs and resource demands of DR testing and supports drills at any time. Helps customers build confidence in their customized recovery plans. One-Click Fault Recovery (Remote DR) Key points Site faults automatically detected Fault recovery plans manually started by users. Users' customized recovery plans automatically executed Other operations are automatically executed. Advantages Ensures rapid and predictable fault recovery to meet service requirements.\nDear All, Today we are going to learn about Storage Scale-up and Scale-out Architecture Scale-up and Scale-out Service data continues to increase with the continued development of enterprise information systems and the ever-expanding scale of services. The initial configuration of storage systems is often not enough to meet these demands. Storage system capacity expansion has become a major concern of system administrators. There are two capacity expansion methods: scale-up and scale-out. The following uses Huawei storage products as an example to describe the two methods Scale-up This traditional vertical expansion architecture continuously adds storage disks into the existing storage systems to meet demands. Advantage: simple operation at the initial stage Disadvantage: As the storage system scale increases, resource increase reaches a bottleneck. Scale-out This horizontal expansion architecture adds controllers to meet demands. Advantage: As the scale increases, the unit price decreases and efficiency is improved. Disadvantage: The complexity of software and management increases SAS Disk Enclosure Scale-up Networking Principles Huawei SAS disk enclosure is used as an example. Port consistency: In a loop, the EXP port of an upper-level disk enclosure is connected to the PRI port of a lower-level disk enclosure. Dual-plane networking: Expansion module A connects to controller A, while expansion module B connects to controller B. Symmetric networking: On controllers A and B, symmetric ports and slots are connected to the same disk enclosure. Forward and backward connection networking: Expansion module A uses forward connection, while expansion module B uses backward connection.\nCascading depth: The number of cascaded disk enclosures in a loop cannot exceed the upper limit. Smart Disk Enclosure Scale-up Networking Principles Huawei smart disk enclosure is used as an example. Port consistency: In a loop, the EXP (P1) port of an upper-level disk enclosure is connected to the PRI (P0) port of a lower-level disk enclosure. Dual-plane networking: Expansion module A connects to controller A, while expansion module B connects to controller B. Symmetric networking: On controllers A and B, symmetric ports and slots are connected to the same disk enclosure. Forward connection networking: Both expansion modules A and B use forward connection. Cascading depth: The number of cascaded disk enclosures in a loop cannot exceed the upper limit. PCIe Scale-out and IP Scale-out PCIe scale-out runs on the PCIe protocol while IP scale-out works based on the IP protocol. IP scale-out is used for Huawei OceanStor V3 and V5 entry-level and mid-range series, Huawei OceanStor V5 Kunpeng series, and Huawei OceanStor Dorado V6 series. IP scale-out integrates TCP/IP, Remote Direct Memory Access (RDMA), and Internet Wide Area RDMA Protocol (iWARP) to implement service switching between controllers, which complies with the all-IP trend of the data center network. PCIe scale-out is used for Huawei OceanStor 18000 V3 and V5 series, and Huawei OceanStor Dorado V3 series. PCIe scale-out integrates PCIe channels and the RDMA technology to implement service switching between controllers. Scale-out Technologies Used by Huawei Storage Systems PCIe scale-out: features high bandwidth and low latency.\nNon-volatile Memory Express (NVMe) is a controller interface standard developed for systems using PCI Express SSDs. (including optimized controller register interfaces, command sets, and I/O queue management) Standardize the communication connection and data transmission between the SSD controller and the operating system. How to control the storage to meet upper-layer services is not defined. NVMe is a standard protocol developed to implement high-performance access to flash media. It is used to access PCIe flash SSDs and gradually evolves to remote access based on other networks. The NVMe protocol has the following features: The protocol is simple, eliminating the burden of historical SCSI compatibility. Supports multiple queues. A maximum of 65535 I/O queues are supported. Each I/O queue supports 64K concurrent I/Os. Supports priority queues. Linux kernel block devices support multi-queue. NVMe devices use multi-queue and multi-core CPU software lock-free mechanism to achieve high performance. The NVMe driver is simpler than the SCSI driver. Back-end access disks do not pass through the SCSI layer or SAS HBA. Compared with SCSI, NVMe benefits mainly from host drivers, which enable high performance of multi-core and multi-queue. NVMe over Fabrics uses the Fabric network to carry the NVMe protocol, enabling high-performance and low-latency remote access to SSDs. Fabrics can be InfiniBand, Ethernet (RoCE and iWARP), or Fibre Channel. Currently, they are released based on RDMA and Fibre Channel. TCP standards. The NVMe over Fabrics protocol model inherits the NVMe over PCIe model, including the subsystem, controller, and namespace.\nDear all, Have a good day! This case shows how to downgrade the firmware for Oceanstor Dorado 5000 V6. The customer has Dorado 5000 V6 for POC. The customer has upgraded the firmware version from 6.0.0 to 6.0.1 by himself inrolling upgrade scenario. Now the customer wants to downgrade the firmware version back to 6.0.0 and perform firmware upgrade again to look for some more metrics while installing firmware upgrade. Its unusual behavior to downgrade from 6.0.1 to 6.0.0. Its a big version change between 6.0.1 and 6.0.0. It's bad things to downgrade when the storage has data. If no data in storage you can try using the upgrade process (install 6.0.0) to downgrade from 6.0.1 to 6.0.0. Its the same method to downgrade and upgrade firmware. You can refer OceanStor Dorado V6 SeriesStorage Systems 6.0.1 Fast Upgrade Guide to downgrade the firmware from 6.0.1 to 6.0.0. The link as below: You can use cmd (create upgrade session) to upgrade firmware too. The create upgrade session command is used to upgrade the software version or software version patch of a controller. Run this command if you need to upgrade the system software package of the storage system. Please pay attention to the Usage Guidelines This command can only be executed on disk arrays. The prerequisites for running this command are that the FTP service or SFTP service on the server is enabled and the server is accessible to the storage system.\nDear all, This post shows you how to compress Video in Windows 10. Windows10 comes with a simple app called Video Editor that allows you to make simplevideos. It's a useful way to quickly compress video in Windows. Anotheradvantage is that it exports a copy of your project, instead of destroying yourfiles, so you can go back and try again. 1.Click or tap on theVideo Editorshortcutfrom theStartMenu, or look for it using the search from your taskbar. TheVideo Editor shortcut from Windows 10's Start Menu 2. Push the\"New video project\"button. TheNew video project button 3. Choose a name for the new video that you're going to create, andpressOK. Choosinga name for the video 4. Drag and drop the video that you want to make smaller, onto theVideoEditorwindow. Alternatively, you can also press theAddbuttonand select the video. Dragand drop the video onto the Video Editor window 5. In theVideo Editorwindow,drag the video from theProject libraryanddrop it on the storyboard from the bottom. Dragand drop the video from the Project library onto the storyboard 6. And now comes the essential part of the \"making the videosmaller\" process: click or tap on the\"Finish video\"buttonfrom the top-right corner of theVideo Editorwindow. TheFinish video button 7. VideoEditoropens a dialogue in which it asks you about the\"Videoquality.\"Click or tap on the preselected option, whichshould be\"High 1080p (recommended).\" TheVideo quality setting 8. To make the video smaller, selectMedium 720porLow540p. TheMedium 720pmakesthe video smaller while not losing too much of its quality in the process.\nNeed to increase SAN (OceanStore 2200 V3) LUN Size 1- Use pre expansion check first to evaluate before expanding LUN if needed : Procedure Log in to DeviceManager as the super administrator. Make sure that the storage environment meets the expansion requirement. In the Basic Information area on the home page, check the device status and total capacity. Ensure that the storage system runs properly and has sufficient storage space. If the device status is Fault , contact Huawei technical engineers to locate and troubleshoot the problem. Start the expansion after the fault is rectified. In the Alarms area, check current alarm information. Click Show All . The Alarms and Events page is displayed, listing all current alarms. If there are alarms related to the disk domain, storage pool, or LUN to be expanded, follow instructions in the Suggestion to handle the alarms. These alarms include Storage Pool Is Degraded , Disk Domain Is Faulty , and LUN Is Faulty . On DeviceManager, confirm and record the host corresponding to the application server, the LUN to be expanded, and the LUN's owning storage pool. If the LUN WWN or host LUN ID is not displayed in the Mapped LUNs area, click and choose WWN or Host LUN ID from the drop-down list. The LUN WWN or host LUN ID is displayed. On the navigation bar of DeviceManager, click Provisioning . The Provisioning page is displayed. Click Host . The Host page is displayed.\nIntroduction to SmartTier Block data tiering: Based on the performance requirements of applications, SmartTier separates SSDs, SAS disks, and NL-SAS disks into the high- performance, performance, and capacity storage tiers. Each tier can be used independently, or two or three tiers can be combined to provide data storage space. SmartTier goes through three phases: data monitoring, data placement analysis, and data relocation. Data monitoring and data placement analysis are automated by the storage system, and data relocation is initiated manually or by a user-defined policy. SmartTier improves storage system performance and reduces storage costs to meet enterprises' requirements for both performance and capacities. By preventing historical data from occupying expensive storage media, SmartTier ensures effective investment and eliminates energy consumption caused by useless capacities, reducing the TCO and optimizing the cost-effectiveness. File data tiering: A storage pool may be composed of multiple media, such as SSDs and HDDs. SmartTier automatically promotes files to high-performance media (SSDs) and demote files to large-capacity media (HDDs, including SAS and NL-SAS disks) based on user-configured tiering policies. Users can specify tiering policies by file name, file size, file type, file creation time, and SSD usage. SmartTier is applicable when file lifecycle management is required, such as financial check images, medical images, semiconductor simulation design, and reservoir analysis. The services in these scenarios have demanding requirements for performance in the early stage and have low requirements for performance later. The following describes an example.\nColleagues, good afternoon. We have prepared for you a new part of training materials on the HCIE Storage certification. This course is \"Storage Performance Tuning Application Practice\". This document describes the background, practice mode, scenario, questions, tasks and suggested answers for Storage Performance Tuning Application. A company purchases a Huawei OceanStor all-flash storage device to run multiple services. It plans to create two LUNs that run on the Linux and Windows, respectively. LUN_Group1_Linux mapped to a Linux host carries critical services, while LUN_Group1_Windows mapped to a Windows host carries non-critical services. To ensure high performance of the storage system, you need to check whether performance bottlenecks exist on the host or storage device and check the performance parameters or features of the storage device. The operating environment has been initialized and set up on the Huawei OceanStor all-flash storage device. Two 2 GB LUNs, LUN_Group1_Linux and LUN_Group1_Windows, have been created and mapped to the Linux and Windows hosts, respectively. A file system has been created and mounted. You need to help storage engineer A complete the following tasks. The specific test configuration is as follows: Note: Cases in this course are examples only. The actual configuration may vary according to operating environments. For details, see the corresponding product documentation. The study material will be attached to the post and will be broken down into parts so that you can study the materials sequentially.\nHyperClone Overview HyperClone allows you to obtain full copies of LUNs without interrupting host services. These copies can be used for data backup and restoration, data reproduction, and data analysis. Working Principles HyperClone provides a full copy of the source LUN's data at the synchronization start time. The target LUN can be read and written immediately, without waiting for the copy process to complete. The source and target LUNs are physically isolated. Operations on the member LUNs do not affect each other. When data on the source LUN is damaged, data can be reversely synchronized from the target LUN to the source LUN. A differential bitmap records the data written to the source and target LUNs to support subsequent incremental synchronization. Synchronization: When a HyperClone pair starts synchronization, the system generates an instant snapshot for the source LUN, synchronizes the snapshot data to the target LUN, and records subsequent write operations in a differential table. When synchronization is performed again, the system compares the data of the source and target LUNs, and only synchronizes the differential data to the target LUN. The data written to the target LUN between the two synchronizations will be overwritten. Before synchronization, users can create a snapshot for a target LUN to retain its data changes. Relevant concepts: Pair: In HyperClone, a pair has one source LUN and one target LUN. A pair is a mirror relationship between the source and target LUNs. A source LUN can form multiple HyperClone pairs with different target LUNs.\nThere are many factors involved in video encoding and transcoding, most of which interact with each other. For example, you can't randomly change the resolution of a video while transcoding and ignore adjusting the bitrate, right? In this section, let's look at some of the most important factors for producing a well-compressed, high-quality bitstream. Not all videos are created equal! Every video is unique and has its flavor, which is what makes video compression an art. The settings or adjustments used to compress high-action NFL videos are different than those used to compress cartoon videos like the Simpsons agree? There are a number of factors to classify videos, such as, Does it contain high-action sports? Is there a lot of grass or water (difficult to compress!) High action movies (war or high-speed car chase?) Head-and-shoulders content, such as news and talk shows Cartoon, anime? Does the content have code text that moves in one direction as the movie moves in the other direction? There are a lot of different types of scenes that set the movie apart and the engineers could have compressed the video better. If you know that the movie contains only flat areas such as cartoons, you can specify a lower bitrate than a high-action motion clip, as shown below. Here's my definition of video codecs - \"Video codecs are a set of tools and algorithms designed to compress video to achieve a predetermined rate-distortion trade-off.\nVideo codecs are often built by consensus and involve boards of engineers, academic scientists, and industry (software and hardware companies). Popular examples of video codecs are H.264/AVC, HEVC, AV1, VP9, EVC, VVC, LCEVC. Each codec is designed with a specific goal. It could be to achieve better performance than the previous generation, or to be royalty-free, or to introduce a new compression method (as LCEVC did). And because each codec is designed and optimized differently, their performance is necessarily different. For example, AV1 achieves higher compression efficiency than H.264/AVC, but uses more time and resources than AVC. And, it's okay! It depends on your needs and resources. Each encoder has a \"rate control\" algorithm that determines how a particular bit rate budget will be used over a period of time (or GOP). The rate control technology used in the codec has a significant impact on its compression efficiency, video quality, and speed. The following are three popular rate control algorithms. It is not possible to discuss these three in depth in this overview article, but stay tuned for an in-depth discussion of rate control. CBR or constant bit rate : The average bit rate remains constant while sacrificing video quality. VBR or variable bit rate : The video quality remains the same while allowing bit rate fluctuations. Capped VBR or Capped Variable Bitrate : The video quality remains unchanged and the bit rate is allowed to fluctuate within the limit or upper limit.\nDepending on the mode you choose for video encoding/transcoding, the encoder optimizes the trade-off between quality and bit rate or file size. The video bit rate or simply, the bit rate is the number of bits of video information transmitted per second. The unit of bit rate is usually kbps or kilobits per second mbps or megabits per second At the beginning of encoding, it is usually necessary to provide the bit rate to the encoder. According to the encoding mode (CBR, VBR, 2-pass VBR, upper limit VBR) , the encoder will use the bitrate value as a guide for compressing the video. As a rule of thumb, the higher the bit rate, the higher the video quality. However, there are always exceptions where, depending on resolution and content (slow, fast, head-and-shoulders, etc. ), increasing the bit rate does not improve video quality. The video resolution or video resolution is the width of the video multiplied by the height of the video. Video resolution is measured in pixels. Video resolution is typically expressed in two ways - Use the height of the video, such as 1080p or 720p. Alternatively, by mentioning the values of width and height - 1920 1080 or 1280 720. For obvious reasons, the resolution of the video plays an important role in encoding. High resolution video, such as 4K, will require more bits and time to compress. Low-resolution video (360p) requires fewer bits and faster compression Therefore, remember the resolution of the video when selecting the encoding bit rate.\nThe time required to compress/encode/transcode video is a key factor in video encoding. Different situations require different solutions, right? If you are live video, you cannot encode 1 frame of video per minute! It's going to be catastrophic! You need to compress video in real time at about 24 fps, 50 fps, or 60 fps. In this case, the encoder typically sacrifices quality to increase speed. However, if you only stream video on demand, you can spend hours compressing each hour of video. In this case, the encoder is tuned for slow compression and several sophisticated tools are used to achieve higher compression efficiency and better video quality. For example, in encoders such as FFmpeg, you have predefined settings called Slow, Slow, Very Slow, Faster, and Faster, which indicate the trade-off between speed, quality, and compression efficiency of the encoder. Depending on your use case (real-time or on-demand), you should select encoding settings (and speed). Group of Pictures (GOPs) are collections of video frames that have clearly defined orders in which they are encoded/decoded and displayed. The length of the GOP, called the \"GOP length\", has a profound impact on video compression efficiency, stream elasticity, and video quality. In short, very large GOP sizes typically provide high compression efficiency, but video quality and elasticity are reduced. And, vice versa. Closed and open GOPs are common in video streams and affect compression efficiency, error resiliency, and switchability in ABR streams. As the name implies, a Closed GOP is closed to frames outside the GOP.\nHuawei has unveiled the HarmonyOS equipped Home Storage device at the Mate 50 series event. And now, its time to grab the thoughtful gadget in your hands. The company has finally pushed off the Huawei Home Storage in the sale phase. According to the latest information, there are two variants of the Huawei Home Storage in this sale. One is the 2TB+2TB version, priced at 2999 yuan [414.91 USD]. While the other is the 8TB+8TB version with a cost tag of 4999 yuan [691.61 USD]. The only difference between the two versions is the model edition. While the 2TB variant is a Western Digital Red Disk Edition (suitable for houses and small businesses), the 8TB model is the Enterprise Edition. You can easily opt for the Home Storage version according to your requirements. So add this reflective device to your wishlist and leap on the below-given link to catch the ultimate gadget in your hand. Dont think to miss this golden chance as the lucky users will get a discount of 100 yuan under certain conditions. The Huawei Home Storage is a smart life product. It easily collaborates with multiple devices to gain the fastest and most efficient data access and transfer services. Besides, it implies an automated synchronization feature on its surface. As a result, new files, images, and other media contents can save to the storage device automatically. Ahead, it powers the HTD286 processor and the latest HarmonyOS 3 interface to perform effective operations and provides a responsive experience to the users.\nWhat is Data Management as a Service (DMaaS)? Data Management as a Service (DMaaS) is a cloud-based storage solution designed to consolidate multiple data sources into a centralized location. An enterprise solution, DMaaS covers the entire data lifecycle, including collection, storage, protection, movement, and analysis. DMaaS is offered via a consumption-based model by a provider, meaning clients only pay for the data infrastructure they use. The primary goal of DMaaS, and data management in general, is to enable a secure, always-on, always-fast connection to data of all kinds across a multi-cloud environment, in order to unlock value from that data and power business innovation. How does Data Management as a Service work? Traditional on-premises data management entails maintaining and managing data that has been produced and gathered over time by an organization. Then, based on business, governance, or regulatory requirements, all data is kept, backed up, and monitored for performance in one of several silos. This method requires a lot of manual work, which makes it a time- and labor-intensive operation. However, the growth of mobile and edge computing has accelerated the creation of both structured and unstructured data, placing further strain on an already difficult and error-prone process. Data management has become much more difficult but is still essential due to the potential of cyberthreats like viruses.\nBy developing a single Software as a Service (SaaS) platform to view all stored data, regardless of function, and better protect that data everywhere, DMaaS transforms a legacy infrastructure into a cloud model, aiding in the understanding of today's enormous data volumes (such as Big Data, data lakes, data warehouses, etc.). With the aid of artificial intelligence (AI) and machine learning, everyday chores may be automated and promptly taken care of in the cloud model, where silos that were previously maintained separately can be searched, accessed, and controlled at once (ML). A simple approach can be used to set up DMaaS. By collaborating with a service provider, businesses can choose among preconfigured storage choices that suit their needs without having to purchase and maintain the infrastructure themselves. The service provider will supply all hardware, software, management, support, and other resources. DMaaS can be created for private, on-premises IT in some cases, but it can only continue to be a service if it is maintained by a third party. Benefits of DMaaS In the modern IT environment, data is one of the most precious resources. You must properly secure and process them in order to obtain insights. Maintaining, storing, and processing the vast volumes of data currently available adds significantly to your company's overhead costs. With the flexibility of being cloud-based, with adjustable costs and simple setup, DMaaS can assist you in overcoming all these difficulties. Your company's productivity, cost effectiveness, and operational agility can all be improved with the aid of the proper data management service.\nYou can also lessen security dangers and data loss with its aid. Being a cloud-based service, DMaaS can bundle all the advantages mentioned above of data management and provide these additional advantages: A well-configured DMaaS can protect your valuable organizational data from several forms of cyberattacks and intruders. It can help you boost your overall efficiency and generate better business. This is because of its inbuilt business intelligence, data processing, and dashboarding abilities. Being a cloud-based service, it can help reduce costs while adding additional value to your business. It completely mitigates the need for heavy upfront infrastructural and setup costs. It provides flexibility to update your services and meets your businesss needs. Due to its pay-per-use model, DMaaS can lead to predictable costs. This makes it easy to account for organizational expenditures. It requires very minimal technical expertise to get started. This is because the service vendors take ownership of the necessary implementation. It also serves as a centralized medium for better visibility across your organizational data. It also helps address siloed situations. Use Cases for DMaaS. DMaaS offers several use cases for businesses and organizations of all domains and sizes.Top 6 use cases for DMaaS are: 1. You can configure it to be an effective data backup and a disaster recovery tool/service for your organization. 2. It can easily get deployed on both on-premise and cloud environments for end-to-end data management services. 3. It can help eliminate data silos. 4.\nHello all, This post talks about the basic concepts ofSnapshot in Dorado V6 file system. I hope you can understand the working principles and characteristics. Creating a file system snapshot is to generate a consistent copy of the source file system at a specific point in time without interrupting services. A snapshot is available immediately upon creation. Data reads, writes, and modifications on the source file system have no impact on the snapshot. Likewise, those on the snapshot have no impact on the source file system. File system snapshots are ready-only. 1. Snapshots can be generated in seconds without affecting host services. 2. Snapshots can be used to quickly restore data to the file system when necessary. 3. Snapshots can serve as data sources for replication, backup, and archiving. 4. Snapshots are not full physical copies of the source data. A snapshot for a large amount of source data occupies only a small space. ROW Deleting a snapshot releases the exclusive data of that snapshot while having no impact on the data of the file system and other snapshots. In other words, only space exclusively occupied by the snapshot is reclaimed and shared data is retained. Keywords : asynchronous reclamation in the background Rollback is performed in the background. The system finds the differences between the current file system and the target snapshot and copies the differential data from the snapshot to the current file system. When the file system is rolled back to snap0, snap1 is not deleted.\nProblemDescription On October 31, the customer reported that intermittent GPFS read/write issue on node 1 in the AIX cluster . The GPFSmember disks on node 1 were abnormal and half of the paths of the disks were infailed state. Handling Proccess: 1.Run the lspath command to query the command output. It is found thatthe host LUN ID of the failed path corresponding to hdisk5 is 5 and the hostLUN ID of the enabled path is 0xa. 2. According to the snap logs of the host, theWWN of the disk corresponding to hdisk5 is 6C8C465100D292F736BE1F2F00000374. 3. Analyze the DH1 log. The ID of the LUN whose WWN is 6C8C465100D292F736BE1F2F00000374 is 884, the ID of the mapping viewwhere LUN 884 resides is 213, and the corresponding host LUN ID is 5. 4. Analyze the DH2 log of the storage device.The LUN whose WWN is 6C8C465100D292F736BE1F2F00000374 corresponds to the ID749. The mapping view ID of LUN 749 is 275, and the corresponding host LUN IDis 5. Brief summaryCurrently the LUN (AIX_sadappp1_2_shared_power26/WWN:6C8C465100D292F736BE1F2F00000374) hashost LUN ID 5 on both DH1 and DH2 storage, but the host LUN ID displayed onhost path is 5 and 0xa(10) which is different from storage currentconfiguration. 5. Further analysis of storage logs shows thatthe host LUN ID of LUN 749 was manually changed on DH2 at 11:44:11 on October15, 2021. (Note: change from 10 to 5. And according to event, except LUN 749,some other LUNs also changed host LUN ID.\n[Problem Description] Unable to access SAN Switch Via GUI (Webtools) [Problem Analysis] Cant open webtools in IE. It show us The client and server dont support a common SSL protocol version or cipher suite on MS edge Java version is 1.8. and SAN switch current version is v8.1.0b , An old version which have some issues to use JAVA webtools on browser by https protocol. For getting access to webtools GUI: Enable 80 port on the SAN switch. Thenwebtoolscan be accessed by http protocol. 1- In SAN switch side: Step1 ipfilter --show //get active IPV4 policy name factory_default_ipv4 Step2 ipfilter --addrule factory_default_ipv4 -rule 1 -sip any -dp 80 -proto tcp -act permit To add new rule for policy factory_default_ipv4 ipfilter --save factory_default_ipv4 save policy ipfilter --activate factory_default_ipv4 activate policy ipfilter --save factory_default_ipv4 save policy again 2- In IE setting - Add SAN switch IP to trusted sites in security page of internet options -Websites start with http://IP - Do not check item: Require server verification(https:) for all sites in this zone 3- In JAVA console In exception site list, input dedicate SAN switch IP with http http://san switch IP1 http://san switch IP2 Accessing SAN switch by http://san switch IP in browser (MS Edge or IE). recommended to use MS Edge [Root Cause] FOS v8.1.0b with Java version 1.8 have issue to using JAVA webtools on browser by https protocol. [Solution] 1- Add each SAN switch IP in IE and java console. 2- Do not use wildcard * for SAN switch in security page on java console.\nThe storage area network ( SAN ) is a network whose primary purpose is the transfer of data between computer systems and storage elements. A SAN consists of a communication infrastructure, which provides physical connections. It also includes a management layer, which organizes the connections, storage elements, and computer systems so that data transfer is secure and robust. The term SAN is typically (but not necessarily) identified with block I/O services rather than file access services. In simple terms, a SAN is a specialized, high-speed network that attaches servers and storage devices. The SAN is sometimes referred to as the network behind the servers. A SAN allows any-to-any connection across the network, by using interconnect elements, such as switches and directors. The SAN eliminates the traditional dedicated connection between a server and storage, and the concept that the server effectively owns and manages the storage devices. This scenario is only an example: This basic storage configuration is presented as an example. However, a configuration can include as many CPUs, RAID controllers, network interfaces, and HDDs as needed. The SAN also eliminates any restriction to the amount of data that a server can access. Traditionally, a server is limited by the number of storage devices that attach to the individual server. Instead, a SAN introduces the flexibility of networking to enable one server or many heterogeneous servers to share a common storage utility. A network might include many storage devices, including disk, tape, and optical storage.\nAdditionally, the storage utility might be located far from the servers that it uses. The SAN can be viewed as an extension to the storage bus concept. This concept enables storage devices and servers to interconnect by using similar elements, such as LANs and wide area networks (WANs). The SANs of today are used to connect shared storage arrays and tape libraries to multiple servers, and they are used by clustered servers for failover. A SAN can be used to bypass traditional network bottlenecks. A SAN facilitates direct, high-speed data transfers between servers and storage devices, potentially in any of the following three ways: Server to storage: This method is the traditional model of interaction with storage devices. The advantage is that the same storage device might be accessed serially or concurrently by multiple servers. Server to server: A SAN might be used for high-speed, high-volume communications between servers. Storage to storage: This outboard data movement capability enables data to be moved without server intervention, therefore freeing up server processor cycles for other activities, such as application processing. Examples include a disk device that backs up its data to a tape device without server intervention, or a remote device mirroring across the SAN. SANs allow applications that move data to perform better, for example, by sending data directly from the source device to the target device with minimal server intervention. SANs also enable new network architectures where multiple hosts access multiple storage devices that connect to the same network.\nData redundancy is implemented through disk data mirroring. Data is backed up on a pair of independent disks. When the raw data is busy, data can be directly read from the mirror copy. Therefore, RAID 1 can improve the read performance. RAID 1 is the highest unit cost in disk arrays, but provides high data security and availability. When a disk fails, the system automatically switches to the mirror disk to read and write data without reassembling invalid data. Therefore, RAID 1 is often used in applications that have strict requirements for fault tolerance, such as finance and finance. The Hemming code (Hamming Code) verifies the stripe storage. Data blocks are distributed on different hard disks. The block unit is bit or byte, which is called Hemming code to provide error checking and recovery. This coding technology requires multiple disks to store check and recovery information so that the RAID 2 redundancy information is expensive (multiple parity disks). The technology implementation is more complex, so it is seldom used in the business environment. Parity check (XOR) stripe storage, shared parity disk, and data stripe storage unit: Byte. Similar to RAID 2, data blocks are distributed on different hard disks. The difference is that RAID 3 uses simple parity check and uses a single disk to store parity check information. If a disk fails, the parity disk and other data disks can generate data again. If the parity disk fails, data usage is not affected.\nRAID 3 provides a good transmission rate for a large amount of continuous data. However, for random data, the parity disk becomes the bottleneck of write operations. Parity check (XOR) stripe storage, shared parity disk, and data stripe storage unit: Block. The RAID 4 also blocks data blocks and distributes them on different disks, but the block unit is blocked or record. The RAID 4 uses a disk as the parity disk. Each write operation requires access to the parity disk. In this case, the parity disk becomes the bottleneck of the write operation. Therefore, the RAID 4 is seldom used in the commercial environment. Parity check (XOR) stripe storage. The parity data is stored in distributed mode. The data strip storage unit is blocked. RAID 5 does not specify a parity disk. Instead, it accesses data and parity check information on all disks. In RAID 5, the read/write pointer can be used to operate the storage array at the same time, providing higher data traffic. RAID 5 is more suitable for small data blocks and random read and write data. The main difference between RAID 3 and RAID 5 is that all disk arrays are involved in RAID 3 data transmission. For RAID 5, most data transfer operations are performed on only one disk, and parallel operations can be performed. In RAID 5, write loss occurs. That is, each write operation generates four actual read/write operations, two of which read old data and parity information, and write new data and parity information twice.\nFor example, if you need to restore A0 in the following figure, you need to add A0, C0, D0, and 0 parity to calculate and obtain A0 to restore data. Therefore, when two disks are faulty, the data in the entire RAID group become invalid. Parity check (XOR) stripe storage: Check data of two distributed storage. The data strip storage unit is blocked. Compared with RAID 5, RAID 6 adds a second independent parity check information block. Two independent parity systems use different algorithms. Data reliability is high. Even if two disks fail at the same time, data usage is not affected. However, RAID 6 needs to allocate more disk space than parity check information. Compared with RAID 5, RAID 6 has a larger write loss. Therefore, write performance is poor. Poor performance and complex implementation methods make RAID 6 very few practical applications. This is a new RAID standard. It has an intelligent real-time operating system and a software tool for storage management. It can run independently from hosts and does not occupy host CPU resources. RAID 7 can be regarded as a storage computer (Storage Computer), which is obviously different from other RAID standards. The RAID 7 level is a RAID mode with the highest theoretical performance so far, because it has been greatly different from the previous mode. In the previous figure, a hard disk is a column of a disk array. In the RAID 7, multiple hard disks form a column, which have their own channels.\nIn this way, you can divide the image into multiple hard disks and connect them to the main channel, it's just more subdivided than the previous level. In this way, when the data in an area are read or written, the data can be quickly located. In the RAID 7, a single hard disk is divided into multiple independent hard disks, has its own read and write channels. RAID 10 is mirrored first and then striped. RAID 01: Striping is performed first, and then mirroring is performed. For example, if four disks are used as an example, RAID 10 divides disks into two groups of mirrors, and then strips the two groups of RAID 1. In RAID 01, two disks are used as RAID 0, and the other two disks are used as the mirroring of RAID 0. The following uses four disks as an example to describe the security differences. 3.2.1 RAID 10 In this case, it is assumed that when the Disk0 is damaged, the entire RAID fails only when one disk of the Disk1 is faulty, and the failure rate is 1/3. 3.2.2 In this case, if the Disk0 is damaged, the stripe on the left cannot be read. In the remaining three disks, ifthe Disk2 and Disk3 disks is damaged, the entire RAID group becomes invalid. Therefore, the failure rate is 2/3. Therefore, RAID 10 is more secure than RAID 01.\n3.3.1 Security Comparison When disk 1 is damaged, RAID 10 does not take effect only when the mirror disk corresponding to disk 1 is damaged. For RAID 5, if any disk in the remaining three disks is faulty, the RAID group becomes invalid. In terms of security, RAID 10 is higher than RAID 5. 3.3.2 Comparison of space utilization The utilization rate of RAID 10 is 50%, and that of RAID 5 is 75%. The larger the number of hard disks, the higher the space usage of RAID 5. In terms of space utilization, RAID 5 is higher than RAID 10. 3.3.3 Comparison of read and write performance Performance differences in read operations The number of disks that can read valid data in RAID 10 is 4. The number of disks that can read valid data in RAID 5 is 4 (the parity information is distributed on all disks). Therefore, the read performance of the two disks must be the same. Performance difference in Continuous Write As shown in the preceding figure, RAID 5 of the four disks can be verified in the memory, and three data records (+1 parity check) are written into the memory. RAID 10 can write two data records and +2 images at the same time. According to the cache principle, the write cache can cache write operations. After the write data is accumulated to a certain period, the write cache is written to the disk. In addition, the process of writing data to the disk array will occur sooner or later.\nTherefore, when RAID 5 and RAID 10 are continuously writing data, the speed of writing data from the cache to the disk is slightly different. If the continuous write is not continuous, the difference is not too large as long as the disk writes limit is not reached As shown in the preceding figure: For example, if you want to change a number from 2 to 4, the I/O operation is performed four times in RAID 5. Read 2 and check 6, and then calculate a new checksum in the cache and write new digit 4 and new parity 8. For RAID 10, only two I/O are required for RAID 10, and four I/O are required for RAID 5. Disk IOPS Comparison Assume that theof the service is 10000, the read cache hit ratio is 30%, the rideis 60%, theis 40%, and the number of disks is 120. Calculate theof each disk in the case of RAID 5 and RAID 10. RAID 5: of a single disk= In the preceding information, 10000*1-0.3*06 indicates the. The ratio is 0.6. The cache hit is eliminated. Actually, only 4200are available. 4 * (10000*0.4) indicates that. For each write, four I/are actually generated in RAID 5. Therefore, the value ofis 16000. To ensure that the two read operations may be hit during the write operation of RAID 5, the accurate calculation is as follows: The calculatedof a single disk is 148, which basically reaches the disk limit.\niSCSI Protocol iSCSI encapsulates SCSI commands and block data into TCP packets and transmits the packets over an IP network. iSCSI uses mature IP network technologies to implement and extend SANs The SCSI controller card is used to connect to multiple devices to form a network, but the devices can communicate with each other on the network and cannot be shared on the Ethernet. If devices form a network through SCSI and the network can be mounted to an Ethernet, the devices can interconnect and share with other devices as network nodes. As a result, the iSCSI protocol evolved from SCSI. The IP SAN using iSCSI converts user requests into SCSI codes and encapsulates data into IP packets for transmission over the Ethernet. The iSCSI scheme was initiated by Cisco and IBM and then advocated by Adaptec, Cisco, HP, IBM, Quantum, and other companies. iSCSI offers a way of transferring data through TCP and saving data on SCSI devices. The iSCSI standard was drafted in 2001 and submitted to IETF in 2002 after numerous arguments and modifications. In Feb. 2003, the iSCSI standard was officially released. The iSCSI technology inherits advantages of traditional technologies and develops based on them. On one hand, SCSI technology is a storage standard widely applied by storage devices including disks and tapes. It has been keeping a fast development pace since 1986. On the other hand, TCP/IP is the most universal network protocol and IP network infrastructure is mature. The two points provide a solid foundation for iSCSI development.\nPrevalent IP networks allow data to be transferred over LANs, WANs, or the Internet using new IP storage protocols. The iSCSI protocol is developed by this philosophy. iSCSI adopts IP technical standards and converges SCSI and TCP/IP protocols. Ethernet users can conveniently transfer and manage data with a small investment. iSCSI Initiator and Target The iSCSI communication system inherits some of SCSI's features. The iSCSI communication involves an initiator that sends I/O requests and a target that responds to the I/O requests and executes I/O operations. After a connection is set up between the initiator and target, the target controls the entire process as the primary device. There are three types of iSCSI initiators: software-based initiator driver, hardwarebased TCP offload engine (TOE) NIC, and iSCSI HBA. Their performance increases in that order. iSCSI targets include iSCSI disk arrays and iSCSI tape libraries. The iSCSI protocol defines a set of naming and addressing methods for iSCSI initiators and targets. All iSCSI nodes are identified by their iSCSI names. This method distinguishes iSCSI names from host names. iSCSI uses iSCSI names to identify initiators and targets. Addresses change with the relocation of initiator or target devices, but their names remain unchanged. When setting up a connection, an initiator sends a request. After the target receives the request, it checks whether the iSCSI name contained in the request is consistent with that bound with the target. If the iSCSI names are consistent, the connection is set up. Each iSCSI node has a unique iSCSI name.\nIntroduction to RAID 2.0 RAID 2.0 RAID 2.0 is an enhanced RAID technology that effectively resolves the following problems: prolonged reconstruction of an HDD, and data loss if a disk is faulty during the long reconstruction of a traditional RAID group. RAID 2.0+ RAID 2.0+ provides smaller resource granularities (tens of KB) than RAID 2.0 to serve as the units of standard allocation and reclamation of storage resources, similar to VMs in computing virtualization. This technology is called virtual block technology. Huawei RAID 2.0+ Huawei RAID 2.0+ is a brand-new RAID technology developed by Huawei to overcome the disadvantages of traditional RAID and keep in line with the storage architecture virtualization trend. RAID 2.0+ implements two-layer virtualized management instead of the traditional fixed management. Based on the underlying disk management that employs block virtualization (Virtual for Disk), RAID 2.0+ uses Smart-series efficiency improvement software to implement efficient resource management that features upper layer virtualization (Virtual for Pool). Block virtualization is to divide disks into multiple contiguous storage spaces of a fixed size called a chunk (CK). RAID 2.0+ Block Virtualization The working principles of RAID 2.0+ block virtualization are as follows: 1. Multiple SSDs form a storage pool. 2. Each SSD is then divided into CKs of a fixed size (typically 4 MB) for logical space management. 3. CKs from different SSDs form chunk groups (CKGs) based on the RAID policy specified on DeviceManager. 4. CKGs are further divided into grains (typically 8 KB). Grains are mapped to LUNs for refined management of storage resources. 5.\nIntroduction to RAID Redundant Array of Independent Disks (RAID) combines multiple physical disks into one logical disk in different ways, improving read/write performance and data security. With the development of RAID technology, RAID can be divided as seven basic levels (RAID 0 to RAID 6). In addition, there are some combinations of basic RAID levels, such as RAID 10 (combination of RAID 1 with RAID 0) and RAID 50 (combination of RAID 5 with RAID 0). Different RAID levels represent different storage performance, data security, and storage costs. RAID Data Organization Forms RAID divides space in each disk into multiple strips of a specific size. Written data is also divided into blocks based on the strip size. The following concepts are involved: Strip: A strip consists of one or more consecutive sectors in a disk, and multiple strips form a stripe. Stripe: A stripe consists of strips of the same location or ID on multiple disks in the same array. Stripe width indicates the number of disks used in an array for striping. For example, if a disk array consists of three member disks, the stripe width is 3. Stripe depth indicates the capacity of a strip. RAID generally protects data by the following methods: Mirroring : Data copies are stored on another redundant disk, improving reliability and read performance. Parity check algorithm (XOR): Parity data is additional information calculated using user data. For a RAID array that uses parity, an additional parity disk is required. The XOR (symbol: ) algorithm is used for parity.\nXOR is widely used in digital electronics and computer science. XOR is a logical operation that outputs true only when inputs differ (one is true, the other is false). 0 0 = 0, 0 1 = 1, 1 0= 1, 1 1 = 0 RAID Hot Spare and Reconstruction If a disk in a RAID array fails, a hot spare is used to automatically replace the failed disk to maintain the RAID array's redundancy and data continuity. Hot spare is classified into the following types: Global: The spare disk is shared by all RAID groups in the system. Dedicated: The spare disk is used only by a specific RAID group in the system. Data reconstruction: indicates a process of reconstructing data from a failed data disk to the hot spare disk. Generally, the data parity mechanism in RAID is used to reconstruct data. Data parity: Redundant data is used to detect and rectify data errors. The redundant data is usually calculated through Hamming check or XOR operations. Data parity can greatly improve the reliability, performance, and error tolerance of the drive arrays. However, the system needs to read data from multiple locations, calculate, and compare data during the parity process, which affects system performance. Generally, RAID cannot be used as an alternative to data backup. It cannot prevent data loss caused by non-drive faults, such as viruses, man-made damages, and accidental deletion. Data loss here refers to the loss of operating system, file system, volume manager, or application system data, not the RAID data loss.\nHello, everyone! Can I change the LUN type from thin to thick type? May the thick LUN bring better performance experience than thin LUN? Express my gratitude in advance! Hello, jingoba! Have a nice day! After the SmartThin function is enabled, the storage system creates thin LUNs and does not allocate the configured capacity to LUNs at a time. In the LUN capacity range, the storage system dynamically allocates storage resources based on the actual capacity of the host to implement on-demand allocation. Compared with thick LUN, there are differences in read/write performance. The actual difference between thin LUN and thick LUN is the mapping between LBA and PBA, 1. If the LBA and PBA are in one-to-one correspondence, then it is thick LUN LBA-Logical block address 2. If the LBA and PBA do not correspond to each other, then thin LUN PBA-Physical block address. Write performance For the first time: The thick LUN is formatted after the LUN is created. Therefore, only the write operation on the host I/O is involved. When new data is written into the thin LUN, space is allocated to the, which brings more data to the I/O. The write process is longer and imposes additional pressure on the back-end hard disk. In this case, the performance of the thick LUN is better than that of the thin LUN. Overwritten write: Because both thin LUN and thick LUN have completed the allocation of the write space, no extra overhead is generated. In this case, the performance of the two is equivalent.\nDHT routing technology In Huawei distributed storage, the block service uses the distributed hash table (DHT) routing algorithm. Each storage node stores a small proportion of data, and the data is routed and stored using the DHT routing algorithm. This DHT algorithm features balance and monotonicity. 1. Balance: Data is distributed to all nodes as evenly as possible, thereby balancing loads among nodes. 2. Monotonicity: When new nodes are added to the system, the system redistributes data among nodes. Data migration is implemented only on the new nodes, and the data on the existing nodes is not significantly adjusted. Traditional storage systems typically employ the centralized metadata management mechanism, which allows metadata to record the disk distribution of the LUN data with different offsets. For example, the metadata may record that the first 4 KB of data in LUN1+LBA1 is distributed on LBA2 of the 32nd disk. Each I/O operation initiates a query request for the metadata service. As the system scale grows, the metadata size also increases. However, the concurrent operation capability of the system is subject to the capability of the server accommodating the metadata service. In this case, the metadata service may become a performance bottleneck of the system. During system initialization, Huawei distributed storage system sets partitions for each disk based on the value of N and the number of disks. For example, the default value of N is 3600 for two-copy backup. If the system has 36 disks, each disk has 100 partitions.\nThe partition-disk mapping is configured during system initialization and dynamically adjusted based on the number of disks. The partition-disk mapping table occupies only a small space, and Huawei distributed block storage nodes store the mapping table in the memory for rapid routing. Huawei distributed block storage does not employ the centralized metadata management mechanism and therefore does not have performance bottlenecks incurred by the metadata service. Huawei distributed block storage logically divides a LUN by every 1 MB of space. For example, a LUN of 1 GB space is divided into 1024 slices of 1 MB space. When an application accesses block storage, the SCSI command carries the LUN ID, LBA ID, and I/O data to be read/written. The OS forwards the message to the VBS of the local node. The VBS generates a key based on the LUN ID and LBA ID. The key contains rounding information of the LBA ID based on the unit of 1 MB. The result calculated using DHT hash indicates the partition. The specific disk is located based on the partition-disk mapping recorded in the memory. The VBS forwards the I/O to the OSD to which the disk belongs. For example, if an application needs to access the 4 KB data identified by an address starting with LUN1+LBA1, Huawei distributed storage first constructs \"key=LUN1+LBA1/1M\", calculates the hash value for this key, performs modulo operation for the value N, gets the partition number, and then obtains the disk of the data based on the partition-hard disk mapping.\nThis parameter needs to be set only when the node to be added is a compute node and Regular installation is selected. Installation Mode Indicates the installation mode of the node. Possible options are Regular installation and Pre-installation . Parameters Username , Password , and Password of root are not required when Node Role is Compute and Installation Mode is Pre-installation . Username Password Indicates the user password. Password of root Indicates the password of user root. Click OK . Click Submit . After the node is configured successfully, click Next . If Network Management Component Installation is Waiting for completion , the progress of installing network management components on the node is 100%. However, you need to wait until the installation is complete on all nodes and a notification is sent to related components. After the notification is sent to the related components, the installation process ends and the status changes to Successful . Click Batch Import Node . In the Please choose files area, click , select the file to be uploaded, and click Open . You can click Download the template , enter parameters of the nodes to be added in the downloaded template, and upload the template. Click Upload . Authenticate all nodes. Click Authenticate and enter the user name and password of a node for authentication. Click Submit . After the nodes are configured successfully, click Next . Configure storage networks. If a front- and back-end shared network is deployed during initialization, you can only deploy the shared network.\nIf front- and back-end separated networks are deployed during initialization, you can only deploy separated networks. If the storage or control IP addresses have been configured, the system displays the storage or control IP address of each node. If the storage or control IP addresses are not configured, the system displays the storage or control IP addresses to be allocated to storage nodes. An IP address range of the control network cannot be in the same network segment as that of the storage network. When the control IP addresses are IPv4 addresses, enter the corresponding subnet mask. When the control IP addresses are IPv6 addresses, enter the corresponding prefix. You can click Add Control IP Address Range to configure multiple control IP address ranges. In addition, cross-network segment configuration is supported. To remove an IP address range, select it and click Remove . You can also click More to modify or remove an added IP address range. The start and end IP addresses of each storage IP address range must be in the same subnet. The storage network configured for a node cannot be in the same subnet as other networks. If the storage IP addresses are IPv4 addresses, enter the corresponding subnet mask. If the storage IP addresses are IPv6 addresses, enter the corresponding prefix. If Use Configured IP Addresses Only is set to No , you need to configure the port of the storage network. Click to go to the Manage Bond Port page. You can configure a port as required.\nYou can click Add Storage IP Address Range to configure multiple storage IP address ranges. In addition, cross-network segment configuration is supported. If storage IP addresses are on different network segments, you need to configure static routes or policy-based routes. If multiple IP addresses are used as storage IP addresses, you need to configure policy-based routes. Click Configure Route to go to the route configuration page. For details, see . To remove an IP address range, select it and click Remove . You can also click More to modify or remove an added IP address range. Yes : Only storage IP addresses configured in advance are used. To select this option, all storage nodes must be configured with storage IP addresses in advance. No : If no storage network IP address is configured in advance, the system will assign new storage network IP addresses to the nodes based on the settings. The Configure Storage Network page varies depending on the selected transmission protocol. Configure the storage network as prompted. describes related parameters. The values of Transmission Protocol , Network Flow Classification , Multi-IP Address , and Bond Mode must be the same as those of the front-end storage network configured during cluster initialization and cannot be changed. Table 2 Parameter Description Use Configured Storage IP Address Only Shared IP Address on DPC Compute Node This parameter is mandatory when Transmission Protocol is set to RoCE and Multiple IP Addresses is set to Yes .\nWhen a DPC compute node has only one front-end storage IP address and you select Yes , the control IP address must be the same as the front-end storage IP address. This parameter can be configured only when a DPC compute node is added for the first time. If a DPC compute node is added later, the value of this parameter will be the same as that configured for the first time and cannot be changed. Click Add Storage IP Address Range and set the IP address range, subnet mask/prefix, and port number of the storage network. If Transmission Protocol of the back-end shared network or front-end storage network is set to RoCE and Multiple IP Addresses is set to Yes , you need to configure control IP addresses. Click Add Control IP Address Range and set the IP address range and subnet mask/prefix of the control network. Click Preview . The system automatically reads the IP addresses configured on the nodes. To improve reliability and performance, you are advised not to use physical ports and bond ports on the storage network at the same time. If there are both physical ports and bond ports on the storage network, in multi-IP address scenarios, all ports must be changed to physical ports in a unified manner, while in non-multi-IP address scenarios, you are advised to use bond ports in a unified manner. Click Submit . After the storage network is configured successfully, click Next . Install nodes.\n(Optional) If the installation fails, you can click Restore Factory Setting to reconfigure the nodes. Click Install . (Optional) After the cluster is installed successfully, you can choose to expand storage pools and replication clusters, and configure iSCSI services as required. Click Finish . 3.Replacing a Single System DiskModule Enter the maintenance mode. Perform this step only when the faulty node has been added to a storage pool. Enter the maintenance mode using DeviceManager: Enter the maintenance mode using a command: Log in to a management node as user dsware using an SSH tool and run the following command to switch the faulty node to the maintenance mode. To run the following command, enter the name and password of CLI super administrator admin as prompted. Log in to DeviceManager. Choose . The hardware page is displayed. In the Nodes list, locate the faulty node and choose . Isolate the faulty node. Log in to the active management node as user fsadmin . Run the following command and enter the password of user root to switch to user root . Run the following command and enter the password of CLI super administrator admin as prompted to log in to the CLI. Run the following command to isolate the faulty node. Run the command circled in the preceding command output to query the isolation status of the faulty node. If the faulty node is isolated successfully, go to the next step. Otherwise, contact technical support. Power off the faulty node on the iBMC interface.\nLog in to the iBMC interface of the faulty node as user Administrator . Click in the upper right corner of the page and select Power Off to power off the faulty node. In the Virtual Console area in the lower right corner of the home page of the iBMC interface, click Start and select a remote console type. The KVM page is displayed. When POWER OFF is displayed, the faulty node is powered off successfully. Label and remove all external signal cables from the faulty node. Remove the faulty node. Figure 1 Remove the riser card from the faulty node. When moving a riser card, pay attention to the cables inside the node that connect to the riser card. Figure 2 Remove the faulty disk module. Loosen the screw on the air duct and lift the air duct. See (1) and (2) in . Figure 3 Press the latch that secures the disk module, tilt the disk module upwards by 20 to 30, and remove the disk module. See (1) and (2) in . Figure 4 Put the removed disk module into an ESD bag. Take the replacement disk module out of its ESD bag. Install the replacement disk module. Tilt the disk module by 20 to 30 and insert it into the slot. Press the latch of the disk module and the disk module at the same time until the disk module is secured by the latch. See (1) and (2) in .\nFigure 5 Place the air duct inside the node and tighten the screw on the air duct. See (1) and (2) in . Figure 6 Install the riser card into the faulty node. When installing a riser card, ensure that the cables inside the node that connect to the riser card are securely connected. Figure 7 Install the faulty node. Raise the ejector lever of the node and push the node into the system subrack along the guide rails. See (1) and (2) in . Figure 8 When the node cannot move forward anymore, rotate the ejector lever to reset it. Then press the lever until you hear a click to completely lock the node into the system subrack. See (1) and (2) in . Figure 9 Connect the node to peripheral devices according to the labels. Ensure that all cables are correctly connected. Press the power button of the node to power on the node. Cancel the isolation of the faulty node. Log in to the active management node as user fsadmin . Run the following command and enter the password of user root to switch to user root . Run the following command and enter the password of CLI super administrator admin as prompted to log in to the CLI. Run the following command to cancel the isolation of the faulty node. Run the command circled in the preceding command output to query whether the isolation of the faulty node is canceled successfully. Exit the maintenance mode.\nPerform this step only when the faulty node has been added to a storage pool. Exit the maintenance mode using DeviceManager: Exit the maintenance mode using a command: Log in to a management node as user dsware using an SSH tool and run the following command to switch the faulty node to the normal mode. To run the following command, enter the name and password of CLI super administrator admin as prompted. Log in to DeviceManager. Choose . The hardware page is displayed. In the Nodes list, locate the faulty node and choose . Check the RAID status of the system disks. Run the following command to view the partition details of the old disk. The number, sizes, and data of partitions on the new disk must be the same as those on the old disk. Using the sector as the unit is recommended. Run the following commands to create a partition table for the new disk. Run the following command to create partitions for the new disk. In the following command, and are obtained in . Run the following command to view the partitions of the new disk. If the error information shown in the following figure is displayed after you run the following command, the new disk is not registered. In this case, run the registration command first. Registration command: Log in to the faulty node as user fsadmin . Run the following command and enter the password of user root to switch to user root .\nBlock Storage Block storage commonly uses an architecture that connects storage devices and application servers over a network. This network is used only for data access between servers and storage devices. When there is an access request, data can be transmitted quickly between servers and backend storage devices as needed. From a client's perspective, block storage functions the same way as disks. One can format a disk with any file system and then mount it. A major difference between block storage and file storage is that block storage provides storage spaces only, leaving the rest of the work, such as file system formatting and management, to the client. Block storage uses evenly sized blocks to store structured data. In block storage, data is stored without any metadata. This makes block storage useful when applications need to strictly control the data structure. A most common usage is for database. Databases can read and write structured data faster with raw block devices. Currently, block storage is usually deployed in FC SAN and IP SAN based on the protocols and connectors used. FC SAN uses the Fibre Channel protocol to transmit data between servers (hosts) and storage devices, whereas, IP SAN uses the IP protocol for communication. The FC technology can meet the growing needs for high-speed data transfer between servers and large-capacity storage systems. With the FC protocol, data can be transferred faster with low protocol overheads, while maintaining certain network scalability.\nFile Storage has the following advantages: Offers long-distance data transfer with a high bandwidth and a low transmission bit error rate. Based on the SAN architecture and massive addressable devices, multiple servers can access a storage system over the storage network at the same time, eliminating the need for purchasing storage devices for every server. This reduces the heterogeneity of storage devices and improves storage resource utilization. Protocol-based data transmission can be handled by the HBA, occupying less CPU resources. In a traditional block storage environment, data is transmitted over the fibre channel via block I/Os. To leverage the advantages of FC SAN, enterprises need to purchase additional FC components, such as HBAs and switches. Enterprises usually have an IP network-based architecture. As technologies evolve, block I/Os now can be transmitted over the IP network, which is called IP SAN. With IP SAN, legacy infrastructure can be reused, which is far more economical than investing in a brand new SAN environment. In addition, many remote and disaster recovery solutions are also developed based on the IP network, allowing users to expand the physical scope of their storage infrastructure. Internet SCSI (iSCSI), Fibre Channel over IP (FCIP), and Fibre Channel over Ethernet (FCoE) are the major IP SAN protocols. iSCSI encapsulates SCSI I/Os into IP packets and transmits them over TCP/IP. iSCSI I widely used to connect servers and storage devices because it is cost-effective and easy to implement, especially in environments without FC SAN.\nObject Storage Users who frequently access the Internet and use mobile devices often need object storage techniques. The core of object storage is to separate the data path from the control path. Object storage does not provide access to original blocks or files, but to the entire object data via system-specific APIs. You can access objects using HTTP/RESTbased uniform resource locators (URLs), like you access websites using browsers. Object storage abstracts storage locations as URLs so that storage capacity can be expanded in a way that is independent of the underlying storage mechanism. This makes object storage an ideal way to build a large-scale system with high concurrency. As the system grows, object storage can still provide a single namespace. This way, applications or users do not need to worry about which storage system they are using. By using object storage, you do not need to manage multiple storage volumes like using a file system. This greatly reduces O&M workloads. Object storage has many advantages in processing unstructured data over traditional storage and delivers the advantages of both SAN and NAS. It is independent of platforms or locations, offering scalability, security, and data sharing: It can distribute object requests to large-scale storage cluster servers. This enables an inexpensive, reliable, and scalable storage system for massive amounts of data. Other advantages of object storage are as follows: Security: data consistency and content authenticity. Object storage uses special algorithms to generate objects with strong encryption.\nHyperMetro Modes Quorum Modes If the link between two DCs is down or one DC is faulty, data cannot be synchronized between the two centers in real time. In this case, only one end of a HyperMetro pair or HyperMetro consistency group can continue providing services. For data consistency, HyperMetro uses an arbitration mechanism to determine service priorities in DCs. HyperMetro provides two quorum modes: The static priority mode applies to scenarios where no quorum server is configured. If no quorum server is configured or the quorum server is inaccessible, HyperMetro works in static priority mode. When an arbitration occurs, the preferred site wins the arbitration and provides services. The quorum server mode (recommended) applies to scenarios where a quorum server is configured. An independent physical server or VM is used as a quorum server. It is recommended that the quorum server be deployed at a dedicated site that is different from the two DCs. In this way, when a disaster occurs in a single DC, the quorum server still works. In quorum server mode , The failure in a DC or disconnection between the storage systems, each storage system sends an arbitration request to the quorum server, and only the winner continues providing services. The preferred site takes precedence in arbitration Dual-Write Working Principles HyperMetro uses dual-write and the data change log (DCL) mechanism to ensure data consistency between the storage systems in two DCs. Both centers provide read and write services for hosts concurrently.\nMultipathing Software Multipathing software ensures redundancy, reliability, and high performance of links between a host and a storage system. UltraPath, multipathing software developed by Huawei, is recommended. Note that if OS native or third-party multipathing software is used, incorrect paths may be selected due to improper configurations or compatibility issues. All descriptions about the multipathing software in this document are based on the assumption that the customer uses Huawei UltraPath. If OS native or third-party multipathing software is used, see the related documents provided by the corresponding multipathing software vendor. The operations described in this document are performed on Windows and Linux hosts. For details about how to query the multipathing software on other operating systems, see the multipathing software user guide of the corresponding operating system. Multipathing status Before querying the multipathing status, ensure that the link status is normal and the owning controller is working properly. On a Windows host, open the UltraPath Console and check whether the link status is normal and whether the owning controller is working properly. On a Linux host, run the upadmin show path command to check whether physical paths are normal. In some scenarios, links may be lost, that is, some initiator ports are absent. On UltraPath, check whether the number of links is the same as the number of physical connections. If link loss occurs in the IOPS-intensive scenario, I/O forwarding performance deteriorates. If link loss occurs in the bandwidth-intensive scenario, a physical connection is reduced, and the bandwidth decreases.\nEC intelligent aggregation technology Erasure coding (EC) increases the computing overhead, and a poor EC design brings more write penalties. Therefore, the performance of products using EC may be significantly lower than that of products using multi-copy storage. Write penalty of EC: This section uses 4+2 redundancy as an example. If the size of data to be written is less than 32 KB (for example, only 16 KB), 16 KB of data is written for the first time. During the second write, the 16 KB data written earlier must be read and combined before being written to disks. This causes the read overhead. This problem does not occur when full stripes are delivered. The intelligent aggregation EC based on append write ensures EC full-stripe write at any time, reducing read/write network amplification and disk amplification by several times. Data is aggregated at a time, reducing the CPU computing overhead and providing ultimate peak performance. In the cache, data of multiple LUNs is aggregated into a full stripe, reducing write amplification and improving performance. Huawei distributed storage provides intelligent I/O aggregation to use different policies for different I/Os, ensuring the read/write performance. Large I/Os are formed into EC stripes and directly written to disks without being cached, saving cache resources. When SSDs are used as cache media, the service life of SSDs can be extended. Small I/Os are written to the cache and an acknowledgement is returned immediately.\n[Problem Description] OceanStor 2600 V3 --- issue with FCconnectivity, ports showing like port throttled from SAN switch end [Handling proccess] Two storage ports on SAN switch port 14 show disabled (port Throttled) Line 1587: 598226 2022-06-27 14:57:230xF0EA0005 Fault Major Cleared 2022-07-07 14:13:39 The optical module rate andthe port rate of ETH port (Controller Enclosure CTE0, SmartIO interface moduleA.IOM0, port number P1, port rate 10Gbit/s, optical module rate 16Gbit/s) donot match. As a result, the port is unavailable. Line 1597: 598223 2022-06-27 14:57:23 0xF0EA0005 Fault Major Cleared 2022-07-0714:13:40 The optical module rate and the port rate of ETH port (ControllerEnclosure CTE0, SmartIO interface module B.IOM0, port number P1, port rate10Gbit/s, optical module rate 16Gbit/s) do not match. As a result, the port isunavailable. CTE0.A.IOM0.P1 AND CTE0.B.IOM0.P1 connected to sanswitch on 2022-06-27 to occur the same issue. Now These two ports were downwithout connect to SAN switch now. Line 1173: 602427 2022-07-06 15:57:49 0xF0EA0005 Fault Major Cleared 2022-07-0714:13:39 The optical module rate and the port rate of ETH port (ControllerEnclosure CTE0, SmartIO interface module A.IOM0, port number P3, port rate10Gbit/s, optical module rate 16Gbit/s) do not match. As a result, the port isunavailable. Line 1180: 602425 2022-07-06 15:57:49 0xF0EA0005 Fault Major Cleared 2022-07-0714:13:39 The optical module rate and the port rate of ETH port (ControllerEnclosure CTE0, SmartIO interface module B.IOM0, port number P3, port rate10Gbit/s, optical module rate 16Gbit/s) do not match. As a result, the port isunavailable. CTE0.A.IOM0.P3 AND CTE0.B.IOM0.P3 connected to sanswitch on 2022-07-06 to occur the issue four storage ports occur this issue.\nMulti-layer redundancy and fault tolerance design Huawei all-flash storage system uses the SmartMatrix multi-controller architecture that supports linear expansion of system resources. The controller enclosure uses the IP interconnection design and supports linear IP scale-out between controller enclosures. The management plane, control plane, and service plane are physically separated (in different VLANs), and served by different components. Each plane can independently detect, rectify, and isolate faults. Faults on the management plane and control plane do not affect services. Service plane congestion does not affect system management and control. All components in Huawei all-flash storage system work in redundancy mode, eliminating single points of failure. Huawei all-flash storage system provides multiple redundancy protection mechanisms for the entire path from the host to the storage system. Service continuity is guaranteed even if multiple field replaceable units (FRUs) allowed by the redundancy scheme are faulty simultaneously or successively. Key technologies for high reliability Three copies across controller enclosures: For data with the same LBA, Huawei all-flash storage system creates a pair between two controllers to form a dual- copy relationship and creates a third copy in the memory of another controller. If there is only one controller enclosure, three copies are stored on different controllers in this controller enclosure, preventing data loss when any two controllers become faulty at the same time. If there are two or more controller enclosures, the third copy can be stored on a controller in another controller enclosure.\nHSSDs use the LDPC algorithm and the FSP technology to ensure data reliability. Enhanced Low-Density Parity-Check (LDPC) algorithm: provides higher error correction capability than that required by flash chips to ensure device reliability. LDPC refers to a kind of linear codes defined through a check matrix. LDPC consists of four modules: Encode, Decode, Soft-bit Logic, and DSP Logic. When data is written to the NAND flash, the system generates the LDPC parity data and writes it to the NAND flash with the raw data. When data is read from the NAND flash, the LDPC parity data is used to check and correct the data. In case an error occurs in data in the NAND flash, the HSSD enables LDPC hard decoding to correct the error. If the error cannot be corrected, the HSSD enables shift read to save data. If shift read fails, the HSSD attempts to enable soft read to save data. If soft read also fails, the HSSD enables read retry to recover data. If the data still fails to be recovered, the data on other dies is used to perform the XOR operation to recover user data. Intelligent FSP algorithm: Based on the characteristics of 3D TLC media, the intelligent FSP algorithm provides faster and more reliable data storage services. Data inspection algorithm After data has been stored in NAND flash for a long term, data errors may occur due to read interference, write interference, or random failures. Risks can be detected and handled in advance through inspection, preventing data loss.\nIntroduction to Hyper Features HyperReplication 1. Synchronous remote replication: Data is synchronized in real time to ensure data consistency and minimize data loss in the event of a disaster. 2. Asynchronous remote replication: Data is periodically synchronized to minimize service performance deterioration caused by the latency of long-haul data transmission. HyperMetro HyperMetro (also called active-active feature) is a key technology of the active-active data center solution. It ensures high-level data reliability and service continuity for users. HyperMetro is an array-level active-active technology. Two active-active storage systems can be deployed in the same equipment room, the same city, or two places that are 100 km away from each other. HyperMirror HyperMirror is the volume mirror software of Huawei hybrid flash storage system. HyperMirror allows users to create two physical copies of a LUN. Each LUN copy can reside in a local resource pool or be an external LUN. Each LUN copy has the same virtual capacity as the mirror LUN. When a server writes data to a mirror LUN, the storage system simultaneously writes the data to each copy of the mirror LUN. When a server reads data from a mirror LUN, the storage system reads data from one copy of the LUN. Even if one copy of a mirror LUN is temporarily unavailable (for example, when the storage system that provides the storage pool is unavailable), the server can still access the mirror LUN.\nThe system memorizes LUN areas where data has been written and synchronizes the changed data with the LUN copy after the copy is available again. HyperVault Huawei hybrid flash storage systems provide the HyperVault feature to implement intra-system or inter-system file system data backup and restoration. HyperVault supports local backup and remote backup. HyperLock With the development of technologies and explosive increase of information, secure access and application of data are attached great importance. As required by laws and regulations, important data such as case documents of courts, medical records, and financial documents can only be read but cannot be written within a specific period. Therefore, measures must be taken to prevent such data from being tampered with. In the storage industry, write once read many (WORM) is the most commonly used method to archive and back up data, ensure secure data access, and prevent data tampering. The WORM feature of Huawei hybrid flash storage systems is also called HyperLock. After data is written to a file, the write permission of the file is removed so that the file enters the read-only state. In the read-only state, the file can be read but cannot be deleted, modified, or renamed. The WORM feature can prevent data from being tampered with, meeting data security requirements of enterprises and organizations. A file system with the WORM feature can be configured only by the administrator. WORM modes are classified into regulatory compliance WORM (WORM-C) and enterprise WORM (WORM-E) based on the administrator's permissions.\nDear All, Today we are going to learn about eReplication BCManager DR Management. DR Management Procedure eReplication DR Management Procedure Step 1 DR environment Sites involved in the DR solution (such as the production data center, same-city DR data center, and remote DR data center) Device resources related to the DR solution (such as storage devices and vCenter Server) Procedure for constructing a DR environment Discover DR resources. Configure DR sites. eReplication DR Management Procedure Step 2 Creating protected groups Select the type of the protected groups to be created based on the type of objects that you want to protect. Specify the site and host whose application instances you want to protect. Select a policy template. Modifying the policy of a protected group and viewing topologies Modify the time cycle policy based on the protection policy of the protected group. Modify, delete, disable, enable, export, or manually execute the protected group. View the logical topology of the protected group. eReplication DR Management Procedure Step 3 Creating a recovery plan Click Create to go to the page for creating a recovery plan. Enter a name for the recovery plan to be created and select the protected groups that you want to recover. Then the recovery plan creation is complete. DR testing Click Test and select the specific host/cluster or testing network at the designated DR data center for DR testing. After the applications are verified, click the Clear button to clear the test data.\n1. We checked the alarms on the storage and found that there are intermittent disconnections on the host(ID:51). Among the 4 links between the storage and the host, two links to controller A will reconnect and the other two links to controller B still disconnect. 2. We firstly checked the storage logs. The receive RSCN means the link between the host and the switch disconnect, then the switch notifies port state change to the storage. There is no disconnection between the storage and the switch. 3.The two links to controller A reconnect 4mins after disconnection. The Rcv PLOGI means storage receives the connection request from the host. However, we didnt find the same request in Controller B which explains why the links between Controller B and the host never connect again. 3. We then checked the switch log. The port and SFP modules in the switch are normal. We found the port on the host is NPIV port so it is possible that the host connects to a switch board or use a virtual HBA card. Also the below screenshot shows that the switch port connects to multiple NPIV port(means multiple hosts) but there is only one faulty NPIV port, which means the problem should be on the switch board or HBA of the host side The storage didnt receive the connection request from the host so the links didnt reconnect. Advise to ask the host support team for help to figure out the problem in the link.\nHello, everyone! I would like to share with you an article about the need for RAID 5 over RAID 1. Is any other RAID type available that can improve things further, while it conserves the advantages and removes the disadvantages of RAID 1? Yes, and this type of RAID is known as . This scenario consists of dividing the user data into N-1 parts (where N is the number of disks that are used to build the RAID) and then calculating a parity part. This parity part permits RAID to rebuild the user data if a disk failure occurs. RAID 5 uses parity or redundant information. If a block fails, enough parity information is available to recover the data. The parity information is spread across all of the disks. If a disk fails, the RAID requires a rebuild and the parity information is used to re-create the lost data. The following figure shows this example. Example of RAID 5 with parity RAID 5 requires a minimum of three disks. In theory, no limitations exist to add disks. This RAID type combines data safety with the efficient use of disk space. Disk failure does not result in a service interruption because data is read from parity blocks. RAID 5 is useful for people who need performance and constant access to their data. In RAID 5+Spare, disk failure does not require immediate attention because the system rebuilds itself by using the hot spare. However, the failed disk must be replaced as soon as possible.\nHello! This post enquires about why HyperSnap restore needs to delete HyperMetro and Hypervault. Please see more details below. In my scenario, I have a HyperMetro volume with snapshot and some remote vault. If I want to restore a local snapshot I need to: delete the remote vault; delete the HyperMetro pair for the volume; restore the local snapshot; recreate a new HyperMetro pair; and after that create a new remote Hypervault (which implies losing the history of the vault). Am I doing it wrong or is there a way to re-sync without having to recreate new HyperMetro and Hypervault volumes every time?Because in the graphical interface the old volumes are not visible, both for the HyperMetro and the Hypervault. Thanks in advance for assisting me! Dear Nvaes, Prerequisites A file system snapshot has been created in the storage system. The share of the source file system has been unmounted. When a file system is running on a VMware VM and the VM is performing replication (cloning VM files), you are advised not to create and roll back a snapshot. When a snapshot is rolled back, the cloned files are rolled back, causing the cloned files unavailable. If you have created and rolled back a snapshot, you are advised to delete the cloned files after the rollback. A snapshot rollback performed on the secondary storage system interrupts access to all snapshots in the file system.\nDear team, How do we fix WceISVista.inf driver errors for Windows OS? Thank you. Dear Muhammad, WceISVista.inf problems are generally caused by Windows 10 Education N x86 driver corruption, or if the hardware associated is faulty in some way. If your INF file is suffering from one of those troubles, replacing it with a fresh file should resolve the issue. INF is utilized by the Pro/ENGINEER Temporary format, which are types of Temporary Files. If you need to replace your WceISVista.inf file, you can find a Windows 10 version in our database listed in the table below. If your exact WceISVista.inf version is not currently available, please request a copy by hitting the \"Request\" button next to the file version you need. As a last resort, if your file is not found below, you can also try contacting Microsoft for the correct version. Placing this new WceISVista.inf file in the same location (overwriting the previous) and your issue should be resolved, but you'll want to check to be sure. You can then re-open Windows 10 Education N x86 to see if the error message is still triggered. Blue Screen WceISVista.inf errors are often caused by related hardware, software, device driver, or firmware problems. These BSODs can stem from Windows 10 Education N x86 problems, or issues with Microsoft-related hardware. Specifically, WceISVista.inf issues caused by: Corrupted, badly configured, or outdated Windows 10 Education N x86-related drivers (WceISVista.inf). WceISVista.inf or Windows 10 Education N x86 registry keys invalid / corrupted. Malware or virus infection corrupting WceISVista.inf.\nLUN performance is poor because of slow disks. OceanStor 18000 series V100R001 Log in to the device and run the show alarm command on the CLI of the primary controller to check whether slow disk alarms are generated. Check the messages log of the primary controller. If send disk slow alarm is displayed in the log, slow disks exist. If slow disk alarms are generated, replace the slow disks. If no slow disk alarms are generated and no send disk slow alarm is displayed in the messages log, check the messages logs of the primary and secondary controllers. Check whether the following slow circle information is displayed in the logs: SLOW_DISK: sd XX has slow Circle, average svctm is XX. If the information is displayed in the logs, slow circles exist. Slow circle: If the average latency of I/Os in a driver exceeds the threshold in 3 seconds, a slow circle exists. Latency threshold: 228----216----144----72----0 (disks) 3000---550----450----250--100 (ms) T series V1R5 Log in to the device and run the show alarm command on the CLI of the primary controller to check whether slow disk alarms are generated. Slow circle: If the average latency of I/Os in a driver exceeds the threshold in 3 seconds, a slow circle exists. Latency threshold: 228----216----144----72----0 (disks) 3000---550----450----250--100 (ms) If other disks in the same link have slow circles, the workload is heavy and no slow disk alarm is reported. Slow disk: If half of 100 circles are slow circles, a controller reports slow disk alarms.\nGo to the Bond dialog box. On the right navigation bar, click . In the Basic Information area of the function pane, click the device icon. In the middle function pane, click the cabinet whose Ethernet ports you want to bond. Click the controller enclosure where the Ethernet ports reside. The system displays the front view of the controller enclosure. Click to switch to the rear view. Click the Ethernet port you want to bond. Click Bond . The Bond dialog box and the selected port is displayed. The format of the port name is controller enclosure ID.interface module ID.port ID . Set the bonding name and available ports for the Ethernet port. Can contain only letters, digits, periods (. ), underscores (_), and hyphens (-). Contains 1 to 31 characters. In Bond Name , enter a name for the port to be bound. The name: From the Available Ports list, select the Ethernet ports you want to bond with the current Ethernet port. Click OK . The Danger dialog box is displayed. Confirm the bonding of the Ethernet ports. Confirm the information in the dialog box and select I have read the previous information and understood subsequences of the operation. . Click OK . The Success message box is displayed, indicating that the operation succeeded. Click OK . Go to the Bond dialog box. On the right navigation bar, click . In the Basic Information area of the function pane, click the device icon.\nIn the middle function pane, click the cabinet whose Ethernet ports you want to bond. Click the controller enclosure where the Ethernet ports reside. The system displays the front view of the controller enclosure. Click to switch to the rear view. Click the Ethernet port you want to bond. Click Bond . The Bond dialog box and the selected port is displayed. The format of the port name is controller enclosure ID.interface module ID.port ID . Set the bonding name and available ports for the Ethernet port. Can contain only letters, digits, periods (. ), underscores (_), and hyphens (-). Contains 1 to 31 characters. In Bond Name , enter a name for the port to be bound. The name: From the Available Ports list, select the Ethernet ports you want to bond with the current Ethernet port. Click OK . The Danger dialog box is displayed. Confirm the bonding of the Ethernet ports. Confirm the information in the dialog box and select I have read the previous information and understood subsequences of the operation. . Click OK . The Success message box is displayed, indicating that the operation succeeded. Click OK . Go to the Bond Ports management page. On the right navigation bar, click . The Provisioning page is displayed. Click Port . The Port page is displayed. Click Bond Ports tab. The Bond Ports page is displayed. Click Create . The Create Bond Port dialog box is displayed. The port name format is controller enclosure ID.interface module ID.port ID .\nSet the name, interface module, and optional ports that can be bonded with the current ethernet port.   Contains only letters, digits, underscores (_), periods (. ), and hyphens (-). Contains 1 to 31 characters. In Name , enter a name for the bond port. The name: Select the Interface Module . From the Optional port list , select the ethernet ports you want to bond. Click OK . The Danger dialog box is displayed. Confirm that you want to bond these ethernet ports. Confirm the information of the dialog box and select I have read and understood the consequences associated with performing this operation. . Click OK . The Success dialog box is displayed indicating that the operation succeeded. Click OK . Log in to DeviceManager. Choose . Click Create . The Create Bond Port dialog box is displayed. The port name format is controller enclosure ID.interface module ID.port ID . Set the name, interface module, and optional ports that can be bonded with the current ethernet port.   In Name , enter a name for the bond port. The name: Contains only letters, digits, underscores (_), periods (. ), and hyphens (-). Contains 1 to 31 characters. Select the Interface Module . From the Optional port list , select the ethernet ports you want to bond. Click OK . The Danger dialog box is displayed. Confirm that you want to bond these ethernet ports.\nFollow-up Procedure You can press the power button on a storage node or remotely power on the node on the iBMC WebUI. Powering Off a Node Using the OS of the Node 1. Log in to a storage node. You can log in to a storage node through the physical KVM in the equipment room. If Huawei servers are used, see . For details about how to remotely log in to a storage node using PuTTY, see . 2. You can run the following commands to power on or off a node. Run the reboot command to restart the node. Run the poweroff command to power off the node. Follow-up Procedure You can press the power button on a storage node or remotely power on the node on the iBMC WebUI. This operation enables you to power on a cluster. Context Services running on the storage system have been stopped. Procedure Power on nodes by referring to Remotely Powering On a Storage Node on the iBMC WebUI or Powering On a Storage Node by Pressing the Power Button on the Node. Repeat 1 to power on all the other nodes in the cluster. After all the nodes in the cluster are powered on, power on the cluster by referring to This operation enables you to power off all nodes in a cluster. Prerequisites No service is running on a storage system.\nScale-up is a traditional vertical expansion architecture. It continuously adds storage devices to the existing storage systems to cope with ever-increasing data volumes. Scale-out is a horizontal expansion architecture. It adds more controllers to meet the requirements of data growth in the unit of nodes. This course introduces the scale-up feature. The Scale-up architecture is a network storage architecture for vertically expanding a storage system by continuously adding new storage devices to cope with ever-increasing data volumes. In such storage systems, the controller (node) configurations are fixed. The can be explained by a simple example. When you only have six or seven fish, a small tank is enough; But after a while, more than 30 little fish were born, and the tank was obviously not big enough. If you use a scale-up solution, you need to buy a vat, take all the sand, water, sets, heating rods, thermometers out of the vat, and reposition them into the vat. This project is not simple oh, not ten minutes or eight minutes can be done, especially water plants, together difficult to separate. (However, this is a drizzle compared to the engineering complexity of migrating data.) Now, if you use the Scale-out solution, you connect the same small cylinder next to the small cylinder, and the two cylinders are connected. The fish can be automatically dispersed into two tanks, and you'll save the toss of sand, water, sets, etc. mentioned above.\nSAS is used to connect disks by leveraging parallel SCSI and serial connection technologies (such as Fibre Channel, SSA, and IEEE1394). On the basis of the serial communication protocol, SAS is a protocol stack for connecting multi-layer storage devices. Moreover, it uses the SCSI-3 extended instruction set and is compatible with SATA devices. SAS enables users to choose appropriate disk types according to their application task requirements. For example, highperformance storage products use SSDs or SAS disks to achieve faster processing capability. Large-traffic data storage products use large-capacity SATA disks or both SATA and SAS disks. SAS consists of three types of protocols that are used correspondingly for data transmission based on various devices connected. The Serial SCSI Protocol (SSP) is used to communicate with SCSI devices. The Serial ATA Tunneling Protocol (STP) is used to communicate with SATA devices. The Serial Management Protocol (SMP) is designed to maintain and manage SAS devices. Non-Volatile Memory Express (NVMe) is an extensible controller chip interface standard, which is designed to provide PCIe SSDs for enterprises, data centers, and client systems. NVMe is a register-level interface. It defines the communication protocol between the operating system and the NVM subsystem, and defines an instruction set and a function set. NVMe is developed by a joint effort of more than 80 companies in the industry and led by 11 sponsor enterprises. It aims to provide a unified standard for promoting PCIe SSDs in the industry. The NVMe protocol stack is simpler by reducing one-layer encapsulation.\nHello everyone In this post, you can learn: 1. Power-on and Power-off Precautions 2. Using DeviceManager to Power On or Off Storage Nodes 3. Using CLI Commands to Power On or Off Storage Nodes 4. Using Storage Nodes' Operating Systems to Power Off Storage Nodes 5. Using Mgmt Ports to Remotely Power On or Off Storage Ports 6. Pressing Power Buttons to Power On or Off Storage Nodes This section describes power-on and power-off precautions and methods. For details about power-on and power-off operations performed for a cluster, see the related sections in the OceanStor 9000 V5 File System Administrator Guide. Power-on and power-off precautions are as follows: You can disable, freeze, power off, or restart a node to ensure data security. To ensure data security, in the event of disabling a storage node, it is required that a minimum of four storage nodes exist in the node pool where the node to be disabled resides. In addition, if disabling a node reduces the reliability of stored data, a message is displayed to indicate that the node cannot be disabled. In the event of freezing, powering off, or restarting a storage node, it is required that a minimum of three storage nodes exist in the node pool. If a node is faulty, prevent the node from staying offline for a long time. It is recommended that you rectify the fault as soon as possible. If the fault cannot be rectified, replace the node or the system disks of the node.\nIf any node in the storage cluster is in the Closed, Offline, Frozen and Online, or Frozen and Offline state, or the system is executing a start, stop, or freeze operation, you cannot disable, freeze, power off, or restart other nodes in this storage cluster until the task is in the Completed state. Check the task state in the Task Management. To avoid data loss, nodes with SSDs cannot be in the powered-off state for over three months. If the nodes need to be powered off for a long time, ensure that they are powered on at least once every three months. During power-on or power-off, do not perform other operations. Do not use F9 to restore the BIOS default settings and do not enable the NUMA function in the BIOS. This section explains how to use DeviceManager to power on or off a storage node. Prerequisites Power-on and Power-off Precautions have been understood. Context Table 1 defines node operating status. Figure 1 shows the node operating status changes. Table 1 Node operating status Status Description Closed The node is powered off. Starting The node is being started. Online The node is started and running correctly. Offline The node is offline due to a fault. Disabled and Online The node is disabled. Services on the node are switched over to other nodes and data on the node is migrated. If not all storage nodes are required, you can disable and power off some nodes to reduce power consumption.\nLog in to the storage node as user root. Logging in to a storage node remotely using PuTTY a. Start PuTTY and enter the management IP address of OceanStor 9000 V5 in Host Name (or IP address) and keep the default values of other parameters. b. Click Open. After the connection is set up, enter the user name omuser and the password. Press Enter to go to the CLI of the operating system. c. To log in to a non-management node or a storage node to which the management IP address does not float to, run the ssh IP address of the back-end storage network command to switch to the node. d. Run the su command and enter the password of user root to switch to user root. 2. Run the following commands to perform power-on or power-off operations. Run the reboot command to restart the node. Run the poweroff command to power it off. If no output is displayed on the KVM, the node has been powered off. NOTE Run the poweroff command to power off the node, and during the restart, if you reset the system on the BIOS (including the BIOS of the SAS card), the NVDIMM data will be restored, which reduces data reliability. For Details, see Power-on and Power-off Precautions. Follow-up Procedure You can power on a storage node by pressing its power button, performing power-on operations on DeviceManager, or using the IPMI or Mgmt port.\nData is one of a companys most valuable resources. Data loss can significantly affect a companys health, market position, reputation and the wellbeing of its customers. Whether caused by security breaches, virus infection or issues with processing, data loss can cause significant damage and costs and require organizations to address them quickly. Fortunately, many tools and resources can help us determine if data loss has happened and create a plan to avoid a recurrence. This article provides several tips on how we can deal with and recover from data loss, focusing on logging. Data loss has many causes and can happen both when data is at rest and when transmitted over a network. It can also occur intentionally or accidentally. A virus infection can delete data, and a cyberattack on a system can steal data. In addition to this, organizations can also experience data loss due to things like hardware impairments, natural disasters and power failures. Human error is the most common reason for data loss, as human beings are imperfect and prone to mistakes that often negatively affect data. Failures that start within one system and then spread to another are called cascading failures. These failures can result in significant losses of data and heavy damage to systems because the failure no longer concentrates within a single spot. Take a deep breath and discontinue any action. It can be tempting to jump straight into fixing the problem, but take some time to assess what happened and consider the options for your organization.\nWrite down what happened in as much detail as possible. Here are some good questions for security leaders to ask themselves: What specific data is your organization missing? What happened before the loss was discovered? Has any software been recently installed or updated or has your organization made any particular changes to the setup? Has the organization onboarded a new employee? Has the security team identified any recent vulnerabilities? Has equipment been physically damaged (for example, water damage)? Because the root cause of data loss varies, its essential to consider all the possibilities and determine whether the initial identified loss is isolated or part of a bigger problem. Next, cybersecurity professionals must decide which data recovery method to use. In the case of hardware damage to machines or servers, it should still be possible to access data backups stored offsite or in the cloud. If IT has been creating regular backups and using cloud storage, your organization will be able to restore data from recent backups. Otherwise, security leaders will need to look at alternative methods like disk recovery. We can use data recovery software to repair data files and databases. It can also assist in scenarios like accidental file deletion or incorrect server formatting. When dealing with sensitive data such as medical files or losing a large amount of essential data, it is advisable to consult data recovery engineers experienced with complex data retrieval. Data loss prevention is a continuous process that must encompass the whole organization.\nOrganizations must strive to stay aware of email phishing and ensure no one is pushing code with security-compromising bugs. Best practices a company can follow include malware protection, strong password policies, firewalls, ongoing risk assessment and file integrity monitoring. The most effective data loss strategies are part of the larger business continuity and disaster recovery (BCDR) plan. This plan is a regularly updated document that sets out the tasks and responsibilities that need to be undertaken (and by whom) in an incident such as the loss of critical data. It aims to reduce the financial impact of the failure; assist the company in complying with any industry regulations in the case of data loss; and empower a company to respond to the situation in a way that allows them to resume operations as soon as possible. The 3-2-1 rule is the gold standard of business data backup, meaning that companies have three copies of their data stores in two different formats that are easy to access. One of these copies is stored off-site to prevent the risk of data loss from physical destruction or theft. For less critical data, you might consider cost-effective but (very) slow options like S3 Glacier Deep Archive. Organizations must regularly verify any backups to ensure that backup data is updated and functional. If we ever do lose data, aggregated logs are crucial in finding out why the data loss happened to prevent any further losses. Many industries require businesses to use log archiving and log analysis to comply with industry regulations.\nYour organization must configure a log aggregator or reporter to suit your use cases and infrastructure. Security teams can do this with an agent in your containers, Kubernetes, as part of your application code or redirect standard logging to an external service or provider. You can also configure an aggregator to pull in data from all systems, including: CI/CD pipelines Cloud infrastructure (VMs, load balancers, databases, etc.) Platforms like Kubernetes Applications Microservices Data loss often results from cascading failures that start in one system and then cause failures in other parts of your infrastructure, ultimately causing data loss. Log aggregation allows you to trace the problem back to its source and resolve it. Centralized log aggregation and management provide a means to organize the data within an IT infrastructure and enable users to view the health of the data across all systems and applications. Many logging platforms offer an alternative to static reporting by flagging anomalies through real-time alerts. For example, a repeated password failure might indicate an unauthorized attempt to access a network, or it might mean someone has not memorized the password. If a team has set up an alert for multiple failed password attempts, the system will notify them. They can also flag scenarios like write failures, misconfigured storage and incompatible formats. Log aggregation also lends itself well to microservices environments where each service, container instance and orchestration tool produces logs.\nHello all, This upgrade procedure is available for Oceanstor Dorado V3/V6 (Solid State Storage)and Oceanstor V3/V5 (Convergent Storage). 1 Obtaining Upgrade Reference Documents 2 Confirming the Patch Version Before the Upgrade 3 Obtaining Tools and Upgrade Packages 3.1 Confirming Tools and Upgrade Packages 3.2 Downloading Tools and Upgrade Packages 3.3 (Optional) Verifying Upgrade Package Integrity 1 Installing the Upgrade Tool 1.1 Installing SmartKit Online 1.2 (Optional) Importing the SmartKit Function Package Offline 2 Setting the Upgrade Policy 3 Evaluating Array Upgrade 4 Collecting Array Information Step 1 On the Device Upgrade page, click Storage Array Upgrade, as shown in Figure 1. Figure 1 Starting storage upgrade Step 2 Click Perform Upgrade to start the upgrade, as shown in Figure 2. Figure 2 Performing the upgrade Step 3 In the Confirm Upgrade dialog box, confirm the information, select the check box, and click OK to start the upgrade, as shown in Figure 3. Figure 3 Confirming the upgrade Step 4 The system automatically imports the upgrade package, as shown in Figure 4. Figure 4 Importing the upgrade package Step 5 After the upgrade package is imported, the system automatically starts a pre-upgrade check, as shown in Figure 5. Figure 5 Pre-upgrade check The pre-upgrade check mainly carries out inspections directly related to the device and its health status, such as the upgrade package compatibility, device service load, redundant links of hosts, and device alarms. Some check items have been covered by the array upgrade evaluation, while other items must be checked after the upgrade package is imported.\nDear team, What are the requirements for Smartkit installation environments? Thanks for hlep. Daer Axe, Configuration Item Requirement Description Supported operating systems Windows XP 32-bit Windows 7 32-bit Windows Server 2003 32-bit Windows Server 2008 32-bit Windows 7 64-bit Windows Server 2003 64-bit Windows Server 2008 64-bit Windows Server 2012 64-bit Windows Server 2016 64-bit Windows 10 64-bit The following tools that use private protocols do not support 64-bit operating systems: Default Management IP Address Modification Tool Unified Storage Deployment You are advised not to use Windows Server 2003 if the DHCP protocol deployment tool is used. Microsoft officially ceased its support for Windows Server 2003 operating system from July 14, 2015. Meanwhile, Windows Server 2003 is not compatible with JRE 1.7.0.80. Therefore, this operating system is not recommended. Recommended Java runtime environment (JRE) version JRE 1.7.0.80 32-bit The software package contains the JRE. Run the OceanStor SmartKit on 32-bit and 64-bit operating systems using 32-bit JRE. Some of the earlier operating system versions do not support JRE 1.7.0.80. Upgrade your operating system to the latest version before using it. CPU Dual-core 2.4 GHz or higher If SmartKit is installed on a VM, the CPU of the VM must be at least 2.4 GHz. Memory 400 MB or more free memory If SmartKit is installed on a VM, the memory of the VM must be at least 400 MB.\nis a direct memory access technology. It enables a host to directly transmit data to the memory of another host over a network without involving either one's operating system. RDMA boasts low I/O latency, high bandwidth performance, and light CPU loads. RDMA directly transfers data to the memory of a computer over a network and quickly migrates the data from one system to a remote system, eliminating the need to exchange data and to copy data between external memories and thereby releasing memory bandwidth and CPU resources. In this way, application system performance is improved. Figure 1 Process of RDMA is an RDMA solution used for Ethernet. Based on the widely deployed IP protocol, RoCE achieves lower latency and higher performance than traditional Ethernet adapters while retaining the routing capability and delivery assurance of IP. It boasts the following performance highlights: RDMA : The direct read/write technology of the hardware memory allows direct Operations on the remote memory from the local end. In this way, multiple copy and context switching operations among the hardware memory, service memory, and remote memory are eliminated. For a traditional Ethernet application, data is copied for multiple times from the network adapter to the application, which consumes too much memory bandwidth and greatly prolongs the execution time. RoCE allows direct copy operations between the memories of applications at the local and remote ends without software interaction. Therefore, the CPU usage and operation latency are greatly reduced. Transfer acceleration/Kemel bypas s: Different from the host software protocol stack.\nRoCE implements acceleration in chips, releasing precious CPU resources for application computing and processing. The computing work is offloaded from the kernel to the network acceleration hardware. In this way, the CPU can use more computing resources for service processing, and a precise and low-latency solution is ensured. For a traditional Ethernet application, the CPU consumes massive resources for processing the network protocol stack. In addition, the CPU must maintain connection contexts reconstruct cache, and process interruption issues. The resource consumption increases in line with the line rate, seriously limiting system scalability. Figure 2 RDMA transfer acceleration/Kernel bypass 25 Gbit/s or 100 Gbit/s RoC E is used for networking and communication between controller enclosures. Data is remotely transferred over the RoCE link by Huawei-developed interface modules without intervention by the CPUs on either side. In this way, data transfer efficiency is greatly improved and access latency is reduced. OceanStor Dorado uses RoCE for networking between controllers. Data is transmitted between controllers over a hop of the RoCE link without forwarding. Besides, controller enclosures are connected to smart SAS or smart NVMe disk enclosures over RDMA links. The following figure uses a high-end OceanStor Dorado storage system as an example. Figure 3 RDMA interconnection for an eight-controller system Data is remotely transferred over RoCE links by interface modules without intervention by the CPUs on either side. This greatly improves data transfer efficiency and reduces the access latency. OceanStor Dorado uses the RoCE technology, which ensures lower latency than PCle and SAS links.\nDear All, Today we are going to discuss about Features of Huawei OceanProtect Backup Storage. OceanProtect Backup Storage Leads Dedicated Backup into the F2F2X (Flash-to-Flash-to-Anything) Era . OceanProtect: An Enterprise-Class Benchmark Backup Storage Rapid Backup Rapid Recovery End-to-end backup acceleration Instant recovery Efficient Reduction Backup data preprocessing and variable-length deduplication Compression/compaction High Reliability Data consistency check System-level reliability Proactive O&M Flash-Based Acceleration to Boost Backup & Restore Performance Instant Recovery and Optimal IOPS Hardware All-flash efficient storage media Intra-node multi-controller A-A architecture Software Full-stripe write + intelligent prefetch, greatly improving service performance High IOPS for instant recovery Benefits of High Data Reduction When using primary storage or other backup storage as the backup target, 1 PB of initial data will increase by 12 times after a dedicated backup period. More capacity More rack space More power consumption High TCO When using dedicated backup storage as the backup target, 1 PB of initial data will increase by 1.3 or 1.5 times after a dedicated backup period. Less capacity needed Much less space occupied Much less power consumed Much lower TCO 4-Step Advanced Deduplication and Compression for Optimal Data Reduction To be supported in OceanProtect 1.1.0. The data reduction ratio depends on the application type and backup policy. Backup Data Preprocessing + Multi-Layer Inline Variable-Length Deduplication for Higher Data Deduplication Ratio Feature-Based Compression and Byte-Level Compaction Achieve Industry-Leading Data Reduction Ratio Feature-based compression: Huawei's patented HZBC algorithm leads the compression ratio, 57% higher than LZ4, 27% higher than GZ fast, and 14% higher than GZ.\nDear All, Today we are going to learn about SmartTier Overview When hot and cold data is distributed randomly, the particular storage characteristics of different types of media are not taken into account. SmartTier automatically matches data to the storage media best suited to that type of data. For example, cold data is stored on NL-SAS disks, and hot data is stored on SSDs. Data flows vertically, improving storage performance and reducing costs. Disk technologies are being developed, and storage systems need to support more storage and more different types of media. Each type of storage media offers its own unique advantages and disadvantages in terms of price and performance. It is often difficult for users to strike the right balance between the two. Data features The most active data is stored in or migrated to a high-performance tier, where the read performance is significantly better. Hot data is stored in or migrated to a performance tier. Cold data is stored in or migrated to a capacity tier without any performance reduction after migration. Dividing Storage Tiers In a storage pool, a storage tier is a collection of storage media that all deliver the same level of performance. Any given storage tier is comprised of the same type of disks and RAID level. A storage pool is a logical combination of up to three storage tiers. The types of disks in a storage pool determine how many storage tiers there are.\nHello everyone In this post, you can learn: 1. Powering Off the Cluster in DeviceManager 2. Restarting the Cluster in DeviceManager 3. Powering Off a Storage Node in DeviceManager 4. Restarting a Storage Node in DeviceManager 5. Remotely Powering On or Off a Storage Node On the iBMC Management Page 6. Powering On or Off a Storage Node by Pressing the Power Button In DeviceManager, you can power off all nodes in a cluster. Context Only the super administrators and administrators can power off the cluster. Prerequisites Services in the storage system have been stopped. Procedure 1. Log in to DeviceManager. 2. Click in the upper right corner and choose Power Off Cluster . 3. Carefully read information in the Danger dialog box and select I have read and understand the consequences associated with performing this operation . NOTICE If you select Power off the management nodes when the service nodes fail to be powered off in the Danger dialog box that is displayed, the management nodes will be powered off even if the service nodes fail to be powered off. If you do not select this option, the management nodes will not be powered off in the scenario where the service nodes fail to be powered off. 4. Click OK to power off the cluster. Follow-up Procedure You can press the power button on a storage node or remotely power on the node on the iBMC management page. In DeviceManager, you can power off all nodes in the cluster.\nIn his dissertation titled \"Logical Design of a Digital Computer with Multiple Asynchronous Rotating Drums and Automatic High Speed Memory Operation,\" German physicist Fritz-Rudolf Gntsch introduced the idea of virtual memory. It described a machine with six 100-word blocks of primary core memory and a 1,000-word address space, with hardware automatically moving blocks between primary memory and secondary memory. At the University of Manchester, paging was first used to increase the working memory of the Atlas Computer by combining its 16,384 words of primary core memory with an additional 98,304 words of secondary drum memory. Although the first Atlas was ordered in 1962, functional paging prototypes had already been created by 1959. The first commercial computer with virtual memory, the B5000, was independently produced by the Burroughs Corporation in 1961. It used segmentation rather than paging. Many issues needed to be solved before virtual memory could be incorporated in common operating systems. Initial implementations of dynamic address translation caused a minor slowdown in memory access because it required expensive, specialized hardware that was challenging to construct. There were concerns that newly introduced secondary storage-based system-wide algorithms might perform less well than previously employed application-specific algorithms. The argument over virtual memory for business computers was settled by 1969 when a team of IBM researchers led by David Sayre demonstrated that their virtual memory overlay system regularly outperformed the finest humanly managed systems.\nS3 Browser is a freeware Windows client for Amazon S3 and Amazon CloudFront. Amazon S3 provides a simple web services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web. Amazon CloudFront is a content delivery network (CDN). It can be used to deliver your files using a global network of edge locations. S3 Browser is a desktop application on Windows and is provided by Amazon for the object service. It supports access to the object service through Amazon S3 APIs. If the object service is compatible with Amazon S3 APIs, you can use this tool to perform common operations, such as creating buckets, uploading objects, and downloading objects. However, APIs and features supported by the object service are different from those supported by Amazon S3. Therefore, when using this tool for access, compatibility issues may occur in some scenarios. Download the software from the S3 Browser official website: For details about the supported S3 Browser versions, and some functions on S3 Browser are not supported currently, visit . Figure 1 shows the procedure for configuring and using S3 Browser. Table 1 describes the key parameters for configuring S3 Browser. Figure 1 Procedure for configuring and using S3 Browser For details about how to manage object service resources using S3 Browser, visit the S3 Browser official website. When using S3 Browser to list objects in a bucket, if there are a large number of objects, set S3 Browser to display the objects in multiple pages.\nA multiprogramming computer's memory is divided among many processes, with some of it being used by the operating system. Memory management is the process of allocating memory to multiple processes. Contiguous or non-contiguous memory/partitions can be used to allocate memory. The non-contiguous allocation of memory, or memory management approach, includes paging and segmentation. Paging is essentially an operating system memory management method. Using this method, the operating system transfers processes in the form of pages from secondary memory into the main memory. A logical idea is paging. With the use of this method, the main memory is divided into discrete frames, which are small physical memory chunks. The size of frames during paging is fixed. The frame size must match the page size in order to avoid external fragmentation and maximize main memory use. This method facilitates quicker data access. Advantages of Paging: Memory management that is effective. Simplicity in partitioning (non-contiguous memory allocation). Allocating memory is simple and inexpensive. Pages are simple to share. No compaction is necessary. no external fragmentation. more efficient swapping. Disadvantages of Paging: Internal fragmentation (only at the last page of the process). Address translation necessitates the use of specialized hardware. The page table is stored in the main memory. Address translation lengthens memory cycle times. Memory reference overhead is caused by multi-level paging. Memory access time is longer. Another method of managing memory is segmentation.\nDear All, Today we are going to learn about SmartMigration Technology. Overview SmartMigration is a key service migration technology. Services on a source LUN can be completely migrated to a target LUN without interrupting host services. The target LUN can totally replace the source LUN to carry services after the replication is complete. SmartMigration features: Reliable service continuity: Service data is migrated online, and it can be completed without interrupting services. Stable data consistency: Data changes on a host are quickly synchronized to both the source and target LUNs during a data migration, ensuring data consistency after migration and making sure you do not lose any data. Flexible performance: Data is moved between different storage media and between RAID levels based on service requirements. Cross-system compatibility: Data can be migrated not only within a single storage system but also between a Huawei storage system and a compatible heterogeneous storage system How SmartMigration Works SmartMigration is leveraged to adjust service performance or upgrade storage systems by migrating services between LUNs. SmartMigration is deployed in two stages The storage system uses the virtualized storage. Virtual data in a storage pool consists of both data volumes and metadata volumes. Data volumes store the actual user data. Metadata volumes record the data storage locations, including the LUN IDs and the data volume IDs. LUN IDs are used to identify LUNs and data volume IDs are used to identify the physical space for the data volumes SmartMigration Service Data Synchronization After creating a SmartMigration task, pair a source and target LUN.\nService data synchronization between the source and target LUNs comprises initial synchronization and data change synchronization Pair: In SmartMigration, a pair is a source LUN and the target LUN that the data will be migrated to. A pair can have only one source LUN and one target LUN. There are two synchronization modes for service. They are independent and can be performed at the same time to ensure that service data changes on the host are synchronized to the source LUN and the target LUN. Data change synchronization A host delivers a write request to the LM module of a storage system. The LM module writes the data to the source LUN and target LUNs and logs the operations. The source LUN and target LUN return the data write result to the LM module. The LM module determines whether or not to clear the log based on the write results. A write success acknowledgment is returned to the host. SmartMigration LUN Information Exchange In a LUN information exchange, LUNs are mapped to data volumes. During the exchange, data volume IDs change, but not the source and target LUN IDs Before a target LUN can take over services from a source LUN, the two LUNs must synchronize and then exchange information. Each LUN and its corresponding data volume are identified by a LUN ID and a volume ID. A source LUN corresponds to a data volume. The LUN IDs are logical entities, but the volumes are physical.\nBefore LUN information exchange: A host identifies a source LUN by its LUN ID. At this point, the mapping relationship exists between the LUN ID and the data volume ID. During LUN information exchange: The source and target volumes trade IDs. This means that the ID for the source LUN is now pointing to the target LUN. After LUN information exchange: Now the source LUN ID has not changed, and users do not notice anything because services are not affected. The source LUN ID is now mapped to the target data volume, and the host starts reading from and writing to the physical space of the target LUN. SmartMigration Pair Splitting Splitting is performed on a single pair. The splitting process includes stopping service data synchronization between the source LUN and target LUN in a pair to exchange LUN information, and removing the data migration relationship after the exchange is complete. In splitting, host services are suspended. After information is exchanged, services are delivered to the target LUN. Service migration is invisible to the users. Pair splitting: Data migration relationship between a source LUN and a target LUN is removed after LUN information is exchanged. After the pair is split, if the host delivers an I/O request to the storage system, data is only written to the source LUN. The target LUN stores all data of the source LUN at the pair splitting point in time. After the pair is split, no connections can be established between the source LUN and target LUN.\nDear All, Today we are going to learn about SmartCompression Technology. Overview SmartCompression reorganizes data to save space and improves the data transfer, processing, and storage efficiency without losing any data. The system supports inline compression, that is, only newly written data is compressed. The storage systems of the Huawei OceanStor Dorado V6 storage systems support inline compression and post-compression. Both are types of lossless compression. Inline compression: Data is compressed before being written to disks. Post-compression: Data is written to disks in advance and then read and compressed when the system is idle. How SmartCompression Works Use Cases of SmartCompression Database: A database is an excellent example of where data compression is useful. Many users would be happy to sacrifice a little performance to recover over 65% of their storage capacity. File service: A file service is another common scenario for data compression. For a storage system with a file service enabled, peak hours occupy half of the total service time and the dataset compression ratio of the system is 50%. In this scenario, SmartCompression slightly decreases the IOPS. Engineering and seismological data: Engineering and seismological data have similar requirements to database backups. This type of data is stored in the same format, but there is not as much duplicate data. Such data can be compressed to save the storage space. Where SmartDedupe and SmartCompression Are Used Together Deduplication and compression can be used together to save more space.\nDear All, Today we are going to disuse about Data Protection Trends and Challenges. Data Is the Key Production Factor for the Digital Economy, More and More Data Are Generated and Processed Each Day. 1. Larger data amount of traditional core applications 2. New platform and workloads (e.g. Container) generate different types of data for backup Data Loss Has Serious Consequences and Should Be Alerted Software and hardware failures. Failure of core databases at a bank A large number of services interrupted for 37 hours UPS overload at an IDC service provider Power failure in the equipment room and system breakdown Services of multiple financial institutions interrupted for 7 hours. Natural disasters Fire in the OVH's data center in Europe 3.6 million+ websites down, and huge amounts of user data could not be restored. Flood in an ISP's data center A large number of devices damaged, and user data permanently lost.\nDear all, What can we do if the storage system cannot be accessed through the serial port? After a maintenance terminal is connected to the serial port on a storage device with a serial cable, the maintenance terminal cannot receive messages from the serial port, the serial port outputs bit errors, or the login prompt is not displayed. Here are 4 possible causes for a login failure through a serial port: The serial port is being used. The serial cable is connected improperly. The serial port connection parameters are incorrectly configured. The serial port is disabled for remote connection. 1. After running the serial port login tool, check whether a message displays indicating that the serial port cannot be opened. If yes, go to Cause 2: The serial cable is connected improperly. If no, go to 2 . 2. Stop the program or process that is using the serial port. 3. On the maintenance terminal, reattempt the login using the serial port. Check whether the maintenance terminal receives messages outputted by the serial port. If yes, no further action is required. If no, go to Cause 2: The serial cable is connected improperly. 1. Check whether the serial port has output. Press Enter , and check whether the corresponding output is displayed. If yes, go to Cause 3: The serial port connection parameters are incorrectly configured . If no, go to 2 . 2. Remove and reinsert, or replace the serial cable. 3. On the maintenance terminal, retry the login through the serial port.\nHi, Guys! Neither. The WORM that we discussed today is related to computer security, but it is not a computer virus. OKLet's go into today's theme: WORM Now is an information era, with the explosive growth of information. A large amount of data is stored in the Internet. Data security is attracting more and more attention. For example, national laws and regulations, criminal cases, and important personal data need to be protected against tampering. The WORM (Write Once Read Many) feature comes into being. It provides multiple read and write technologies. It is a commonly used method for data security access and archiving in the industry. It aims to prevent data from being tampered with and archive and archive data. What is WORM? WORM means Write Once Read Many. It is a data security protection feature. After a file is written, the WORM feature immediately removes the write permission of the file to enable it to enter the read-only state. In this state, files can only be read and cannot be modified, greatly improving data security. How does WORM work? The WORM feature adds WORM properties to common file systems so that files in the WORM file system can be read only during the protection period. After a WORM file system is created, the file system is mapped to the application server using the NFS or CIFS protocol.\n1. Powering On and Off the Storage System 2. Power On and Off interface module 3. Rebooting the Storage System 4. Emergency power-on and power-off 5. Powering On a Key Management Server 6. Powering On and Off Disk Enclosures Powering On and Off the Storage System Precautions Procedure NOTE: Follow-up Procedure Prerequisites Context NOTE: NOTE Prerequisites Precautions Procedure NOTE method 1. method 1 Procedure NOTE Powering Off the Storage System method 1 Precautions Procedure NOTE Follow-up Procedure Prerequisites Context Procedure . Procedure 1Log in to DeviceManager. 2. Choose System . 3.Click to switch to the rear view. 4. Click the interface module you want to power on. The Interface module dialog box is displayed. 5. Click Power On . The Success message box is displayed, indicating that the operation succeeded. 6. Click OK . The interface module is powered on. Before replacing the interface module, power off it at first. Prerequisites All services related to the interface module have been stopped. Procedure 1. Log in to DeviceManager. 2. Choose System . 3. Click to switch to the rear view. 4. Click the interface module you want to power off. The Interface module dialog box is displayed. 5. Click Power Off . The security alert dialog box is displayed. 6. Confirm the information in the dialog box and select I have read and understand the consequences associated with performing this operation . 7. Click OK . The Success message box is displayed, indicating that the operation succeeded. 8. Click OK . The interface module is powered off.\nProcedure 1. On the navigation bar of the ISM, click . 2. Click Device View in the main window to access the front view of the storage device. 3. Click in the front view to switch to the rear view of the storage device. 4. Click the interface module that you want to power on . 5. In the dialog box that is displayed, click Power On. The Information dialog box is displayed indicating the success of powering on the interface module. 6. Click OK and you have finished powering on the interface module. Before replace the interface module, power off it at first, you can power off it on the ISM. Prerequisites All services related to the interface module have been stopped. Procedure 1. On the navigation bar of the ISM, click . 2. Click Device View in the main window to access the front view of the storage device. 3. Click in the front view to switch to the rear view of the storage device. 4. Click the interface module that you want to power off. 5. In the dialog box that is displayed, click Power Off . The Danger dialog box is displayed. 6. Read the content of the dialog box carefully and select I have read the warning message carefully . to confirm the information. Then, click OK . The Information dialog box is displayed indicating the success of powering off the interface module. 7. Click OK and you have finished powering off the interface module.\nDear all, Selecting an appropriate capacity expansion method is subject to factors such as the amount of capacity you want to expand. The capacity and performance of storage systems can be expanded by adding: Disks Disk enclosures Disk bays Controllers or controller enclosures Links Storage pools, LUNs, and file systems You can select an appropriate capacity expansion method based on storage system conditions and capacity requirements of services running on the storage systems. Expanding Capacity By Characteristic Application Scenario Adding disks No need to stop services Easy to operate Fast Cost-effective The storage system of which capacity you want to expand has enough empty disk slots and the total capacity of disks to be inserted into the empty disk slots meets your service capacity requirements. Adding disk enclosures No need to stop services Large capacity expandable The storage system of which capacity you want to expand does not have enough empty disk slots or the total capacity of disks to be inserted into the empty disk slots does not meet your service capacity requirements. For example, 5 TB of capacity is required but only 2 TB is available after disks are inserted into all empty disk slots. Adding disk bays No need to stop services Large capacity expandable The total capacity of fully loaded existing disk bays does not meet your service capacity requirements.\nAdding controllers No need to stop services Large capacity expandable Accelerating performance The number of disk enclosures in your storage system has reached the upper limit and there is no empty disk slot. The storage system does not meet performance requirements of services. Adding links No need to stop services The storage system does not meet reliability or performance requirements of services. Expanding storage pools More available capacity for services No need to stop services The capacity or number of storage pools does not meet service requirements. Idle disks exist in your storage system. Expanding LUNs or file systems More available capacity for services The capacity or number of LUNs or file systems does not meet service requirements. Free capacity exists in your storage pools. The type of the new disks must be the same as that of the disks in the storage pool, and the number of the new disks is greater than or equal to the minimum number of disks allowed to be added. You can use eDesigner for evaluation. A storage pool supports disks of two different capacity specifications. Upon capacity expansion, add disks of only one qualified specification at a time. The following scenarios are involved. When expanding the capacity of a storage pool in a storage system with one controller enclosure, add disks of only one qualified specification at a time.\nIf the storage pool contains disks of only one capacity specification, you can either add disks of the same type and capacity specification as the existing ones (recommended) or add disks of the same type but a larger capacity specification than the existing ones. For example, if a storage pool contains disks and the capacity of each disk is 960 GB (disk capacity specification), you can add either disks of which the disk capacity specification is 960 GB (recommended) or disks of which the disk capacity specification is greater than 960 GB. If the storage pool contains disks of two capacity specifications, you can only add disks of the greater capacity specification. For example, if a storage pool contains disks of both 960 GB and 1.92 TB capacity specifications, you can only add disks of 1.92 TB capacity specification upon capacity expansion. When expanding the capacity of a storage system with two or more controller enclosures by adding disks or disk enclosures, you must follow all the capacity expansion principles for a single controller enclosure storage system. In addition, if a storage pool contains disks owned by two or more controller enclosures, ensure that the number, capacity specification, and slots of the disks and disk enclosures are the same for those controller enclosures. The available capacity of storage systems must be properly planned to ensure sufficient capacity for service data. You can use eDesigner to plan the available capacity and calculate the capacity that needs to be purchased.\nDMaaS is a cloud service that provides enterprises with centralized storage of disparate data sources. \"As a service\" references a pay-per-use business model that does not require customers to purchase or manage infrastructure for data management. In this business model, customers back up their data to a DMaaS service provider. This is usually done by installing an agent on the data source to be backed up, although in the case of cloud data sources a simple authentication process may be the first step. DMaaS is generally an operating expense that goes up and down based on the amount of service a customer uses. It is technically possible to provide DMaaS using on-premise infrastructure or a private cloud provided by a DMaaS provider, but all infrastructure must be provided and managed by the DMaaS provider to be considered a service. While it is possible to do data management as a service in this way, doing so is prohibitive for logistical and cost reasons. DMaaS has to be done as a service -- it's not data management as a service if a company has to buy, install, and maintain a lot of infrastructures to perform data management. The \"as-a-service\" moniker should be in keeping with the traditions of the services that created and defined the concept, such as Salesforce, Microsoft Office 365, and Google Workspace. For example, none of these companies require customers to install or manage virtual or physical infrastructure to provide or use services.\nCompanies using such services simply tell the provider their specific needs, such as how many users should be registered, and how much storage each user needs. The infrastructure required to provide this service will be automatically provisioned and managed by the vendor. Data management as a service uses cloud services to provide scalability, insights, and accessibility to a company's various data sources. This centralized data collection is then used to provide data protection and other services. Data sources include file servers, application servers, and database servers, as well as virtual machine (VM) databases. Most companies also have data on one or more cloud providers and many desktops, laptops, and mobile devices. Comprehensive data management solutions protect and manage data from all sources in a single cloud-based system; however, some data management options only protect and manage a subset of these data sources. Other services mentioned above include proactive compliance, data analysis, legal hold and centralized search. A centralized view across all data sources provides the best view for many of these services. For example, if an e-discovery request requires a company to find and save all instances of a given employee's work, being able to perform a single search across server, laptop, and cloud instances makes it easier to satisfy the request. Three advantages of DMaaS hosting over other data management solutions include: A complete DMaaS system protects all of a company's data assets while gaining additional value while reducing costs.\nThe centralized storage of data required by DMaaS eliminates waste and facilitates other parts of the business. Using data management services (rather than buying and maintaining infrastructure to do it yourself) reduces capital expenditures and makes data management costs more predictable. The long-term goal of DMaaS is to integrate physical data center infrastructure management with many other services, including IT workload management, energy management, connectivity, and business costing. But it takes time to reach the other side of success. In terms of feature richness, DMaaS is not yet comparable to 'traditional' DCIM, but over time we expect it to grow even more. DMaaS allows the data center landscape to span DCIM, spanning the proprietary management of a single site. Over time, additional data and services may be added, including integrated workload management, energy management, customer relationship, and business systems, weather, employee services, and security and network management. DMaaS applies DCIM reduces risk while increasing efficiency, improving capacity forecasting, and enhancing business agility. However, it remains an under-deployed technology due to challenges such as changing the operating environment to support DCIM and difficulty measuring overall ROI. The promise of DMaaS is expected to alleviate these challenges and increase the value of DCIM. When computing environments are distributed across traditional on-premises data centers, cloud and colocation sites, and edge computing applications, data centers are more difficult to manage. The traditional DCIM model requires software to be installed at each data center site, which can be challenging when comparing performance across environments.\nDMaaS is a cloud-based remote monitoring service designed to support various deployment sites. DMaaS solutions are evolving to include a full suite of services, such as backup and archive, disaster recovery, analytics, and security, all on one platform. For example, these solutions use machine learning to identify anomalies and catch ongoing cyberattacks. When choosing a comprehensive DMaaS solution, organizations should look for several key attributes in providers, such as: A SaaS-based platform that provides a searchable view of all data, no matter where it resides, through a unified dashboard with a simple user interface. The platform should support data from various cloud services. Ability to subscribe to various data management services such as backup, disaster recovery, archive, files, and dev/test from one provider instead of using a different provider for each service. Users should get up and running quickly by connecting to the service immediately. The platform should provide a consistent and seamless experience across hybrid environments, using a single GUI to manage all of its data. They will be able to move and restore data to any location within these environments. The platform should be able to leverage advanced cloud services such as machine learning capabilities to detect cyber attacks like ransomware, or data classification services to address compliance and privacy concerns. In addition to the simple core benefit of allowing the IT team to manage data while the infrastructure is managed by a proven solution provider, a comprehensive DMaaS solution provides the flexibility to move data across hybrid environments while managing data centrally place.\nThis significantly simplifies operations and makes it easier for organizations to extract business intelligence and insights to compete more effectively in their respective industries. DMaaS will allow your organization to subscribe to discrete data management offerings addressing a wide range of use cases within a single DMaaS solution. Heres a quick list of the top ten: Offsite Backup and Recovery and BaaS SaaS and Cloud Apps Protection Long-Term Retention and Archiving Disaster Recovery Air-Gap Security and Ransomware Recovery Compliance and Data Governance Files and Objects Development and Test Cloud Data Lake Analytics is an intelligent storage management platform designed for a wide range of applications, offering convergence, intelligence, and openness. Simply stated, it simplifies storage management and Operations and Maintenance (O&M), improves the operation efficiency of data centers, and automates storage throughout its entire lifecycle. DME connects to mainstream cloud management platforms and Information Technology Service Management (ITSM) through open APIs, scripts, and plug-ins, enabling the smooth integration into the existing environments and service processes of customers. Users simply need to log in to a software interface to complete resource provisioning, intelligent O&M, and data protection for data services, simplifying multi-device storage management. DME centrally manages Huawei storage, third-party storage, switches, and hosts, all through a unified management interface and open southbound APIs. Data Management as a Service (DMaaS) is a cloud-based service model that provides organizations with a comprehensive set of data management tools and capabilities as a subscription service.\nIn this article we will Introduce The SATA, SAS, fc and NL-SAS interfaces Interfaces and protocols on hard drives The types of physical interfaces of hard drives A protocol can be thought of as a set of communication commands and rules. Disk drives, solid state media, and storage arrays interact with other devices based on a single protocol. Of course, disk arrays (RAIDs) often use different protocols. In the storage world, each protocol often has its own physical interface specifications, meaning that the protocol can be associated with the physical interface of a drive. For example, a SATA drive is associated with the SATA protocol and the physical SATA interface. The same thing applies with SAS and fc. Each has its own interface and protocol. Today, the following four protocols and interfaces dominate the world of disk drives: Serial Advanced Technology Attachment (SATA) Serial Attached SCSI (SAS) Nearline SAS (NL-SAS) Fiber Channel (FC) SATA Interface (SATA) SATA is one of the most well-known physical media used in desktops, servers and laptops. This technology was able to make its way into the advanced enterprise technology market, but in the world of storage arrays, NL-SAS technology quickly replaced it. In enterprise technology, SATA drives are synonymous with terms such as low cost, low performance, and high capacity. The main reason for this is that the ATA instruction set is not as rich and efficient as the SCSI instruction set and is therefore not suitable for high performance workloads.\nDear team, What does BBU do in Huawei Oceanstor storage? Thank you. Dear friend, A Power-BBU module consists of a power module and a BBU. Only AC power modules are supported and they allow the controller enclosure to work properly at maximum power. BBUs provide enough power to ensure that any data in flight is de-staged to the vault area in the event of a power failure. If a BBU is faulty, it can be isolated without affecting the normal running of the storage system. If a power failure occurs, BBUs ensure that the storage system writes cached data to the built-in disks of the controllers, preventing data loss. After the external power supply resumes, the driver reads data from the built-in disks of the controllers to the cache. In a system using the lithium batteries, the battery capacity is updated and detected by charging and discharging the batteries. In this way, the problems can be detected in advance that the battery capacity attenuates, the batteries fail to meet the power backup requirements of the system, and thus the data backup fails when the batteries are not used for a long time. Then, the reliability of data protection upon the system power failure can be improved. Each controller enclosure has two power modules (PSU 0 and PSU 1) to supply power to controllers A and B. The two power modules form a power plane and are redundant of each other.\nWhat Does Storage? Storage is a process through which digital data is saved within a data storage device by means of computing technology. Storage is a mechanism that enables a computer to retain data, either temporarily or permanently. Storage devices such as flash drives and hard disks are a fundamental component of most digital devices since they allow users to preserve all kinds of information such as videos, documents, pictures, and raw data. Storage may also be referred to as computer data storage or electronic data storage. Storage is among the key components of a computer system and can be classified into several forms, although there are two major types: Volatile Storage (Memory): Requires a continuous supply of electricity to store/retain data. It acts as a computer's primary storage for temporarily storing data and handling application workloads. Examples of non-volatile storage include cache memory and random-access memory (RAM). Non-Volatile Storage: A type of storage mechanism that retains digital data even if its powered off or isnt supplied with electrical power. This is often referred to as a secondary storage mechanism and is used for permanent data storage requiring I/O operations. Examples of volatile storage include a hard disk, USB storage and optical media. Storage is often confused for memory, although in computing the two terms have different meanings. Memory refers to short-term location of temporary data (see volatile storage above), while storage devices, in fact, store data on a long-term basis for later uses and access.\nFive nines is the term used for describing the availability of a computer or a service at 99.999 percent of the time it is required. In other words, the system or service is only unavailable for 5.39 minutes throughout the year for planned or unplanned downtime. Five nines is recommended and required for mission-critical requirements and for certain areas such as e-commerce. However, five nines availability has always been a challenge for a service or network and is often impossible to guarantee. Five nines ensures high availability and reliability of the computer or service. It is often the desired percentage availability of a given system or service. However, there is no ruling committee or body which formalizes the meaning or definition of five nines. In order to achieve five nines, most employ monitoring systems in order to proactively resolve all issues. One of the approaches to achieving five nines is by duplicating the components so that backup components are always available. Along with cost involved, redundancy is also a problem with this approach. Another approach has been to build a shared component system in which another active system could be made available in case of a failure. Ensuring five nines over a period of time is challenging. The standard is expensive due to cost of the physical infrastructure as well as software components. Additional components add to complexity and risk. High-capacity planning and multiple Tier 4 data centers are recommended for five nines.\nHell, guys. Today let's talking about Storage Class Memory(SCM) and NVMe. All-SSD Flash Arrays are widely used in enterprise-class storage. Compared with traditional HDDs, all-SSD flash arrays have significantly improved latency, performance, and reliability. The following describes the development history of SSDs. The SSD storage medium and interface technology are constantly developing and evolving. SSDs are divided into several phases. In the first phase, SATA SSD or SATA/SAS SSD are used. In this phase, SLC and eMLC are used. The second phase is PCIe SSD . The biggest problem of PCIe SSDs is non-standard. Many private protocols work independently. PCIe SSDs are classified into Host-based SSDs and device-based SSDs based on FTL positions. Until the NVMe era, unified interfaces and protocol standards are used. NVMe has three major product forms: The first type is U.2 compatible with SATA/SAS SSDs, the second type is PCIe compatible SSD cards, and the third type is M.2 commonly used by consumer products. Although NVMe has greatly improved the interface standards and data transmission efficiency, the mainstream storage media are still based on NAND flash. How should storage media develop? Intel Optane series drives have been proved to have greater storage advantages only when they are paired with storage class memories (SCMs). In this case, data storage will usher in a great leap forward, and the future of NVMe is the SCM. It will be based on industry protocol standards rather than proprietary technology. In fact, Intel/Micron released 3DX (PRAM) to combine NVMe and SCM for the first time.\nSince then, major vendors have increased investment in SCM media. Intel calls the Optane Memory Apache Pass (AEP) a revolutionary SCM designed for high performance and flexibility. Optane NVMe SSD is called the Coldstream, which is the fastest SSD with the best availability and serviceability in the world. Currently, SCM focuses on filling the capacity and performance gap between SRAM and Storage. Currently, there are many types of SCM media under development, but the mainstream SCM media include PCM , ReRAM , MRAM , and NRAM . Phase-Change RAM (PRAM) uses the electrical conductivity difference between the crystalline state and the amorphous state of a special alloy material to represent 0 or 1 data. Its advantages are simple structure, easy to achieve large capacity and low cost. It is mainly used for cache acceleration and cache memory applications . Considering the maturity, popularity, and write penetration of PRAM, it is usually used with DRAM or SRAM to form a high-speed cache application resource pool with the tiering capability while filling the performance and capacity gap between RAM and Storage. The typical example is Intel's 3D Xpoint. Resistive RAM (ReRAM) applies different voltages between the power-on and power-off electrodes to control the formation and fuse status of the internal conductive wire of the cell and displays different impedances (memory resistors) to indicate data. Currently, the typical vendors are HPE and Crossbar. On the Internet, the existing cross-CPU memory access is limited by the network latency and cannot fully utilize the SCM medium persistence feature.\nRedundant array of independent disks (RAID) is a method of storing duplicate data on two or more hard drives. It is used for data backup, fault tolerance, improving throughput, increasing storage functions, and enhancing performance. RAID is attained by combining two or more hard drives and a RAID controller into a logical unit. The OS sees RAID as a single logical hard drive called a RAID array. There are different levels of RAID, each distributing data across the hard drives with their own attributes and features. Originally, there were five levels, but RAID has advanced to several levels with numerous nonstandard levels and nested levels. The levels are numbered RAID 0, RAID 1, RAID 2, etc. They are standardized by the storage networking industry association and are defined in the common RAID disk data format (DDF) standard data structure. RAID was first patented by IBM in 1978. In 1987 a team of electrical engineers and computer science specialists from the University of Berkley in California defined RAID levels 1 through 5. Their work was published by the Association for Computing Machinery's Special Interest Group on Management of Data in 1988. It was called a case of redundant arrays of inexpensive disks (RAID). The objective was to combine multiple inexpensive devices into an array, which featured more storage, dependability and faster processing. Later, RAID marketers eliminated the term \"inexpensive\" so there was not a low cost association by consumers and changed the term to Independent.\nDear all, Can you believe that5D Optical Disc Could Store 500TB for Billions of Years? Hard drives and flash storage have become more reliable over the years, but only on human timescales. What if we need more persistent data storage? Decades? millennium? The key to this vision could be 5D optical storage, which has 10,000 times the data density of Blu-ray discs. But writing data to the glass plate this way was always too slow - until now. A new technique developed at the University of Southampton has greatly accelerated the process without compromising the reliability of the data. This type of data storage uses three layers of nanoscale dots in glass disks. The size, orientation, and location of points (three dimensions) give you five \"dimensions\" for encoding your data. A 5D disc is still readable 13.8 billion years later, but it would be surprising if someone was nearby reading them at the time, the researchers said. In the short term, 5D optical media can survive heating to 1,000 degrees Celsius. You can see earlier, smaller versions of the disc above. This isn't the first time 5G optical data storage has appeared. It was just unrealistically slow before. A laser is used to add data to the disc, but if the laser is moved too fast, the structural integrity of the disc is compromised. The technique devised by PhD researcher Lei Yuhao uses a femtosecond laser with a high repetition rate.\nHello all, Oracle created an add-on version of the Solaris operating system it acquired when it acquired Sun Microsystems in 2009. The new version of the operating system is called the Common Build Environment (CBE). As Oracle senior software engineer Darren Moffat explained this week, a CBE is similar to a beta in that it includes a pre-release version of the upcoming Solaris release. These releases are called Support Repository Updates (SRUs) and are now released monthly. Any security fixes provided in Oracle's quarterly Critical Patch Updates (CPUs) are provided in the SRU. All SRUs are for Solaris 11.4 - the current and possibly last version of the operating system. Oracle's Solaris license already allows free testing and development or personal use. But as Moffat explained in his post, the free-to-use version of Solaris provided by Oracle didn't keep pace with all the changes provided in the SRU. As such, Oracle believes the new CBE product is necessary because the move to a continuous release cycle and a monthly SRU release cadence keeps developers from getting a free-licensed version of Solaris that includes the latest updates. Former Oracle exec warns Big Red's audit process is also a 'sales enablement tool' read more But the CBE is not always fully synchronized with the code provided in the SRU. \"We intend to release CBE releases on a regular basis,\" Moffat wrote. This means that CBE will not match the SRU or CPU release cadence.\nWhen they actually land, the CBE will include all the changes found in previous SRUs and CPUs -- but probably months behind. This free product is intended for open source software developers or non-production personal use and is available under Oracle's Early Adopter License. Upgrading to a full SRU is possible - if the user is willing to sign an Oracle support contract. Today marks the first delivery of our \"Common Build Environment\" (CBE) releases for the Oracle Solaris 11.4. To enable us to make new features and fixes available quicker and to more systems Oracle Solaris now uses a continuous delivery model of SRU/micro releases rather than much larger minor releases every few years. The GA release of a major or minor was historically the release intended for non-production use for developement of free/open source software, testing, proof of concept deployments. With the switch to a continuous delivery model many new features that have been added to Oracle Solaris 11.4 are not available in a release with a non-production use license. The SRUs also contain updates to the free and open source software that is included with Oracle Solaris. The source code repository with build instructions and patches for the open source software is available on our . Some of the Oracle Solaris patches enable free/open source software to take advantage of functionality delivered after the 11.4.0 release. You can upgrade an existingOracle Solaris 11.4.0system to the CBE release today and get access to thenew featuresreleased in SRU/micros since 11.4.0.\nIntegrating data from diverse sources creates a coherent view. Ingestion includes cleansing, ETL mapping, and transformation. Data integration produces effective, actionable business intelligence. Data integration isn't standardized. Data integration solutions require a network of data sources, a master server, and clients accessing the master server's data. Typically, the client requests data from the master server. The master server collects internal and external data. The extracted data is consolidated into a single set. The client uses this. There are numerous methods for integrating data, depending on the size of the company, the need being met, and the resources available. Manual data integration is essentially the process by which an individual user manually collects relevant data from numerous sources by directly accessing interfaces, cleans it up as needed, and consolidates it into a single warehouse. This is inefficient and inconsistent, and it makes little sense for all except the tiniest enterprises with limited data resources. Middleware data integration is an integration method in which a middleware application operates as a mediator, assisting in the normalization of data and its incorporation into the master data pool. (Consider adapters for ancient electrical equipment with out-of-date connection points.) Legacy apps are frequently incompatible with others. When a data integration system is unable to get data from one of these applications on its own, middleware is used. Application-based integration is a method of integrating data that uses software applications to search, extract, and integrate data.\nTodays consumers have become used to having access to every service online instantly and expect it to function without interruption no matter what. As a business owner, you have many features to consider when choosing the right system and infrastructure for your critical online applications. One of the features you have to consider when choosing the right server for your business is whether to enable RAID on your system, but more importantly, what type of RAID to choose to fit your technical needs. Below we will go through all the pros and cons of each RAID level and give suggestions on which type to choose for your set up. RAID, short for redundant array of independent (originally inexpensive) disks is a disk subsystem that stores your data across multiple disks to either increase the performance or provides fault tolerance to your system (some levels provide both). There are two ways of implementing the system. Software raid and hardware raid. Hardware raid is directly managed by a dedicated hardware controller to which the disks are connected. The raid calculations are managed by an on-board processor which offloads the strain on the host processor CPU. However, the performance of todays CPUs has increased so much, that this advantage has become more or less obsolete. HW controllers do provide an extra failsafe element with its BBU (Battery Backup Unit) that protects your data in case of an unexpected power loss to the server. Software RAID is part of the OS and is the easiest and most cost effective implementation.\nIt does not require the use of an additional (often costly) piece of hardware and the proprietary firmware. Here is a list of the most used RAID levels: RAID 0 (Disk striping): RAID 0 splits data across any number of disks allowing higher data throughput. An individual file is read from multiple disks giving it access to the speed and capacity of all of them. This RAID level is often referred to as striping and has the benefit of increased performance. However, it does not facilitate any kind of redundancy and fault tolerance as it does not duplicate data or store any parity information (more on parity later). Both disks appear as a single partition, so when one of them fails, it breaks the array and results in data loss. RAID 0 is usually implemented for caching live streams and other files where speed is important and reliability/data loss is secondary. RAID 1 (Disk Mirroring): RAID 1 writes and reads identical data to pairs of drives. This process is often called data mirroring and its primary function is to provide redundancy. If any of the disks in the array fails, the system can still access data from the remaining disk(s). Once you replace the faulty disk with a new one, the data is copied to it from the functioning disk(s) to rebuild the array. RAID 1 is the easiest way to create failover storage.\nData archiving moves inactive data from production systems to long-term storage. Archival data can be used anytime. A data archiving strategy optimizes system resources, letting users to easily access archive storage devices or plans for easy retrieval and cost-effective information storage. It outlines how users should relocate data for maximum performance and legality. Secure data archiving allows long-term data retention. It secures mission-critical data for later use. Once in the archived data management system, information is accessible and protected. Data archiving is essential for businesses and organizations that continually receive new information and must keep current data. Government rules, the law, and company policy all favor greater, longer-retained data. Data archiving services assist firms stay current at lesser expense. Organizations develop standards for archiving data, including how to classify it. These data preservation criteria automate identification and archiving. Policies often address security, retention, and other aspects. As time passed and the number of electronic channels utilized for corporate communication increased, data archiving expanded to cover a wide range of different data kinds. Here is a list of today's most typically preserved communication records: Email (with attachments) Social media platforms (Facebook, Twitter, Instagram etc.) Platforms for internal and external cooperation (Teams, Zoom, Slack, Meet etc.) Platforms for instant messaging (e.g. WhatsApp) Text messages, voicemail, and mobile phone calls Websites Some of the key reasons to archive your data is: I. It prevents data loss. II. It Ensures that only authorized users can access authorized documents. III.\nHello everyone, System-on-a-chip technology is very popular now, today I will give you a view about SoC. SoC stands for system on a chip. This is a chip/integrated circuit that holds many components of a computerusually the CPU, memory, I/O ports, and secondary storageon a single substrate, such as silicon. Having all of these components on one substrate means SoCs use less power and take up less space than their multi-chip counterparts. SoCs are becoming increasingly popular with the growth of IoT and edge and mobile computing. SoC is an integrated circuit that integrates a computer or other electronic system into a single chip. The system-on-chip can process digital signals, analog signals, mixed signals, and even higher frequencies. System-on-chip is often used in embedded systems. System-on-a-chip integrations are large, typically millions to tens of millions of gates, and the term \"system-on-chip\" is often used to refer to more powerful processors that can run certain versions of Windows and Linux. System-on-chip's stronger capabilities require an external memory chip, such as some system-on-chips equipped with flash memory. System-on-chips can often be connected to additional external devices. System-on-chip (SIC) requires higher integration scale of semiconductor devices. To better perform more complex tasks, some system-on-chips employ multiple processor cores. Figure 1. Structure of SoC A system-on-a-chip (SoC) is a microchip with all the necessary electronic circuits and parts for a given system, such as a smartphone or wearable computer, on a single integrated circuit (IC).\nAn SoC for a sound-detecting device, for example, might include an audio receiver, an analog-to-digital converter (ADC), a microprocessor, memory, and the input/output logic control for a user - all on a single chip. Figure 2. Nvidia Tegra 650 SoC (Source: www.nvidia.com) System-on-a-chip technology is used in small, increasingly complex consumer electronic devices. Some such devices have more processing power and memory than a typical 10-year-old desktop computer. In the future, SoC-equipped nanorobots (robots of microscopic dimensions) might act as programmable antibodies to fend off previously incurable diseases. SoC video devices might be embedded in the brains of blind people, allowing them to see and SoC audio devices might allow deaf people to hear. Handheld computers with small whip antennas might someday be capable of browsing the Internet at megabit-per-second speeds from any point on the surface of the earth. SoC is evolving along with other technologies such as silicon-on-insulator (SOI), which can provide increased clock speeds while reducing the power consumed by a microchip. HiSilicons latest flagship SoC is the 5nm, 5G enabled Kirin 9000, powering the Huawei Mate 40 series. Its the successor to the Kirin 990 found in the Huawei P40 series and the Honor 30 Pro Plus. As we have come to expect from a chip powering expensive top-tier models, there are plenty of high-performance components packed inside. An octa-core Cortex-A77 and A55 configuration paired with a 24 core Mali-G78 graphics unit make this HiSilicons most powerful chip to date.\nAlthough not quite as cutting edge as its competitors, who are using newer Arm CPU cores. The company has also improved its in-house image and video processing units to support high-end photography features, along with a very competitive integrated 5G modem package. Another of the Kirin 9000s most notable features is the inclusion of a triple-clusterNeural Processing Unit (NPU) based on Huaweis in-house DaVinci architecture. Reduce volume : Combine several integrated circuits with different functions on a printed circuit board, and the volume is larger; if integrated into an SoC chip, the volume becomes smaller. Cost reduction : Multiple integrated circuits need to be packaged and tested, and the cost is high; if integrated into an SoC chip, only one integrated circuit needs to be packaged and tested, and the cost is low. Reduce power consumption and improve computing speed: The printed circuit board is used to combine several integrated circuits with different functions. The electrical signal must be transmitted over a long distance on the printed circuit board to perform operations. The power consumption is higher and the computing speed is faster. Slow; if integrated into an SoC chip, the electrical signal can be calculated by transmitting a short distance in the same integrated circuit, the power consumption is low, and the calculation speed is faster. Improve system functions : Integrate integrated circuits with different functions into one SoC chip, which is smaller in size and can integrate more \"functional units\" to form a more powerful chip.\nIn 2019, Pakistan ranked as the regions fastest-growing country for digital freelancers. The government also appointed an MIT-trained executive to head the Digital Pakistan initiative. Much of its digital innovation is focused on clusters that enhance traditional industries, like agriculture, which makes up almost 19% of the nation's GDP and employs 40% of its labor force. Emerging technology has been disrupting traditional farming and boosting productivity and efficiency. Several apps are starting to transform the agricultural sector of Pakistan, enabling farmers to improve crop yields by better livestock management, soil preparation, more timely weather updates, and e-marketplaces. Chile combines digital technologies with abundant natural resources. Like most Latin American and developing countries, Chile has many industries that focus on natural resources, including mining and salmon farming. Today, for example, more than 200 companies are involved in its salmon industry. These include manufacturers of aquiculture equipment, breeding cages, and nets; salmon food companies; floating houses and warehouses; laboratories; vaccine and drug producers; air and land transportation companies; underwater services; quality control; training centers, research centers, and educational institutions; financial and insurance companies; and specialized legal consulting and advisory services. ICT is having a major impact on helping this cluster better communicate and coordinate successfully in a competitive global market. Middle Eastern nations are promoting digital transformation in every part of their infrastructures, building digital innovation around clusters of innovation. Plans and vision statements abound for broadband, AI, and 5G, and play a key role in galvanizing national support for critical national goals.\nHowever, real incentives and investments are needed to make these visions reality. Many nations are using government funds to enable local demand for ICT in the form of smart city programs such as the UAE's National Smart Government and Cities plans. Saudi Arabia has aggressively invested in 5G capabilities in line with the targets of Saudi Vision 2030, the nation's strategic framework for reducing the kingdom's dependence on oil; diversifying its economy; and developing public service sectors such as health, education infrastructure, recreation, and tourism. In August 2019, the Ministry of Communications and Information Technology (MCIT) announced the government's ICT Strategy 2019-2023, which comprises 13 national priorities. These include improving competition in fixed retail and wholesale markets, increasing the share of local content in the IT industry, and improving the tech skills of the local ICT workforce. The government has also funded 24 initiatives for building a connected and innovative Saudi Arabia, including establishing technology clusters in oil and gas, healthcare, and logistics. Many involve government mega projects to jump start these local clusters, for example, transport and mobility schemes like Riyadh Metro, social infrastructure developments such as the Ministry of Housings Sakani program, and energy mega projects like the state-owned Aramcos Berri and Marjan oil fields. Other initiatives involve working with national and regional champions (particularly IT companies and telcos) to accelerate segments such as IoT, data analytics, AR/VR, smart homes, and autonomous vehicles. Saudi Arabias GCI rank has jumped 10 places since 2016 to reach 33 in GCI 2020.\nDear team, I have ORA-01843 error, what is the right format of the date in Oracle? Thanks. Dear Axe, Error ORA-01843 occurs when the user specifies a date that is not a valid calendar month. Valid months are: January, February, March, April, May, June, July, August, September, October, November, and December for format code MONTH. For the MON format code, valid month values are: Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, and Dec. To correct error ORA-01843, you must find the error and enter a valid month value in its correct form. There are mainly two reasons the user sees this error. The first reason is that the incorrect NLS_DATE_FORMAT is being used. This will often occur when you are working with data from the United States where the commonly used syntax order is mm/dd/yyyy . However, the proper format should be the international standard in which the date precedes the month. Another reason may be that you are attempting to insert a written month name but use a numeric month in the mask instead. You may consider using the TO_DATE function to specify the date format mask. This function will convert a string to a date.\nThe format for the TO_DATE function is: TO_DATE( string1 [, format_mask] [, nls_language] ) The following is an example: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(15/07/99, DD/MM/YY); This can also be written as: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(071599, MMDDYY); The same example can be written yet another way: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(1999/07/15, YYYY/MM/DD); All result in a date value of July 15, 1999. Another option is that you can change the sessions date format with the following line of code. Users should be careful with choosing this option as it may have consequences for other SQL. It may be best to find the specific point of the code and correct the syntax of the month if this is not a frequent occurrence. ALTER session set NLS_DATE_FORMAT=DD/MM/YYYY; To avoid seeing error ORA-01843, be sure to write valid values for months. It is important to note that error ORA-01843 may not be thrown even when there is an error in month. Therefore, it is even more important to be aware of proper month values that are valid. Make sure that you are writing dates in correct NLS_DATE_FORMAT, especially if data is from the United States in which the standard differs from the worldwide standard practiced in other countries. Dear Axe, Error ORA-01843 occurs when the user specifies a date that is not a valid calendar month. Valid months are: January, February, March, April, May, June, July, August, September, October, November, and December for format code MONTH.\nFor the MON format code, valid month values are: Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, and Dec. To correct error ORA-01843, you must find the error and enter a valid month value in its correct form. There are mainly two reasons the user sees this error. The first reason is that the incorrect NLS_DATE_FORMAT is being used. This will often occur when you are working with data from the United States where the commonly used syntax order is mm/dd/yyyy . However, the proper format should be the international standard in which the date precedes the month. Another reason may be that you are attempting to insert a written month name but use a numeric month in the mask instead. You may consider using the TO_DATE function to specify the date format mask. This function will convert a string to a date. The format for the TO_DATE function is: TO_DATE( string1 [, format_mask] [, nls_language] ) The following is an example: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(15/07/99, DD/MM/YY); This can also be written as: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(071599, MMDDYY); The same example can be written yet another way: SELECT * FROM MYTABLE WHERE MYTABLE.DATEIN = TO_DATE(1999/07/15, YYYY/MM/DD); All result in a date value of July 15, 1999. Another option is that you can change the sessions date format with the following line of code. Users should be careful with choosing this option as it may have consequences for other SQL.\nDear all, I will share with you a very interesting case today. This case is aboutself-balancing bicycle. After being injured in a bicycle accident, Zhihuijun, one of the Huawei engineers involved in this work, put forward the idea for this project. In order to achieve such an efficient self-driving bicycle, the team must equip the vehicle with an automatic control system with a sensor network and a computing chip with sufficient computing power to alleviate the existence of the driver. In addition, engineers must implement a set of perception and control algorithms based on previous hardware. The prototype is more like an electric bicycle, thanks to its lithium-ion battery rated to last 2 to 3 hours. However, it is reported that it has also been enhanced with all the necessary sensors, and the bicycle is equivalent to an autonomous car. They include accelerometers, gyroscopes, and LiDAR scanners, not to mention the artificial intelligence that controls the hardware and gives functions such as obstacle avoidance and automatic route following. However, if the bike cannot manage its own standing skills, none of this matters. Zhihuijun asserts that this is possible by using a disc-shaped control module connected to a precision sensor that can sense slight tilts in the 3D printed frame and indicate small changes in balance to compensate, thereby keeping it upright. The sensors gather data about the surroundings, identify objects, their distance, and the bicycle's relative position and speed.\nDear All, Today we are going to learn about SmartQoS Overview of SmartQoS traffic control management SmartQoS dynamically allocates storage resources to meet certain performance goals for specified applications. The storage system uses LUN-, file system-, or snapshot-based I/O priority scheduling and I/O traffic management to ensure critical services are not interrupted. SmartQoS is an essential value-added feature for a storage system, especially when there are certain applications with demanding QoS requirements. When multiple applications are deployed on the same storage device, users maximize their benefits through the proper configuration of SmartQoS. Performance control reduces adverse impacts of applications on each other and ensures the performance of critical services. SmartQoS limits the resources allocated to non-critical applications to ensure high performance for critical applications. I/O Priority Scheduling The I/O priority scheduling technology of SmartQoS is based on LUN priorities. Each LUN or file system has a priority, which is configured by a user and saved in a database. When a host sends an I/O request to a storage array, the storage array gives a priority to the I/O request based on the priority of the LUN or the file system that will process the I/O request. Then the I/O carries the priority throughout this processing procedure. When a LUN or a file system is created, its I/O priority needs to be specified. If not, the LUN or the file system is assigned a low priority by default. After a LUN or a file system is created, its I/O priority can be manually changed.\nI/O Traffic Control I/O traffic control uses a token-based system. When a user sets an upper limit for the performance of a traffic control group, that limit is converted into the number of corresponding tokens. In a storage system, where the IOPS is limited, each I/O operation corresponds to a token. If bandwidth is limited, tokens are allocated to sectors. Each traffic control queue has a token bucket.SmartQoS periodically puts a certain number of tokens into the token bucket of each traffic control queue. The number of tokens is determined by the performance upper limit set for the traffic control group. For example, if the performance upper limit is set to IOPS = 10,000, the token distribution algorithm sets the maximum number of tokens in the token bucket for the traffic control group to 10,000. When processing a traffic control queue, the queue checks whether the token bucket has enough tokens. If it does, an I/O request is processed, and its corresponding token is consumed. If it does not, SmartQoS does not process the I/Os operations in this queue until there are enough tokens in the token bucket. Scenario For cost reduction, some users will not build their dedicated storage systems independently. They prefer to run their storage applications on the storage platforms offered by storage resource providers. This lowers the total cost of ownership (TCO) and ensures the service continuity.\nHello Huawei Community! This post is written aboutexporting system data using DeviceManager in Dorado V6. Please have a look below for more details. Periodically export the system data of a storage system and save it in a safe place. This helps you know the operating status of the storage system and take countermeasures against possible system faults and unexpected disasters. If a system failure occurs, the exported system data can be used to locate and analyze the failure. The system data to be exported includes configuration information, system logs, disk logs, and diagnostic files. System logs record the information about the configuration information, events, and debugging operations on a storage system and can be used for analyzing the running status of the storage system. The system log file is in *.tgz format. Configuration information indicates the real-time running status of a storage system and includes user information and LUN configuration information. The configuration information file is in *.txt format. DHA runtime logs are disks' daily running logs, including daily disk health statistics, I/O statistics, and disk service life. The DHA runtime log file is in *.tgz format. DHA runtime logs collect S.M.A.R.T/LogPage information (collected at 2 o'clock every morning), I/O statistics (collected every 2 hours), and disk service life (collected at 2 o'clock every morning). A log package (1 KB) is generated for each disk each day.\nIn this post, you will learn how to configure RAID in Windows 11? You can use storage pools and storage space to do this. Therefore, we will create and configure storage pools in Windows 11. If your computer is low on storage space, you can connect multiple hard drives to your computer. You can use these multiple hard drives individually as needed, but you can configure all these multiple hard drives as one hard drive. Now the same thing happens on the server where they combine multiple hard drives into one hard drive, but the setup is a bit complicated, like RAID. The process I explain to people through this step-by-step guide is actually performed on a virtual hard drive. But you can also do this using an external hard drive connected to your computer, but if you are using a desktop, you can do this, but I might suggest you check your PC motherboard to see if the raid controller is supported. If it can support the red controller, then you can combine all of these drives from the raid controller setup into a single hard drive. This way, you can also perform a fresh install of the Windows operating system on all multiple drives, just like a single drive. Step 1. Connect an external hard drive or use a virtual hard drive If you want to create a storage pool on Windows 11, you can use either an external hard disk drive or a virtual hard disk drive.\nFor this article, I'm going to use a virtual hard disk, so I need to create one. To create a virtual hard disk, open Disk Management. You can open Disk Management using the Run dialog box by typing Diskmgmt.msc or by right-clicking the Start menu and selecting Disk Management. Open Disk Management To create a virtual hard disk, go to the Actions menu and select Create VHD. Create VHD Click Browse and save the VHD in the specific location you want to save. Then enter the size of the virtual hard drive you want and in my case I'll put 5 GB of storage. And you have to choose VHDX as well as dynamic allocation, which is also recommended. VHD : stands for virtual hard disk drive, used for older operating systems. You can create virtual hard drives with up to 2 TB of storage. VHDX : It stands for Virtual Hard Drive X and is intended to replace VHD. It can support up to 64 TB of virtual hard disks. Fixed Size : As the name suggests, it stores a fixed amount of storage space on a physical computer. Suppose that if you create a 10 GB virtual hard disk, it immediately consumes 10 GB of storage on the actual hard disk. Dynamically expansion : As the name suggests, it expands the amount of storage mentioned over time as it is used. Let's say if I create a 5 GB virtual storage space, the actual hard drive will take up about 4 MB of space.\nHello everyone, In this case, we did an upgrade disk firmware of a Dorado 5000 V6. We got following steps: Step 1 Log in to the CLI using the admin account. Step 2 Run show disk general |filterColumn includecolumnList=ID,Manufacturer,Model,Firmware\\sVersion,Bar\\sCode command to querythe hard disk information. If the value of Model starts with HSSD, continue withstep 3. Step 3 Run show file_system general command to query the NAS information. 1. In Step 2, if a disk whose Model is LT1800RO or LT0900RO and Firmware Version isearlier than H507, the check result is Not passed. 2. If the disk meets any of the following conditions, the check result is Optimization recommended. Otherwise, the check result is Passed. 1). In step 2, if the value of Model is HUC101818CS4200 or HUC101812CS4200 andthe value of Firmware Version is less than AAG4, the check result isoptimization recommended. 2). In step 2, if the value of Model is HUS156060VLS600 and the value of Firmware Version is less than A8G0, the check result is optimization recommended. 3). In step 2, if the value of Model is HSSD-D5222AM5900,HSSD-D5222AM5C60,HSSD-D5222AM59A0, HSSD-D5222AM5600, HSSD-D5222AM5AA0, HSSD-D5222AM5CA0,HSSD-D5222AM5901, HSSD-D5222AM5A81, HSSD-D5222AM5C61, HSSD-D5222AM59A1,HSSD-D5222AM5AA1,HSSD-D5222AM5CA1, HSSD-D5222AM56A0, HSSD-D5222AV5800, HSSD-D5222AH5A60,HSSD-D5222AH5400, HSSD-D522JAM5400, HSSD-D522JAH5200, HSSD-D522JAH5400,HSSD-D5222AM6G60, HSSD-D5222AV5C20, HSSD-D5222AH5200, HSSD-D5222AM6G20,HSSD-D522JAM5600,HSSD-D5222AM5960, HSSD-D5222AM5961, HSSD-D5222AM5A90,HSSD-D5222AM5A91, HSSD-D5222AM5C80, HSSD-D5222AM5C81 and the value of FirmwareVersion is less than 3172, the check result is optimization recommended. 4). In step 2, if the value of Model isHSSD-D3J2JAM5400, HSSD-D3J2LAM5400, HSSD-D3J2JAM5600, HSSD-D3J2LAM5600 and the value of Firmware Version is less than 9203, and the command output is displayed in step 3, the check result is optimization recommended. 5).\nIn step 2, if the value of Model isHSSD-D322XAM0200,HSSD-D3220AM0200,HSSD-D3220AM0400,HSSD-D322XAM0600,HSSD-D332AAM1200,HSSD-D332AAM140T,HSSD-D332ZAM1600,HSSD-D332ZAM1400,HSSD-D322ZAM1400,HSSD-D322ZAM1600,HSSD-D332AAM4600,and the value of Firmware Version is less than 5506, the check result isoptimization recommended. 6). In step 2, if the value of Model is HSSD-D5223PM5A00,HSSD-D5223PM5B00,HSSD-D5223PM5D00,HSSD-D5223PM5A01,HSSD-D5223PM5B01,HSSD-D5223PM5D01and the value of Firmware Version is less than 1864, the check result isoptimization recommended. 7). In step 2, if the value of Model is HSSD-D5222AM5A80 and the value of Firmware Version is less than 3272, the check result is optimization recommended. 8). In step 2, if the value of Model isHSSD-D6D23AL960N, HSSD-D6D23AL1T9N, HSSD-D6D23AL3T8N, HSSD-D6D23AL7T6N and the value of Firmware Version is less than 2152, the check result is optimization recommended. 9).In step 2, if the value of Model isHSSD-D6223AL960N,HSSD-D6223AL1T9N,HSSD-D6223AL3T8N,HSSD-D6223AL7T6N,HSSD-D6223AL15TN,HSSD-D6323AL3T8N,HSSD-D6323AL7T6Nand the value of Firmware Version is less than 3235, the check result is optimizationrecommended. 10).In step 2, if the value of Model isHSSD-D6D23DL960N,HSSD-D6D23DL1T9N,HSSD-D6D23DL3T8N,HSSD-D6D23DL7T6N,HSSD-D6D23DL1T0N,HSSD-D6D23DL2T0N,HSSD-D6D23DL4T0N,HSSD-D6523DL1T9N,HSSD-D6523DL3T8N,HSSD-D6523DL7T6N,HSSD-D6D23DL15TN,HSSD-D6D93DL3T8N,HSSD-D6593DL7T6Nand the value of Firmware Version is less than 3247, the check result isoptimization recommended. 11). In step 2, if the value of Model is HSSD-D7294DL15TNHSSD-D7294DL15TEHSSD-D7224DL15TNHSSD-D7294DL1T9NHSSD-D7294DL3T8NHSSD-D7294DL7T6NHSSD-D7294DL1T9EHSSD-D7294DL3T8EHSSD-D7294DL7T6EHSSD-D7294DL3T8SHSSD-D7223AL960NHSSD-D7223AL1T9NHSSD-D7223AL3T8NHSSD-D7223AL7T6NHSSD-D7223AL15TNHSSD-D7523AL960NHSSD-D7523AL1T9NHSSD-D7523AL3T8NHSSD-D7523AL7T6NHSSD-D7223AL3T8EHSSD-D7223AL7T6EHSSD-D7894DL3T8NHSSD-D7594DL3T8NHSSD-D7594DL7T6NHSSD-D7894DL3T8EHSSD-D7594DL7T6EHSSD-D7284PV800NHSSD-D7284PV1T6NHSSD-D7284PV3T2NHSSD-D7584PV1T6NHSSD-D7284PL3T8NHSSD-D7284PL7T6N and the value of FirmwareVersion is less than 6222, the check result is optimization recommended. 1. If the check result is Not passed, upgrade the disk firmware. 2. If the check is recommended for optimization, the firmware version is too low, and in order to improve the robustness of the hard drive, it is recommended to contact a technical support engineer to upgrade the hard drive firmware version.\nDear All, Today we are going to learn about SmartThin. Introduction to SmartThin A traditional storage system deployment has the following problems: Services can be impacted or even interrupted when the storage capacity is expanded. Storage space is not well utilized. Storage is inefficient. SmartThin can allocate the storage space on demand, improving resource utilization and more fully meeting service requirements. If the actual amount of data that needs to be stored is more than expected, LUN space can be adjusted dynamically. Free space can be allocated to any LUN that needs it, which helps improve utilization. In addition, LUN capacity can be expanded online without affecting services. Using SmartThin has two major advantages: Storage capacity is not allocated to LUNs when LUNs are created. It is allocated on demand when LUNs are being used. LUN space can be adjusted dynamically. How SmartThin Works SmartThin virtualizes storage resources. SmartThin manages storage devices on demand. SmartThin does not allocate all of its capacity in advance. It presents users a virtual storage space larger than the physical storage space. SmartThin allocates the space based on user demands. If users need more space, they can add back-end storage units to expand capacity as needed. The system does not need to be shutdown, and the whole expansion process is transparent to the users. SmartThin creates thin LUNs based on a RAID 2.0+ virtual storage resource pool, that is, thin LUNs coexist with thick LUNs in the same storage resource pool.\nRAID is the abbreviation of \"Redundant Array of Independent Disks\". The Chinese meaning redundant array of independent disks, also known as \"Redundant Array of Inexpensive Disks\", is a redundant array of inexpensive disks. It was proposed by Professor D.A. Patterson of Berkeley, California in 1988. Simply put, RAID is a technology that combines multiple independent hard disks (physical hard disks) in different ways to form a disk group (logical hard disk), which provides higher storage performance and data redundancy than a single hard disk. The different ways of composing a disk array become RAID Levels. The function of data redundancy is to use the redundant information to recover the damaged data after the user data is damaged, thus ensuring the security of the user data. The adoption of RAID brings great benefits to the storage system (or the server's built-in storage), where increasing the transfer rate and providing fault tolerance is the biggest advantage. RAID increases the transfer rate by using multiple disks simultaneously. Significantly increase the data throughput (Throughput) of the storage system by simultaneously storing and reading data on multiple disks. In RAID, many disk drives can transfer data at the same time. These disk drives are logically a disk drive, so RAID can be used to achieve several times, dozens of times, or even hundreds of times the speed of a single disk drive. This is also the problem that RAID originally wanted to solve.\nHello all, This case is about how to checkphysical space is used by LUNs with the deduplication function on Dorado 3000 V6. Customer want to know how much physical space is used for LUN with deduplication on Dorado 3000 V6. Dorado 3000 V6. We should check the storage pool global reduction ratio and available capacity for storage pool & subscribed capacity for the storage pool. Its more precise for reduction on storage pool level. It does not need to check the individual LUN reduction ratio. Its for internal data statistics. We should check Capacity and Subscribed Capacity in output use show lun general lun_name=? Capacity is the total capacity for this LUN. The space showed on the application/OS side. Can be over-provisioned. Subscribed Capacityis the actual used capacity quota or capacity used/write by applications. We should check Reduction Ratio in output use show lun reduction_info lun_id=? Reduction RatioCapacity reduction ratio. If we create a LUN with 1T provisioning with thin and 260G application data write in it. Our reduction ratio is 2.6:1. So Subscribed Capacity is 260G. 260G/2.6=100G. 100G physical capacity used in the storage pool. It means 100G physical space can store 260G application data by deduplication and compression Capacity 1T LUN size Subscribed Capacity 260G application data written from OS side Reduction Ratio: 2.6:1 reduction ratio is 2.6:1 Physical space occupied = Subscribed Capacity/ Reduction Ratio We should mention Available capacity for storage pool which describes how much physical capacity is available.\nIf a company has data, the question is how important it is for a particular company to make considerable efforts to move to a higher level of maturity in this direction, Such a transition requires mastering a number of practices, such as architecture management, master data, data quality, and security. It makes sense to do this if the company understands that in the end it will receive a significant result for its business. The difficulty lies in the fact that the business benefits not from data management, but from data-based analytical applications, and therefore it can be difficult to convince management of the need for long-term investment in Data Governance We often translate this term as data management, but what is data management then? Data Governance is a comprehensive enterprise data management strategy. With the help of data management methodology, the company will be able to extract maximum business value from its data and become more agile. Data Governance Data Governance forms the basis of all data management in the organization. Data Governance provides efficient use of reliable data. Efficient data management is a very important task that requires centralized control mechanisms. In this post we will answer the below questions: What is Data Governance? Why is Data Governance important? How important is it for organizations to pay attention to this issue? What are the challenges of Data Governance? What are the useful experiences in this field? Data Governance includes the people, business processes, and technologies needed to manage and protect an organization's data assets.\nData Governance aims to ensure that the organization's data is comprehensible, accurate, complete, reliable, secure and accessible and usable. Data Architecture Data Quality Meta-data (metadata) Data Warehousing & Business Intelligence Reference & Master Data Documents and data content Documents & Content Data Integration & Interoperability Data security Data security Data Storage & Operation Data Modeling & Design The main goal of Data Governance is to create methods and frameworks with clear responsibilities and processes for standardizing, integrating, protecting and storing business data. In general, the main goals of Data Governance are: Reduce the risks and dangers of data Explain and determine internal rules for data use Implement data compliance requirements Improving internal and external communication Create value from the organization's data Reduce organizational costs Help ensure the organization's continued presence in the competitive arena through risk management and optimization Facilitate the implementation of the above The Data Governance program will always have an impact on the strategic, tactical and operational levels of organizations. Data Governance program in order to organize and efficient use of data in the company text and in order to implement other data-driven projects as a continuous and repetitive process. In addition to the responsibilities mentioned, the following important aspects of the Data Governance program should be made clear: Where When What How Although Data Governance is not yet institutionalized and pervasive in organizations, most organizations have implemented the Data Governance program for some of their departments or some of their applications.\nThus, the creation of Data Governance in a methodical way in organizations means a great change in the field of data from informal rules to formal controls in the organization. Formal data governance is usually implemented in a company or organization when that company has reached a level where functional functions can no longer be effectively implemented. Data Governance is a prerequisite for countless tasks and projects and will have valuable benefits. Here are some benefits of Data Governance: Consistent and consistent data and processes across the organization are a prerequisite for better and more comprehensive decision support. Growing and enhancing IT perspectives at the operational, tactical, and strategic levels by enacting new rules to change processes and data. Optimization of data management costs by central control mechanisms (these mechanisms are increasingly widely used in the age of data explosion) Increase productivity through synergy created (for example by reusing processes and data) Build higher self-esteem than data through guaranteed data quality, certified data as well as complete data process documentation Ability to meet globally or nationally defined laws and standards Security of internal and external data of the organization by monitoring and reviewing privacy policies Increase process efficiency by reducing lengthy coordination processes Creating clear and accurate communication through standardization (this is a key prerequisite for macro-based organizational activities) Today more than ever, Data Governance will help organizations be more accountable.\nAlso, Data Governance to create new and innovative contexts in business such as big data analysis, which with traditional thinking will be very difficult new approaches. Currently, the most important drivers that will force organizations to reconsider their current approaches are: Create a data-driven view to support digital business models Improving the quality of organizational data and managing core data Create data management capabilities in big data environments Establish standards to increase the ability to respond to external influences Self-service business intelligence, users want to perform analysis independently of information technology. Data compliance: Transparent and understandable data processes to comply with legal requirements In addition to the above, there are other developments and requirements that have made Data Governance all the more important. For example, advanced analytics, social media, 360-degree customer vision, cloud-based business intelligence, information strategies, and adherence to data protection guidelines for internal or external use, such as CRM or SCM. The need for data governance is not hidden from anyone, but many organizations do not implement Data Governance, despite its many benefits, due to the complexity or uncertainty of its success. Implementing a Data Governance program is not an easy task, and the following important challenges lie ahead with implementing a data governance program: Organize data sovereignty Implementing data governance requires an open organizational culture. For example, it should be checked that data governance laws are applicable in the organization, even if these laws require the creation of certain maps and responsibilities.\nEventually, data governance will become a new policy that will help confer, distribute, and remove responsibilities and competencies. This will require a very sensitive and precise approach. Data Governance requires proper acceptance and working communication between all people involved in this process. The right people need to be in the right place to implement policies. Project managers in particular need to have an understanding of the technical aspects as well as understanding the business and preferably a comprehensive conceptual view of the organization. Convincing the organization's stakeholders that they need a Data Governance program and getting funding to implement the program is often difficult. In addition, change in the organization is not desirable and there will be resistance to it. Businesses need to be flexible to meet needs that are changing very quickly. Creating the right balance between flexibility and data governance standards is critical to business needs. Useful experiences and success factors govern data Data governance is not a project to be implemented in the organization as a big bang. It can be said that Data Governance is a comprehensive plan in the organization that must implement complex and long-term projects. So there is always the risk that people involved may lose their motivation and interest over time. In view of the above, it is recommended that a controllable or practical prototype project be initiated and that this approach be continued intermittently.\nIn this way, the project is controllable and the gained experience can be used for more complex projects and the expansion of data dominance in the organization. Typically the steps of a Data Governance project are: Define goals and understand the benefits of data governance Analyze the current situation Prepare a roadmap Justification of stakeholders and project budget Planning and compiling data governance Run the data governance program Monitor and control data sovereignty These steps must be repeated for each new program, as well as if changes were made to the previous program. Continue to implement creative data governance Before starting a data governance program in an organization, questions about the reasons for implementing Data Governance should be answered to avoid unnecessary and unnecessary tasks. Similarly, existing processes should be evaluated to determine whether existing processes can be adapted to new needs within the Data Governance program, rather than unnecessarily developing new ones. The strategic, tactical and operational levels of the company, as well as its organizational, business and technical aspects form the basis of the company matrix in data governance. With this structure, Data Governance projects can be defined with the specifications, processes, maps and tasks related to each. It is worth noting that the different levels of data governance plan and the aspects mentioned and the role of data governance in the company should be very specific. The DAMA framework provides all data governance issues with documented criteria.\nAccording to the materials provided for the implementation of Data Governance, the current situation can be compared with the expected situation structurally. By doing this, we can determine the entry point into the discussion, set priorities, and design a roadmap based on the main actions. Maps are essential in the Data Governance program. Today, there are software tools that will provide Data Governance templates for metadata management (metadata), data quality, core data management, and data integrity. Theories about maps in Data Governance differ slightly, but the main maps mentioned in data governance are as follows: Strategic Data Governance Committee (Strategic Leadership Committee) Data Governance Management Committee (Tactical Level) Data Manager Data owner Data monitor Data users The following tips will help you implement the Data Governance program: The Data Governance program should not be implemented in any way without the support of senior management in the organization Do not start with a big bang. Data Governance is a continuous and repetitive process that contains sub-projects. Start with small pilot projects and expand your experience across the company. Data Governance programs can run for many years. However, related projects should not take more than three months. Set goals clearly and carefully. The success of Data Governance is a priority. Stakeholder involvement and transparency in the work process are important. It is recommended to establish a free and transparent communication with all stakeholders, far from secrecy. Do not reinvent the cycle, but try to use existing patterns.\nDear all, This post will give you an overview ofHyperMetro-Inner, its working principle and how toExpand controller on a HyperMetro-Inner Network. The high-end storage systems support HyperMetro-Inner, which ensures servicecontinuity in the event that an entire controller enclosure or multiple controllersfail. Figure 1 Working principle of HyperMetro-Inner HyperMetro-Inner uses back-end full interconnection and continuous cachemirroring to tolerate seven faulty controllers out of eight. The back-end fullinterconnection and three cache copies ensure data integrity and service continuity in the event that any two controllers or one controller enclosure fails. Back-end full interconnection: Back end interconnect I/O modules (100 Gbit/sRDMA) and smart disk enclosures enable one disk enclosure to be concurrentlyaccessed by eight controllers in two controller enclosures. Therefore, even if sevencontrollers are faulty, the remaining controller can access all SSDs. Continuous cache mirroring: If a controller is faulty, a new mirror copy is selectedfor the write cache. If the number of properly working controllers is greater thanthat of mirror copies, the number of write cache copies remains unchanged. In theextreme cases where only one controller is working properly, the write cache is notlost. Three cache copies: The write cache uses three copies (configured by default afterHyperMetro-Inner is enabled) and ensures one copy across controller enclosures. Iftwo controllers and one controller enclosure are faulty, at least one valid writecache copy is available, ensuring zero data lost and service continuity. If you use a HyperMetro-Inner network after controller expansion, import the HyperMetro-Inner license and check the number of member disks in storage pools before expansion.\nDatabase as a service (DBaaS) is one of the fastest growing cloud services. The service allows organizations to take advantage of database solutions without having to manage and maintain the underlying technologies. DBaaS is a cost-efficient solution for organizations looking to set up and scale databases, especially when operating large-scale, complex, and distributed app components. In this article, we will discuss Database as a Service, how it works, and its benefits to your organization from both technology and business perspectives. Database as a Service is acloud-basedsoftware service used to set up and manage databases. A database, remember, is a storage location that housesstructured data. The administrative capabilities offered by the service includes scaling, securing, monitoring, tuning and upgrade of the database and the underlying technologies, which are managed by the cloud vendor. These administrative tasks are automated, allowing users to focus on optimizing applications that use database resources. The hardware and IT environment operating the database software technologies is abstracted. Users dont need to focus their efforts on the database implementation process itself. The service is suitable for: IT shops offering cloud-based services End users such as developers, testers, and DevOps personnel Depending on the service, the DBaaS service can be a managed front-end SaaS service or a component of the comprehensiveInfrastructure as a Service (IaaS) or Platform as a Service (PaaS) stack. Heres how a typical DBaaS, as part of the IaaS, works: The first step involves provisioning avirtual machine (VM)as an environment abstracted from the underlying hardware.\nThe database is installed and configured on the VM. Depending on the service, a predefined database system is made available for end users. Users can access this database system using an on-demand querying interface or a software system. Alternatively, developers can use aself-service modelto set up and configure databases according to a custom set of parameters. The DBaaS platform handles the backend infrastructure and operations. Database administrators (DBAs) can use simple click-on functionality to configure the management process. These include, but arent limited to: Monitoring Upgrades and patches Disaster recovery Security The DBaaS platform scales the instances according to the configuration and policies associated with the managed database systems. For example, for disaster recovery use cases, the system replicates the data across multiple instances. The building blocks of the underlying components, such as a server resource, are controlled by the platform and rapidly provisioned for self-service applications of database deployment. Without a managed database service or a DBaaS, youll have to manage and scale hardware components and technology integrations separately. This limits your ability to scale a database system rapidly to meet the technology requirements of a fast-paced business. DBaaS technology saves valuable resources on setting up and managing database systems and the IT environment. The technology reduces the time spent on the procedure from weeks and days to a matter of a few minutes. This is especially true for self-service use cases inDevOps environmentsthat require rapid and cost-effective operations capabilities for their IT systems.\n--------------------------------------------------------------------------------------------------------------------------------------------------- hioadm format -d [ -m ] [ -t ] Parameter Description Value Name of an SSD. Example: sdd Formatting mode. 0 : quick formatting (Secure Erase Settings=0) 1 : secure formatting (Secure Erase Settings=1) If this parameter is not specified, the default value 0 is used. Sector size and type. 0 : The sector size is 512 B, and the metadata size is 0 B. 1 : The sector size is 4 KB, and the metadata size is 0 B. 2 : The sector size is 512 B, and the metadata size is 8 B. 3 : The sector size is 4 KB, and the metadata size is 64 B. 4 : The sector size is 4 KB, and the metadata size is 8 B. 5 : The sector size is 520 B, and the metadata size is 0 B. 6 : The sector size is 4160 B, and the metadata size is 0 B. If this parameter is not specified, the sector size and type are not changed. The Linux operating system supports all the preceding sector types (5 and 6 are applicable only to SAS drives). The Windows operating system supports sector types 0, 1, 5, and 6 (5 and 6 are applicable only to SAS drives). The ESXi system supports only the 512 B sector type. If this parameter is not set, the sector type is not changed.\nHello guys, I will discuss what's the difference between the MEMS and NEMS. I hope this article will be helpful for you. The expanding and developing fields of micro-electromechanical systems (MEMS) and nano-electromechanical (NEMS) are highly interdisciplinary and rely heavily on experimental mechanics for materials selection, process validation, design development, and device characterization. These devices range from mechanical sensors and actuators, to microanalysis and chemical sensors, to micro-optical systems and bioMEMS for microscopic surgery. Their applications span the automotive industry, communications, defense systems, national security, health care, information technology, avionics, and environmental monitoring. This chapter gives a general introduction to the fabrication processes and materials commonly used in MEMS/NEMS, as well as a discussion of the application of experimental mechanics techniques to these devices. Mechanics issues that arise in selected example devices are also presented. Nanoelectromechanical systems or NEMS is the next logical miniaturization step from MEMS, and is used to describe devices integrating electrical and mechanical functionality on the nanoscale. Often employing the 2010 Nobel physics prize-winning material, graphene, together with the last centurys wonder material, carbon nanotubes, NEMS systems have found much application in atomic force microscopy (AFM). Currently, the most significant technologies for the fabrication of three-dimensional MEMS microstructures are bulk micromachining, surface micromachining, LIGA (from the German Lithographie, Galvanoformung, Abformung - a process involving X-ray lithography, electroplating and plastic moulding), electrical discharge machining (EDM), and substrate bonding. NEMS systems rely on two basic principles of fabrication: photo/electron beam lithography, and molecular self-assembly.\nDeveloping a MEMS device has previously been seen as two separate systems. Where one system controls the computations and the other respond accordingly. It is apparent that as technology increases, the size of electronics usually decreases.NEMS introduce a way of integrating the two systems of MEMS into a single silicon chip. NEMS a major focus of nanotechnology involves passing electrons through nano semiconductors in order to emit light. They are small so they can fit just about anywhere in a relative perspective. Electrostatic effects can easily damage as compared to a component with a l surface area. Less material to produce a product usually means a lower cost, however this depends on the material and the means of production. Less consumed energy. Overall it seems that there are more advantages of MEMS and NEMS disadvantages so an increase in practical application can be observed. MEMs and NEMs have enabled the advent of many new devices. Many of these can be found in the medical field. Scalpels with embedded ICs can measure pressure and cutting depth. Device have been manufactured to be implanted into patients to release the exact dosages of medicine at the correct time. MEMS are enabling systems to resight for the blind. This article fo on the increasing use of MEMS fo treating diseases and injuries of th central nervous system (brain and spine), including paralysis, Parkin disease, and drug-resistant depressant.\nHuawei's DPA Appliance is a data protection device with integrated backup software, servers, and storage components. Its distributed architecture enables linear performance and capacity development, allowing a single system to protect, build, and manage user data and applications. This improves data protection, reduces investment, and simplifies data management. The Data Protection Appliance is an enterprise-class copy data management device that helps users achieve SLA requirements. Data Protection Appliance protects, recovers, and repurposes data. DPA covers a wide variety of data protection solutions here we are going to enlist them briefly, Backup/restore: The Data Protection Appliance provides mature and efficient data backup and recovery functions to protect Windows, Linux, Unix, virtualization platforms like VMware, FusionCompute, and XenServer, databases like Oracle, MySQL, and SQL Server, and other popular applications, implementing unified data protection and centralized O&M. Copy replication: The local backup data or production data is damaged by a natural disaster or data center error. In this case, copy replication can be used to recover backup data quickly and securely. Advanced backup: Copy data management employs data virtualization, snapshot, and clone to retain copy data's historical status and offer rapid access. Backup continuously: Continuously captures production volume I/O changes based on volume backup. If the backup network isn't congested, RPO can be 0. Virtual snapshots can recover data at any time. Continuous replication: Data on the production host is continually replicated to the target host online using database log-level real-time replication. If the production host is broken, the target host's data can swiftly recover services, providing service continuity.\nDear team, Java Update process has not been completed and an error appears: Error Code 1603. Java Update did not complete. Thanks for the help. Dear Axe, This is a known issue, and we are still investigating the root cause. Meanwhile, you can try the following to install Java. Option 1: Restart your system and uninstall old versions Restart your system before installing Once you see the 1603 error, restart your system. Download and install theoffline installerpackage. When prompted, choose Save in the download dialog box, and save the download package in a convenient place (e.g. save on the desktop). Double click on the downloaded installation file to start the installation process. Uninstall Java versions before installing If the above instructions fail to resolve the issue, it is recommended that youuninstall all existing Java versionsfrom the system. Reboot the system after you uninstall all Java versions, before trying to install. Option 2: Disable Java content through the Java Control Panel This option disables Java content in the browser prior to installing. 1. Once you see the 1603 error, close the installer. 2. Find and launch the Java Control Panel 3. Uncheck (de-select) Enable Jav a content in the browser option In the Java Control Panel, click the Security tab. Uncheck the option Enable Java content in the browser . Click Apply and then OK to confirm the changes. 4. Reinstall Java and re-enable Java content in the browser Download and install theoffline installerpackage.\nHello all, What's the meaning of The Error Code 0x0 0x0 ? How can I get rid of this error? Thanks. Dear Phany, Here are some ways to fix it. 1. Closing Conflicting Programs: Its important to remember that a runtime error occurs when two or more applications are interfering with one other. Stop these competing programs as the first step in resolving the issue. Open the Task Manager by pressing Ctrl-Alt-Del simultaneously. This will show you a list of running applications. Stop the applications one by one by highlighting each one and clicking the End Process button at the bottom of the window. This error notice will need to be monitored for recurrence, so keep an eye on it. After determining which software is generating the issue, you may proceed to the next stage in the troubleshooting process, reinstalling the application. 2. Update Your Antivirus Software or Install The Most Recent Windows Updates Runtime errors caused by viruses need to be removed as soon as they are discovered. Update your anti-virus software and conduct a full scan of your computer to ensure that you have the most up-to-date virus definitions and fixes. 3. Run Disc Cleanup Running into a runtime issue might be due to a lack of free space in your computer. Its a good idea to back up your data and clear up space on your hard disc. Restarting your computer is another option.\nOpen your Explorer window and right-click your main directory to launch Disk Cleanup (this is usually C: ) Disk Cleanup may be found in the Properties menu. 4. Reinstall Graphics Driver Do this if the problem is caused by a poor graphics driver: The graphics driver may be found in the Device Manager. Restart your computer and uninstall the video card driver by right-clicking on it and selecting uninstall. Thanks. Dear Phany, Here are some ways to fix it. 1. Closing Conflicting Programs: Its important to remember that a runtime error occurs when two or more applications are interfering with one other. Stop these competing programs as the first step in resolving the issue. Open the Task Manager by pressing Ctrl-Alt-Del simultaneously. This will show you a list of running applications. Stop the applications one by one by highlighting each one and clicking the End Process button at the bottom of the window. This error notice will need to be monitored for recurrence, so keep an eye on it. After determining which software is generating the issue, you may proceed to the next stage in the troubleshooting process, reinstalling the application. 2. Update Your Antivirus Software or Install The Most Recent Windows Updates Runtime errors caused by viruses need to be removed as soon as they are discovered. Update your anti-virus software and conduct a full scan of your computer to ensure that you have the most up-to-date virus definitions and fixes. 3.\nHello all, Today we will talk about theparity bitused by RAID arrays. A parity bit is a bit that is added to ensure that the number of bits with the value one in a set of bits is even or odd. Parity bits are used as the simplest form of error detecting code. There are two variants of parity bits: even parity bit and odd parity bit. When using even parity, the parity bit is set to 1 if the number of ones in a given set of bits (not including the parity bit) is odd, making the entire set of bits (including the parity bit) even. When using odd parity, the parity bit is set to 1 if the number of ones in a given set of bits (not including the parity bit) is even, keeping the entire set of bits (including the parity bit) odd. In other words, an even parity bit will be set to \"1\" if the number of 1's + 1 is even, and an odd parity bit will be set to \"1\" if the number of 1's +1 is odd. Even parity is a special case of a cyclic redundancy check (CRC), where the 1-bit CRC is generated by the polynomial x+1. If the parity bit is present but not used, it may be referred to as mark parity (when the parity bit is always 1) or space parity (the bit is always 0). Parity is the process of ensuring accurate data transfer between nodes during communication.\nParity bits are appended to the raw data bits to create an even or odd number of bits; Number of bits of value 1. The source then transmits this data over the link and checks and validates bits at the destination. Data is considered accurate if the number of bits (even or odd) matches the number transferred from the source. Parity check is created to eliminate data communication errors. It is a simple network data verification method with an easy-to-understand working mechanism. For example, if the raw data is 101001, there are three ones. When even parity is used, a parity bit with the value 1 is added to the left of the data so that the number of 1s is even; The transferred data changes to 11010001. However, if parity is used, the parity bit value is zero; 01010001. If the original data contains an even number of 1s (1101001), then if parity is used and the transferred data becomes 11101001, the parity bit with the value 1 is added to the left of the data so that the number of 1s is odd. The receiver agrees to use the same parity as the sender, and the parity is either even. If this protocol is not configured correctly, communication is not possible. Once the data arrives at the receiver, if the data is transferred incorrectly, the parity bit value will be incorrect; Therefore, an error occurred during transmission.\nParity is used for communication, although more advanced protocols such as Microcom Network Protocol (MNP) and ITU-T V.42b have been replaced as standards for modem communication. It is still used for memory storage device testing, for example, to run a memory check while reading data. Parity is a very basic method that detects simple errors, but not errors caused by electrical noise changing the number of bits. In fact, both the receive and transmit bits have errors that cancel each other out. While the chance of this happening on a PC is largely remote, a third bit can be allocated for parity in large computer systems where data integrity needs to be ensured. Redundant Array of Independent Disks (RAID) also uses an enhanced form of parity-based protection to check horizontal and vertical parity. Write a second set of parity data on all drives to avoid loss in the event of an error. When a RAID drive parity check fails, the parity information is used to rebuild the data along with the data on other disks. The bits on the remaining drives are added. If they add up to odd numbers, the correct information on the failed drive must be even and vice versa. Parity data is used by RAID arrays (Redundant Array of Independent/Inexpensive Disks) to achieve redundancy. If a drive in the array fails, remaining data on the other drives can be combined with the parity data (using the Boolean XOR function) to reconstruct the missing data.\nFor example, suppose two drives in a three-drive RAID 5 array contained the following data: Drive 1: 01101101 Drive 2: 11010100 To calculate parity data for the two drives, an XOR is performed on their data: 01101101 XOR 11010100 10111001 The resulting parity data, 10111001, is then stored on Drive 3. Should any of the three drives fail, the contents of the failed drive can be reconstructed on a replacement drive by subjecting the data from the remaining drives to the same XOR operation. If Drive 2 were to fail, its data could be rebuilt using the XOR results of the contents of the two remaining drives, Drive 1 and Drive 3: Drive 1: 01101101 Drive 3: 10111001 as follows: 10111001 XOR 01101101 11010100 The result of that XOR calculation yields Drive 2's contents. 11010100 is then stored on Drive 2, fully repairing the array. XOR logic is also equivalent to even parity (because a XOR b XOR c XOR ... may be treated as XOR(a,b,c,...) which is an n-ary operator which is true if and only if an odd number of arguments are true). So the same XOR concept above applies similarly to larger RAID arrays with parity, using any number of disks. In the case of a RAID 3 array of 12 drives, 11 drives participate in the XOR calculation shown above and yield a value that is then stored on the dedicated parity drive. Following are the key points to remember for RAID level 5. Minimum 3 disks.\nMost important Deep learning algorithms are a comprehensive guide to the most widely used deep learning architectures. It provides an overview of the theoretical background and practical applications of the most mainstream deep learning methods, including the most widely used algorithms, such as Convolutional Neural Networks (CNNs), Long Short Term Memory Networks (LSTMs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Radial Basis Function Networks (RBFNs), Multilayer Perceptrons (MLPs), Deep Belief Networks (DBNs), Restricted Boltzmann Machines (RBMs) CNNs recognize satellite images, analyze medical images, forecast series, and detect anomalies. CNNs send data through numerous layers and extract features to perform convolutions. The Convolutional Layer's Rectified Linear Unit (ReLU) corrects the feature map. The pooling layer corrects feature maps for the following feed. Pooling is a downsampling approach that minimizes feature map dimensions. Later, single, long, continuous, and linear vector 2-D arrays are flattened in the map. The Fully Connected Layer builds a 2-D array from the Pooling Layer's flattened matrix and classifies the image. A layer of Convolutional Data CNN features a convolution layer, which is comprised of a few different filters and is used to accomplish the convolution function. CNNs that use the Rectified Linear Unit (ReLU) algorithm feature a layer called the ReLU that is used to carry out operations on the elements. The final product is a feature map that has been repaired. Pooling Layer After that, the rectified feature map is sent into a layer that does pooling.\nThe size of the feature map can be reduced by the use of a down-sampling procedure known as pooling. The flattening operation is performed by the pooling layer, which results in the resulting two-dimensional arrays from the pooled feature map being converted into a single, long, continuous, linear vector. Layer That Is Completely Connected When the flattened matrix from the pooling layer is given as an input, a fully connected layer is formed. This layer is responsible for the classification and identification of the images. Long Short-Term Memory (LSTM) networks are Recurrent Neural Networks (RNN) that are trained to learn and adapt to long-term dependencies. It can remember and recall past information for an extended period, and this is its default behavior. Due to their ability to store memory of past inputs across time, LSTMs are widely employed for time series prediction. This analogy is based on their chain-like structure, which consists of four layers that connect and communicate with one another in different ways. In addition to time series prediction applications, they can be used to build voice recognizers, produce medications, and compose music loops. LSTM operates sequentially. First, they tend to forget unimportant information acquired in the prior condition. Then, they selectively update some cell-state values and generate particular portions of the cell-state as output. The operating diagram is shown below. RNNs follow the work approach by feeding (t-1) time if time is t. t's output is fed at t+1. Inputs of any length undergo the same processes.\nRNNs store past data and don't increase input size if the model size is raised. Unfolded RNNs resemble this. GANs are deep learning algorithms used to generate training-data-like data. GAN consists of a generator that learns to generate false data and a discriminator that learns from it. GANs are used to clarify astronomy images and simulate gravitational dark matter lensing. It's used in video games to upscale 2D textures to 4K quality. They're also utilized to render human faces and 3D objects. GANs generate fake and genuine data in simulation. During training, the generator provides bogus data that the discriminator learns to recognize as false. GANs send updated results. RBFNs are feed-forward neural networks with radial activation functions. Input, hidden, and output layers are utilized for time-series prediction, regression testing, and classification. RBFNs do these functions by measuring data similarities. Input vectors send data into the input layer, validating identification and rolling out findings by comparing earlier data sets. Input layer neurons are sensitive to these facts, and layer nodes classify them efficiently. Neurons reside in the hidden layer but integrate with the input layer. Hidden layer Gaussian transfer functions are inversely proportional to neuron output distance. Output layer linear combinations of radial-based data using Gaussian functions as neuron parameters. Deep learning uses MLPs. It's a feed-forward neural network with perceptron layers. Perceptrons have activation functions. MLPs have similar input and output layers. Hidden between these two levels is another. MLPs are used to construct pictures, voice, and translation software.\nThe input layer is where MLPs receive data. The layer's neurons form a graph for a one-way connection. This input data's weight is between the hidden and input layers. MLPs use activation functions to find ready-to-fire nodes. Tanh, sigmoid, and ReLUs are activation functions. MLPs are used to train models to comprehend the layers' co-relation to generate the desired output from a given data source. This algorithm is utilized in dimension reduction, regression, classification, topic modeling, and DBNs. RBIs have apparent and hidden layers. Both layers feature hidden units and bias units connected to output nodes. RBMs have forward and backward pass phases. RBMs work by turning inputs into numbers for the forward pass. RBMs weigh each input, and the backward pass transforms these weights into reconstructed inputs. Later, both translated inputs are mixed with weights. These inputs are subsequently transferred to the visible layer for activation, and an easily reconstructed output is formed. When selecting an algorithm, accuracy, training time, and usability must always be considered. Numerous users prioritize precision, whereas novices focus on algorithms they are most familiar with. When faced with a dataset, the first thing to consider is how to achieve results, regardless of what those results may look like. Beginners typically select algorithms that are simple to implement and yield quick results. This is acceptable so long as it is the initial step in a procedure.\nStorages are one of the devices that can be used to store information. In the following, we will examine the types of information storage methods and technologies used in them. Types of information storage Introducing Tape Storage Tool Information storage terms on the computer What is Block Storage? What is Block Level Storage? Advantages of Block Level Storage system What is File Level Storage? What is Cold Storage? What is Tiered storage? What are Flash Ready and Flash Storage? Disadvantages of Flash Storage Every processing system needs storage equipment in order to record and store the processed information. Any space in which we store information is called storage. Therefore, when choosing a server, storage space should be considered accordingly. 1- Internal storage equipment: This equipment includes all types of hard disks and hard drive controllers that are installed inside the server case. 2- External storage equipment: This equipment includes a variety of storage devices based on hard disk and magnetic tape or tape that are connected to the server or network as complementary equipment. The types of storage tools used in the IT world are tapes that are used in the field of storage and are used to back up and back up data and information. These drives are sometimes installed directly on servers. If it is necessary to replace the next tape automatically after filling one Tape, Autoloader should be installed on the devices.\nIf you want to use multiple drives to back up multiple devices at the same time, you must use the Tape Library. Hard disk-based data storage device is divided into 4 categories based on network structure, hardware infrastructure, operating system and required software and user: DAS or Direct Attached Storage is a type of storage that connects directly to the computer or server Storage Controller, meaning that the hardware control of this storage is with the server and is not part of the storage network. This technology is needed when the amount of data you need is more than the hard drives on the server, so by adding DAS, you increase the number of drives under your server management. Das storage is one of the best storage for network expansion because it does not require any additional tools to use and to use it, just buy a cable with the device. Of course, there is a need for some configurations, but the simplicity of this configuration is such that users can do this configuration without special expertise or advice from network experts. The very reasonable price of das storage is another important advantage so that the use of this storage is very common in small and medium businesses as well as branches of large companies. Another advantage of this storage is its flexibility; So you can use SAS, SATA and SSD in one enclosure at the same time. This feature is critical for growing businesses that need occasional network equipment upgrades.\nNAS stands for Network Attached Storage and is one of the most widely used data storage systems in the world today as a server. The nas device is commonly used to centrally manage corporate files and share them with users. Storage on this system is very secure and fast; Because NAS uses RAID technology and users can change and use the read settings according to their needs. Another advantage of NAS storage is the ability for multiple users to access data simultaneously. For medium and large companies where fast access to shared data is essential, a NAS storage solution is a good idea. Another point in this method of storage is the possibility of remote access to files, which is possible through the remote system. NAS storage also allows users to use web-based applications, which is another great advantage, especially when combined with a remote access system, allowing users to access their data without the need for physical presence and the use of system software. SAN stands for Storage Area Network and is actually a network of storage; Storage with Block level access for the server; This means that the server can directly access the hard disk blocks without intermediaries. This method has a high speed. When it comes to flexibility and adaptability, no technology can match block-level storage. This method is very common in large companies because SAN storage offers users a great deal of flexibility, accessibility and efficiency; Benefits that are important for large companies.\nHuawei's R&D efforts have resulted in a breakthrough that has greatly improved the performance computing and storage networks: fully lossless Ethernet. At Huawei Connect 2021, Wang Lei, President of the Data Center Network Domain of Huawei Data Communication Product Line, invited customer experts to discuss how Huawei's hyper-converged data center network can help improve supercomputing power and artificial intelligence. The experts also shared their insights into and visions for supercomputing applications. In Liu Cixin's science-fiction novel The Three-Body Problem, \"the world's most powerful computers,\" used to simulate nuclear explosions, \"can perform 500 trillion floating-point operations per second.\" That seems like an incredibly huge number. However, on July 1, 2021, China's Pengcheng Cloud Brain II topped the IO500 ranking at the World Supercomputing Conference, exceeding one quintillion floating-point operations per second (FLOPS), 2,000 times faster than the most powerful computers in The Three-Body Problem. Real-world supercomputing has gone beyond the imaginings of science fiction. Huawei Tech: We know that supercomputers have tremendous computing power. In laymans terms, what does that mean? Wang Lei: Exascale computing is truly powerful. Technically speaking, it is 1018 FLOPS, equivalent to what half a million of our latest laptops can do combined. AlphaGo, the AI that beat Lee Sedol at Go a few years ago, was powered by a petaflop-level [more than one quadrillion] FLOPS supercomputer. Today's exascale computers are 1,000 times as powerful as the computer that powered AlphaGo. Huawei Tech: Is building a supercomputer simply a matter of stacking many computers together?\nBack when we just started investment and research in computing, our researchers found that simply binding servers together could not create a linear increase in computing power. For example, they found that doubling the number of GPU servers increased computing power by just 4%. Through analyzing the computing process, we found that the problem was caused by packet loss, an inherent problem with conventional Ethernet. Packet loss of just 0.1% can result in computing power loss of 50%, meaning half the server computing power is wasted. To address this, Huawei began examining the question of how we might create lossless Ethernet networks. We finally solved this problem two years ago, thereby realizing 100% utilization of servers' computing power. Huawei Tech: The digital economy is said to have entered the computing era. But will supercomputing have any impact on the daily lives of ordinary people? Wang Lei: For most people, the whole topic of supercomputing seems like something very remote, because this technology is mainly used in relatively high-level applications such as weather forecasting, earthquake monitoring, and human genetic testing. However, supercomputing is much closer to our daily lives than most of us are aware of. For example, in recent years, its played a role in increasing the variety of new, affordable cars. Vehicle crash testing is one of the most time- and investment-intensive processes in automobile manufacturing. Using physical vehicles for testing means each crash results in a scrapped testing vehicle, and the cost can add up to millions of RMB.\nHowever, using supercomputers to simulate crash tests can shorten the development cycle of new cars from 36 months to 12 months. Now, with Huawei's hyper-converged data center network, that process can be expedited even further. Customer 1: We can look at supercomputing and its implications from the public health perspective. In the early days of the pandemic, we didnt understand COVID-19 so well. Through extensive analysis, it was later found that the cytokine storm was an important factor increasing the morbidity rate. Supercomputing played a major role in the process of deepening our knowledge. Scientists and doctors working together found that the overreaction of the human immune system to the invading virus affected certain normal bodily functions and led to the failure of those functions. With the support of supercomputing, a way to cut off the cytokine storm signal pathways was discovered. This knowledge was put to good use in Wuhan, where it saved lives. Customer 2: If we compare AI computing power to electric power, the AI computing center is like a large-scale power station. AI applications, like electricity, will be widely used in numerous industries and households. The use of AI will make urban management more precise self-driving vehicles and license plate recognition are examples of AI in our daily lives and urban management. Albert Einstein once said, \"We cannot solve our problems with the same level of thinking that created them.\" The history of humanity has been a long process of creating and solving problems.\nHello, everyone! Today I'm going to introduce you the cloud computing and cloud storage. When it comes to cloud storage, the first thing people think of is cloud computing. Cloud storage is a new concept that has been extended and developed in the concept of cloud computing. Cloud computing is the development of distributed computing, parallel processing (Parallel Computing) and grid computing (Grid Computing). It is the automatic splitting of huge computing processing programs into countless smaller subroutines through the network. The huge system composed of multiple servers is analyzed and analyzed, and the processing result is transmitted back to the user. Through cloud computing technology, network service providers can process tens of millions or even billions of information in a matter of seconds, achieving the same powerful network services as \"supercomputers.\" The goal of cloud computing systems is to migrate independent, personalized operations running on a PC or a single server to a large number of servers \"clouds\" that are responsible for processing user requests and outputting results. It is a system with data operations and processing as its core. When the core of computing and processing of cloud computing systems is the storage and management of large amounts of data, cloud computing systems need to be configured with a large number of storage devices, then the cloud computing system is transformed into a cloud storage system, so cloud storage is a data storage system. And management as the core of the cloud computing system.\nHello all, Can I configure the same UIDs or user names in the local host for Oceanstor OceanStor 9000 V5? Thanks. Dear Axe, You are advised not to configure the same UIDs or user names in the local host, LDAP domain, or NIS domain. If the same UIDs or user names exist, the user mapping result will not be the expected result. After a user is mapped, the owner information of the files or directories owned by CIFS users (the files or directories that are created by CIFS users or the owner information of the files or directories are changed to CIFS users) is the information of the NFS users mapped from CIFS users on the NFS client. If no mapping rules have been configured for CIFS users, the owner information of the files or directories is the IDs (calculated using IDMAP, a hash algorithm) of the CIFS users on the NFS client. If the client is an NFSv4 client, the owner information is displayed as nobody . After a user is mapped, the owner information of the files or directories owned by NFS users (the files or directories that are created by NFS users or whose owners are changed to NFS users) is NFS user names on the CIFS client: When NFS users are NIS domain users, the owner information is displayed as NIS_DOMAIN\\user name . When NFS users are LDAP domain users, the owner information is displayed as LDAP_DOMAIN\\user name .\nWhen CIFS users are mapped to NFS users, quota statistics will be collected for the NFS users or owning user group. If an NFS user creates a soft link using a full path on a Linux client, a mapped CIFS user cannot access the soft link on a Windows client. Thanks. Dear Axe, You are advised not to configure the same UIDs or user names in the local host, LDAP domain, or NIS domain. If the same UIDs or user names exist, the user mapping result will not be the expected result. After a user is mapped, the owner information of the files or directories owned by CIFS users (the files or directories that are created by CIFS users or the owner information of the files or directories are changed to CIFS users) is the information of the NFS users mapped from CIFS users on the NFS client. If no mapping rules have been configured for CIFS users, the owner information of the files or directories is the IDs (calculated using IDMAP, a hash algorithm) of the CIFS users on the NFS client. If the client is an NFSv4 client, the owner information is displayed as nobody .\nA Device Fails to Start After It Is Turned On If a device fails to start after it is turned on, services on the device cannot work. After being turned on, the device keeps trying to enter the network booting interface but cannot start and enter the operating system. Possible cause 1: A system disk (namely, a disk running the operating system) is incorrectly inserted System disks in a RH2288 V3 12-slot Nodes System disks in a 5288 V3 36-slot Nodes Possible cause 2: The basic input/output system (BIOS) booting device is incorrectly configured. Possible cause 3: The system is not set to start from the RAID group. Possible cause 4: A system disk fails. Flowchart for troubleshooting a device start failure Procedure Possible cause 1: A system disk is incorrectly inserted. a. Reinsert the system disk. b. Turn on the device and check whether it can start. - If yes, no further operations are required. - If no, go to Possible cause 2 . Possible cause 2: The BIOS booting device is incorrectly configured a. Turn on the device and during the system startup process, press Esc when the message is displayed, choose SCU , input the BIOS password to enter the BIOS screen. b. On the Boot tab page, select Legacy and press Enter . c. Under Boot Type Order , select Hard Disk Drive and press F5/F6 to set Hard Disk Drive as the prior start device. Then press Enter .\nd. Select #0300 ID43 LUN0 LSI Logical and press F5/F6 to set the system disk as the prior start device. e. Press F10 to save and exit the BIOS interface. Turn on the device and check whether it can start. - If yes, no further operations are required. - If no, go to . Possible cause 3: The system is not set to start from the RAID group. a. After the BIOS is configured, the device automatically restarts. During the startup, the message is displayed for pressing Ctrl+C. Press Ctrl+C to go to the SAS BIOS settings. b. Select Eval Board and press Enter . The Adapter Properties SAS2008 page is displayed. c. Select SAS Topology and press Enter to go to the management interface. d. Select the RAID group where the system disk resides, as shown in the gray striped area. The slot IDs of the member disks of this RAID group are 25 and 26. Press Alt+B to set the system to start from this RAID group. Press ESC and select Save changes then exit the menu . e. Press ESC twice, select Exit the Configuration Utility and Reboot , and press Enter . The device automatically restarts. Check whether the device can start correctly. - If yes, no further operations are required. - If no, go to . Possible cause 4: A system disk fails. a. Replace the faulty system disk following Replacing a System Disk Module in . b. Turn on the device and check whether it can start.\nDear all, This post will give an overview ofOpen Services Gateway Initiative. The OSGi (Open Services Gateway Initiative) specification is a Java framework for developing and deploying modular software programs and libraries. The framework was initially managed by the OSGi Consortium, an open standards organization. In October 2020, the OSGi Consortium formally announced the transfer of its standardization working group to the Eclipse Foundation to save costs, as OSGi is now developing specifications that go beyond the original intent of the Consortium. The transition of the OSGi standards group to the Eclipse Foundation makes sense because they have a broader focus on open source software. The OSGi specification consists of the following two parts: Standard for building modular components called bundles or plug-ins. Plug-ins are based on component-based software engineering practices where standalone code can be written and packaged, so they can be easily reused in a modular fashion. The OSGi specification defines an infrastructure for the life cycle of a bundle and determines how bundles interact and bind with other services. Java virtual machine-like container. These containers act as repositories for bundles and can be used to publish, discover, and bind services in a service-oriented architecture. This is essentially a bundle of service platform frameworks that run on top of it. These containers are flexible enough to provide multiple applications directly from a single container. In addition, packages in one container can be exposed through an application programming interface for use by packages in another container.\nThe framework in which the bundle resides consists of several layers that handle service registration tasks, network security, module interactions, and management of various states in the framework lifecycle. The OSGi framework is component-based, and the components are called bundles. OSGi provides a modular architecture for today's large-scale distributed systems as well as small embedded applications and devices networks. Building systems from in-house and off-the-shelf modules significantly increases the reuse of software products and solutions and extends their lifecycles, thereby reducing development and maintenance costs. The OSGi programming model realizes the promise of component-based systems. OSGi technology is successful because it provides a very mature, field-proven system of components that can work in a large number of environments. The OSGi component system is used to build any type of application, from simple to highly complex applications such as IDEs, application servers, email systems, content management systems, application frameworks, residential gateways, and on-board telematics systems. OSGi is widely adopted in a wide range of industries and applications such as the Internet of Things, M2M, smart home, telematics, assisted living, healthcare, automotive, media, control systems, energy management, smart meters, telecommunications, enterprise software platforms, and robotics. The original goal of OSGi began with embedded system vendors and network vendors working together to create a set of standards for a remotely manageable Java-based service framework. OSGi was originally conceived as a service gateway for managing smart appliances and other internet-enabled devices in the home.\nGreetings! This post enquires about theincreased SSD disk space. Please see below. ISSUE DESCRIPTION I wonder if it is possible to increase the space on a hard disk already embedded. Could anyone please help me out and clarify? Thanks in advance! Dear Luciano, It's hard toincrease space on a hard disk already embedded. You can change to a different RAID leave to safe space. Here are some explanding for storage. For block services, you can expand the capacity using the following methods:Increasingthe LUN capacityIncreasingLUN capacity is more complex than adding LUNs. When the available space of a file system is about to run out and the storage pool has available...Preferentially Delete Old Snapshot: Delete old snapshots to reclaim space forincreasing Expanding LUN Capacity The following operations must be performed in sequence to expand the LUN capacity: Expand the LUN capacity on the storage system. Huawei provides technical support Expanding a Disk Domain This operation allows you toincreasethe capacity of a disk domain. Thank you. Dear Luciano, It's hard toincrease space on a hard disk already embedded. You can change to a different RAID leave to safe space. Here are some explanding for storage. For block services, you can expand the capacity using the following methods:Increasingthe LUN capacityIncreasingLUN capacity is more complex than adding LUNs.\nDear community, Do you know about Cybersecurity Mesh? I will share with you what you need to know about it. Cybersecurity mesh becomes a building block of a Zero Trust security strategy ensuring all data, services, devices, and applications are accessed securely regardless of where they are whether by human or machine. All connections to access the data are considered unreliable unless verified. The cybersecurity mesh is a key component of a zero-trust network philosophy, whereby any device is by default not trusted to access the broader network. Perimeter-focused security often fails because as much as 34 percent of data leaks and breaches originate on the inside of the network itself. A distributed cybersecurity mesh that utilizes zero trust adapts to emerging threats and changing access needs. Threats can be detected in real-time and assets such as data and devices can be protected better than simple VPN passwords. The mesh ensures that all data, systems, and equipment are treated equally and securely it doesnt matter where they are located in (or out) of the network. Any connection to access data is by default considered unreliable until it is verified by the security protocol. Zero Trust Architecture You can create a Cybersecurity Mesh by designing and implementing an IT security infrastructure that is not focused on building a single perimeter around all devices or nodes of an IT network, but instead you create a smaller individual perimeter around each access point. This is a horizontal approach to a network rather than a traditional top-bottom approach.\nIn the mesh, the access points can be managed from a centralized point of authority. This Cybersecurity Mesh can establish a more robust and flexible approach to a networks security. When each node has its own perimeter the IT network manager can maintain and track different levels of access to various parts of a given network. As anywhere operations continue to evolve, the cybersecurity mesh will become the most practical approach to ensure secure access to, and use of, cloud-located applications and distributed data from uncontrolled devices. 1. Cybersecurity mesh will support more than 50 percent of IAM requests: today, most digital assets, identities, and devices exist outside of the enterprise, which complicates traditional security models. When it comes to IAM requests, Gartner predicts cybersecurity mesh will support the majority of IAM requests and enable a more explicit, mobile, and adaptive unified access management model. With the mesh model, enterprises get a more integrated, scalable, flexible, and reliable approach to digital asset access points and control than traditional security perimeter protection. 2. Delivery of IAM services will lead to a rise in managed security service providers (MSSPs): MSSP firms can provide enterprises with quality resources and necessary skillsets to plan, develop, acquire, and implement comprehensive IAM solutions. Gartner predicts that by 2023, 40 percent of IAM application convergence will primarily be driven by MSSPs that focus on delivering best-of-breed solutions with an integrated approach; this process will shift the influence from product vendors to service partners. 3.\nIdentity proofing tools will be added to the workforce identity life cycle: more robust enrollment and recovery procedures are urgently needed thanks to the massive increase in remote interactions, which make it harder to differentiate between attackers and legitimate users. Gartner states that by 2024, 30 percent of large enterprises will implement new identity-proofing tools to address common weaknesses in workforce identity life cycle processes. 4. Decentralized identity standards emerge: centralized approaches to managing identity data make it harder to provide privacy, assurance, and pseudonymity. With the decentralized approach empowered by the mesh model, blockchain technology ensures privacy and allows individuals to validate information requests by providing the requestor with just the minimum required amount of information. By 2024, Gartner predicts that a true global, portable, decentralized identity standard will emerge in the market to address business, personal, social and societal, and identity-invisible use cases. 5. Demographic bias within identity proofing will be minimized: more enterprises have become interested in document-centric approaches to identity proofing. The rise of remote work in 2020 called attention to the many ways bias with respect to race, gender, and other characteristics can occur in online use cases. Therefore, by 2022, 95 percent of organizations will require that identity-proofing vendors prove that they are minimizing demographic bias. A cybersecurity mesh architecture provides a composable approach to security based on identity to create a scalable and interoperable service. The common integrated structure secures all assets, regardless of location, to enable a security approach that extends across the foundation of IT services.\nHello everyone In this post, you can learn about the power-on and power-off operations of all Huawei storage devices. Powering On and Off Huawei OceanStor V3 and V5 Series Storage Devices 1. Powering On and Off the Storage System 2. Power On and Off interface module 3. Rebooting the Storage System 4. Emergency power-on and power-off 5. Powering On a Key Management Server 6. Powering On and Off Disk Enclosures Powering On and Off Huawei OceanStor 18000 V3 and V5 Series Storage Devices 1. Powering On and Off the Storage System 2. Power On and Off interface module 3. Restarting the Storage Device 4. Emergency power-on and power-off 5. Powering On a Key Management Server 6. Powering On and Off Disk Enclosures 7. Restarting the SVP Powering On and Powering Off Huawei Dorado V3 and V6 Series Devices 1. Powering On and Off the Storage System 2. Power On and Off interface module 3. Restarting the Storage Device 4. Emergency power-on and power-off 5. Powering on the Storage System (Remotely on the CLI) Powering On and Off Huawei OceanStor 9000 V5 Storage Devices 1. Power-on and Power-off Precautions 2. Using DeviceManager to Power On or Off Storage Nodes 3. Using CLI Commands to Power On or Off Storage Nodes 4. Using Storage Nodes' Operating Systems to Power Off Storage Nodes 5. Using Mgmt Ports to Remotely Power On or Off Storage Ports 6.\nDear All, Today we are going to learn about the Working Principles of NFS and CIFS. Working Principles of NFS One program can use remote procedure call (RPC) to request a service from a program located in another computer on a network without having to understand the network's details. RPC runs on top of other transmission protocols such as Transmission Control Protocol (TCP) or User Datagram Protocol (UDP), which carry the message data between communicating programs. In the OSI network communication model, RPC traverses the transport layer and application layer. RPC simplifies development of applications. RPC works based on the client/server model. The requester is a client, and the service provider is a server. The client sends a call request with parameters to the RPC server and waits for a response. On the server side, the process remains in a sleep state until the call request arrives. Upon receipt of the call request, the server obtains the process parameters, outputs the calculation results, and sends the response to the client. Then, the server waits for the next call request. The client receives the response and obtains call results. Typical Application of NFS: Shared Storage for Cloud Computing Cloud computing uses the NFS server as the internal shared storage. Cloud virtualization software (such as VMware) optimizes the NFS client, so that the VM storage space can be created on the shared space of the NFS server. The NFS client is optimized based on cloud computing to provide better performance and reliability.\nDear All, Today we are going to learn about NIC TOE NIC and iSCSI HBA + initiator software NIC + Initiator Software Host devices such as servers and workstations use standard NICs to connect to Ethernet switches. iSCSI storage devices are also connected to the Ethernet switches or to the NICs of the hosts. The initiator software installed on hosts virtualizes NICs into iSCSI cards. The iSCSI cards are used to receive and transmit iSCSI data packets, implementing iSCSI and TCP/IP transmission between the hosts and iSCSI devices. This mode uses standard Ethernet NICs and switches, eliminating the need for adding other adapters. Therefore, this mode is the most cost-effective. However, the mode occupies host resources when converting iSCSI packets into TCP/IP packets, increasing host operation overheads and degrading system performance. The NIC + initiator software mode is applicable to scenarios that require the relatively low I/O and bandwidth performance for data access. TOE NIC + Initiator Software The TOE NIC processes the functions of the TCP/IP protocol layer, and the host processes the functions of the iSCSI protocol layer. Therefore, the TOE NIC significantly improves the data transmission rate. Compared with the pure software mode, this mode reduces host operation overheads and requires minimal network construction expenditure. This is a trade-off solution. iSCSI HBA An iSCSI HBA is installed on the host to implement efficient data exchange between the host and the switch and between the host and the storage device.\nDear All, Today we are going to learn about NAS. What is NAS? Network-attached storage (NAS) connects storage devices to the live network and provides data and file services. The most used network sharing protocols for NAS are Common Internet File System (CIFS) and Network File System (NFS). Enterprises need to store a large amount of data and share the data through a network. Therefore, NAS is a good choice. For a server or host, NAS is an external device and can be flexibly deployed through the network. In addition, NAS provides file-level sharing rather than block-level sharing, which makes it easier for clients to access NAS over the network. NAS provides storage resources through file-level data access and sharing, enabling users to quickly share files with minimum storage management costs. NAS is a preferred file sharing storage solution that does not require multiple file servers. NAS also helps eliminate bottlenecks in user access to general-purpose servers. NAS uses network and file sharing protocols for archiving and storage. These protocols include TCP/IP for data transmission as well as CIFS and NFS for providing remote file services. UNIX and Microsoft Windows users can seamlessly share data through NAS or File Transfer Protocol (FTP). When NAS sharing is used, UNIX uses NFS and Windows uses CIFS. General-Purpose Server and NAS Devices NAS devices are optimized based on general-purpose servers in aspects such as file service functions, storage, and retrieval.\nAs shown in the figure, a general-purpose server can be used to carry any application and run a general-purpose operating system. Unlike general-purpose servers, NAS is dedicated to file services and provides file sharing services for other operating systems using open standard protocols. To improve the high availability of NAS devices, some NAS vendors also support the NAS clustering function. The components of a NAS device are as follows: NAS engine (CPU and memory) One or more NICs that provide network connections, for example, GE NIC and 10GE NIC. An optimized operating system for NAS function management NFS and CIFS protocols Disk resources that use industry-standard storage protocols, such as ATA, SCSI, and FC The NAS environment includes clients that access NAS devices over IP networks using standard protocols. NAS Protocols NFS is a traditional file sharing protocol in the UNIX environment. CIFS is a traditional file sharing protocol in the Microsoft environment. It is based on the Server Message Block (SMB) protocol. CIFS has high requirements on network transmission reliability, so it usually uses TCP/IP. NFS is used for independent transmission and thereby uses TCP or UDP. One disadvantage of NFS is that clients must be equipped with dedicated software. CIFS is integrated into the operating system and does not require additional software. NFS is a stateless protocol, whereas CIFS is a stateful protocol. If a fault occurs, the NFS connection can be automatically recovered, whereas the CIFS connection cannot be automatically recovered. CIFS sends only a small amount of redundant information.\nDear All, Today we are going to learn about raid levels. What Is RAID? Redundant Array of Independent Disks (RAID) combines multiple physical disks into one logical disk in different ways, for the purposes of read/write performance and data security improvement. Implementations: hardware RAID and software RAID Functionality of RAID: Combines multiple physical disks into one logical disk array to provide larger storage capacity. Divides data into blocks and concurrently writes/reads data to/from multiple disks to improve disk access efficiency. Provides mirroring or parity for fault tolerance. Hardware RAID and software RAID can be implemented in storage devices. Hardware RAID uses a dedicated RAID adapter, disk controller, or storage processor. The RAID controller has a built-in processor, I/O processor, and memory to improve resource utilization and data transmission speed. The RAID controller manages routes and buffers, and controls data flows between the host and the RAID array. Hardware RAID is usually used in servers. Software RAID has no built-in processor or I/O processor but relies on a host processor. Therefore, a low-speed CPU cannot meet the requirements for RAID implementation. Software RAID is typically used in enterprise-class storage devices. Data Organization Forms Disk striping: Space in each disk is divided into multiple strips of a specific size. Data is also divided into blocks based on strip size when data is being written. Strip: A strip consists of one or more consecutive sectors in a disk, and multiple strips form a stripe.\nStorages are one of the devices that can be used to store information. In the following, we will examine the types of information storage methods and technologies used in them. Types of information storage Introducing Tape Storage Tool Information storage terms on the computer What is Block Storage? What is Block Level Storage? Advantages of Block Level Storage system What is File Level Storage? What is Cold Storage? What is Tiered storage? What are Flash Ready and Flash Storage? Disadvantages of Flash Storage What is File Level Storage? File Level Storage System is the most common storage system in hard drives, NAS systems and.. In this storage, the storage disk is configured with specific protocols (such as NFS, SMB / CIFS, etc.) and the files are stored in In Bulk and accessed. File Level Storage devices are often used to share files between users. Although Block Level Storage is very flexible, File Level Storage is invincible when it comes to simplicity. In addition to simplicity, this technology is centralized and highly accessible (HA), which provides a place to store files and folders, which is the most vital needs of organizations. These File Level devices (usually NAS devices) provide a lot of storage space and at the same time cost less than Block Level Storage. File Level Storage uses common file level protocols such as SMB / CIFS in Windows and NFS in Linux and VMware. In Block Level Storage, a volume must be created, the operating system booted, and then connected to this created volume.\nBut in File Level, the storage device controls the files and folders on the device, which means that in many cases, the File Level Storage or NAS device must control User Access Control and Permissions. Some devices also integrate with existing security systems. In the case of backups, these devices may need to be controlled because they do not use a standard operating system. So if you decide to use these devices, pay attention to this point. If you need Authentication, Permission, and Backup, settings are easier on File Level devices than on Block Level Devices. In many cases, this procedure is as simple as using a simple configuration tool. But if you need high levels of storage performance, File Level is not for you and Block Level is a better choice. Block Level devices are generally configurable in terms of capacity and performance. Although file level devices also have performance advantages, capacity is usually a bigger concern. Mass File Storage: When your users only need to have a place to store files, File Level devices are suitable. VMware: In addition to using Block Level Storage, VMware hosts can also connect to storage via NFS. Advantages of File Level Storage System It is easy to implement and use. Files and folders are stored both in the systems they store and in the systems that users have access to. Compared to Block Level Storage, they are generally cheaper and less expensive to maintain. NAS-based storage systems are usually dependent on it.\nStorages are one of the devices that can be used to store information. In the following, we will examine the types of information storage methods and technologies used in them. Types of information storage Introducing Tape Storage Tool Information storage terms on the computer What is Block Storage? What is Block Level Storage? Advantages of Block Level Storage system What is File Level Storage? What is Cold Storage? What is Tiered storage? What are Flash Ready and Flash Storage? Disadvantages of Flash Storage Block Storage is a type of data storage that is commonly used in SAN environments, that is, where data is stored in volumes, also known as blocks. Each block acts as a separate hard drive and is configured by the storage admin. These blocks are controlled by the server operating system and are generally accessible by the FCoE and FC or iSCSI protocols. Suitable for storing all kinds of applications such as file systems and databases. Because volumes act like separate hard drives, Block Storage is suitable for storing a variety of applications such as file systems and databases. Of course, block storage devices are more complex and expensive than file storage devices, but at the same time they are flexible and provide higher performance. In Block Level Storage systems, raw blocks (storage volumes) are created and each block is controlled as a separate hard drive. These blocks are usually controlled by server operating systems. Each block or Storage Volume can be formatted independently with the appropriate file system.\nIn fact, Block Level Storage is a hard drive in the server, with the difference that this hard drive is installed in a separate case and can be accessed using fiber optics or iSCSI. Block Level Storage is invincible when it comes to flexibility and versatility. In these devices, raw storage volumes are created and then the server operating system is connected to these volumes and uses them as independent and separate hard drives. This feature makes Block Level Storage suitable for almost any type of application, such as File Storage, Database Storage, VMFS Volume, and so on. You can put any type of system file on Block Level Storage. If you are using Windows, you can use NTFS to format volumes, and if you have VMware servers, you can use VMFS. Block Level Storage is actually a hard drive on the server. When it comes to backing up, many storage files have replication capabilities, but backup tools such as DPM or Data Protection Manager can be used to back up your files. Because it looks and works like a hard drive, the backup process has no special shape. Because of the complexity of management, block-based storage devices are more complex than file-based storage devices, but you also have more flexibility. Carefully manage storage and share it between servers. Manage storage protection levels (eg RAID) Monitor the performance of storage devices to ensure that the required performance of servers and applications is provided. Monitor and manage storage connection infrastructure.\nData Storage Network: Interfaces and protocols on the hard drive Part 2 SAS, as its name implies, is a serial point-to-point protocol that uses the SCSI command suite and the advanced SCSI queuing mechanism. SCSI is another powerful organizational technology that offers a richer set of commands and a better queuing system, and products based on this technology use better hardware requirements than SATA drives. All of this makes SCSI-based drives, such as SAS and fc, the best choice for high-performance, critical workloads. Of course, achieving good performance is costly. SAS drives are more expensive than SATA drives with similar capacities. Another key advantage of SAS is that SATA ii and newer drives can connect to the SAS or backplane network and work alongside SAS drives. This makes SAS a flexible option when building storage arrays. SAS drives also have two ports, making them a great option for external storage arrays, as well as providing more flexibility. In the case of external storage arrays and SAS drives, each port on the SAS drive can be connected to different controllers in the external storage array. This means that if a port or even a controller fails, the drive is still available through the remaining port. This fault tolerance threshold for faulty ports allows organizations to continue operating through a secure port without interrupting business operations. Although SAS drives have two ports, these ports work in on / off mode, meaning that only one port is active and issues commands to the drive at a time.\nChip-based system or SoC is one of the new concepts. If you are interested in smartphones, you must have heard about it. This relatively small chip provides all the processing infrastructure needed to build a computer and has far more advantages than the CPU. Battle for the future of processing After more than 50 years, the central processing unit, known as the CPU for short, has finally found a competitor called the IMG Source: https://en.wikipedia.org/wiki/System_on_a_chip For decades, when buying a personal computer, you had no choice but to process the processor, but now you can look around and see a variety of devices, from smartphones and tablets to even some new laptops that use a chip-based system. Of course, the CPU and SoC have similar features, and almost everything we know about processors can be generalized to the chip-top system. A system-on-a-chip, or SoC, is an integrated circuit or IC that houses the components of a complete computer or electrical system on a single chip. SoCs may include central processing units (CPUs), graphics processing units (GPUs), memory, input / output controllers such as USB, power control circuit, wireless networks such as Wi-Fi, Bluetooth and 4G. IMG Source: https://en.wikipedia.org/wiki/System_on_a_chip Basic definition of SoC SoC is derived from the phrase \"System On A Chip\". This integrated chip or electronic circuit has various components that include processor (microprocessor or microcontroller), memory, I / O ports and secondary storage memory. All these components are mounted on a single piece like a chip.\nPlacing a large number of essential components on a single chip means reducing energy consumption and space occupied on the main board (motherboard) of the device. The popularity of the on-chip system is growing rapidly thanks to the expansion of the concept of the Internet of Things (IoT) and cross-border and mobile processing. For example, Intel, the world processor giant, bought a SoC manufacturer called Silicon Engineering Group in 2018. We've seen other deals before, such as Intel buying Altera. One of the most common examples of SoC applications is game consoles. Nintendo used Nvidia's Tegra X1 chip on the switch console, and AMD's new chip on the Flute and Gonzalo are set to be used in the next generation of PlayStation and Xbox, respectively. Raspberry Pi computers, Arduino boards and STEM kits all use a chip-based system. The chip surface system is often used in STEM kits due to its ease of use and therefore also plays a positive role in teaching the design of these kits to interested people. In addition, another area in which these chips are widely used are the smartphones and tablets that we use on a daily basis.\nTypes of SoC There are generally three types of SoC or chip-based systems: The chip system that uses a microcontroller (a chip containing a processor, RAM, ROM, and possibly other components) Example: Arduino boards Top system of chips using a microprocessor (a chip containing a processor) Example: Chip-based system used in smartphones; For example, Qualcomm's Snapdragon 845 chipset (which is also used for virtual reality headsets) or Apple A12 Bionic, which is used in iPhone XS, iPhone XS Max and iPhone XR. Also the operating system of Intel and Rasperry Pi FPGA series chips System-specific chips, which may have a microcontroller or microprocessor. These chips are known as ASICs. Basic CPU definition Although there is always emphasis on technology and performance of processors, but it is not bad to know that the processor is ultimately very fast like a calculator! The CPU requests information from memory and then performs arithmetic operations (such as addition and multiplication) or logical and conditional operations (and, or, is not) on that information. The more expensive and complex the processor, the more information it can process and the faster your computer will perform. Image of a number of processors and RAM A processor is not independently a personal computer; A personal computer is actually a framework of a set of chips. Memory is needed to store information, an audio chip is responsible for decoding and amplifying audio signals, the GPU displays images on your monitor, and there are hundreds of other smaller components, each with its own important function.\nWhat is the difference between SoC and CPU? The most important advantage of a chip-based system is its dimensions. Although a chip-top system is usually only slightly larger than the processor, it has far more capabilities. If you use a processor, it is very difficult to build a computer with an area of less than 10 square centimeters, because you need several other chips for proper performance. But with a chip-top system, you can fit a complete computer into smartphones and tablets, while still having enough space to store the battery. A chip-based system requires much less power due to its high integration and much shorter communication paths, and this is especially important in mobile gadgets such as smartphones. In addition, by reducing the number of separate physical chips, the cost of production is reduced and you can have a cheap but efficient computer. But the only drawback of a chip-top system is the lack of complete flexibility in it. On a PC you can easily replace a processor, graphics card or RAM with better options, but this is not possible on a smartphone, for example. It may be possible to buy a new chip-based system in the future, but it's still a waste of money since all the components are on the same chip. For example, suppose you only need more RAM and you do not want to pay for a more powerful CPU or GPU on a new chip.\nThe chip-top system is a package, and unfortunately you can not replace or upgrade only part of it. Where did the idea for the chip-based system come from? Google has been designing the chips it needs for several years. Custom-designed chips are a much better choice than general-purpose chips made by companies such as AMD or Nvidia, given the company's processing needs. With the advent of cloud computing, the demand for processing power in Google data centers has skyrocketed, forcing the company's engineers to build custom chips to meet new needs. For example, in 2015, Google unveiled the Tensor Processing Unit (TPU), a chip used in Google Data Centers to improve the performance of machine-based applications and services such as real-time voice search, image recognition, and interactive language translation. Provide a better experience. Instead of integrating the various components on the motherboard, which were electronically spaced a few inches apart, we used chip-based system designs to have several different functions or separate chips on a single chip Advantages of using a chip-based system Custom chips as optimal and targeted hardware components have somehow underestimated Moore's law. With their integrated design, these chips can well respond to the exponential increase in demand for processing power. With deeper integration in the hardware layers, higher performance becomes available with less power consumption, which could open up new horizons in terms of optimization and scalability Chip systems also offer the possibility of further customization and can be designed for specific applications.\nThe main challenge is that due to the variety of cloud services, the process of designing and manufacturing these systems and chips must be done quickly. In fact, in SoCs, the CPU is part of the chip. The most important advantage of SoCs and their popularity is their very small size; The same thing is true of smartphones, tablets and other powerful gadgets today. in addtion to the power of SoCs, today's smartphones have been able to do many things on a personal computer. Another advantage of system-on-one-chips is faster and easier communication between different components, which reduces energy consumption and reciprocally increases battery life; This issue is very important in mobile phones and portable gadgets. The use of SoCs significantly reduces the cost of producing the product; Because each part does not need to be produced separately and then installed independently on the board. The most important weakness of SoCs is their inflexibility. On PCs, you can easily replace or upgrade your GPU or CPU or RAM; Because these parts are mounted independently on the main board. But it is not possible to upgrade these components in smartphones. It may be possible to replace SoCs in gadgets in the future; But even then you have to pay a high price; Because in the SoC, there is RAM, GPU and other components mentioned above, and in this case they are all interchangeable. Finally, the chip-top system is the next step, and they have the power to take the place of the processors.\nLUN stands for Logic Unit Number, which means the logical unit number used to identify a logical unit. In essence, a LUN is a unique identifier for identifying an individual or set of physical or virtual storage devices that executes I / O commands with a host computer and is standardized by a small system computer interface or SCSI is defined. In other words, LUN is a device stored by the SCSI protocol or Small System Computer Interface that encloses SCSI like channel fiber or iSCSI. SCSI is an interconnected I / O connection that can facilitate the exchange of data between servers and storage devices via transmission protocols. A logical number may be used by any device that supports read and write operations, such as a tape drive, but is more commonly used on a logical disk such as that created in a SAN. However, this is not technically correct, and the term \"LUN\" is often used to refer to a logical disk. To give a practical example, a typical multi-disk drive has multiple physical ports, each with a SCSI address. An administrator may format the disk array as a RAID and then split the RAID into several separate storage volumes. To display each volume, a SCSI token is configured to represent a logical unit. Each SCSI token may represent multiple logical units and therefore represent multiple volumes, but this does not mean that these volumes are interconnected.\nA computer that accesses a volume in the disk array specifies the volume to read or write associated with the corresponding logical number. In another example: a disk drive has a physical SCSI port that usually represents only a single target, which in turn usually provides only one unit logical unit whose logical unit number is zero. This logical unit represents the total memory of the disk drive. In early versions of LUN, the initiator delivers a Command Descriptor Block ( CDB ) to a target (physical unit), and in the CDB a three-bit field of logical unit numbers to identify the logical unit within the target. In current SCSI, the LUN delivers a CDB starter to a specific logical unit, so the logical unit number appears in the transport layer data structure, not the CDB . A logical unit number is the only way to identify a logical unit. There is also a SCSI Device ID that identifies a logical unit uniquely in the world. Labels or serial numbers stored in the storage volume of a logical unit often act to identify the logical unit. However, a logical unit number is the only way a starter can move its commands to a specific logical unit, so starters often create a mapping table from LUN to other identifiers through a discovery process. . LUN identifies a logical unit only in the context of a specific initiator. Therefore, two computers that have access to the same disk volume may recognize it by different LUNs .\nLUN0 is a LUN that must be present in every target. A logical unit with LUN0 is a special one that must be implemented by several specific commands, the most important of which are Report LUNs. This way a beginner can find all the other LUNs in the target, but LUN0 does not need to provide other services such as storage volume. Many SCSI targets have only one logical unit Others have few logical units that correspond to separate physical devices and have fixed logical unit numbers. A large storage system may have up to thousands of logical units, which are logically defined by the administrator command, and the administrator may select a logical unit number or the system may select it. From a computer point of view, SCSI LUN is only part of the complete SCSI address. A complete address of the device consists of the following parts: c-part: controller ID adapter host bus t-part: Identify the target ID of the SCSI tag on that controller d-part: Disk ID that specifies a Logic Unit Number in that target. s-part: Identifies the slice ID that identifies a particular partition on that disk. In the Unix family of operating systems, these identifiers are often combined with a single name. Storage infrastructure and logical unit type play a role in performance and reliability.\nSome types of LUNs are as follows: Mirrored LUN: LUN error tolerance with identical copies on two physical drives for data redundancy and backup of data Concatenated LUN: Merges several Logic Unit Numbers into one logical unit or volume. Striped LUN: Writes data to multiple physical drives and increases performance by distributing I / O requests across drives. Striped LUN evenly: Splits data evenly across three or more physical drives. If the physical drive fails, the data can be recovered from the information on the remaining drives. Equality calculation may affect writing performance. SANs control host access to logical unit numbers to enforce data security and integrity. Switch-based LUN masking and zoning manage the SAN resources available to connected hosts. LUN zoning provides isolated I / O paths through a FC SAN fabric between end ports to ensure decisive behavior. Host is the area in which it is located. LUN zoning is usually set at the switch layer, which can help improve security and eliminate network hotspots. LUN coverage restricts hosts access to SCSI targets and their logical unit numbers. LUN coverage is typically done on a storage controller but can also be applied to a host bus adapter or HBA or switch layer. With LUN coverage, multiple hosts and regions can use the same port on a single storage device. However, they can only see SCSI-specific targets and the logical unit numbers assigned to them.\nHigh performance computing (HPC) rivals supercomputing power, but it is more relevant and docile. It is suitable for small and medium-sized businesses. HPC scenarios include extensive space research, medical research, or any scenario that requires ultra-fast, large-scale computing without too much fuss. High-performance computers consist of link processing power comparable to high-performance server system clusters. Each system in this linked cluster is referred to as a node. Just like teams, communication between these nodes is key to better optimizing the system. You can be assigned multiple nodes depending on how your organization wants to handle the data. Alibaba Cloud incorporates resiliency in this way. You can scale up and down depending on the type of computing scenario you are working with. HPC consists of component services such as compute power, network assistance, and storage. A software program is an overlay that is used to communicate with each other among the resources that form a cluster. Multiple algorithms can maximize the resource utilization of the optimal system. Each component that aligns with each other must execute at the optimal rate in order to synchronize with each other. This maximizes performance and provides a reliable and optimized system. The main motivation is to solve complex problems without dedicated systems. Building nodes to solve larger problems is the simplest explanation for HPC. It could be a research lab that scientists use to dig deeper into energy-based research, environmental analysis or space research. Alibaba Cloud E-HPC can be used in financial modeling, artificial intelligence, and machine learning scenarios.\nIt can also be used in the media or entertainment industries, where editing or streaming large amounts of parallel data requires continuous and complex computational power. All of these situations are the tip of the iceberg. HPC can be used in any scenario where you need more computing power than usual or where you need to analyze or evaluate large amounts of data. Alibaba Cloud offers it as High Performance Computing as a Service (HP CaaS). Alibaba Cloud's elastic high-performance computing solution aims to provide a performance-packaged solution that can be deployed with one click. Alibaba Cloud E-HPC leverages Alibaba Cloud infrastructure and leverages all industry-leading solutions in one package to meet complex computing requirements. Let's take a look at the delivery mode of Alibaba Cloud HPCaaS: Alibaba Cloud E-HPC provides support for Infrastructure as a Service (IaaS) and Platform as a Service (PaaS), and Software as a Service (SaaS). The chart below shows the differences between the three services: Highly Elastic and scalable Clusters or nodes can be added without adding physical machines Physical clusters have to be configured, and a team of engineers have to be there to perform management and maintenance operations Elasticity is dependent on the resources of the self-built HPC. The performance is limited to the number of deployed nodes and clusters. Software and hardware counterparts are managed based on which cloud service provider and type of cloud service you are using.\nThe ability to select anECSor EGC instance as needed without additional configuration requirements Requires configuration and physical resource orchestration for compute or graphical compute needs. Different Instances can be orchestrated depending on the level of hardware and software algorithms in place Depends on the Cloud Service provider and your SLA If it is aHybrid Cloud setup, it depends on the private cloud setup Alibaba Cloud security and authorizationproducts back the E-HPC solution. There is no need for third-party security product deployment. Fully-managed products give you a hassle-free system. Security systems, physical data center security, and online security-related products and services have to be deployed and controlled based on analysis. A constant need for maintaining a security system falls on the user. Security suites and products have to be manually arranged or can be automatically managed by a service provider UnlikeAlibaba Cloud E-HPC, this incurs additional costs Security products can be leveraged using the cloud services provider or by installing these products manually Auto-scaleup and auto-scale down are supported without the need for user interaction. Multiple products from the Alibaba Cloud lineup are automatically utilized to gather metrics and adjust resources. Scaling is dependent on the operating management software and system administrator. Scaling is dependent on the software counterpart and what infrastructure has been deployed for the solution. Scaling is managed by the Cloud Services Provider. If you are using Alibaba Cloud services, such asECS and EGS instances, alongside products likeServer Load Balancer,Cloud Monitorcan be used to gather and adjust the scaling.\nData is the cornerstone of any modern software application, and databases are the most common way to store and manage the data used by applications. With the explosive growth of web and cloud technologies, databases have evolved from traditional relational databases to more advanced database types such as NoSQL, columnar, key-value, hierarchical, and distributed databases. Each type has the ability to handle structured, semi-structured and even unstructured data. On top of that, databases are constantly dealing with mission-critical and sensitive data. When this is combined with compliance requirements and the distributed nature of most datasets, managing databases becomes very complex. Therefore, organizations need powerful, secure, and user-friendly tools to maintain these databases. This is where database management systems come into play - by providing a platform to manage databases. let's see. A database management system (DBMS) is a software tool that allows users to easily manage databases. It allows users to access and interact with the underlying data in the database. These operations can range from simply querying data to defining database schemas that fundamentally affect the structure of the database. Additionally, a DBMS allows users to interact with the database securely and concurrently without disturbing each user , while maintaining data integrity. Typical database administration tasks that can be performed with a DBMS include: Configure authentication and authorization . Easily configure user accounts, define access policies, modify restrictions and access scopes. These actions allow administrators to restrict access to underlying data, control user actions, and manage users in the database.\nProvides data backup and snapshots . A DBMS can simplify the database backup process by providing a simpler and more direct interface to manage backups and snapshots. They can even move these backups to a third-party location, such as cloud storage, for safekeeping. Performance tuning . A DBMS can use integrated tools to monitor the performance of the database and enable users to tune the database by creating optimized indexes. It reduces I/O usage to optimize SQL queries for optimal database performance. Data Recovery. During recovery operations, the DBMS provides the recovery platform with the necessary tools to easily restore the database fully or partially to its previous state. All of these administrative tasks can be accomplished using a single administrative interface. Most modern DBMSs support processing multiple database workloads from centralized DBMS software, even in distributed database scenarios. Additionally, they allow organizations to have a manageable top-down view of all data, users, groups, locations, etc. in an organized manner. The following figure illustrates a schematic diagram of a DBMS system: All DBMSs come with a variety of integrated components and tools necessary to perform almost all database administration tasks. Some DBMS software even offers the ability to extend core functionality by integrating with third-party tools and services directly or through plug-ins.\nIn this section, we describe common components that are common across all DBMS software, including: Storage engine Query language Query processor Optimization engine Metadata catalog Log manager Reporting and monitoring tools Data utilities Storage engine The storage engine is the core component of a DBMS, which interacts with the file system at the OS level to store data. All SQL queries that interact with the underlying data go through the storage engine. Query language Interacting with a database requires a database access language, from creating a database to simply inserting or retrieving data. A suitable DBMS must support one or more query languages and language dialects. Structured Query Language (SQL) and MongoDB Query Language (MQL) are two query languages used to interact with databases. Among many query languages, query language features can be further categorized according to specific tasks: Data Definition Language (DDL). This consists of commands that can be used to define the database schema or modify the structure of database objects. Data Manipulation Language (DML). Commands that work directly with data in the database. All CRUD operations belong to DML. Data Control Language (DCL). This involves permissions and other access controls for the database. Transaction Control Language (TCL). Commands to handle internal database transactions. Query processor This is the intermediary between the user query and the database. The query processor interprets the user's queries and turns them into actionable commands that the database can understand to perform the appropriate function.\nOptimization engine The optimization engine allows the DBMS to provide insight into database performance in terms of optimizing the database itself and queries. When combined with database monitoring tools, it can provide a powerful toolset to get the best performance from your database. Metadata catalog This is a centralized directory of all objects in the database. When an object is created, the DBMS uses the metadata catalog to keep a record of the object and some metadata about it. This record can then be used to: Authenticate user requests for appropriate database objects Provides an overview of the complete database structure Log manager This component will keep all logs of the DBMS. These logs will include user logins and activities, database functions, backup and restore functions, and more. The log manager ensures that all these logs are properly logged and easily accessible. Reporting and Monitoring Tools Reporting and monitoring tools are another standard component that comes with a DBMS. Reporting tools will enable users to generate reports, while monitoring tools will be able to monitor database resource consumption, user activity, and more. Data utility In addition to all of the above, most DBMS software comes with additional built-in utilities to provide the following capabilities: Data Integrity Check Backup restore Simple database repair Data verification There are many different types of DBMSs, but we can divide the most commonly used DBMSs into three types. This is the most common type of DBMS.\nThey are used to interact with databases that contain structured data in tabular format with predefined relationships. Additionally, they use Structured Query Language (SQL) to interact with the database. Microsoft SQL, MySQL, and Oracle databases are some of the popular DBMSs that fall into this category. These DoDBMSs are used to manage databases containing data stored in JSON-like structures with limited or no relational structure. They are powered by query languages such as MongoDB Query Language (MQL) for database operations. MongoDB, Azure Cosmos DB are some prominent examples of DoDBMS. As the name suggests, this type of DBMS is used to manage columnar databases that store data in columns rather than rows, with an emphasis on high performance. Some databases that use columnar formats are Apache Cassandra, Apache HBase, etc. DBMSs were introduced to solve fundamental problems associated with storing, managing, accessing, securing, and auditing data in traditional file systems. Software users and organizations can gain the following benefits by using a DBMS: A DBMS provides the ability to control users and enforce security and compliance management policies. This controlled user access improves database security and makes data less vulnerable to security breaches. A DBMS enables users to securely access databases no matter where they are. As a result, they can quickly handle any database-related task without complicated access methods or concerns about database security. Most importantly, a DBMS allows multiple users to collaborate effectively when interacting with the database.\nHello all, This post is talking about AWS HPC Technology. Access a broad range of cloud-based services, like machine learning (ML) and analytics, plus HPC tools and infrastructure to quickly design and test new products. Gain on-demand access to compute capacity. Skip the wait and save more time to focus on solving complex problems without worrying about cost and infrastructure constraints. Find the perfect fit for your infrastructure needs and solve real-world business problems with your choice of the largest selection and capacity of HPC services, fast networking, and storage on AWS. Support virtually any workload with the secure, resizable compute capacity of Amazon Elastic Cloud Compute (EC2) and the latest-generation processors. Run HPC applications at scale with Elastic Fabric Adapter (EFA), a network for Amazon EC2 instances with high-level inter-node communications capabilities. Quickly build HPC compute environments with AWS ParallelCluster, an open-source tool that simplifies the deployment and management of HPC clusters. Quickly process massive datasets on-demand and at scale with Amazon FSx for Lustre, a high-performance file system with sub-millisecond latencies. Scale hundreds of thousands of computing jobs across all AWS compute services and features with AWS Batch, a cloud-native batch scheduler. Deliver a high-performance remote desktop and 3D application graphics with NICE DCV, a bandwidth-efficient and high-performance streaming protocol. Scale HPC applications to thousands of CPUs and GPUs with Elastic Fabric Adapter (EFA). Run distributed ML applications faster with a purpose-built, low-latency, and low-jitter channels for inter-instance communications.\nWhen it comes to creating the VMs there could be many things that are tiresome or could be repetitive workload, which can make the work lengthy but dont worry there is still hope you can get your job done more easily and quickly there are some important concepts which can make this work hassle-free. These Terms are Cloning, Templates, and Snapshot, They make it easy for you to create a sample template to create a similar type of VMs Also they can help you exactly copy one VM to make a clone out of it. At the same time, you can save the status and settings of your VM by taking its snapshot. All of them have their significance but you have to about their exact use and limitation so here in the following you are going to see them in detail: Before making large modifications to programs or operating systems in virtual machines, IT managers prepare snapshots. If a program, configuration modification, or OS patching goes wrong, the virtual machine can be reverted. Commonly, snapshots are mistaken for backups. Snapshots allow you to revert a machine to a previous point, but they are not a backup solution. Snapshots are point-in-time backups. Underlying files aren't copies or clones. If you delete a virtual machine, even with snapshots enabled, you can't retrieve it. Therefore, you must adopt a virtual machine backup solution. You can clone a virtual machine as long as there are no competing tasks on the source virtual machine.\nHello friend! hope it can help: Solution: 1. Insert the power plug again. a. If the IPC is powered on, no further operation is required. b. f the fault persists, go to 2. 2. Check the status of the power indicator. If the power indicator is off, the power supply is faulty. Check the power supply adapter or power cable. For the description of power indicator status, see the user manual delivered with the IPC. NOTE: For a PTZ camera that has no power indicator, check whether the PTZ of the camera automatically rotates after you power on the camera. 3. Check whether the fault is rectified. a. If yes, no further operation is required. b. If no, contact your service provider for assistance. Power supply from PoE 1. Check whether the power supply device is running properly. - If yes, go to 2. - If no, replace the power supply device, and go to 3. 2. Replace the network cable. 3. Check whether the fault is rectified. - If yes, no further operation is required. - If no, contact your service provider for assistance. Thanks! Hello friend! hope it can help: Solution: 1. Insert the power plug again. a. If the IPC is powered on, no further operation is required. b. f the fault persists, go to 2. 2. Check the status of the power indicator. If the power indicator is off, the power supply is faulty. Check the power supply adapter or power cable.\nDear all, This post will guide you toupgrade a non-commercial version to a commercial one for storage products.This is the only way for upgrading a non-commercial version to a commercial one. To determine whether the running version is non-commercial, contact Huawei technical support. This approach is highly risky. It is used to format system startup partitions, reinstall the operating system, and clear the existing service configuration data. Do not perform it without the guidance of technical support engineers. Log in to the CLI on any controller. Run the change user_mode current_mode user_mode=developer command to switch to the developer mode, as shown in Figure 1. Figure 1 Entering the developer mode In developer mode, run the export configuration_data ip=x.xx.xx.xx user=xxx password=xxx db_file=XX.dat protocol=SFTP command, as shown in Figure 2. Figure 2 Exporting configuration data In this command, ip is the IP address of the host; user and password are the user name and password of the host; the value of db_file is user-defined, but its file name extension must be .dat ; protocol can be SFTP or FTP. In developer mode, run the clear configuration_data action=reboot command. Wait until the system clears the configuration data and restarts, as shown in Figure 3. Figure 3 Clearing all configuration data and restarting After the configuration data is cleared, the password is changed to the initial one. After the system starts up, log in to each controller and run the minisystem command in developer mode on the CLI to enter the minisystem, as shown in Figure 4.\nIf there are not enough IP addresses, you can run the sshtoremoteExt command in the minisystem to redirect to the desired controller. For example, to redirect to controller 0, run sshtoremoteExt 0 ( 0 is the controller ID) and then enter the password (same as that of user admin ) as prompted, as shown in Figure 5. To check the current controller on which you are performing operations, run the showsysstatus command, as shown in Figure 6. Figure 4 Entering the minisystem Figure 5 Redirecting to controller 0 Figure 6 Checking the current controller Run the free -m command in the minisystem of each controller to check its remaining memory, as shown in Figure 7. Ensure that the remaining memory is 100 MB greater than the size of the upgrade package to be uploaded. Figure 7 Checking the remaining memory In the minisystem of each controller, run the sys.sh cleardirtydataflag command to clear dirty data from the memory, as shown in Figure 8. Figure 8 Clearing dirty data Use the FTP/SFTP tool to upload the upgrade package to the /home/permitdir directory of any controller, and run the setupsystem -a command in the minisystem of this controller. The system prompts you to enter the upgrade package name. Enter the package name and press Enter . The system automatically checks whether the upgrade package matches the device type. If they match, enter y as prompted to format and reinstall the operating system, as shown in Figure 9.\nA Yagi antenna is a directional antenna consisting of a driven element such as dipole or folded dipole and additional parasitic elements, typically a reflector and one or more directors. It radiates in only one direction and is most commonly used in point-to-point communications. A Yagi antenna is used for communications in a medium range of three to five miles between two points. It can also be used as a bridge antenna to connect clients to an access point. This term is also known as a Yagi-Uda array or patch antenna. The Yagi antenna was invented by Shintaro Uda and his colleague Hidetsugu Yagi in 1926. A similar design to the Yagi antenna is found all over the United States and is referred to as a log-periodic antenna. A Yagi antenna has two to three straight antenna elements, which are set to a length of roughly half the electrical wavelength they are designed to support. It is considered a balanced type but can also be unbalanced depending on whether it is used with a balun at the feed line joint, which joins the drive element of the antenna. The benefits of the Yagi antenna include good range and ease of aiming the antenna compared to other directional dishes and designs. Since the Yagi antenna is directional, it focuses its entire signal in a cardinal direction. This results in increased gain over an antenna dispersing energy in a 360-degree circle, such as the omni-directional model of other antenna designs.\nYMODEM is an asynchronous communication protocol for modems developed by Chuck Forsberg as a successor to Xmodem and Modem7. It supports batch file transfers and increases transfer block size, enabling the transmission of a whole list or batch of files at one time. It was initially implemented in the Control Program for Microcomputers (CP/M) \"Yet Another Modem\" program. YMODEM is sometimes called YMODEM batch. YMODEM is a modification of Xmodem 1k that allows multiple batch file transfers. It is a half-duplex protocol, as it does not send and receive control signals in both directions at same time. This helps reduce buffer overrun problems. YMODEM is similar to Xmodem in its operation except that it sends the file name, time stamp and size in regular Xmodem blocks (block 0) before transmitting the file. YMODEM 1K uses 1 KB block size, which was one option provided in the original YMODEM standard. YMODEMg is considered to be a variant of YMODEM, designed to be used along with modems supporting error control. The g option for YMODEM is driven by the receiver, which initiates batch transfer by transmitting a \"g\". When the sender recognizes the g, it bypasses the wait for an acknowledgment signal (ACK) to every transmitted block, sending succeeding blocks at maximum speed. The sender expects a g to initiate transmission of a file and ACK on the end-of-transmission signal at each file end. Unlike other similar protocols, YMODEM does not provide any recovery or software error correction, but expects the modem to provide equivalent services.\nQuick Response (QR) codes QR codes can be handy for saving keystrokes and eliminating typos. They make it easy to respond to marketing campaigns or gather more information about a product. However, QR codes can also be used for malicious activities. Malicious QR codes are difficult to spot since their contents are obscured until scanned. A recentMobileIron surveyfound that nearly 63% of respondents were unable to distinguish between a safe and malicious QR code. Distinctions can be made only after the code is unscrambled by a QR code reader. QR codes can do more than display a website address. They can also be used to initiate an action that costs money, such as making a phone call, or sending a payment. Below are some ways thatmalicious QR codescan be used to compromise devices, and steal information and money. QR codes can be used to install software. The download and installation can happen in the background, without the users knowledge or permission. Difficult to remove, these sneakymalware appsstay out of sight by hiding their app icons. Phishingis a common use for malicious QR codes. QR codes used for phishing take advantage of several factors to accomplish the scam. They take advantage of smaller mobile screen sizes to hide the full URL destination. The smaller screen allows scammers to show a legitimate-looking portion of a phishing website in the mobile browser. QR codes can be used tosend a text or email message.\nSome scams may include sending messages to a premium rate phone number that generate income for attackers. Malicious or spam text messages can also be sent to the phones contacts. These messages are sent without the victims knowledge or consent. QR codes can be used to enable location on your device which can reveal your location or steal information about your network and device. This information can be saved and sent to an attackers server at a later date. Other malware on the device can also take advantage of the location data. QR codes can be used to create calendar events in your phones calendar. Spam events like this can be an annoyance and may also contain malicious links or include social engineering content to urge victims to take an action. An attacker couldgain controlof all or parts of a phone's operating system, exploiting vulnerabilities through QR code reader software using a command injection or buffer overflow. Since malicious QR codes arent easily recognizable, its best to be cautious about using them. Below are a few ways to prevent accidental exposure of your information. Consider the source and look for evidence of tampering. Printed codes can easily be covered with stickers. Be wary and check for a sticker overlay before scanning. Choose to scan codes provided by trusted resources only. Consider the consequences of providing payment data to a malicious website. Phishing sites are designed to look like legitimate websites. Avoid sharing personally identifiable information, login credentials, or payment data.\nA database repository is a logical, but also sometimes physical grouping of data from related but separate databases. This is usually done when there is a 'higher purpose' for the data, but the data items needed to do this reside on different databases. In these cases a repository is necessary to bring together the discrete data items and operate on them as one. Database repositories are usually discussed and implemented in the realm of data warehousing and business intelligence. This usually requires a level of aggregation of data that the lowel-level databases simply cannot provide, thus necessitating the creation of a higher-level structure. Consider the case of a large bank. Such an institution will likely be composed of several different subsidiaries, not in a physical, geographically-diverse sense, but rather in a functional or lines-of-business sense. There will be the traditional bank account division, in addition to a loans division, a forex and treasury division, an investment banking division, and a custody/ safe deposit division. All these divisions run their own separate information systems, which of course implies separate databases. However, each division must report its own financials back to the head office. The Chief Financial Officer (CFO) needs to aggregate all the financial data from the various divisions to gauge their profitability, because these feed directly into the bank's overall financial position. You can see that the CFO's office is not really concerned with the operational part of the various databases, he is only really interested in the data that deals with financials.\nWindows Azure is a cloud computing platform developed by Microsoft that can be used to build and host online Web applications through Microsoft data centers. Management of the scalable Web applications is also performed at Microsofts data centers. Windows Azure was originally codenamed Red Dog and was initially called Windows Cloud when it first launched in October 2008. Windows Azure is designed to make IT management easier. The main purpose of developing Windows Azure was to minimize the overhead and personnel expenses associated with the creation, distribution and upgrade of the Web applications. The Windows Azure platform is considered a platform as a service, which is an imperative component of a cloud computing platform. It consists of various on-demand services hosted in Microsoft's data centers and is commodotized through three product brands. The services and applications developed using the Azure platform run on the Windows Azure operating system, which provides a runtime environment for Web applications along with an extensive set of services that facilitate the building, hosting and management of applications without requiring maintenance to expensive onsite resources. Windows Azure is designed to support both Microsoft and non-Microsoft platforms. The three main components that constitute Windows Azure are: Compute layer Storage layer Fabric layer Windows Azure also includes an automated service management feature that allows the upgrading of applications without affecting their performance. Windows Azure is designed to support a number of platforms and programming languages.\nIOPS (input/output operations per second) is the standard unit of measurement for the maximum number of reads and writes to storage locations. IOPS is frequently referenced by storage vendors to characterize performance in solid-state drives (SSD), hard disk drives (HDD) and storage area networks. Along with transfer rate, which measures how fast data can be transferred from contiguous storage locations, IOPS can be used to measure storage performance. While transfer rate is measured in bytes, IOPS is measured as an integer. As a measurement, IOPS can be compared to revolutions per minute (rpm) of a car engine. If a vehicle is in neutral, stating that the engine is capable of spinning at 10,000 rpms in that moment is meaningless. Without taking into account the data block size (or I/O size), read/write activity or I/O stream, IOPS as a stand-alone measurement says little. Throughput measures how many units of information a system can process in a period of time. It can refer to the number of I/O operations per second, but is typically measured in bytes per second. On their own, IOPS and throughput cannot provide an accurate performance measurement. Latency measures the time between issuing a request and receiving a response. With regards to IOPS, latency is a measure of the length of time it takes for a single I/O request to be completed from the application's point of view. While not providing a complete picture, combining latency, IOPS and throughput measurements can help gauge performance. IOPS is often measured with some testing tools.\nHello all, Do you knowBaud Rate? This post will give you an overview ofBaud Rate. The maximum rate of signal state changes per second on a communications circuit. If each signal state change corresponds to a code bit, then the baud rate and the bit rate are the same. It is also possible for signal state changes to correspond to more than one code bit, so the baud rate may be lower than the code bit rate. Baud rate is the measure of the number of changes to the signal (per second) that propagate through a transmission medium. The baud rate may be higher or lower than the bit rate, which is the number of bits per second that the user can push through the transmission system. Bits will be converted into baud for transmission at the sender side and the reverse conversion will happen at the receiver end so that the user receives the bitstream that was sent. A few simple definitions before we move ahead: Bit rate the number of binary bits, 1s or 0s to be transmitted per second Baud rate the number of line symbols transmitted per second Channels the number of transmission channels So to convert bit rate to baud rate you multiple baud rate by the number of bits per symbol by the number of channels being used: Bit rate = Baud rate * Bits per symbol * Channels Bit rate and Baud rate The following table shows the most used baud rates.\nThe DeviceManager is a device management program developed by Huawei Technologies Co., Ltd. The DeviceManager has been loaded to the storage system before delivery.\nThe login operations on other operating systems need to be adjusted accordingly.         You are navigated to the DeviceManager login page.  Your web browser may display that the website has a security certificate error. If the IP address is correct, you can neglect the prompt and continue to access the storage system. If you have a usable security certificate, you are advised to use corresponding commands to import the certificate to improve system security. For details about the corresponding commands, see .  If you select , the login succeeds. You are advised to select the option to always trust the publisher's content. Otherwise, this dialog box is displayed during the next login.  If you select , the login fails.     Optional:          Local user: You will log in to the storage system in local authentication mode. The super administrator can log in to the storage system using the authentication mode only.   LDAP user: You will log in to the storage system in LDAP domain authentication mode. You can log in to the storage system in LDAP domain authentication mode only after the LDAP server is properly configured.              If is selected, the user name and password must be a domain user name and password.\nDear team, I have a question about the port shutdown in the link flapping scenario. Thanks. Dear friend, The primitive sequence order description came from the FC protocol definition, so, its correct. All the vendors should strictly follow this state machine. So, when storage detected LF2 state, it will send NOS to switch, vice versa, switch also need to send NOS when the port enters LF2 state. After storage receives OLS, it must send LR to remote port; after storage receives LRR, it must send Idle to remote port. Storage sends LR to switch when Link timeout occurs, Link timeout when one or more R_RDY primitive signal are not received within 2 seconds after BB_Credit_CNT has reached BB_Credit. Storage sends NOS in the following 3 conditions: 1. Loss of Signal detected 2. Loss of Synchronization timeout 100ms 3. Link Reset(LR) timeout over 100ms Normally, the switch only detect NOS after receiving 3 continuous NOS primitive signal. So, CISCO would like to shut down the port after detecting NOS event by automation script, its okay. But, we cant detect NOS state log or NOS count. Because, the NOS primitive signal is handling by Interface module hardware logic, not driver. Theres no such log by default, only if we use some diagnose firmware. For the same reason, storage cant shut down port after receiving the NOS primitive signal. Because interface module hardware will follow the FC protocol to send and handle NOS. But, it will not pass the NOS primitive signal to upper layer driver.\nHello everyone This post describes how to power on and off Huawei SNS series switches. Before powering on the switch, complete the installation check to ensure that all devices are correctly installed. After powering on the FC switch by following the correct power-on sequence, observe indicators to check that the FC switch is powered on successfully. 1. Connect the power cords to both power supplies, and then to power sources on separate circuits to protect against power failure. Ensure that the cords have a minimum service loop of 6 inches available and are routed to avoid stress. 2. Provide power to the switch. a. SNS2124 The SNS2124 does not have an on/off switch. This means that power is supplied to the SNS2124 as soon as you connect it to a power source. b. SNS2224/SNS2248 Power on the power supplies by flipping both power switches to the I symbol. The power supply LEDs display amber until POST is complete, and then change to green. The switch usually requires 1 to 3 minutes to boot and complete POST. 3. After powering on is complete, verify that the switch power and status LEDs on the left of the port side of the switch are green. Perform the following steps to provide power to the SNS3096. 1. Connect the power cords first to both power supplies in the chassis and then to power sources on separate circuits to protect against AC failure.\nEnsure that the cords have a minimum service loop of 6 inches available and are routed to avoid stress. The power supplies power up as soon as they are plugged in. The power supply LEDs display green. The power LED on the front of the switch turn green as well. The system status LED on the front panel will be amber until POST completes and then it will turn green. If a second power supply is installed but NOT plugged into a power source, the AC status light on the power supply will be out and the DC status light will be amber. If the second power supply IS plugged into a power source, then both LEDs will be green. CAUTION: Power is supplied to the switch as soon as the first power supply is connected. 2. After power-on self-test (POST) is complete, verify that the switch power and status LEDs on the left of the port side of the switch are green. Power on the switch SNS5192/SNS5384 Complete the following steps to provide power to the chassis. DANGER: Use the supplied power cords. Ensure the facility power receptacle is the correct type, supplies the required voltage, and is properly grounded. 1. Connect the two AC power cords to the two power supplies. 2.\nConnect the power cords to a power source with voltage of 200 V AC to 240 V AC, 47 Hz to 63 Hz or optionally to a power source with voltage of 110 V AC to 120 V AC, 47 Hz to 63 Hz. If using any application blades in the chassis, the 200 V AC to 240 V AC option is necessary in order to achieve power supply redundancy. CAUTION: Use of the high-voltage line (200 V AC to 240 V AC) is highly recommended because of better power-conversion efficiency. For a fully-loaded SNS5192/SNS5384, 200 V AC to 240 V AC is required for high availability (ability to hot swap a failed power supply without affecting system operation). 3. Switch the AC power switches on the power supplies to I. The AC power switches light green when switched on and power is supplied. 4. The SNS5192/SNS5384 performs a POST each time it is powered on. POST takes approximately 10 minutes and is complete when the indicator light activity displays the operational state. You can bypass POST by using the fastBoot command. You can also disable POST for successive reboots on the SNS5192/SNS5384 using the diagDisablePost command. CAUTION: Do not connect the switch to the network until the IP addresses are configured. Power on the switchSNS2624 1. Connect the power cord to the power supply, and then to the power source. Ensure that the power cord has a minimum service loop of 6 inches available and routed to avoid stress.\nThe system power supply LED displays amber until power-on self-test (POST) is complete, and then change to green. The switch usually requires several minutes to boot and complete POST. 2. After POST is complete, verify that the switch power and switch status LEDs are green. For more information about how to interpret POST, BOOT, and diagnostics tests, refer to . Power on the switch SNS3664/SNS3696E 1. Connect the power cords to both power supplies, and then to power sources on separate circuits to protect against power failure. Ensure that the power cords have a minimum service loop of 6 inches available and are routed to avoid stress. 2. Power on the power supplies by flipping both switches to the on position (the \"I\" symbol). The power supply LEDs display amber until power-on self-test (POST) is complete, and then change to green. The switch usually requires several minutes to boot and complete POST. NOTE Power is supplied to the device as soon as the first power supply is connected and turned on. 3. After POST is complete, verify that the switch power and switch status LEDs are green. For more information about how to interpret POST, BOOT, and diagnostics tests, refer to Monitoring the Device. Power on the switch SNS5604/SNS5608 Perform the steps to provide power that are applicable to your power supply model. Observe the following for all power connections: Before connecting power, refer the following.\nElectrical caution and danger statements in Safety precautions and Facility requirements Power supply specifications section in the SNS5604/SNS5608 Directors Technical Specifications for power supply requirements of your device. Connect each power supply to a different power source or circuit to provide full redundancy. Route the power cords so they will be out of the way when connected to the power source. Ensure that the power cords have a minimum service loop of 15.2 cm (6 in.) and are routed to avoid stress. Remember that power is supplied to the device as soon as the first power supply is connected to a power source. Connecting power cord to AC power supplies Context Complete the following steps to connect the power cord from the facility AC power source to the device 's AC power supply. Before connecting to power, be sure to observe all \"Power Precautions\" in Safety precautions. In addition, refer to the power supply specifications and requirements in SNS5604/SNS5608 Directors Technical Specifications. Procedure 1. Remove the logo bezel protective cover if it is still installed over the top air vents on the port side of the chassis. Refer to for instructions. NOTICE Remove the logo bezel protective cover on the port side of chassis before applying power. This cover is attached over the air vents. If not removed, the chassis can overheat and will eventually shut down. 2. Install all power supplies provided for your device if not already installed. Refer to Installing a power supply for procedures. 3.\nWhen installing device in a rack, route power cables from power distribution units (PDUs) so they do not cover air vents in chassis. 4. Connect the provided AC power cords to a power source with voltage of 200240 VAC, 50/60 Hz or optionally to a power source with voltage of 100120 VAC, 50/60 Hz. DANGER High Touch Current. Earth connection essential before connecting supply. NOTE Use of the high-voltage line (200240 VAC) is highly recommended because of better power-conversion efficiency. With 120 VAC primary input, the power distribution unit (PDU) supplies roughly half the available wattage, which can limit blade and port configurations. For a \"fully-loaded\" chassis with maximum supported blades and optics, two power supplies connected to 200240 VAC are required for full N+N redundancy. For details on power supplies required for operation and high availability, refer to \"Power supply requirements\" and \"Power consumption\" tables in the SNS5604/SNS5608 Directors Technical Specifications. 5. Route the cords so they will be out of the way when connected to the power source. Ensure that the power cords have a minimum service loop of 15.2 cm (6 in.) available and are routed to avoid stress. 6. Plug the power cords into power supplies. The power supply LED will light green when power is applied. Note that after one power supply is plugged into AC power, LEDs on the remaining installed power supplies will flash green until they also have power applied. The director performs a power-on self-test (POST) each time it is powered on.\nPOST takes approximately 10 minutes, during which time status LEDs on installed blades and other FRUs may display amber. Power LEDs on all FRUs display green when power-on self-test (POST) is complete and all FRUs are functional. You can bypass POST by using the fastBoot command. You can also disable POST for successive reboots using the diagDisablePost command. NOTE Do not connect the device to the network until the IP addresses are configured. 7. After POST is complete, verify that the power LEDs on blades and other FRUs are green. For information about LED patterns, refer to Monitoring the Device. 8. Ground the chassis by attaching a ground wire from building ground to an appropriate crimp connector and attaching the connector to the 2AWG Panduit LCD2-14AF lug located to the left of the bottom fan assembly near the bottom of the chassis. Connecting power cord to HVAC/HVDC power supplies Context Use steps in this section to apply power to the dual-function high-voltage AC and DC (HVAC/HVDC) power supply. This power supply converts high-voltage DC or AC input to appropriate DC power for the device. Make sure that you observe the electrical caution and danger statements in Safety precautions when connecting this power supply. NOTE The equipment installation must meet NEC/CEC code requirements. Consult local authorities for regulations. Power is supplied to the device as soon as the first power supply is connected to a power source. The maximum input voltage for connection to the HVAC/HVDC power supply should not exceed 305 VAC and 400 VDC.\nThe maximum input voltage for connection to the HVAC power supply must not exceed 305VAC. Procedure 1. If connecting to AC power, attach an AC power plug to the unterminated wires on the HVAC/HVDC power cord that meets your facility and local code requirements. If connecting to DC power, verify how you will attach these unterminated wires to your site's DC power terminal blocks. For more information on the HVAC/HVDC power cord available for these power supplies, refer to Using HVAC/HVDC power cords. 2. Ground the chassis by attaching a ground wire from building ground to an appropriate crimp connector and attaching the connector to the 2AWG Panduit LCD2-14AF lug located to the left of the bottom fan assembly near the bottom of the chassis. 3. Remove the logo bezel protective cover if it is still installed over the top air vents on the port side of the chassis. Refer to Removing logo bezel protective cover for instructions. NOTICE Remove the logo bezel protective cover on the port side of chassis before applying power. This cover is attached over the air vents. If not removed, the chassis can overheat and will eventually shut down. 4. Install all power supplies provided for your device if not already installed. Refer to Installing a power supply for procedures. 5. When installing the device in a rack, route power cables from power distribution units (PDUs) so they do not cover air vents in chassis. 6.\nBefore connecting the power cord to a power supply, first remove the cable restraint cover, if it is installed under the power cord connector. Remove the cover by unscrewing the two torx head screws. Save the retainer cover and screws for reinstallation after plugging in the power cord. NOTICE When removing the metal cable restraint cover under the high voltage power supply inlet, remove the 2 Torx head screws only. a. Torx head screws b. Metal cable restraint cover 7. Connect power cords to installed power supplies. The connector on the power cord is keyed so that it only fits one way into the power supply connector. Note that the connector's latch should be positioned under the connector and will latch when the power cord connector is fully inserted into the power supply. a. Power cable b. Connector latch 8. Attach the cable restraint cover under the power cord connector using its two torx head screws (refer to step 4). NOTE This retainer cover protects the power cord from being accidentally unlatched and disconnected from the power supply. 9. If connecting to an AC power source, use the following steps. (If connecting to a DC power source, go on to step 8.) a. Make sure that you observe the electrical caution and danger statements in Safety precautions when connecting this power supply. b. Make sure that AC power plug is attached to the power-source end of the HVAC/HVDC power cord that meets your facility and local code requirements.\nFor more information on the HVAC/HVDC power cord available for these power supplies, refer to Using HVAC/HVDC power cords. DANGER Make sure that the power source circuits are properly grounded, then use the power cord supplied with the device to connect it to the power source. c. If connecting to an AC power source, connect to a power source with voltage of 200277 VAC, 50/60 Hz (recommended). NOTE Use of a high-voltage line (200277 VAC) is highly recommended because of better power-conversion efficiency. For a \"fully-loaded\" chassis with maximum supported blades and optics, two power supplies connected to 200277 VAC are required for full N+N redundancy. For details on power supplies required for operation and high availability, refer to \"Power supply requirements\" and \"Power consumption\" tables in the SNS5604/SNS5608 Directors Technical Specifications. DANGER High Touch Current. Earth connection essential before connecting supply. NOTICE Use a separate branch circuit for each power cord, which provides redundancy in case one of the circuits fails. 10. If connecting to an DC power source, use the following steps. (If connecting to a AC power source, go to step 7.) a. Make sure that you observe applicable electrical caution and danger statements in Safety precautions when connecting this power supply. b. Terminate the negative (-) and positive (+) unterminated wires on the HVAC/HVDC power cord to the DC power source. Connect the ground wire to building ground. For more information on the HVAC/HVDC power cord available for these power supplies, refer to Using HVAC/HVDC power cords.\nNOTE Make sure that there is an adequate circuit breaker in the DC input circuit to the system based on input wiring to the product and input voltage. 11. Route the cords so they will be out of the way when connected to the power source. Ensure that the power cords have a minimum service loop of 15.2 cm (6 in.) available and are routed to avoid stress. 12. After power is applied, the power supply LED will light green. The director performs a power-on self-test (POST) each time it is powered on. POST takes approximately 10 minutes, during which time status LEDs on installed blades and other FRUs may display amber. Power LEDs on all FRUs display green when power-on self-test (POST) is complete and all FRUs are functional. You can bypass POST by using the fastBoot command. You can also disable POST for successive reboots on the device using the diagDisablePost command. NOTE Do not connect the device to the network until the IP addresses are configured. 13. After POST is complete, verify that the power LEDs on blades and other FRUs are green. For information about LED patterns, refer to Monitoring the Device. 14. Ground the chassis by attaching a ground wire from facilites ground to an appropriate crimp connector and attaching the connector to the 2AWG Panduit LCD2-14AF lug located to the left of the bottom fan assembly near the bottom of the chassis.\nHi everybody, I use dorado 5000 v6 (6.1.x). I move big data from old storage to Huawei Storage (not with smart migration). Every day and on specific time Huawei Storage have high I/O response time (about 30 minutes). I think SmartDedupe is reason. How can i set SmartDedupe settings (retation period or limitation etc.). Thanks, best regards. Dear friend, For OceanStor Dorado 5000, OceanStor Dorado 6000, OceanStor Dorado 8000, and OceanStor Dorado 18000,Table 1lists the application request size and the SmartDedupe and SmartCompression status for each application type. Table 1 Application Type Application Request Size SmartDedupe SmartCompression Default 8 KB Enabled Enabled Oracle_OLAP 32 KB Disabled Enabled Oracle_OLTP 8 KB Enabled Enabled Oracle_OLAP&OLTP 8 KB Enabled Enabled SQL_Server_OLAP 32 KB Disabled Enabled SQL_Server_OLTP 8 KB Enabled Enabled SQL_Server_OLAP&OLTP 8 KB Enabled Enabled SAP_HANA 8 KB Enabled Enabled Vmware_VDI 8 KB Enabled Enabled Hyper-V_VDI 8 KB Enabled Enabled FusionAccess_VDI 8 KB Enabled Enabled Others 8 KB Enabled Enabled Note: Others applies only to storage systems of 6.1.3 and later versions. If the preset application types do not meet service requirements, run the create workload_type general name=? io_size=? dedup_enabled=? compression_enabled=? template_type=LUN id=? command in developer mode to create an application type. For example, if you have both Oracle_OLAP and Oracle_OLTP services and require high bandwidth, you can create an application type and set the application request size to 16 KB. Please refer to . Thanks.\nHello all, This post is talking aboutHPC 21.0 Software Architecture. OceanStor Pacific series is an intelligent distributed storage product that supports mass scale-out and provides block, file, object, and HDFS services. Users can deploy storage services as required. As shown in the preceding figure, Operation Administrator and Maintenance (OAM) provides management and control services of OceanStor Pacific series including resource management, service management, system management, user management, installation and deployment, upgrade, capacity expansion and reduction, as well as inspection and information collection. The access layer receives and processes the access requests of various services. For unstructured services, the access layer consists of the following functional modules: The NAS srv module provides standard NAS protocol (NFS or SMB) for access processing. The OBJ srv module provides standard S3 protocol for object access processing. The HDFS srv module provides standard HDFS protocol for access processing. The Distributed Parallel Client (DPC) module is designed for high-performance computing (HPC) scenarios and is compatible with standard POSIX/MPI-IO semantic access. For structured services, Virtual Block Service (VBS) provides standard iSCSI access and provides local SCSI semantic access for high-performance scenarios. The service layer provides structured and unstructured services. All unstructured services are processed on the Distributed Related Object Service (DROS) platform in a unified manner and they share the same metadata, data, and value-added features. Structured services are processed by an independent block module. The index layer provides fine-grained space layout management capabilities above the persistence layer.\nData virtualization is the process of aggregating data from different sources of information to develop a single, logical and virtual view of information so that it can be accessed by front-end solutions such as applications, dashboards and portals without having to know the data's exact storage location. Many organizations run multiple types of database management systems, such as Oracle and SQL servers, which do not work well with one another. Therefore, enterprises face new challenges in data integration and storage of huge amounts of data. With data virtualization, business users are able to get real-time and reliable information quickly, which helps them to take major business decisions. The process of data virtualization involves abstracting, transforming, federating and delivering data from disparate sources. The main goal of data virtualization technology is to provide a single point of access to the data by aggregating it from a wide range of data sources. This allows users to access the applications without having to know their exact location. The most recent implementation of the data virtualization concept is in cloud computing technology.\nSaaS sprawl (Software-as-a-Service sprawl) is a phenomenon that occurs when the number of third-partycloud applicationsbeing used on a network reaches a point where administrators can no longer manage them effectively. SaaS sprawl typically occurs when project teams and individual users download and install cloud apps to meet an immediate work need without waiting for their IT department to review and approve the cloud vendors software product. The ease of procuring, deploying, and using software-as-a-service (SaaS) applications has made this type ofshadow ITeasier to deploy than ever before. COVID-19 work-from-home mandates inspired many business departments to allow the unsupervised use of cloud apps without stopping to consider how the apps would fit into the organization's workflows. Although the large majority of cloud apps are used in isolation, its still important for IT departments to manage the data that moves in and out of them. At the beginning of the COVID-19 pandemic, many companies used spreadsheets to manage their SaaS cloudstacks, but this quickly proved to be both time consuming and inefficient. In response to changing business needs,software asset management(SAM) vendors quickly made improvements to theircloud integration platformsand rolled out additional management tools to help IT administrators combat SaaS sprawl. Depending upon the targeted market segment, such tools may be referred to as or . The advantages of an effective SaasOps platform include the ability to: Gain visibility into cloud app use across the organization. Help organizations drive business value from data stored in third-party apps. Minimize budget wasted on SaaS apps that have overlapping functions.\nDigital transformation and the growing need for digital resiliency have made it clear to enterprises that their Data Center Networks (DCNs) must be extensively automated and modernized in order to support strategic business outcomes. Currently, DCNs involve separate technologies and networks that satisfy different requirements. Enterprises have an Ethernet network to support general-purpose computing, a Fibre Channel (FC) network to support storage networking, and an InfiniBand network to support High-Performance Computing (HPC). All three networks are needed because their requirements and use cases are different. General-purpose computing: Ethernet is widely used for general-purpose computing. Servers host virtualization technologies such as Virtual Machines (VMs) or containers to provide services for employees and external users through general-purpose computing networks, sometimes called application networks, service networks, and front-end networks. Storage networking: FC has been adopted for storage networking. Storage nodes are interconnected through storage networks, including Storage Area Networks (SANs). SANs support high-performance Solid-State Drives (SSDs) by delivering increased Input/Output Operations Per Second (IOPS) and low-latency connections between server hosts and storage arrays. HPC: InfiniBand is often used for HPC networks. These environments run very computationally intensive, latency-sensitive workloads, including big data analytics and advanced simulations. In these environments, where low latency is prized, applications tend to run on Bare Metal (BM) and servers are rarely virtualized. Computing units such as Central Processing Units (CPUs) and Graphics Processing Units (GPUs) are configured for HPC or Artificial Intelligence (AI) training, and server nodes are interconnected through HPC networks. The problem with having three different networks is higher costs.\nSince the networks involve three different sets of procurement, deployment, and operations, separate Capital Expenditure (CAPEX) outlays are required. In addition, there are high requirements on operations specialization for personnel as they need to be knowledgeable about different protocols and have technology-specific expertise associated with each technology. Tools are also separate in this model, resulting in redundant costs. Enterprises increasingly demand networking between data centers and clouds to cope with the requirements of hybrid Information Technology (IT), multicloud, and digital resiliency. Although every network technology has played a crucial role, only Ethernet is well placed to become a foundational technology for hyper-converged networking across all domains of the DCN, namely, general-purpose computing, storage networking, and networking for HPC. Ethernet switching is commonplace in the market. As a result, the technology benefits from economies of scale, and the innovations that occur within its thriving open ecosystem. Based on the Ethernet ecosystem, Ethernet's continuing evolution makes it a great candidate for hyper-converged networks. It not only makes such networks possible, but also makes them viable for many use cases. Ethernet also has a significant bandwidth advantage over the other technologies. It has already evolved to 400GE, and will evolve to 800GE soon. Ethernet extends its advantage over the other technologies in terms of bandwidth and throughput, too. Impressive advances in achieving low latency and losslessness have also been made.\nMeanwhile, Ethernet alternatives such as Remote Direct Memory Access over Converged Ethernet (RoCE) and various Non-Volatile Memory express (NVMe) over Fabrics (NoF) are now entering the market, and will become more common in the years to come. As such, Ethernet is well placed to address SAN and HPC use cases. According to market research firm IDC's worldwide survey of enterprises across several major industry verticals, the respondents working in the finance, banking, media, entertainment, and gaming sectors, as well as those with three or more data centers, have the highest interest in a converged Internet Protocol (IP) data and storage network that is lossless, low latency, and high performance. Figure 1: Interest in a Converged IP Data and Storage Network That is Lossless, Low Latency, and High Performance (by Industry) Figure 2: Interest in a Converged IP Data and Storage Network That is Lossless, Low Latency, and High Performance (by Number of Data Centers) N = 205 Source: Autonomous Driving Datacenter Networks Survey for Huawei, IDC, August, 2020 According to survey results, more than 55% of finance and banking respondents claimed that they were likely or extremely likely to deploy a converged IP data and storage network, compared to 46% of media, entertainment, and gaming respondents. Approximately 50% of those with a single data center said that they were likely or extremely likely to deploy such a network within 12 to 24 months, compared to 46% of those with two data centers, and 43% of those with three or more data centers.\nFigure 3: Likelihood of Deploying a Converged IP Data and Storage Network That is Lossless, Low Latency, and High Performance in 12 to 24 Months (by Industry) Figure 4: Likelihood of Deploying a Converged IP Data and Storage Network That is Lossless, Low Latency, and High Performance in 12 to 24 Months (by Number of Data Centers) N = 205 Source: Autonomous Driving Datacenter Networks Survey for Huawei, IDC, August, 2020 Source: IDC White Paper, sponsored by Huawei, The Hyper-converged Datacenter Network: How Ethernet Addresses the Need for Low Latency, Losslessness, and Full Life-Cycle Network Automation, doc #US48217221, September 2021 Hyper-converged DCNs offer a wide range of benefits. Network consolidation with a single Ethernet network dramatically reduces the costs associated with redundant network infrastructure, including procurement, design, build, and operations costs. Eliminating redundant networks also simplifies and improves the efficiency of network operations and management. Automated network operations across the full lifecycle of a hyper-converged Ethernet DCN extend from Day 0 planning and design to maintenance and closed-loop change management and optimizations. Root-cause analysis can be achieved quickly, leading to faster troubleshooting and remediation, which in turn yields highly available applications and services. To cope with the challenges of providing an all-Ethernet hyper-converged DCN, Huawei has introduced its CloudFabric 3.0 Hyper-Converged DCN Solution, which consists of CloudEngine switches and iMaster-NCE, an intelligent network management and control system. The solution offers a wide range of leading features. Fully lossless Ethernet: Ethernet is natively prone to packet loss, which has gone unresolved for 40 years.\nData is the core of intelligent management. Intelligent management, scheduling, mining, and analysis of data help applications deliver personalized experience and companies implement precision control over the design, production and logistics process to reduce management costs. Data is defining new business models and creating new commercial opportunities. The digital and intelligent transformation demands high-performance infrastructures, of which all-flash storage is an essential part. Born with high performance, all-flash storage is the new engine for mission-critical services. By vertically integrating its capabilities in chips, networks, and management, Huawei released the OceanStor Dorado all-flash storage series, which adopts an intelligent SSD controller chip, intelligent multi-protocol processing chip, and intelligent BMC management chip to build an end-to-end service acceleration platform, delivering triple performance and maximizing resource utilization. Intelligent SSD controller chip, accelerating data read and write Since data is stored on SSDs, the performance and stability of SSDs are critical. An SSD comprises of a control unit (SSD controller + DRAM) and a NAND flash storage unit. The control unit is responsible for data read/write. The Flash Translation Layer (FTL) is used to store the mapping between the user LBA and the physical page in the SSD. When the storage controller reads data from the SSD, it provides an LBA address. The control software in the SSD flash finds the physical address corresponding to the LBA address, reads the data from the flash, and returns the data to the host. In the case of writing, the control software writes data and then updates the FTL mapping table.\nTherefore, the FTL is the core of the entire SSD, and it determines the response time of data read/write. To achieve the ultimate in storage speed, Huawei leverages an innovative SSD controller chip to accelerate data read and write. The FTL algorithm is moved from the control software layer to the SSD controller chip, so that all FTL reads and writes are performed by the chip, significantly reducing the number of software interactions and hence the I/O response latency. An example can help you better understand the reason. A computer using Windows 95 needs to load a long string of codes after being powered on, which takes two to three minutes. After switching to Windows 2010, the computer has a stronger CPU to load the codes. We can enter the startup interface instantaneously. According to tests by Huaweis performance and interoperability lab, the read latency of Huawei SSDs is as low as 80 s in low-load scenarios, which is only 60% of the same type of SSDs in the industry; the performance is two times higher than that of competitors. However, thats not all of it. To achieve end-to-end acceleration, Huawei developed the FlashLinkTM technology by combining proprietary SSD controller chip, SSDs, NVMe architecture, and a storage operating system designed for flash from the ground up. Huaweis Dorado storage can deliver 3x higher performance after enabling of value-added features such as deduplication, compression, and snapshot, while maintaining 0.5 ms latency, eliminating performance shortage issues in peak hours.\nBased on the flash-oriented storage operating system, Huawei Storage has developed an innovative disk-controller collaboration algorithm. As is known to all, garbage collection is one of the main factors that affects the performance of SSDs. The innovative disk-controller collaboration algorithm enables a storage controller to learn about the data layout in SSDs in real time and adjust the data layout to ensure consistent data layout between the storage controller and SSDs. Data in the controller is written into SSDs in the format required by the SSDs, avoiding subsequent data migration and garbage collection and ensuring consistently high performance of the flash storage system. This is the basic principle of the disk-controller collaboration algorithm. The detailed technologies involved include the sequential write of large blocks, independent partitioning of metadata, and end-to-end I/O priority adjustment. More details on these technologies will be introduced in later issues. Based on the intelligent SSD controller chip and FlashLink technology, Huaweis lightning-fast and rock-solid OceanStor Dorado all-flash storage can deliver high performance while keeping the response time within 0.5 ms. Intelligent multi-protocol interface chip, accelerating data read and write of front-end network interfaces Front-end interface modules are essential to a storage system. Application data is transmitted from servers to storage arrays through these modules. Currently, mainstream front-end interface modules include 8G/16G/32G FC, 1/10/25/40/100 GE, and 10G FCoE. Each front-end interface module supports only one protocol, which is a kind of waste of resources. To improve the efficiency, Huawei develops a multi-protocol interface chip that integrates GE/10GE/FC/FCoE protocol interfaces.\nCustomers can use one interface chip to transmit data carried over IP and FC protocols. On a 10GE or 8/16G FC network, only optical module components need to be replaced, instead of the entire module. The flexible conversion between protocols greatly improves network flexibility and reduces the network construction and maintenance costs of data centers. More importantly, the internal hardware logic module of the intelligent multi-protocol interface chip supports many protocol stack functions such as checksum and FC. The processing flow, logic, and functions are switched over from the CPU software to the chip, helping ensuring high I/O concurrences and low latency of storage services. In other words, the intelligent multi-protocol interface chip offloads the checksum and FC workloads from the CPU, which can improve network processing performance, release the x86 CPU processor resources, accelerate network access and data exchange, and improve the overall storage performance. Huaweis performance tests show that when configured with the same front-end interface module (16G FC) and test model (7:3 read/write, 8K I/O blocks), OceanStor Dorado can deliver 3x performance over competitors products. Finally, lets look at the FCoE protocol. It supports both FC and Ethernet-based transmission, thereby protecting FC-based software and hardware investments. The intelligent multi-protocol interface chip integrates the functions of the Ethernet NIC and FC network HBA card, that is, one interface can support two types of network I/Os, eliminating extra cables and switches and simplifying network management. Previously, the FCoE protocol runs on the host CPU, which consumes a large number of CPU resources.\nThe host CPU cannot parse other network protocols at the same time, failing to provide the required high performance. OceanStor Dorado adopts the intelligent multi-protocol interface chip to process and parse the FCoE protocol, offloading the host CPU and improving the overall network performance and server availability. Traditional networkingFCoE (I/O consolidation) Intelligent BMC management chip, accelerating fault management and rectification Whether faults can be quickly identified and eliminated is a core indicator of the reliability of IT devices. The intelligent BMC management chip is the heart of OceanStor Dorado. It is built in with fault diagnosis and fault prewarning libraries to improve fault diagnosis accuracy. Fast diagnosis is the prerequisite for fast recovery. The intelligent management chip provides a management computing capability of 2000 Dhrystone Millions Of Instructions Per Second (DMIPS), which is five times higher than that of competitors. If a controller, front-end interface module, or management module fails, the switchover can be performed in seconds. No data is lost, services are not interrupted, and users are completely unaware of the switchover. The intelligent BMC management chip excels in energy saving. It monitors the health, power consumption, and temperature of each module in a refined manner, and combines the static and dynamic power consumption control technologies to adjust the heat dissipation of the system and reduce the chip temperature, thereby controlling static power consumption of the chip.\nDatacenter customers love the flexibility of distributed cloud and digital workspaces HIDC provides this demand. Digital evolution is nothing new, thanks to the Covid-19 pandemic, more attention has been paid than ever before. Over the past two years, companies around the world have made great strides in accelerating the timing of fully integrating technology into their business model, one of which is migrating to the cloud and using cloud technology. Productivity is now one of the most essential needs for any business, the use of cloud and data center solutions has become mandatory. But which cloud and data center solutions are right for you? The choices are many and depend on individual needs. Three important post-corona data center trends that you need to know During the recent global acceleration of digital evolution, two of the most common solutions that businesses have turned to are hybrid cloud and distributed cloud. Hybrid cloud flexibility allows businesses to choose which data and applications to stay in the private cloud environment and which to offer through third-party services, and distributed cloud-centric local solutions to combat latency and meet specific needs. Offers. Another reason for the popularity of both cloud models is their ability to increase compliance with legal requirements, which is very important in the global economy. In addition, companies can expect improvements in workforce remote support, agility and scalability, security controls, and their final analysis. With increasing combined and decentralized loads, a greater level of complexity is created in the management of these environments.\nHIDC can help you manage this complexity and provide the best service. Every moment you lose in the global serial market can diminish your valuable market share, especially for organizations for which every minute leads to the loss or gain of a large share. According to IDC, on average, an employee spends more than 2 hours a day searching for information and software to do their job properly. Although it may not be accurate for all data center employees, we all know that there are scenarios in which employees do not have the resources they need to do their job. That's why digital workspaces have been so popular over the past year because they can use the cloud to provide custom work environments in real time to any device, anywhere. Instead of looking for convenient resources, integrated workspaces allow employees to access their schedules and data through a secure interface. SaaS, web applications, corporate data, virtual applications, and desktops are all accessible through a high-quality user interface that does not depend on the user's device, location, or network. Ensuring that the workforce has exactly what they need, no more and no less, can not only increase productivity and efficiency, but can also reduce overhead costs. If people could not invest in these factors, they would lag behind their competitors in the market.\nDear all, This post will show you how to Turn Automatic Translation On or Off in Chrome . 1. The first thing you want to do is open Chrome, click the menu icon, and then click on Settings . Alternatively, you can type chrome://settings/ into your address bar to go directly there. 2. Once in the Settings menu, click on Advanced . Scroll down a little bit more until you see the Languages heading, then click on Language . 3. By default, Chrome has translation enabled. If you want to disable this feature, click the toggle button in the off position. If youre going to continue to use the translate feature, do nothing. Disable Offer to translate pages that aren't in a language you read, under the Language heading 4. When navigating to a site thats automatically been translated by Chrome, a Google Translate icon appears in the Omnibox. To see whats available for the site or language-specific options, click the Translate icon . 5. From here, you can choose to Show Original to translate the page back into the original language, or you can click the dropdown Options button for a few other choices, like having it always translate the language, never translate the language, or never translate the current site. You also can change language settings. If you have more than one language added to your browser, Chrome will normally just offer to translate web pages to your browsers primary language.\nHello, everyone! This post will share with you the interface modules. A GE electrical interface module has four 1 Gbit/s electrical ports. Indicators on a GE electrical interface module Indicator Status and Description Power indicator/Hot Swap button Steady green: The interface module is working correctly. Blinking green: There is a hot swap request to the module. Steady red: The module is faulty. Off: The interface module is powered off or hot swappable. Link/Active indicator of the GE electrical port Steady green: The link to the application server is normal. Blinking green: Data is being transferred. Off: The link to the application server is down or no link exists. Speed indicator of the GE electrical port Steady yellow: The speed is the highest. Off: The speed is not the highest. A 40GE interface module provides two 40 Gbit/s optical ports. A 100GE interface module provides two 100 Gbit/s optical ports. A 12 Gbit/s SAS shared expansion module on an engine provides twelve 4 x 12 Gbit/s mini SAS ports to connect the engine to a disk enclosure through a mini SAS HD cable. When the transfer rate of the connected device is less than that of the expansion port, the expansion port automatically adjusts the transfer rate to that of the connected device to ensure the connectivity of the data transfer channel. For a Fibre Channel optical module, the enabled auto-negotiation function allows it to auto-negotiate a maximum of three speeds. The Ethernet optical module does not support auto-negotiation.\nThe order in which these tasks were performed was disordering The storage system processes the API requests sent by the host. These API requests are queued on the storage system and executed in serial mode. After the save image command is executed on the host, LUNs are mapped, volumes are mounted, data is copied, LUNs are unmapped, and volumes are unmounted. Therefore, run the save image command on the host side in serial mode. No problem occurs. However, if the save image command is executed on the host side in parallel, two situations may occur. The first is different images. In this case, each parallel task on the host side is for different LUNs on the storage device, so there will be no problem. The second type is the same mirroring. In this case, all concurrent tasks are for the same LUN on the storage device. In this case, the first task starts to copy data after the volume is mounted, and other tasks are still in the process before data copying. After the first task copies data, the system starts to unmap this lun. Then, the host cannot scan the LUN, and other tasks are interrupted. Also we found this should be a bug of OpenStack from the internet. link: https://bugs.launchpad.net/glance-store/+bug/1965679. completely the same issue. So we can confirm that this issue is related to OpenStack side. Huawei can not resolve it. From the document, we think there should be a lock when cinder was used as glance backend storage.\nResearch is defined as careful consideration of study regarding a particular concern or problem using scientific methods. On the other hand, research is a systematic inquiry to describe, explain, predict, and control the observed phenomenon. It involves inductive and deductive methods. Exploratory : As the name suggests, researchers conduct exploratory studies to explore a group of questions. The answers and analytics may not offer a conclusion to the perceived problem. It is undertaken to handle new problem areas that havent been explored before. This exploratory process lays the foundation for more conclusive data collection and analysis. Descriptive : It focuses on expanding knowledge on current issues through a process of data collection. Descriptive research describes the behavior of a sample population. Only one variable is required to conduct the study. The three primary purposes of descriptive studies are describing, explaining, and validating the findings. For example, a study was conducted to know if top-level management leaders in the 21st century possess the moral right to receive a considerable sum of money from the company's profit. Explanatory : Causal or explanatory research is conducted to understand the impact of specific changes in existing standard procedures. Running experiments is the most popular form. For example, a study that is conducted to understand the effect of rebranding on customer loyalty.\nResearch is conducted with the purpose to: Identify potential and new customers Understand existing customers Set pragmatic goals Develop productive market strategies Address business challenges Put together a business expansion plan Identify new business opportunities Good research follows a systematic approach to capture accurate data. Researchers need to practice ethics and a code of conduct while making observations or drawing conclusions. The analysis is based on logical reasoning and involves both inductive and deductive methods. Real-time data and knowledge is derived from actual observations in natural settings. There is an in-depth analysis of all data collected so that there are no anomalies associated with it. It creates a path for generating new questions. Existing data helps create more research opportunities. It is analytical and uses all the available data so that there is no ambiguity in inference. Accuracy is one of the most critical aspects of research. The information must be accurate and correct. For example, laboratories provide a controlled environment to collect data. Accuracy is measured in the instruments used, the calibrations of instruments or tools, and the experiments final result. 1. Quantitative Research As the name suggests, quantitative refers to the numbers where data is collected based on numbers, and a summary is taken from these numbers. Graphs help to quantify the results in quantitative research. There are four main types of Quantitative research: Descriptive, Correlational, Causal-Comparative/Quasi-Experimental, and Experimental Research. attempts to establish cause-effect relationships among the variables. These types of designs are very similar to true experiments, but with some key differences. 2.\nQualitative Research Qualitative refers to the non-numerical elements in the research. When the information or data cannot be grasped in terms of numbers, qualitative research comes for the rescue. Though not reliable as much as quantitative, qualitative research helps to form a better summary in terms of theories in the data. Qualitative research is a process of naturalistic inquiry that seeks an in-depth understanding of social phenomena within their natural setting. It focuses on the \"why\" rather than the \"what\" of social phenomena and relies on the direct experiences of human beings as meaning-making agents in their everyday lives. 3. Descriptive Research Facts are considered in descriptive methods and surveys and case studies are done to clarify the facts. These help to determine and explain with examples, the facts, and they are not rejected. Many variables can be used in descriptive research to explain the facts. 4. Analytical Research Analytical research uses the facts that have been confirmed already to form the basis for the research and a critical evaluation of the material is carried out in this method. Analytical methods make use of quantitative methods as well. 5. Applied Research Applied research is action research where only one domain is considered and mostly the facts are generalized. Variables are considered constant and forecasting is done so that the methods can be found out easily in applied research. The technical language is used in the research and the summary is based on technical facts. 6.\nFundamental Research Fundamental research is the basic or pure research done to find out an element or a theory that has never been in the world yet. Several domains are connected and the aim is to find out how traditional things can be changed or something new can be developed. The summary is purely in common language and logical findings are applied in the research. 7. Exploratory Research Exploratory studies are based on the theories and their explanation and it does not provide any conclusion for the research topic. The structure is not proper and the methods offer a flexible and investigative approach to the study. The hypothesis is not tested and the result will not be of much help to the outside world. The findings will be topic-related which helps in improving the research. 8. Conclusive Research Conclusive Research aims at providing an answer to the research topic and has a proper design in the methodology. A well-designed structure helps in formulating and solving the hypotheses and gives the results. The results will be generic and help the outside world. Researchers will have an inner pleasure to solve the problems and help society in general. 9. Surveys Not least considered, Surveys play a main role in the research methodology. It helps to collect a vast amount of real-time data and helps in the research process. It is done at a low cost and can be done faster than any other method. Surveys can be done in both quantitative and qualitative methods.\nAlways, quantitative surveys must be considered above qualitative surveys as they provide numerical outputs and the data is real. Surveys are mainly used in the business to know the demand for a product in the market and to forecast the production based on the results from the survey. 10. Case Studies Case studies are another method of research methodology where different cases are considered and the proper one for the research is selected. Case studies help to form an idea of the research and helps in the foundation of the research. Various facts and theories can be considered from the case studies that help to form proper reviews about the research topic. Researchers can either make the topic general or specific according to the literature reviews from the studies. A proper understanding of the research can be made from the case study. Case studies tend to focus on qualitative data using methods such as interviews, observations, and analysis of primary and secondary sources (e.g. newspaper articles, photographs, official records). Sometimes a case study will also collect quantitative data. In research, a critical evaluation of the topic is important to analyze and verify the research. This helps the researcher to explore the research more effectively. Various methods in the research help to explore the research from different perspectives and to analyze it in a fact-driven manner. Quantitative methods and surveys help to gain numerical outputs that help in all the research. Results can be formed easily without explaining much in the thesis with the help of numbers.\n1. Powering On and Off the Storage System 2. Power On and Off interface module Restarting the Storage Device 4. Emergency power-on and power-off 5. Powering On a Key Management Server 6. Powering On and Off Disk Enclosures 7.Restarting the SVP Before powering on the storage system, complete the installation check to ensure that all devices are correctly installed and all cables are correctly connected. After powering on the storage system by following the correct power-on sequence, observe indicators on bays to check that the storage system is powered on successfully. Power-On Sequence Regarding the system power-on sequence, two basic rules must be followed: If the system includes disk bays, they must be powered on before the system bays are powered on. System bays must be powered on in an ascending sequence from system bay 0 to system bay n. Disk bays can be powered on in any sequence. NOTE Devices must be grounded before the storage system is powered on. Otherwise, devices may be damaged. The overall system power is controlled by power switches on power distribution units (PDUs). Each bay is equipped with PDUs that reside at the left and right sides in the rear of the bay. Your PDUs can be either European standard or North American standard. Choose the correct power-on sequence according to the type of PDUs configured. Power-On Sequence for Systems Configured with European Standard PDUs For a system configured with European standard PDUs, the correct power-on sequence is as follows: 1.\nIn this case, contact Huawei technical support engineers. To power on the storage system, press the power button. If the power indicator of the controller enclosure is blinking green, the storage system is being powered on. Do not hold down the power button for more than five seconds, as this will cause the storage system to be powered off. After the controller enclosure is powered on, other disk enclosures connected will be automatically powered on. Figure 10 Power button on an engine 6. (Optional) If the KVM is not powered on, switch on the power button on the rear panel of the KVM to power on it. 7. After the KVM is powered on, enter your user name and password to log in to the KVM console. The default user name and password are empty. That is, you can log in to the system by pressing Enter twice. After the login, select the channel that connects to the SVP to go to the Windows system (the default channel is 1). Sequence for Powering on DC PDBs (Four Controllers) Power supplied to the entire storage system can be controlled using switches on the DC PDBs. Each bay has two DC PDBs. One is located between 36 U and 37 U, and the other one is located between 39 U and 40 U. NOTE OceanStor 18000 series storage systems support DC power modules. 1. Turn on the switches of each DC PDB in all disk bays. There is no sequence requirement.\nFigure 11 shows the switches of a DC PDB. Figure 11 Switches of a DC PDB 2. Turn on the switches of each DC PDB in all system bays. 3. For a multi-controller storage system that uses a switch-based network, power on the switches used for Scale-out. 4. Press the power button of the SVP, as shown in Figure 12 . Figure 12 Power button of the SVP Press the power button of all engines, as shown in Figure 13 . NOTE In a scenario with multiple controllers, all the controllers must be powered on within three minutes. If the engines fail to power on within the specified period, the storage system fails to power on. In this case, contact Huawei technical support engineers. To power on the storage system, press the power button. If the power indicator of the controller enclosure is blinking green, the storage system is being powered on. Do not hold down the power button for more than five seconds, as this will cause the storage system to be powered off. After the controller enclosure is powered on, other disk enclosures connected will be automatically powered on. Figure 13 Power button on an engine NOTE The power-on of the storage system takes 15 to 30 minutes. Power-On Check When the system is successfully powered on, the status indicator on the front door of system bay 0 is steady green, as shown in Figure 14.\nConfirm the information in the displayed Danger dialog box and enter the password of the currently logged in user. Then select I have read and understand the consequences associated with performing this operation. NOTE If you enter the wrong password more than three times within 5 minutes, the current user will be logged out and exit DeviceManager. 4. Click OK. The Success dialog box is displayed, indicating that the operation succeeded. 5. Click OK. You have restarted the storage device. Perform emergency power-on and power-off according to the equipment room manual. After power-on, the device automatically recovers. After connecting network cables of the key management servers, connect power cables and power on the key management servers. Prerequisites You have installed and connected network cables of the key management servers. You have prepared the power cables. You need to power on both of the two key management servers. Procedure 1. Wear ESD gloves and ESD wrist straps. 2. Unpack power cables and adapter cables. 3. Insert one end of the power cable to the power socket on the rear of the key management server, and the other end to the external power supply. 4. If the external power supply is not turned on, turn it on. NOTE The key management servers have no power switches. After their power cables are connected and the external power supplies are turned on, key management servers will automatically power on. The LCD on the key management server will flash slowly for approximately one minute and display the initialization information.\nOLTP is an application type of the database service where a large number of users perform online transaction operations. OLTP (online transaction processing) is a class of software programs capable of supporting transaction-oriented applications on the Internet. In computing, a transaction is a sequence of discrete information exchanges that are treated as a unit. Many everyday acts involve OLTP, including online banking, online shopping and even in-store shopping when the point of sale (POS) terminal is tied to inventory management software. Two important characteristics of an OLTP system are concurrency and atomicity. Atomicity guarantees that if one step is incomplete or fails during the transaction, the entirety will not continue. Concurrency prevents multiple users from altering the same data at the same time. In order for a transaction to be completed successfully, all database changes must be permanent, a condition known in computing as atomic statefulness. To avoid single points of failure, OLTP systems are often decentralized. For example, Google Cloud Spanner is a distributed relational database service that runs on Google Cloud. It is designed to support global online transaction processing. The OLTP application has the following load characteristics: From the perspective of the database: 1. The data volume involved in each transaction is very small. 2. The database data must be up to date. Therefore, the database availability is high. 3. A large number of users access the network at the same time. 4. The database is required to respond quickly. Generally, a transaction needs to be completed within several seconds.\nFrom the perspective of storage: 1. Each I/O is very small, usually 2 KB to 8 KB. 2. Access to hard disk data is very random. 3. At least 30% of the data is random write operations. 4. REDO logs (redo log files) are written frequently. Backup and recovery, as part of a high availability strategy, can be performed on a low level of granularity to efficiently manage the size of the database. OLTP systems usually remain online during backups and users may continue to access the system while the backup is running. The backup process should not introduce major performance degradation for the online users. Partitioning helps to reduce the space requirements for the OLTP system because part of a database object can be stored compressed while other parts can remain uncompressed. Update transactions against uncompressed rows are more efficient than updates on compressed data. Partitioning can store data transparently on different storage tiers to lower the cost of retaining vast amounts of data. For data maintenance operations (purging being the most common operation), you can leverage partition maintenance operations with the Oracle Database capability of online index maintenance. A partition management operation generates less redo than the equivalent DML operations. A common scenario for OLTP environments is to have monotonically increasing index values that are used to enforce primary key constraints, thus creating areas of high concurrency and potential contention: every new insert tries to update the same set of index blocks. Partitioned indexes, in particular hash partitioned indexes, can help alleviate this situation.\nHey, guys! Today I will share with you the Huawei storage performance analysis tools. 1. View real-time monitoring data on DeviceManager or SystemReporter. 2. View historical performance monitoring data on the eService performance analysis website. 3. Use eDesigner to evaluate storage's reachable performance indicators. How to analyze the performance log. Uploading a performance file package to http://oceanstor-analyser.huawei.com:8080/ 1. A data package exported using SmartKit can be directly uploaded and parsed. 2. Performance files collected in other ways must be compressed into one package before being uploaded and parsed. Do not upload small packages separately. 3. Currently, .zip, .rar, and .7z packages are supported. 4. Do not place performance files of different devices into the same package. Otherwise, the performance data may fail to be parsed. 5. You are advised to upload the config file after uploading performance data. The sharing function enables you to share the performance data with the personnel who analyze the data together. Performance analysis reports can be directly generated based on performance data. Currently, View Data in the lower left corner is the function most used. You can check the following path: Storage - Enterprise Storage - Product Capability Evaluation Products : converged storage V3 and V5 series, Dorado V3 and V6 series, T V2 and 18000 V1 series. Service models : mainstream services such as OLTP, OLAP, VDI, all-random, and NAS.\nDear all, This post instructs you what is Hyper-V, and how to install it on Windows 11 step by step. Hyper-V is Microsofts hypervisor software, which allows you to create and run virtual machines on your PC. Virtual machines (VMs) let you create isolated instances of an operating system that doesnt affect your host PC. There are a few options for virtualization software out there, but Hyper-V is native to Windows, and its ideal if youre managing Windows VMs. Hyper-V runs each virtual machine in its own isolated space, which means you can run more than one virtual machine on the same hardware at the same time. You might want to do this to avoid problems such as a crash affecting the other workloads, or to give different people, groups or services access to different systems. Computing Environment - A Hyper-V virtual machine includes the same basic components as a physical computer, such as memory, processors, storage, and networking. All of these parts have features and options that you can configure in different ways to suit different needs. Both storage and networking can be considered their own categories because you can configure them in a number of ways. Disaster recovery and backup - For disaster recovery, Hyper-V Replica creates copies of virtual machines that are intended to be stored in another physical location, so you can restore virtual machines from the copies. For backup, Hyper-V offers two types.\nMost enterprise storage systems are now based on all-flash, particularly in the case of primary data storage. Yet, in these budget-strapped times, it pays to look at drives that blend flash SSDs with hard disks to provide extra secondary capacity at an affordable price. Multiple vendors offer these solid-state hybrid drives, which combine a traditional high-capacity HDD with high-speed SSD technology. HDDis an acronym standing for . In other words, an HDD is a traditional hard drive with mechanical heads that move across spinning platters. HDDs offer high-capacity storage at a low cost per gigabyte. However, the reliance on mechanical components limits an HDD's overall performance. SSDstands for . Rather than storing data on magnetic platters like HDDs do, SSDs store data in NAND flash memory. Because SSDs don't include any moving parts, they far outperform HDDs, making them the best choice for overall performance. Better still, SSDs generally consume less power and give off less heat than HDDs, while also running silently. There are, however, a few disadvantages associated with SSDs. SSDs generally have lower capacities than HDDs, with a higher price per gigabyte -- although higher-capacity models have become available over the last few years. Additionally, the memory cells used to store data wear out with use. Early SSDs had relatively short life spans, but newer models have failure rates that are more in line with HDDs. SSHD stands for . It's also known as a .\nAn SSHD is a device that is designed to give you the best of both worlds by combining flash memory storage and HDD storage into a single device. SSHDs have solid-state storage built in but also use HDD technology as a way of providing additional storage space. SSHDs work by caching frequently used files to flash storage, while retaining less commonly used files on spinning media. This means that an SSHD drive initially performs similarly to an HDD, but as the drive learns which files are used most often -- and caches those frequently used files -- it begins to deliver SSD-like performance. But, before committing to a SSHD, it pays to understand the advantages and weaknesses of HDDs, SSDs and SSHDs. Generally speaking, HDDs offer the highest capacity with the lowest cost per gigabyte, while SSDs tend to have smaller capacities but far better performance at a higher cost per gigabyte. SSHDs have historically fallen somewhere in the middle. Even though using an SSHD is still a viable option in many cases, the SSHD market seems to be shrinking. A search on Newegg for the term SSHDrevealed only a few dozen options. This is likely due to the fact that SSDs have become more affordable in recent years and because SSD manufacturers have been able to improve storage capacity and drive longevity. As such, it often makes sense to simply use an SSD instead of an SSHD.\nFor basic introduction you can have a look at my previous post. Advantages: Deduplication has the following advantages: Lower bandwidth consumption when copying data connected with remote backups, replication, and disaster recovery; Increased retention durations; Reduced tape backups and faster recovery time targets There are several techniques used for Deduplication of data here we are going to explore some of them. Firstly, I am going to list down their names than will further explain them. Inline deduplication Post-process deduplication Fixed-length deduplication Variable-length deduplication Similarity-based deduplication Inline deduplication examines data as it enters a backup system. As data is written to backup storage, redundancies are removed. Although inline deduplication uses less backup storage, it can cause bottlenecks. For high-performance main storage, storage array suppliers advocate turning off inline data deduplication techniques. This is an asynchronous backup procedure that eliminates redundant data after it has been written to storage? Duplicate data is eliminated and replaced with a pointer to the block's initial iteration. The post-processing method allows users to deduplicate certain workloads and quickly restore the most recent backup without hydration. The tradeoff is that backup storage capacity is more than that required by inline deduplication. This deduplication technique breaks data into chunks of a specified size. Only the unique blocks are saved once the blocks have been compared. Fixed-length is significantly less efficient than the alternative, although it is easy to implement. Variable-length deduplication divides a file system into pieces of varying sizes, allowing the deduplication operation to achieve higher data reduction ratios than fixed-length blocks.\nData governance is nothing more than documentation if it isn't carried out. Data governance establishes all norms and processes, whereas data management implements them so that data may be compiled and used for decision-making. Data governance contributes to your organization's Data Intelligence. It assists you in making sense of your data so that you can use it as an asset. Data governance is primarily about developing procedures and processes to aid in data comprehension and standardization. No single data governance model fits all organizations. There are several models, some of which are better suited to smaller or bigger enterprises. The first model is for small businesses that have a single owner This architecture assures that the data is generated by local users, who are often the master data's consumers. In this situation Big data, if not correctly handled, can result in massive data discrepancies. Data governance companies must define data ownership explicitly and limit it to a small group of specialists within the business. Data consistency may be controlled by automated systems, and discrepancies can be swiftly corrected via audits. Individual business users manage their master data in this data governance architecture. In this situation, various business divisions collaborate with shared consumers, supplies, and providers. The master data approach is best suited for small and medium-sized businesses with several plants and/or firms involved. It can lead to redundant master data and inconsistent data, which can result in inconsistency or erroneous reporting.\nGood day Storage fans! Following the previous article on , it was time to move on to another pretty 'demanded' keyword observed through the Community search bar, that is HyperReplication. A key concept in the storage industry, HyperReplication has managed to impose itself as a leading technology in the enterprise environment. Let's discover together exactly why from the below paragraphs! BACKGROUND INFORMATION HyperReplication is the remote replication feature developed by Huawei. The feature provides flexible and powerful data replication functions to achieve remote data backup and recovery, continuous support for service data and disaster recovery (DR). TYPES OF DEPLOYMENT MODES When a storage system runs block services, HyperReplication supports the following two modes: synchronous remote replication - data is synchronized in real time to achieve full protection for data consistency, minimizing data loss in the event of a disaster; as ynchronous remote replication -data is synchronized periodically to minimize service performance deterioration caused by the latency of long-distance data transmission.\nBASIC CONCEPTS This section describes the basic concepts related to the HyperReplication feature: Remote Replication Pair -the relationship between a primary logical unit number (LUN) and a secondary LUN in a remote replication task; Synchronization -the process of copying data from the primary LUN to the secondary LUN; Splitting - theprocess of stopping data synchronization between primary and secondary LUNs; Primary/Secondary Switchover - theprocess of exchanging the roles of the primary and secondary LUNs in a pair relationship; Data Status - by determining data differences between a primary and a secondary LUN, the HyperReplication feature identifies the data status of the current pair; Writable Secondary LUN -hosts can send data to secondary LUNs, so after the HyperReplication feature is configured, the secondary LUN is read-only by default and if the primary LUN is faulty, the administrator can cancel write protection for the secondary LUN and set the secondary LUN to writable; Link Compression -an inline compression technology; Optimizing Data Synchronization Performance -after optimization, data is transferred to the remote storage system through the optimal path, avoiding the impact of I/O forwarding on performance and improves data synchronization efficiency. BENEFITS lists the benefits of the HyperReplication feature: Function Purpose Benefit THE BOTTOM LINE A more than crucial aspect within the storage industry, HyperReplication always comes in handy whenever it is 'called upon' to be employed. More information on HyperReplication can be found by accessing this .\nHello all, Someone may ask that what is storage. I will say storage is the media for storing and protecting data. There are 2 senses narrow and broad: Storage is to save data to certain storage media in a proper, secure, and effective manner to meet the requirements ofdifferent application environments and ensure effective access to the data. Specifically: Storage is physical media for temporary or long-term data storage. Storage is a method or behavior for ensuring data integrity and security. Storage combines the two aspects to providecustomers with a data storage solution. Structured data: Refers to data stored in databases and logically represented by a two-dimensional table structure.Structured data includes data in databases such as SQL, Oracle, and DB2. Unstructured data: Refers to data that cannot be logically represented by a two-dimensional table structure.Unstructured data includes documents, texts, pictures, XML, HTML, images, as well as audio and video information inall formats. Storage capacity needs increase mainly due to the rapid growth of unstructured data. IOPS: Indicates the number of I/Os processed by a storage array per second. Generally, the performance of randomreads and writes of small files such as database files depends on the IOPS. Bandwidth (MB/s): Indicates the maximum output bandwidth that a storage array can provide per second. Themaximum bandwidth of a storage array generally refers to the sequential read bandwidth of the cache. Theperformance of continuous reads and writes of large files such as videos depends on the bandwidth. 1. Solution: Disaster recovery (DR) solutions Backup solutions 2.\nSmartVirtualization eliminates incompatibility concerns across multiple storage systems, enabling users to manage heterogeneous storage systems and utilize storage resources from both legacy and new storage systems, thereby safeguarding client investments. Some Merits of SmartVirtualization are as follows: The local storage system is compatible with standard heterogeneous storage systems, allowing accomplish centralized storage resource planning and administration. When a local storage system uses the storage space provided by LUNs in a heterogeneous storage system (external LUNs for short), the local storage system does not generate complete copies of physical data, so the storage space of the local storage system will not be wasted. In addition to employing external LUNs as local storage resources, a local storage system can set value-added functions such as SmartMigration for these LUNs to provide high security and dependability of service data. Use smart virtualization to make a local storage system take over the resources of a heterogeneous storage system. Then, the local storage system can manage and allocate storage space on the heterogeneous storage system. This makes space allocation across different storage systems easier and consolidates the resources of multiple storage systems. enables you to migrate your databases to the cloud inalmost no downtime. All data changes to the source database that occur during the migration are continuously replicated to the target, allowing the source database to be fully operational during the migration process Storage resource management is a technique and methodology used to make network memory storage better and easier.\nWhen it comes to service migration, SmartMigration is a key technology. It can move service data from one storage system to another and from one storage system to another without causing problems. In a storage system, It migrates service data without interfering with host services, and it allows service data to be migrated across Huawei storage systems as well as between compatible storage systems from other vendors.SmartMigration synchronizes and splits service data so that all of the data from the source LUN to the target LUN can be moved to the new LUN at once. Once the SmartMigration task is created, The first step is Service data synchronization for that we need to Use a Source LUN Use a Target LUN Pair them up There will be two Service data synchronization: initial synchronization data change synchronization Both of them can be done at the same time but keeping them synced in both source and target LUNS. First, we are going for Initial Synchronization in which All initial service data in source LUNS are copied to target LUNS. This means Full data copy There is no need to stop the host services during synchronization. Sending a write request is what the host does when it changes data. When the storage system is done synchronizing, it writes changed service data to both the source and target LUNs with dual write technology.\nThat Records differential data that fails to be written to the target LUN during the data change synchronization and copy the data that hasn't been written from the source LUN to the target LUN based on the DCL. When the copy is finished, the storage system sends the host write success acknowledgment. The storage system returns a write failure if data cannot be written to the source LUN. The host re-sends the data to the source LUN only, not the target LUN, after receiving the write failure. During migration, this method assures data consistency on the source and target LUNs. 1. The host sends an I/O write request to the LM module( of the storage system. 2. The LM module processes the write request. It writes the data to the source LUN and the target LUN and then writes the write operation to the log so that it can be seen. 3. The source LUN and the target LUN both send the data write result to the LM module, which sends it to the LM module. 4.\nArtificial intelligence (AI) and machine learning (ML) promise to transform whole areas of the economy and society, if they are not already doing so. From driverless cars to customer service bots,AI and ML-based systemsare driving the next wave of business automation. They are also massive consumers of data. After a decade or so of relatively steady growth, the data used by AI and ML models has grown exponentially as scientists and engineers strive to improve the accuracy of their systems. This puts new and sometimes extreme demands on IT systems, includingstorage. AI, ML and analytics require large volumes of data, mostly in unstructured formats. All these environments are leveraging vast amounts ofunstructured data, says Patrick Smith, field CTO for Europe, the Middle East and Africa (EMEA) at supplier Pure Storage. It is a world of unstructured data, not blocks or databases. Training AI and ML models in particular uses larger datasets for more accurate predictions. As Vibin Vijay, anAI and MLspecialist at OCF, points out, a basic proof-of-concept model on a single server might expect to be 80% accurate. With training on a cluster of servers, this will move to 98% or even 99.99% accuracy. But this puts its own demands on IT infrastructure. Almost all developers work on the basis that more data is better, especially in the training phase. This results in massive collections, at least petabytes, of data that the organisation is forced to manage, says Scott Baker, CMO at IBM Storage. Storage systems can become a bottleneck.\nThe latest advanced analytics applications make heavy use of CPUs and especiallyGPU clusters, connected via technology such as Nvidia InfiniBand. Developers are even looking at connecting storage directly to GPUs. In AI and ML workloads, the learning phase typically employs powerful GPUs that are expensive and in high demand, says Brad King, co-founder and field CTO at supplier Scality. They can chew through massive volumes of data and can often wait idly for more data due to storage limitations. Data volumes are generally large. Large is a relative term, of course, but in general, for extracting usable insights from data, the more pertinent data available, the better the insights. The challenge is to provide high-performance storage at scale and within budget. As OCFs Vijay points out, designers might want all storage on high-performancetier 0flash, but this is rarely, if ever, practical. And because of the way AI and ML work, especially in the training phases, it might not be needed. Instead, organisations are deploying tiered storage, moving data up and down through the tiers all the way from flash to the cloud and even tape. Youre looking for the right data, in the right place, at the right cost, says Vijay. Firms also need to think about data retention. Data scientists cannot predict which information is needed for future models, and analytics improve with access to historical data. Cost-effective, long-term data archiving remains important. There is no single option that meets all the storage needs for AI, ML and analytics.\nThe conventional idea that analytics is a high-throughput, high-I/O workload best suited to block storage has to be balanced against data volumes, data types, the speed of decision-making and, of course, budgets. An AI training environment makes different demands to a web-based recommendation engine working in real time. Block storage has traditionally been well suited for high-throughput and high-I/Oworkloads, where low latency is important, says Tom Christensen, global technology adviser at Hitachi Vantara. However, with the advent of modern data analytics workloads, including AI, ML and evendata lakes, traditional block-based platforms have been found lacking in the ability to meet the scale-out demand that the computational side of these platforms create. As such, a file and object-based approach must be adopted to support these modern workloads. Block-based systemsretain the edge in raw performance, and support data centralisation and advanced features. According to IBMs Scott Baker, block storage arrays support application programming interfaces (APIs) that AI and ML developers can use to improve repeated operations or even offload storage-specific processing for the array. It would be wrong to rule out block storage completely, especially where the need is for high IOPS and low latency. Against this, there is the need to build specific storage area networks for block storage usually Fibre Channel and the overheads that come with block storage relying on an off-array (host-based) file system. As Baker points out, this becomes even more difficult if an AI system uses more than one OS.\nAs a result, system architects favour file or object-based storage for AI and ML. Object storage is built with large, petabyte capacity in mind, and is built to scale. It is also designed to support applications such as the internet of things (IoT). Erasure coding provides data protection, and the advanced metadata support in object systems can benefit AI and ML applications. Against this, object storage lags behind block systems for performance, although the gap is closing with newer,high-performance object technologies. And application support varies, with not all AI, ML or analytics tools supporting AWSs S3 interface, the de facto standard for object. Cloud storage is largely object-based, but offers other advantages for AI and ML projects. Chief among these are flexibility and low up-front costs. The principal disadvantages of cloud storage are latency, and potential data egress costs. Cloud storage is a good choice for cloud-based AI and ML systems, but it is harder to justify where data needs to be extracted and loaded onto local servers for processing, because this increases cost. But the cloud is economical for long-term data archiving. Unsurprisingly, suppliers do not recommend a single solution for AI, ML or analytics the number of applications is too broad. Instead, they recommend looking at the business requirements behind the project, as well as looking to the future.\nUnderstanding what outcomes or business purpose you need should always be your first thought when choosing how to manage and store your data, says Paul Brook, director of data analytics and AI for EMEA at Dell. Sometimes the same data may be needed on different occasions and for different purposes. Brook points to convergence between block and file storage in single appliances, and systems that can bridge the gap between file and object storage through a single file system. This will help AI and ML developers by providing more common storage architecture. HPE, for example, recommends on-premise, cloud and hybrid options for AI, and sees convergence between AI and high-performance computing. NetApp promotes its cloud-connected, all-flash storage system ONTAP for AI. At Cloudian, CTO Gary Ogasawara expects to see convergence between the high-performance batch processing of the data warehouse and streaming data processing architectures. This will push users toward object solutions. Block and file storage have architectural limitations that make scaling beyond a certain point cost-prohibitive, he says. Object storage provides limitless, highly cost-effective scalability. Object storages advanced metadata capabilities are another key advantage in supporting AI/ML workloads. It is also vital to plan for storage at the outset, because without adequate storage, project performance will suffer. In order to successfully implement advanced AI and ML workloads, a proper storage strategy is as important as the advanced computation platform you choose, says Hitachi Vantaras Christensen.\nData deduplication is a method of reducing storage capacity requirements by eliminating redundant copies of data. Deduplication can be performed inline, while data is written into the storage system, or in the background, after the data has been copied to disc, to eliminate duplicates. inline deduplication relies on processes that occur between the data origin servers and the data backup destinations, or, in other words, functions that occur during the process rather than after it has been completed. While post-processing deduplication separates redundant data after it has already been transferred, Inline deduplication can slow down data backups or otherwise obstruct the process; however, it also means that the end product will be free of superfluous or inefficient data. Inline processing would give higher space efficiency and potentially higher compression rates. When continuous performance is required and there is ambiguity about capacity optimization potential and how it may affect performance, post-processing is the preferable method. Capacity optimization occurs after data is saved, so there is little influence on performance while data is written. Inline processing would give higher space efficiency and possibly even higher compression rates where data to be written has obvious larger potential for capacity optimization (for example, if there is a lot of redundant data). Inline gives higher storage performance since writes are minimized and disc wear is reduced as a result. However, recognizing the nature of the data being written and calculating the efficiency of data reduction would be required beforehand by the storage/IT administrator.\nSmartVirtualization is a heterogeneous virtualization solution developed by Huawei. When a local storage system (OceanStor Dorado V6 series storage system) is connected to another type of Huawei storage system or a third-party storage system, this feature enables the local storage system to use and manage storage resources of the peer storage system as local storage resources despite of the different software and hardware architectures. SmartVirtualization can manage storage resources on heterogeneous storage systems, but not configure them.SmartVirtualization eliminates the incompatibility issues across different storage systems, so users may manage heterogeneous storage systems and employ storage resources from both older and modern storage systems, protecting client investments. External LUNs in heterogeneous storage systems can be used to offer physical storage capacity for the OceanStor Dorado V6 series storage systems via SmartVirtualization. First, we need to understand what an eDevLUN and External LUN is, A local storage system's storage pool reorganizes mapped external LUNs as raw storage devices based on data organization form. An eDevLUN is a raw device. An eDevLUN occupies only the metadata storage space required by the local storage system. The service data is still on the heterogeneous storage system. It is possible to configure SmartMigration for eDevLUNs so that application servers can access data on external LUNs in a heterogeneous storage system. An eDevLUN consists of data and metadata. A mapping relationship is established between data and metadata. A LUN in a heterogeneous storage system, also called remote LUN. 1.\nThe process of terminating service data synchronization between the source LUN and target LUN, exchanging LUN information, and then removing the data migration link between the source LUN and target LUN. Splitting is carried out on a single pair. The splitting procedure is as follows: I. Service data synchronizations between the source LUN and the target LUN in a pair are interrupted. II. Then, the two LUNs exchange LUN information. III. then, the data migration relationship is canceled. IV. During the split, host services are suspended. V. After the information exchange, services are sent to the target LUN. The process is invisible to users. Now comes the question of how does the LUN information is exchanged Let's take an example L0 is the LUN ID of the source LUN and L1 is the LUNID of the Target LUN Before the target LUN L1 can take over services from the source LUN L0, it needs to exchange information with the L0. Using the source LUN ID and source data volume ID, a host can figure out what a source LUN looks like and how much space it takes up. Source LUN ID and source data volume ID are mapped to each other so that the host can read the physical space of the source LUN from the source LUN. The target LUN ID and target data volume ID are also linked in the mapping. Source and target data volume IDs change during LUN information exchange.\nDear all, Today we talk about the customer benefits of Huawei OceanStor Dorado V6 all-flash storage. In the past few years, the explosive growth of data and mining of data values have led to the innovation of IT systems, especially storage devices. The main storage media changes from HDDs to SSDs, and the main storage protocol changes from SAS to NVMe. The end-to-end access latency of storage systems is shortened from 10 ms to 1 ms or even lower, and storage systems are becoming more intelligent with high performance and reliability. Business developments and technical innovations pose higher requirements on the design and construction of customers' IT infrastructure, and choosing a proper storage system is a crucial part in building a modern IT infrastructure. Stable storage performance and high reliability are the basis for building an intelligent and scalable IT system; efficient storage is a key factor in reducing IT system costs; efficient data flow, intelligent O&M, non-disruptive upgrade, and long-term supply assurance are crucial for long-term IT system evolution and development. Guided by the concept of \"Data + Intelligence\", Huawei OceanStor Dorado V6 all-flash storage systems redefine storage architecture. With self-developed software and hardware (including chips), Huawei OceanStor Dorado V6 all-flash storage systems implement intelligent, native all-flash storage to provide stable, high-performance, and highly reliable services and meet the storage requirements of intelligent IT systems.\nHuawei OceanStor Dorado V6 all-flash storage systems use Huawei-developed chips for front-end interconnection, CPU processing, cluster switching, back-end interconnection, and disk processing, providing low latency and large throughput from end to end. The storage systems support 32 Gbit/s Fibre Channel, 100 Gbit/s RDMA, and NVMe for front-end interconnection, cluster switching, and back-end interconnection, and can offload specific tasks to hardware to free CPU resources, ensuring low latency and high bandwidth. The core processing unit of the storage systems is the Kunpeng 920 chip, which incorporates multiple cores and processors. With the key software designs, such as balanced distribution, lock-free design, and high scalability, a storage system can have up to 1000 cores to ensure efficient service processing. FlashLink enables close collaboration between controllers and SSDs. The use of multistreaming, full-stripe write, garbage collection, and read/write priority control effectively reduces write amplification on SSDs, making the most of the SSD performance throughout the lifecycle. The AI chip learns and analyzes the workload to identify long-interval sequential flows and data associations, which greatly improves the data prefetch capability of the cache, achieving up to 20 million IOPS (SPC-1 test model) at 0.1 ms latency. Huawei OceanStor Dorado V6 all-flash storage systems employ a new-generation hardware platform and an ultra-stable SmartMatrix full-mesh architecture to enhance data reliability and service continuity, ensuring always-on storage services. In terms of data reliability, the storage systems provide end-to-end data redundancy protection, validity check, and recovery mechanisms.\nData protection measures at various levels can be used on demand, such as multiple cache copies, data protection across controller enclosures, RAID, data integrity protection, snapshots, remote replication, and active-active data center solution. The system checks data integrity and rectifies any error in the end-to-end data read and write processes to prevent unexpected recoverable faults (such as bit reversal) in hardware. This effectively prevents data error spreading when devices are in an uncontrollable and intermittent sub-health state, ensuring data security. In terms of service continuity, the system accurately monitors the device health status to quickly identify faults. If a fault is detected, the system isolates and attempts to rectify the fault through redundancy takeover. If the fault is rectified, the involved component continues providing services. If the fault fails to be rectified, an alarm is reported to prompt users to replace the faulty component. These data reliability and service continuity assurance techniques enable OceanStor Dorado 8000 V6 and Dorado 18000 V6 to tolerate failure of seven out of eight controllers without service interruption, and allow non-disruptive software upgrades. As the business of enterprises grows, a single storage system will carry multiple service systems, which have varied requirements on performance, capacity, data protection, and cost. This poses a challenge on storage reliability, performance, and capacity, as well as the customers' overall IT planning and management. Huawei OceanStor Dorado V6 allflash storage systems have balanced the capacity and performance for hybrid application models in which various services and workloads share storage resources.\nHello Community! In today's blog post we are going to address HyperMetro, an extremely important term found in the storage industry. Naturally, HyperMetro is without a doubt one crucial notion in this sector, which can be seen in the amount of queries our users have run on theCommunity search bar throughout the past year. Let's find out more about HyperMetro in the upcoming paragraphs! BACKGROUND INFORMATION HyperMetro is Huawei's active-active storage solution that enables two storage systems to process services simultaneously, establishing a mutual backup relationship between them. If one storage system malfunctions, the other one will automatically take over services without data loss or interruption. With HyperMetro being deployed, you do not need to worry about your storage systems' inability to automatically switch over services between them and will enjoy rock-solid reliability, enhanced service continuity, and higher storage resource utilization. TYPES OF DEPLOYMENT MODES HyperMetro supports both single-data center (DC) and cross-DC deployment modes: single-DC deployment - the active-active storage systems are deployed in two equipment rooms in the same campus and hosts are deployed in a cluster; cross-DC deployment - the active-active storage systems are deployed in two DCs in the same city or in two cities located in close proximity (the distance between the two DCs should be within 300 kms).\nBASIC CONCEPTS This section provides the key concepts associated with HyperMetro: Protected Object -the protected objects are LUNs or protection groups, that is, HyperMetro is configured for LUNs or protection groups for data backup and disaster recovery; Protected Group (PG) and LUN Group - an LUN group can be directly mapped to a host for the host to use storage resources.\nYou can group LUNs for different hosts or applications, while a protection group (PG) applies to data protection with consistency groups; HyperMetro Domain -a HyperMetro domain allows application servers to access data across DCs, consisting of a quorum server and the local and remote storage systems; HyperMetro Pair -a HyperMetro pair is created between a local and a remote LUN within a HyperMetro domain; HyperMetro Consistency Group -a HyperMetro consistency group (CG) is created based on a protection group; Dual-Write -dual-write enables the synchronization of application I/O requests with both local and remote LUNs; DCL -data change logs (DCLs) record changes to the data in the storage systems; Synchronization -HyperMetro synchronizes differential data between the local and remote LUNs in a HyperMetro pair, while one can also synchronize data among multiple HyperMetro pairs in a consistency group; Pause -pause is a state indicating the suspension of a HyperMetro pair; Force Start -to ensure data consistency in the event that multiple elements in the HyperMetro deployment malfunction simultaneously, HyperMetro stops hosts from accessing both storage systems; Preferred Site Switchover - preferred site switchover indicates that during arbitration, precedence is given to the storage system which has been set as the preferred site (by default, this is the local storage system); FastWrite - FastWrite uses the First Burst Enabled function of the SCSI protocol to optimize data transmission between storage devices, reducing the number of interactions in a data write process by half.\nHello everybody! This post contains anOceanStor Dorado 18000 V6.0.1 upgrading case. Please have a look below for more information on the topic. During the replacement of Controller 0D, SmartKit shows the below: We cannot go forward with the replacement. After the replacement, we are still facing issues when upgrading the firmware. OceanStor Dorado 18000 V6 ; Current Version 6.0.1.SPH20 ; Target Version 6.1.0+6.1.0.SPH13 . Because the controller is replaced, this system will be upgraded. Risks that must be rectified before the upgrade The SmartKit version is old. Update SmartKit and perform the upgrade evaluation again to ensure that no new risks exist before the upgrade. Otherwise, contact Huawei engineers. Links For downloading . For the . For upgrading the . For the . Possible risks According to the logs, the port disconnection major alarms are deleted (Controller enclosure CTE0, interface module IOM.H6, port number P1). Confirmed with the customer whether the port was still in use. If it is in use, restore the connection. If it is not in use, ignore it. Download the software package and guide from the following websites. 1. Before the upgrade, the storage array must install the hot patch 6.0.1.SPH21 , here are the download links: ; ; . 2. The upgrade to 6.1.0 using hot patches takes effect at the same time. Select the fast upgrade method. Here is the patch package: ; ; ; ; . Note : Please check the latest hot patches of SPH13 or later versions.\nCreating a Basic Storage Pool cxtxdxs2 is the raw device disk in the /dev/rdsk directory. Default mount point of a storage pool When you run the preceding command to create a storage pool, the default mount point of the top-level file system is /pool-name. If the directory does not exist, the system automatically creates the directory. To create a storage pool using a different default mount point, use the In this way, you can create a pool named tank and a ZFS file system with the mount point /xx/zfs. Basic Query Commands Basic Status Query Commands NAME STATE READ WRITE CKSUM Creating a ZFS File System The parameter is pool-name/[filesystem-name]/filesystem-name. In this way, a JJ file system can be created in the tank file system and a mount path is automatically created for mounting. You can also customize the file system mount point when creating it. # zfs create -o mountpoint=/export/zfs tank ZFS automatic mount point NAME PROPERTY VALUE SOURCE You can also manually reset the mount point. NAME PROPERTY VALUE SOURCE pool/filesystem mountpoint /mnt local Before resetting the mount point, you need to unmount the file system. After the setting is complete, the file system is automatically mounted. Mounting a ZFS File System Unmounting can be directly canceled by using the traditional umount command. List ZFS Basic Information Precautions: 1. The name and mount point of the ZFS file system can be changed, but the name of the Zpool that originally created the ZFS file system cannot be changed. 2.\nHello Community! This post showcases the troubleshooting method for whenSVP login cannot be accessed via Remote Desktop. Please see below for more details. SVP (Service Processor) cannot be accessed via Remote Desktop Connection. The below is the screenshot for reference: The device model is an OceanStor 18500 V3 . The service processor (SVP), working with the keyboard, video and mouse (KVM), is the core component for managing, configuring and maintaining the OceanStor 18500 V3/18800 V3 storage system. Maintenance and management tools are installed on the SVP and used for local or remote monitoring, management, configuration and authentication. Below you may find the front panel of the SVP: Logging in to DeviceManager Before the storage system is delivered, DeviceManager is loaded to the SVP. You can log in to DeviceManager in four ways: SVP, web, management network port and tablet. When logging in via KVM, it is confirmed the Windows OS works normall, as shown in the figure. It is suggested to reboot SVP, then check whether it is recovered (SVP is only used for management, it has no impact on the service when rebooting SVP). Method 1. Using DeviceManager Customers can log in to DeviceManager using KVM and click the reboot svp button.Here is . Method 2: Using CLI The reboot SVP command is used to reset an SVP VM and host. If you log in to CLI using the management IP address of the controller, this command cannot be executed. Before running this command, use the SVP management IP address to log in to CLI.\nHello dear all, This post describes the upgrade methods supported by OceanStor Dorado V6. This table shows upgrade methods and application scenarios Fast upgrade is implemented by replacing processes and without resetting controllers. During the upgrade, host links are not interrupted and hosts are unaware of the upgrade. This upgrade method is recommended for OceanStor Dorado V6 series storage systems. In a rolling upgrade, controllers are upgraded in several batches and reset for the upgrade to take effect. By working with the host's multipathing software, the storage system switches over services between controllers during the upgrade to prevent interruption. Note: During a rolling upgrade, controllers are restarted in several batches, and front-end links are switched over between controllers. This may cause host compatibility issues. Before the upgrade, evaluate host compatibility by following the instructions in 8.4.6 Evaluating Compatibility. If host services have been stopped before the upgrade, compatibility evaluation is not needed. OceanStor Dorado 8000 V6 and 18000 V6 use an active-active front-end architecture. Resetting a single controller does not interrupt front-end links or trigger a switchover. If you do not need to upgrade front-end interface modules or the front-end interface modules support hot upgrade, the rolling upgrade does not trigger host compatibility issues, and compatibility evaluation will be automatically skipped during the pre-upgrade site survey. For the default batch upgrade policy, see 13.2 Default Batch Upgrade Policies. If NFSv3 is used, a rolling upgrade can be performed without interrupting services. If SMB 2.0 or SMB 3.0 is used, you must manually enable failover.\nA few decades ago, we had only one type of main storage technologywhich is a header and spinning-platter designthehard drive. But today, we have solid-state technology that comes in many form factors, such asmSATA,M.2, and2.5\". Storage drives are mostly an overlooked item in a computer system; but it should be held equally as important as your systems processor and RAM. With an SSD as a primary drive, computer systems boot up in well under a minute and start apps faster, when compared to other systems outfitted with a hard drive and faster processor. You might be thinking, If SSDs are so great, why would we need something better? To figure out why future drives need to offer larger capacities and faster access times than what we have now, were going to take a quick dive into computers of the future. Current computer systems operate on a binary format of 1s or 0s; however, consumer-level systems in the far future will work with this pair of digits, as well as somewhere in betweenat the same timethanks to quantum technology. This level of processing power will speed up calculations millions of times faster and can probably crack your 128-digit password before you finish typing it out. And the folks at Google already have working quantum machines for research and government use. With systems this fast and consuming vast amounts of data, storage will need to step up its game.\nWell look at a few innovations in the storage arena that can meet those requirementssome of them are ready nowwhile others are a long way off for consumer availability. One other factor about drives not mentioned, but should be, is durability. With large amounts of irreplaceable data being generated each day, preserving this content is critical. Most manufacturers rate their drives reliably in under a decade or sooner. After that time, its best to back up the contents to a newer drive. To solve this dilemma, research has been done to encode vast amounts of data on a medium that has been available for thousands of years. With a few strands of synthetic DNA, you could store all the apps, videos, photos, and more held on a computer. Using this technique, DNA-based storage drives will have two distinct major advantages: size and reliability. Theoretically, a fraction of an ounce of DNA can fit 300,000 times more data than a 1TB hard drive and hold that data for the next millennium. This storage process wont be ready for a while, but hopefully, it will keep future quantum-based systems busy when available. For technology that will be available to the average consumer soon, 96-layer 3D NAND technology has been recently developed in a partnership with Western Digital and Toshiba, and will provide larger storage capacities in SSDs than the typical 512GB and 1TB we are accustomed to seeing. This technology will be available in 3-bits-per-cell and 4-bits-per-cell architectures to deliver performance and reliability at an affordable price point.\nThe removal of redundancies from data before or while it is written to a backup device is referred to as inline deduplication. It decreases the quantity of redundant data in an application as well as the backup disc targets' capacity requirements. Inline deduplication was created to reduce storage overhead for backup data. Data deduplication began as a technique to save space for backup files. Inline deduplication analyses fresh data ready to be transmitted to storage against data that already exists in storage and discards any redundant data it finds, whether it employs the hash identifier approach or the byte-level comparison method. The inline deduplication software uses algorithms to automatically attach the identifying hashes and compare them to those in the saved data. The data is saved if there is no match. Because the process occurs as data is being transferred to backup storage, it may cause performance issues when compared to post-process deduplication. This was a concern in the early days of deduplication, but contemporary processors and memory can readily manage the extra workload of inline deduplication, without any performance degradation.\nDifferences : Difference 1: The snapshot copy does not contain the data changed by the snapshot itself. The snapshot of the snapshot contains the data changed by the snapshot itself. For example: Snapshot copy: If the parent snapshot data is A, change the parent snapshot data to B, and then create a copy for the parent snapshot. In this case, the snapshot copy data is A. If the copy is reactivated, the snapshot copy data becomes the source LUN data (the source LUN data may be changed). Snapshot of a snapshot: If the parent snapshot data is A, change the parent snapshot data to B. No matter how the source LUN data is changed, create a child snapshot for the parent snapshot. In this case, the child snapshot data is B. Difference 2: A snapshot copy can be rolled back only to the source LUN or other snapshots, but cannot be rolled back to the parent snapshot. You can roll back the source LUN, parent snapshot, and other snapshots. Application scenarios: Snapshot copy application scenario: After a snapshot is created for the source LUN (data A is used), the customer performs read/write operations on the snapshot (data B is used). After the operations are complete, the customer wants to roll back data to the source LUN. In this scenario, only a snapshot copy can be created for the snapshot. Snapshots cannot be used in this scenario.\nCache is a key module for improving performance and user experience. When analyzing cache performance, pay attention to the impact of cache configuration on system write performance. The write policy of the cache is write back. When each write I/O reaches the cache, a write success response is returned to the host. The cache reorders and combines the data and then writes the data to disks. During service running, the LUN health status may be set to write protection if a battery fault occurs, the system has only one controller, the controller is overheated, or the LUN fault page exceeds the threshold. In this case, write I/O requests cannot be written to disks, but data stored in the disks can be read. When write protection is enabled, data in the hard disk is not modified, which effectively protects the hard disk data. You can query LUN properties on DeviceManager or CLI to obtain the health status and cache write policy of the LUN. On DeviceManager: To query the LUN properties on DeviceManager: To query the LUN properties on the CLI, run show lun general .\nHello team, Dear Phany, For SNS2248, if DC power supply is selected, the input should be from 40 V to 60 V, 4.5A. Power supply specifications Specification SNS2124 SNS2224 SNS2248 Maximum output 75 W. 150 W. 150 W. Input voltage AC: 85 V to 264 V, nominal 100 V to 240 V. AC: 85 V to 264 V, nominal 100 V to 240 V. AC: 85 V to 264 V, nominal 100 V to 240 V. DC: 40 V to 60 V, nominal 48 V. AC Input frequency Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. AC inrush current 21.5 Amps at 240 V AC Cold Start. Maximum of 35 A at 240 V AC for 10 ms or less. Limited to 50 A peak at 240 V AC for any initial current surge or spike of 10 ms or less at cold start-up. Any additional inrush current surges or spikes in the form of AC cycles or multiple AC cycles greater than 10 ms, and less than 150 ms, must not exceed 15 A peak. Input line protection - AC lines are fused. AC lines are fused. DC lines use -Ve&+Ve reverse polarity protection. System Power Consumption 57 W with all 24 ports populated with 8 Gbit/s SWL optics. 48 W with empty chassis and no optics.\n80 W with all 24 ports populated with 16 Gbit/s SWL optics. 60 W with empty chassis and no optics. AC: 110 W with all 48 ports populated with 16 Gbit/s SWL optics. 72 W with empty chassis and no optics. DC: 112 W with all 48 ports populated with 16 Gbit/s SWL optics. 53 W with empty chassis and no optics. Heat dissipation 277 BTU/hr. 338 BTU/hr. 32 port: 338 BTU/hr. 48 port: 375 BTU/hr. Thanks. Dear Phany, For SNS2248, if DC power supply is selected, the input should be from 40 V to 60 V, 4.5A. Power supply specifications Specification SNS2124 SNS2224 SNS2248 Maximum output 75 W. 150 W. 150 W. Input voltage AC: 85 V to 264 V, nominal 100 V to 240 V. AC: 85 V to 264 V, nominal 100 V to 240 V. AC: 85 V to 264 V, nominal 100 V to 240 V. DC: 40 V to 60 V, nominal 48 V. AC Input frequency Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. Range: 47 Hz to 63 Hz; Nominal: 50 Hz to 60 Hz. AC inrush current 21.5 Amps at 240 V AC Cold Start. Maximum of 35 A at 240 V AC for 10 ms or less. Limited to 50 A peak at 240 V AC for any initial current surge or spike of 10 ms or less at cold start-up.\nHi team, We enabled file system deduplication, but only applies to new content that is added to the filesystem. I'd like to know if there is any way to apply the process to existing files. We have activated this functionality once the filesystem is pretty full. Kind regards. Dear Phany, I can confirm that only the data that is newly written is deduplicated on the OceanStor 5500 V3 storage. Definition The SmartDedupe&SmartCompression feature is developed by Huawei to provide smart data deduplication and data compression functions. The feature is sometimes regarded as two features: SmartDedupe and SmartCompression. SmartDedupe is a data downsizing technology that deletes duplicate data blocks in a storage system to save physical storage capacity, meeting growing data storage needs. The 5300 V3/5500 V3/5600 V3/5800 V3/6800 V3 storage system supports online deduplication, that is, only the data that is newly written is deduplicated. NOTE: The data that is newly written is the data written after file system SmartDedupe is enabled. SmartCompression reorganizes data to reduce storage space consumption and improve the data transfer, processing, and storage efficiency without any data loss. The 5300 V3/5500 V3/5600 V3/5800 V3/6800 V3 storage system supports online compression, that is, only the data that is newly written is compressed. NOTE: The data that is newly written is the data written after the file system SmartCompression is enabled. The only way to have the old data could be deduplicated would be to copy and write again.\nHere, we divided the storage components into three sections according to the abstraction level to which they belong: Lower-level layers Middle-level layers Higher-level layers The below figure shows an example of each networking stack. Lower-level layers As Figure a. shows, only three stacks can directly interact with the physical wire: Ethernet, SCSI, and Fibre Channel. Because of this configuration, these models are considered the lower-level layers. All of the other stacks are combinations of the layers, such as Internet SCSI (iSCSI), Fibre Channel over IP (FCIP), and Fibre Channel over Ethernet (FCoE), which are also called the middle-level layers. Ethernet is typically used on conventional server-to-server or workstation-to-server network connections. The connections build up a common-bus topology by which every attached device can communicate with every other attached device by using this common bus. Ethernet speed is increasing as it becomes more pervasive in the data center. Middle-level layers The middle-level layer consists of the transport protocol and session layers. Internet Small Computer System Interface (iSCSI) is a transport protocol that carries SCSI commands from an initiator to a target. The iSCSI data storage networking protocol transports standard SCSI requests over the standard Transmission Control Protocol/Internet Protocol (TCP/IP) networking technology. iSCSI enables the implementation of IP-based SANs, enabling clients to use the same networking technologies, for both storage and data networks. Because iSCSI uses TCP/IP, iSCSI is also suited to run over almost any physical network.\nBy eliminating the need for a second network technology just for storage, iSCSI has the potential to lower the costs of deploying networked storage. The Fibre Channel Protocol (FCP) is the interface protocol of SCSI on Fibre Channel (FC). It is a gigabit speed network technology that is primarily used for storage networking. Fibre Channel is standardized in the T11 Technical Committee of the International Committee of Information Technology Standards (INCITS), an ANSI-accredited standards committee. FCP started for use primarily in the supercomputer field, but FCP is now the standard connection type for SANs in enterprise storage. Despite its name, Fibre Channel signaling can run on both twisted-pair copper wire and fiber optic cables. Fibre Channel over IP (FCIP) is also known as Fibre Channel tunneling or storage tunneling. It is a method to allow the transmission of Fibre Channel information to be tunneled through the IP network. Because most organizations already have an existing IP infrastructure, the attraction of being able to link geographically dispersed SANs, at a relatively low cost, is enormous. FCIP encapsulates Fibre Channel block data and then transports it over a TCP socket. TCP/IP services are used to establish connectivity between remote SANs. Congestion control and management and also data error and data loss recovery are handled by TCP/IP services and do not affect Fibre Channel fabric services. The major consideration with FCIP is that it does not replace Fibre Channel with IP; it allows deployments of Fibre Channel fabrics by using IP tunneling.\nHello all, Here are some FAQs about how to handle the hard disk bad sector problem. Q: What is a bad sector of a hard disk? Q: What are the causes of bad sectors? Q: What types of bad sectors are classified based on bad sector attributes? Q: How to repair bad sectors after they are generated? A: A bad sector refers to a small area of a hard disk that reports errors and cannot be accessed successfully. The size of the bad sector is 512-bit. New bad sectors (also called bad sectors) are often caused by some type of entity corruption. If a document or folder uses this bad sector, it indicates that the document has become incomplete or corrupted due to unreadable bytes. A: The most common causes of bad sectors are as follows: The original quality of the drive is flawed. Due to technical reasons, mechanical hard disks cannot achieve 100% non-bad sectors during production, and some physical bad tracks are usually on the chip. The manufacturer will list such bad tracks in the factory bad track list and completely isolate them and will no longer be used. Such bad tracks are referred to as factory bad sectors. Poor personal use habits, such as collision, frequent data reading and writing, improper computer shutdown, forced power-off, and so on, may cause bad sectors of the hard disk. Unstable voltage: When a disk is running, the disk rotates at a high speed by using the current.\nThen, the magnetic head moves to the disk to generate static electricity to read and write data. If the voltage is unstable, the disk and magnetic head are damaged, causing bad sectors. If the hard disk is overheated, the control circuit overheats and damages the normal running of the hard disk. There is too much dust in electronic products, and hard disks are no exception. If there is a high dust environment, some dust will run into the disk, which will affect the read and write of the disk head, causing bad sectors to spread. Sonic or ultrasound is probably one of the hard drive killers. A: There are logical bad sectors and physical bad sectors. Logical bad sector Such bad sectors are usually caused by improper read and write operations of software. Improper I/O of some software may make sector data disorder, even endanger the partition table and the important data of the hard disk, resulting in the failure to read and write the sector. If the hard disk is powered off forcibly, the data being written to the hard disk will be written to the incorrect sector, resulting in incorrect sector data. Some unqualified software may cause logical bad sectors even if it runs normally. Physical bad sector There are two types of bad sectors, one is the factory bad sector and the other is the bad sector generated during use. With current technology, the hard drive line cannot produce 100% bad sector-free hard drives.\nWorkers moving discs, small vibrations during machine production, etc., have some flaws on the hard drive. These sectors are isolated during the low-level formatting of the hard disk and are never used. Therefore, these sectors have little impact on the quality of the hard disk. If the hard disk is shaken violently during use, the magnetic head scratches the disk, causing physical bad sectors or even serious disk scratches. If the hard disk is degraded, the magnetic function inside the disk is attenuated. If the magnetic head becomes unstable, the disk will be damaged and dust will enter the hard disk body, which will damage the disk surface and further generate bad magnetic area, which will seriously affect the disk performance and data security. A: Repair logical bad sector Use the built-in tool of the operating system to repair the file system. The hard disk can still be used. If logical bad sectors cannot be directly scanned for recovery, you can perform low-level formatting on the disk to repair all logical bad sectors on the disk and isolate physical bad sectors. Repair physical bad sector Also called a hardware bad sector, usually, a physical change occurs on a disk, or a disk head cannot read data normally or takes a long time to read data. If the data can be read, back up the data as soon as possible to prevent bad sectors from spreading. The most serious damage is that the magnetic head collides and the data cannot be read.\nHello team, We inquiry about the optimum network topology: bond and failover configuration for OceanStor Dorado 5000 V6. Thanks. Dear Axe, Regarding the bond port details, the bandwidth increase depends on the use of multiple sessions, not on the bond port function itself (which is based on IEEE 802.3ad protocol). But the link redundancy when using bond port will work no matter the application, if correctly configured (meaning that when a port/link fails, the other in the bond relationship will continue to provide services). And if you plan to connect to a share from multiple hosts, then multiple sessions will be established and you will also benefit from increased the bandwidth of the bond port function. Finally, from the provided details I understand that your main requirement is IP failover, which is theoretically discussed here and succinctly exemplified here. In this case, your proposed topology will be optimal, because you will benefit from both the bond port and IP failover redundancy, and the bandwidth increase if multiple sessions are used. Configuration process: First, add both service ports of the same controller to a bond port. Do this operation for both controllers ports, so you will end up with two bond ports (for example named CTE.A and CTE.B). Now create the logical port, which only one will exist. This logical port will be used to mount the shares, for example in your Windows hosts.\nDear all, This post shares the HPCSolution of HPE. Solution : Lustre + MSA low-end storage + D6020 high-density enclosure Customer-oriented : Small- and mid-range customers, sensitive to costs Customized solution : ClusterSTOR E1000 (integrated delivery) Customer-oriented : High-end customers, mainly in North America Ultimate bandwidth : 80 GB/s per 2 U O&M tool : Cray ClusterStor data services While NFS remains the most widelyadopted file system, it has dropped from being utilized at 54% of the sitesdown to 46%. NFS was one of the first file systems that could handle theinitial scale of early HPC systems. Its first mover status, coupled with thefact that its still adequate in smaller scale HPC systems today, is reflectedin its continued, albeit shrinking, wide adoption. Lustreutilization has grown from 21% to 32.5%, and Lustresopen-source approach has enabled it to mature its feature set across a widenumber of areas including performance, resiliency, reliability, andscalability. This maturity has also given it stability and the capability ofscaling to meet the demands of petascale andemerging exascale configurations. GPFS/Spectrum Scale adoption hasgrown at a slightly lower rate, from 23% to 26.8%. This modest growth inadoption can be attributed to a slower feature advancement pace due to itsproprietary nature along with a market shift away from IBMs dominance in theHPC sector and a change in the pricing model away from user-license-based tocapacity-based. This data confirms the selection of Luster asthe parallel file system for the new HPC storage for the new era: Cray ClusterStorE1000 storage system.\nHello everybody, Today we are talking about the most popular and recent topic: the Metaverse. It may look like an enhanced version of virtual reality (VR) - but some people think that the Metaverse may be the future of the Internet. In fact, people believe that VR is what modern smartphones are to the first bulky phones of the 1980s. In Metaverse, you can use headphones to enter a virtual world connected to various digital environments, instead of a computer. Unlike VR, which is currently mainly used for games, this virtual world can be used for almost anything - work, games, concerts, movie trips - or just hanging out. Most people imagine that when you use it, you will have a 3D avatar - your own appearance. The concept of Metaverse originated from science fiction or pointed to the \"ultimate form\" of the Internet. The term comes from Neal Stephenson's science fiction novel \"Snow Crash\" in 1992. It describes a world where people interact with various softwares in a three-dimensional space as virtual images. At present, games of the Metaverseconcept, especially blockchain games - due to the limitations of technology and practitioners, the rendering, game content and gameplay creation - are far behind mobile games and terminal based games; blockchain games need to be distributed. Typical storage also causes the game to respond slowly, so the experience is not good. In fact, many games, especially RPG games, have created a virtual world with their own social scenes, upgrade modes, their own worldview, their own economic system.\nSome games offer the opportunity for users to create their favorite avatars, change faces, clothes, get upgrades, purchase equipment, socialize...even get married, have children, raise pets and join adventures with your brothers-in-arms. In fact, in addition to not having an open and co-constructed ecosystem, they can be regarded as a virtual world created by themselves. Therefore, many people think that game companies may be the most suitable to take the lead in creating a Metaverseworld and become the entrance to the meta-universe. Ready Player One -Directed by Steven Spielberg It is a decentralized open-source ecosystem on the chain. The application scenarios are not just entertainment, but socializing, studying, working, shopping, watching dramas, watching exhibitions, sports and even investment and financial management on the platform. Parallel digital world. In order to realize such a Metaverse, multiple forces are needed to build it together, so no giant or individual can complete this work independently. Therefore, we believe that games and VR are only the entrance to the Metaverse, but besides games and VR, there are many investment opportunities. There are many ways to classify Metaverse. Next, I will try to classify them according to the logic of investors looking for projects and explain what I think about the investment opportunities of Metaverse - the will be slightly repeated due to different classifications. Roughly speaking, Metaverse includes several technologies, for example, chip technology, network communication technology, virtual reality technology (VR/AR/MR/XR), game technology (game engine, game code, multimedia resources), AI (Artificial intelligence) technology, blockchain technology.\nHello all, The HUAWEI DEVELOPER CONFERENCE 2021 (TOGETHER) was held on October 22. We've got an incredible lineup of sessions this year. Together, we'll explore what's new with HarmonyOS, smart homes, smart offices, HMS Core and more! We can make a new Metaverse. Join us for a hands-on experience with tech that's driving the future. Of course, in order to understand what Huawei is doing, we must first know what a \"parallel world\" is. First of all, the well-known \"Spider-Man: Into the Spider-Verse\" has brought the setting of a \"parallel world\". To put it simply, the early setting of \"Parallel World\" itself was the original author's attempt to free the superheroes from the old-fashioned characters, such as the entire villain US team, the darkened Spider-Man, the female version of Deadpool and so on. At the same time, as the parallel world was realized, the technology circle gave birth to the \"virtual parallel world\"-Metaverse. In the future, most people will live in another parallel world, which is connected to the real world, si this will greatly satisfy peoples spiritual needs. Seeing this, someone must ask: \"Isn't this bullshit?\nCould it be that if you put this as an internet-addicted teenager, then the whole immersive surfing can satisfy your spiritual needs?\" Facebook : Become a Metaverse company in the next five years Alibaba : Apply for trademarks of \"Ali Yuan Universe\" and \"******* Yuan Universe\" Tencent : The next upgrade is \"True Internet\" Mihad Tour of \"Original God\" : In 2030, a virtual world will be created in which one billion people around the world are willing to live This key technology derived from Huawei's self-developed algorithm and it can use an ordinary RGB mobile phone to finely segment the object and the background, while also generating a high-precision 3D geometric model with only one click. Speaking of \"low lag\", the same \"just-needed\" metaverse also puts forward high requirements for \"always online\" and \"real-time rendering\". To put it bluntly, the network must not only be fast, but also very stable~ Coincidentally, Huawei, which has a communications background, really has a \"killer\" in this regard. Wireless Kit network environment prediction: According to the user's personal network situation, it can intelligently match the number of threads, iP routing and timeout time. To put it bluntly, it is tailored to avoid congestion. Network Kit access behavior prediction: It can predict network access behaviors, warm up the network in advance, thereby reducing waiting, which is like planning navigation and warming up the car before going out, saving additional time.\nGood day Community! This post is about tackling the issue whenOceanStor Dorado 18000 V6 has a different reduction ratio in 2 DHs. Please see the below sections for more details. Two OceanStor Dorado 18000 V6 6.0.1 SPH21 storage devices form an active-active mode. The data reduction ratios of the two storage devices differ greatly. 1. For example, the data reduction ratio of the DH1 site is lower than that of the DH2 site. 2. The reason why the data reduction ratio of DH1 is lower than that of DH2 is that the post-deduplication progress of all controllers at DH1 is slower than that of DH2 . 3. The reason for the slow post-deduplication is that the concurrent post-deduplication tasks at DH1 are always limited to the minimum. 4. Further analyzed the logs. It was found that a large number of asynchronous remote replication synchronization and snapshot creation and deletion flow control tasks existed in the logs of DH1 site. 3 indicates a synchronization task of asynchronous remote replication. 14 and 29 indicate a snapshot creation or deletion task. If there are so many snapshot creation and deletion tasks, the snapshot is an internal snapshot of asynchronous remote replication. 5. The priority of post-deduplication tasks is lower than that of snapshot creation and deletion tasks. Therefore, when internal snapshots of asynchronous remote replication are frequently created and deleted, the scheduling is occupied. As a result, the concurrent restoration tasks of post-deduplication cannot be scheduled by flow control. DH1 and another storage form an asynchronous remote replication.\nHi Shahid3333! The answer are BC . RAID 0 (also known as a stripe set or striped volume) splits ('stripes') data evenly across two or more disks, without parity information, redundancy, or fault tolerance. RAID 1 consists of an exact copy (or mirror) of a set of data on two disks; a classic RAID 1 mirrored pair contains two disks. This configuration offers no parity, striping, or spanning of disk space across multiple disks, since the data is mirrored on all disks belonging to the array and the array can only be as big as the smallest member disk. RAID 5 consists of block-level striping with distributed parity. Unlike in RAID 4, parity information is distributed among the drives. It requires that all drives but one be present to operate. Upon failure of a single drive, subsequent reads can be calculated from the distributed parity such that no data is lost. RAID 5 requires at least three disks. RAID 6 extends RAID 5 by adding another parity block; thus, it uses block-level striping with two parity blocks distributed across all member disks. Hi Shahid3333! The answer are BC . RAID 0 (also known as a stripe set or striped volume) splits ('stripes') data evenly across two or more disks, without parity information, redundancy, or fault tolerance. RAID 1 consists of an exact copy (or mirror) of a set of data on two disks; a classic RAID 1 mirrored pair contains two disks.\nDear all, This post is an introduction of Linux Dirty Pipe Vulnerability (CVE-2022-0847). Vulnerability disclosure link: https://dirtypipe.cm4all.com/ On March 8, 2022, the open-source community revealed a new first-level security vulnerability in the Linux kernel, Dirty Pipe Vulnerability . The vulnerability was introduced in the Linux kernel version 5.8, which can 'kill' subsequent versions of the kernel, as well as all Downstream vendor products that use the relevant Linux kernel. Of course, it is necessary to note that the kill here is not a remote command execution for any system, but a kernel-oriented exploit attack. The final effect is to write no more than one page of content to any readable file! So when can we attack the kernel directly? There are so many scenes like this! For example, in our usual development/build server, we can directly perform privilege escalation (obtain root privileges) based on the ssh shell connection. The following figure is a common user-oriented cloud server environment: A typical privilege escalation is to execute a malicious program to escalate the uid to root privileges. Another scenario is that after the attacker remotely breaches the system and seizes the control of the business process, he further escalates his rights to root and takes complete control of the system. In order to avoid getting caught in the details (in fact, the main reason is that the reader does not have the patience to read it), before the detailed code analysis, it is better to explain the core idea of this vulnerability in vernacular.\nfirst of all, we need to align some basic terms first. The first is the pipe pipeline mechanism, because since the name of the vulnerability is \"pipeline vulnerability\", it must be related to pipe. In Linux, Pipe pipes are divided into named pipes and anonymous pipes. Let us first intuitively feel what a named pipe is, as shown in the following figure: A named pipe is an entity file with a name. Take another look at anonymous pipes: The pipe character command we often use | actually creates an anonymous pipe. Redirect the output of process 1 to process 2. [To avoid complications, the processing description of shell process transfer is ignored here] Although Pipe has many forms, in essence, a pipe is a means of inter-process communication, allowing two processes to send and receive data through a pipe. The second prerequisite knowledge that needs to be known is the kernel's Page Cache mechanism (cache management mechanism). We know that the IO read and write speed of the disk is very slow, so generally when we access a disk file, we first load its content into physical memory, and subsequent accesses directly take the copy in memory to read data. Because of the memory copy of a file, it may be opened and used by many processes later. (Think about it, usually our WeChat software may open a local text, and the word software may also open the same text).\nHello there, Today I want to share you Huawei OceanStor 9000 Scale-Out NAS Technical White Paper. In the data explosion era, data available to people has been increasing exponentially. Traditional standalone file systems have to add more disks to expand their capacity. Such file systems are no longer capable of meeting modern storage requirements in terms of capacity scale, capacity growth speed, data backup, and data security. New storage models are introduced to resolve this issue: Centralized storage File metadata (data that provides information about other data, such as the file location and size) and data information are stored centrally. Back-end SAN and NAS are mounted to front-end NFS. This model of storage system is difficult to expand, not to mention providing petabytes of capacity. Asymmetrical distributed storage It has only one metadata service (MDS) node, and stores file metadata and data separately. Such storage systems include Lustre and MooseFS. One issue with a single MDS node is single point of failure, which can be avoided using heartbeat mechanism, but the performance bottleneck with single-point access is inevitable. Fully symmetrical distributed storage It employs a fully symmetrical, decentralized, and distributed architecture. Files on storage devices can be located using the consistent hash algorithm, an implementation of distributed hash table (DHT). Therefore, this model of storage system does not need to have an MDS node. It has storage nodes only and does not differentiate between metadata and data blocks.\nihave storage How can i cllecting network information how to use Dear user, The information to be collected includes the basic information, fault information, storage device information, network information, and application server information. Collect the types of information specified in Table 3-1 and send the collected information to maintenance engineers. Table 3-1 Types of information to be collected Information Type Item Action Basic information Device serial number and version Provide the serial number and version of the storage device. You can log in to the DeviceManager and query the serial number and version of the storage device in the General area. Customer information Provide the customer's contact person and contact means. Fault information Occurrence time Record the time when a fault occurs. Symptom Record the symptom when a fault occurs, for example, an error dialog box or an event notification. Operations performed before a fault occurs Record the operations performed before a fault occurs. Operations performed after a fault occurs Record operations that are performed before reporting the fault to maintenance personnel. Storage device information Hardware module configuration Record the configuration of the hardware modules in the storage device. Indicator status Record status of the storage device indicators, especially the indicators in orange or red. For details about indicator status, see the of the corresponding product model. System data Manually export the operating data, and system logs of the storage device. Alarms and events Manually export the alarms and events of the storage device.\nIs there any difference for LunGroups and Luns also HostGroups and Hosts??? we are having a strange issue with Vmware 7.x and with 3 hosts directly connected to dorado 3000 v6. If we map lun to 3 host oneby one,we can expand the disk on dorado ,but cannot do it on Vmware site; If we use Lungroup and Hostgroups , we can do both of them without any problems? was thinking that , the lungroups and hostgroups are just for less maintaince tasks but i guess there are more behind Hello, dear! The basic information about LUN, LUN group, Host, and Host Group are as follows: LUN Storage space in a storage pool is divided into logical units called LUNs. A host can use storage space provided by LUNs after LUNs are mapped to it. LUN group A collection of multiple LUNs. If the data of an application is stored on multiple LUNs, you can create a LUN group for these LUNs. Operations on a LUN group apply to all its member LUNs. A LUN group can contain one or more LUNs. Host A physical or virtual machine that can access a storage system. Host group A collection of multiple hosts. If an application is deployed on a cluster consisting of multiple hosts, these hosts will access the data volumes of the application at the same time. In this case, you can create a host group for these hosts. A LUN can be mapped to only one host.\nALUA is an industry-standard protocol for determining optimal pathways between a storage system and a host. ALUA allows the initiator to inquire about path parameters such as primary path and secondary path from the target. It also enables the target to relay events to the initiator. It is advantageous because multipathing software can be written to support any storage array. To establish primary and secondary pathways, proprietary SCSI commands are no longer required. When ALUA is enabled, the host multipathing software categorizes physical pathways to discs as Active Optimized (AO) or Active Non-optimized (AN). The AO pathways are preferred by the host for delivering services to the storage system. An AO path is the best I/O access path between a host and the LUN's owning controller. An AN path is a suboptimal I/O access path that connects the host to a non-owning controller. Huawei's HyperMetro active-active storage solution allows two storage systems to process services concurrently, providing a mutual backup relationship between them. If one storage system fails, the other will take over services automatically, with no data loss or disruption. When you adopt HyperMetro, you won't have to worry about your storage systems' incapacity to instantly transfer over services between them, and you'll experience rock-solid reliability, improved service continuity, and increased storage resource utilisation.\nNow we are going to look for failover of HyperMetro Scenario and non-Hyper metro Scenario There are two types of failures: Path Failure Lun Owing Controller Failure/SP Failure The Active optimize paths are the main points so If one of the AO paths fails, the host will route I/Os to another AO path. If all of the AO paths on the owning controller fail, the host will provide I/Os to the AN paths on the non-owning controller, as seen in \"Path failure.\" If the owner controller of a LUN fails, the system will activate the other controller as the new owning controller, as illustrated in the \"SP failure\" section. ALUA Working Principles and Failover in HyperMetro Scenario: There are two modes Load Balancing Mode Local preferred Mode When HyperMetro is in load balancing mode, the host multipathing software designates the pathways to the owning controllers on both HyperMetro storage arrays as AO paths and the paths to the other controllers as AN paths. The AO routes are used by the host to access the storage arrays. If one of the AO paths fails, the host will route I/Os to another AO path. If the controller that owns a LUN fails, the system will activate the other controller to maintain load balancing. When HyperMetro is configured to operate in local preferred mode, the host multipathing software specifies the pathways to the owning controller on the local storage array as AO paths.\nBasic SSD Features SSD Controller Architecture NAND flash devices. A typical SSD controller consists of a host interface control block, a flash translation layer (FTL)control block, and a NAND control block. See Figure 1. Figure 1 The architecture of SSD The architecture of an SSD consists of several components that work together to provide fast and reliable storage. Controller : The controller is the brain of the SSD and manages the flow of data between the host system and the NAND flash memory. It also performs wear leveling, error correction, and other functions to ensure the reliability and longevity of the SSD. NAND flash memory : NAND flash memory is the type of memory used in SSDs. It is a non-volatile memory, meaning that it retains its data even when power is removed. NAND flash memory is organized into pages and blocks, and data is written and erased in blocks. DRAM cache: Some SSDs include a DRAM cache to temporarily store data that is being read or written. This helps to speed up data transfer and improve overall performance. Interface: The interface is the connection between the SSD and the host system. The most common interfaces used in SSDs are SATA, PCIe, and NVMe. Firmware : The firmware is the software that runs on the SSD controller and manages its operation. It includes algorithms for wear leveling, error correction, and other functions that are essential for the performance and reliability of the SSD. The architecture of an SSD is designed to minimize latency and maximize throughput.\nNumbers are always the way to major a quantity to analyze something or make a decision, in computer system terminology there are several number systems such as Decimal system, Hexadecimal, and binary system. although computer uses all of them in different scenarios but most commonly used is the binary system that is in the base of 2. The other systems such as hexadecimal are used in storing images whereas the decimal number system has a base of ten. Numbers written without a base are decimal numbers by default. Here is description of the binary number system which is used mainly in computer systems, where two means 0 and 1 to represent the states on/ off A bit is the smallest unit of measurement used in data measurement. A single bit can have either a 0 or a 1 value. It may include a binary value (for example, On/Off or True/False), but nothing else. A binary digit is a logical 0 or 1 that represents a component's passive or active status in an electric circuit. A group of 4 bits is called a nibble As a result, the fundamental unit of measurement for data is the byte or eight bits. A byte can hold 28 or 256 separate values, which is enough to represent basic ASCII characters such as letters, numerals, and symbols. 1 byte =8bits A computer word, like a byte, is a fixed number of bits processed as a unit that changes from computer to computer but is constant for each computer.\n--percent Displays the installation progress in percentage. --excludedocs Do not install the document files in the software package. --includedocs installation document --replacepkgs Forcibly reinstalls the installed software package. --replacefiles Replace files that belong to other software packages. --force Ignore conflicts between software packages and files. --noscripts Do not run pre-install and post-install scripts --prefix Install the software package to the path specified by. --ignorearch does not verify the structure of the software package. --ignoreos does not check the operating system where the software package runs. --nodeps does not check dependencies. --ftpproxy is used as the FTP proxy. --ftpport Specifies the port number of the FTP server. Common options: -v Displays additional information. -vv Displays debugging information. --root Make RPM use the specified path as the root directory so that both the preinstaller and postinstaller are installed in this directory. --rcfile Set the rpmrc file to --dbpath Set the path of the RPM document inventory to Command format: # rpm -e(or --erase) [options] pkg1... pkgN Parameter list: pkg1... pkgN (software package to be deleted) Detailed options: --test Executes only the deleted test. --noscripts Do not run pre-install and post-install scripts --nodeps does not check dependencies. Common options: -vv Displays debugging information. --root Ask RPM to use the specified path as the root directory so that both the preinstaller and postinstaller are installed in this directory.\n--rcfile Set the rpmrc file to --dbpath Set the path of the RPM document inventory to Command format: # rpm -U(or --upgrade) [options] file1.rpm... fileN.rpm Parameter list: file1.rpm... fileN.rpm (software package name) Detailed options: -h (or --hash) Output the hash mark (``#'') during installation. --oldpackage allows \"upgrading\" to an earlier version --test Performs only the upgrade test. --excludedocs Do not install the document files in the software package. --includedocs installation document --replacepkgs Forcibly reinstalls the installed software package. --replacefiles Replace files that belong to other software packages. --force Ignore conflicts between software packages and files. --percent Displays the installation progress in percentage. --noscripts Do not run pre-install and post-install scripts --prefix Install the software package to the path specified by. --ignorearch does not verify the structure of the software package. --ignoreos does not check the operating system where the software package runs. --nodeps does not check dependencies. --ftpproxy is used as the FTP proxy. --ftpport Specifies the port number of the FTP server. Common options: -v Displays additional information. -vv Displays debugging information. --root Ask RPM to use the specified path as the root directory so that both the preinstaller and postinstaller are installed in this directory. --rcfile Set the rpmrc file to --dbpath Set the path of the RPM document inventory to Command format: # rpm -q(or --query) [options] Parameter list: pkg1... pkgN (Querying Installed Software Packages) Detailed options: -p (or ``-'') Queries the software package file. -f Queries the software package. -a Queries all installed software packages.\nHello all, How to quickly configure block services OceanStor 2200 V300R006? Don't worry, this post helps you quickly configure block services for OceanStor V3 series storage systems. OceanStor 2000 V3 Series: OceanStor 2200 V3/2600 V3 OceanStor 5000 V3 Series: OceanStor 5300 V3/5500 V3/5600 V3/5800 V3 OceanStor 6000 V3 Series: OceanStor 6800 V3 OceanStor 18000 V3 Series: OceanStor 18500 V3/18800 V3 1. A maintenance terminal is connected to the storage system through a management network port or the management network port of SVP (18000 series storage systems). You can manage and maintain the storage system on the maintenance terminal running the DeviceManager program developed by Huawei. 2. The storage system provides storage space for application servers 3. Storage systems can be connected to application servers running different operating systems including Windows, Linux, and UNIX over an Internet Small Computer Systems Interface (iSCSI), Fibre Channel (FC), or InfiniBand (IB) network. According to data transmission protocols, an application server functions as the initiator for data transmission, and a storage system serves as the target for the information. The initiator sends data read and write requests to the target. The target receives, processes, and responds to the requests. 4. Application servers run client programs. The storage system can connect to application servers running different operating systems including Windows, Linux (SUSE, and Red Hat), UNIX (Solaris, AIX, and HP-UX), and VMware ESXi. 1. The storage system automatically identifies all disks. 2. Disk domains are composed of the same or different types of disks.\nHello, everyone! Today, I would like to share on the need for edge storage. Enterprise IT depts are, understandably, concentrated on datacenter storage and cloud infrastructure. These are massive, scalable systems that are centrally administered. Nevertheless, it is not always practicable or desirable to move data from the periphery to the center. The massive amounts of data generated by connected sensors, Cctv surveillance, and other IoT devices might easily overtake network connectivity. Even if this were not the case, latency, service quality, and reliability are all reasons to avoid centralizing all data. Processing data locally and just delivering a subset of it such as results, exceptions or fault data, or even time-based data samples minimizes bandwidth usage. However, it requires effective local storage. We've seen an explosion in IoT (internet-connected devices) in recent years, which has been the primary driving behind edge computing solutions. Edge computing has enabled data storage to be fairly close to where it's gathered as more gadgets connect to the internet and require real-time computing capability. Edge caching is another type of edge storage that the telecoms industry is looking into. Data is temporarily kept here before being moved to other, centralized or cloud-based systems for processing. Edge caching could be used for data compression, encryption, and load balancing, among other things. In the media and internet industries, edge caching is already used in content delivery networks (CDNs).\nDear team, I want to know how to connect a Serial Port of Dorado 3000 v6via aserial cableto manage and maintain. Thanks. Dear Axe, There are two types of serial cables: RJ-45 to DB9 serial cable and double RJ-45 serial cable. Choose one according to the type of the serial port on the maintenance terminal. Typically, the storage system is connected to a maintenance terminal through an RJ-45 to DB9 serial cable, as shown in Figure 1 and Figure 2. Figure 1 Connecting a serial port to a maintenance terminal (applicable to Dorado 3000 V6) Figure 2 Connecting a serial port to a maintenance terminal (applicable to Dorado 5000/6000 V6) Procedure Wear an ESD wrist strap, ESD gloves, and ESD clothes. Prepare a serial cable used to connect the serial port on the controller enclosure to the maintenance terminal, and attach a label to the serial cable. Connect the RJ-45 end of an RJ-45 to DB9 serial cable to the serial port on the controller enclosure. Connect the DB9 end of the RJ-45 to DB9 serial cable to the serial port on the maintenance terminal. Thanks. Dear Axe, There are two types of serial cables: RJ-45 to DB9 serial cable and double RJ-45 serial cable. Choose one according to the type of the serial port on the maintenance terminal. Typically, the storage system is connected to a maintenance terminal through an RJ-45 to DB9 serial cable, as shown in Figure 1 and Figure 2.\nDear all, How do I detect and mitigate Red Hat Linux CVE-2021-4034? Thanks. Dear Phany, A local privilege escalation vulnerability was found on polkit's pkexec utility. The pkexec application is a setuid tool designed to allow unprivileged users to run commands as privileged users according predefined policies. The current version of pkexec doesn't handle the calling parameters count correctly and ends trying to execute environment variables as commands. An attacker can leverage this by crafting environment variables in such a way it'll induce pkexec to execute arbitrary code. When successfully executed the attack can cause a local privilege escalation given unprivileged users administrative rights on the target machine. For customers who cannot update immediately and doesn't have Secure Boot feature enabled, the issue can be mitigated by executing the following steps: 1. Install required systemtap packages and dependencies as per - pointed by https://access.redhat.com/solutions/5441 2. Install polkit debug info: debuginfo-install polkit 3. Create the following systemtap script, and name it pkexec-block.stp: probe process(\"/usr/bin/pkexec\").function(\"main\") { if (cmdline_arg(1) == \"\") raise(9); } 4. Load the systemtap module into the running kernel: stap -g -F -m stap_pkexec_block pkexec_block.stp 5. Ensure the module is loaded: lsmod | grep -i stap_pkexec_block stap_pkexec_block 434176 0 6.\nIn a storage system, eight NL-SAS disks form a RAID 6 group and LUNs are created on it. The default prefetch policy is intelligent prefetch, and the write policy adopts write back with mirroring. A dd command is executed on a Red Hat host to test the performance of reading data from raw LUNs mapped from the storage system. The command output indicates that the host has low read performance (only about 150 MB/s). Why? Cause: On the storage array side, a user views the LUN performance data and finds that I/O pressure on the LUNs is light with only 1024 IOPS. This fault is not caused by a performance bottleneck on the storage array. Solution: Perform the following steps to optimize the performance: Log in to the Red Hat host as user root , and run the dd if=/dev/sds of=/dev/null bs=256K command to set the block size. Run the dd if=/dev/sds of=/dev/null bs=256K iflag=direct command to set the I/O mode to direct I/O (Set an O_DIRECT flag when starting a block device). Changing block device scheduling algorithms: Run the echo noop /sys/block/sd*/queue/scheduler command, where sd* can be sdc , sdd , or sds , depending on your actual conditions. Application scenario: In most existing cases, the scheduling algorithms of host block devices are set to noop, so that I/O sequencing and consolidation can be implemented on the storage systems rather than the host block devices. Adjusting prefetch policies: Modify read_ahead_kb in the command /sys/block/sd*/queue/read_ahead_kb .\nIs it possible bound ports over controllers ? I just want to access cifs over one ip. 2 10 gbit adapters are available. ..Chris Dear sprinchr, Yes, you can bond port. Port bonding provides more bandwidth and redundancy for links. After Ethernet ports are bonded, MTU changes to the default value and you must set the link aggregation mode for the ports. On Huawei switches, you must set the ports to work in static LACP mode. NOTE: 1. The port bond mode of a storage system has the following restrictions: On the same controller, a bond port is formed by a maximum of eight Ethernet ports. Only the interface modules with the same port rate (GE or 10GE) can be bonded. The port cannot be bonded across controllers. Non-Ethernet network ports cannot be bonded. SmartIO cards cannot be bonded if they work in cluster or FC mode or run FCoE service in FCoE/iSCSI mode. The MTU value of the SmartIO port must be the same as that of the host. Read-only users are unable to bind Ethernet ports. Each port can be added only to one bond port. Physical ports are bonded to create a bond port that cannot be added to the port group. 2. Although ports are bonded, each host still transmits data through a single port and the total bandwidth can be increased only when there are multiple hosts. Determine whether to bond ports based on site requirements. 3. The link aggregation modes vary with switch manufacturers.\nWhat is the Function of the High and Low Watermarks in the Cache? How to set their values . The high or low watermark of a cache indicates the maximum or minimum amount of dirty data that can be stored in the cache. An inappropriate high or low watermark of a cache can cause the write performance to deteriorate. When the amount of dirty data in the cache reaches the upper limit, the dirty data is synchronized to disks at a high speed. When the amount of dirty data in the cache is between the upper and lower limits, the dirty data is synchronized to disks at a medium speed. When the amount of dirty data in the cache is below the lower limit, the dirty data is synchronized to disks at a low speed. Do not set a too large value for the high watermark. If the high watermark value is too large, the page cache must be small. When the front-end I/O traffic surges, the I/Os become unstable and the latency is prolonged, adversely affecting the write performance. Do not set a too small value for the low watermark. If the low watermark value is too small, cached data is frequently written to disks, reducing the write performance. Do not allow a too small difference between the high and low watermarks. If the difference is too small, the back-end bandwidth cannot be fully utilized. The recommended high and low watermarks for a cache are 80% and 20%, respectively.\nDear all, Now Id like to share with you the history ofAll-flash Storage. It has been 15 years since SSDs started to enter the enterprise storage mix. The costs of SSDs have gradually decreased and reliability of the medium has risen rather dramatically since then as more and more models have been added. Popular vendors and start-ups are providing their own versions of all-flash array products. In looking at how all-flash storage has evolved, five major leaps have taken place: SSD RAW, SSD Hybrid, SSD Optimized, and SSD Native, and SCM. All-flash Storage: Thefirst generation in design simply focused on how to make use of the high performanceof SSD and the high IOPS it yields along with the increases in bandwidthcapacity, lowered latency, and so on. The system itself did not offer anyenterprise-class features and relied exclusively on the host side to providedata protection. Since no value-added features were offered, this generation ofSSD could only graft in the large-capacity and high speeds of SSD intothe overall mix. The medium was not used much with the applications running inproduction systems. SSD layouts required improved reliability and disasterrecovery capabilities before becoming more mainstream and the deployment andO&M on the client side required further simplification. The newness of merely injecting higher performance into theuse case started to wane as enterprises waited for improved functionality. Representativevendors: TMS; Violin Memory Second-Gen:SSD Hybrid Thesecond-generation of all-flash storage made some improvement on theinadequacies in the first generation and placed all-SSD configurations into thegeneral-purpose array.\nSuch enterprise-class features as remote replication,snapshot, and thin provisioning were also made available. The advantages anddisadvantages were still rather apparent. This generation of flash productsfully inherited the features that came with the first generation (includingseamless interconnection to traditional HDDs) while requiring no change in userhabits. The disadvantage was that the design was still based in conventionalHDD architectures with only minor improvement geared to the attributes offlash. This meant that the full performance capabilitiesof SSD remained unleashed. At-scale deployments of this generation requiredseveral million USD in investment, which is why many enterprises still held offon wide adoption. Representativeproducts: IBM Storwize F-series; HP 3PAR 7450; EMC VNX-F Third-Gen:SSD Optimized Asmore enterprise warmed up to the maturing SSD technology, the third generationfeatured plenty of optimizations geared specifically to the attributes of themedium. However, the efficiency in how data is stored became the mainbottleneck for this generation and the high price kept many for adoptingall-flash in their data centers. Also, this generation generally lackedcost-lowering data reduction capabilities. System performance also degraded asprotection capabilities increased (the better the RAID protection, the lowerthe system performance). Enabling other data protection features such assnapshot lowered system performance even further. Although not perfect, manycustomers placed their performance-sensitive online transaction services on theSSD-optimized zones even with the tradeoffs in performance from enablingcertain functions. Representativeproducts: EMC VMAX-F/Unity-F series; HDS VSP-F series; HP 3PAR 8450 Fourth-Gen: SSS Native Thenew designs optimized for flash established the medium as the cost-saver andperformance injector for organizations.\nHello all, This case is about thatCIFS shares become inaccessible and start asking for username and password inOceanStor 2200 V300R006C20SPC100. The problem is that customer presents CIFS Shares for the SQLServers in order to dump the backups (unauthenticated access to the CIFS) and he finds that after two or three days the shares become inaccessible and start asking for username and password (although they are not configured with authentication). Just \"uncheck\" and \"check\" the CIFS Service box (see attached screenshot) and the service will start working again. In the log of controller 0A, there are thousands of log records as below. It indicates that clients build above 11000 sessions to controller 0A of storage, it exceeds the limitation of each controller, then storage rejects the connection, in fact, each controller can just support 11000 sessions. If you disable the CIFS service on the storage side, storage will release all sessions, a new connection will work then. The storage boot up at 2021-10-22 08:12:36. The log was collected at about 2021-11-1 17:43:6 when the SMB connections are 4, but the sessions are 3372 Below are the four connections from different clients. Just one session on the connection from xx. xx.101.34/ xx. xx.101.35/ xx. xx.101.33 , all the other sessions are on connection from xx. xx.101.12 . So the problem is that client xx. xx.101.12 just set up sessions with storage, but never release, when all sessions of storage are used up, the client cant connect to storage anymore. Client xx.\nHello all, Today we talk about Intelligent balancing architecture in Dorado V6. The storage system uses the global cache and global storage pools. Data written to the cache is evenly distributed to all controllers. Data written to a storage pool is evenly distributed to all its member SSDs. The storage system is divided into multiple vNodes. Each vNode corresponds to a group of CPU and memory resources. For example, each controller in OceanStor Dorado 18000 V6 has four CPUs, and each CPU belongs to a vNode. In this way, CPU resource scheduling is reduced, improving CPU usage. Figure 1 Intelligent balancing architecture Intelligent Balancing Algorithm The storage system uses an intelligent balancing algorithm to balance host I/Os on vNodes. 1. The storage system calculates the hash value (shard) of a host I/O based on its LBA and the hash factor, that is: Shard = hash (I/O LBA, hash factor) 2. The intelligent distribution algorithm distributes data to different vNodes based on shard values, ensuring that host data to LUNs is evenly distributed to all vNodes. Intelligent Front-End Balancing Multi-level, efficient, intelligent distribution ensures load balancing between vNodes. When Huawei UltraPath is used, UltraPath works closely with the storage system. UltraPath uses an intelligent distribution algorithm to calculate the shard value of each I/O, searches for the vNode in the storage system based on the shard value, and distributes the I/O to the front-end link corresponding to the vNode. This prevents data forwarding between vNodes.\nBrocade switches support multiple zones, regular zones and special zones. The regular zone is what we usually call the zone. The main function is to isolate the device and divide the fabric into multiple partitions. The special zone has TI zone, QoS zone and LSAN zone. Unless otherwise stated, the zones mentioned here are all regular zones. A regular zone can be classified into a port zone, a WWN zone, and a mixed zone according to the types of members it contains. Port zone: The members of the zone are all switch ports, and each port is uniquely determined by the Domain ID and Port Index. The advantage of this type of zone is that the device connected to the switch does not need to be re-zoned after the device is replaced. However, the device needs to be re-zoned after the device is replaced with the port connected to the switch. That is, the port zone is location-dependent. Such as: zone01: (1,1; 1, 2; 1, 3). Advantages: Simple to create, easy to understand, suitable for SAN network switches with few connected devices Disadvantages: After the device is replaced with other ports, it cannot communicate with the members in the original zone. It is related to the location and is not convenient to manage in a large SAN network. WWN Zone: The member of the zone is the WWN of the device. The WWN can be the device node WWN or the device port WWN (WWPN). The WWPN is used to create the zone.\nIntroduction: Solid state drives are one of the biggest hardware advances in computer memory that have accelerated these types of systems. Its read and write speeds are several times faster than rotating disks with similar speeds of 7200 RPM or 10000 RPM. This increase in speed has not only booted and turned the system on and off, but has even affected all aspects of computers and servers. This technology has improved all its components such as software speed and system performance We recommend that you use SSDs, but there are differences between NVMe SSDs and standard SATA drives, and you should also know if all M.2 drives are classified as NVMe. In the following, we will talk to you about the differences between their types. What is NVMe? The first thing to note is that SSDs are very fast, but the only limiting factor for the speed of this type of drive is not their internal hardware, but the use of SATA III port, which is also used for rotating hard drives. Non-Volatile Memory Express, or NVMe for short, is a communication port that allows newer SSDs to read / write faster than flash memory. Basically, this technology allows the flash drive to run an SSD directly through the high-speed PCIe bus instead of the old, low-speed SATA port. In other words, this technology is the way hardware uses to better communicate with computers and servers. NVMe is not a new type of flash memory but just a new communication port.\nAlso, despite the difference in appearance, the NVMe drive can be produced with M.2 or PCIe ports. Both ports (M.2 and PCIe) use high-speed PCIe bus instead of SATA to communicate electrically with the system. Are all M.2 drives NVMe? The answer to this question is no. M.2 is just a physical feature. M.2 drives can be available in either SATA (eg Crucial MX500 M.2) or NVMe (such as Samsung 970 Pro / EVO). In fact, SATA or NVMe can be described as a gateway for electrical communication with the system. In fact, M.2 SSD drives and 2.5-inch SATA SSDs work with almost the same specifications, but as mentioned earlier, NVMe M.2 drives have a completely different structure. How fast is NVMe compared to SATA? Newer motherboards use SATA III with a maximum data transfer rate of 600 Mbps (or 300 Mbps for SATA II, in which case it's time to switch). Through this port, most SSDs offer read / write speeds of close to 500 to 530 MB / s. To compare the SATA 7200 RPM drive, depending on the year of manufacture, condition and type of parts, the data transfer rate is about 100 Mbps. On the other hand, the 3rd generation NVMe drives offer write speeds of up to 3500 Mbps and the 4th generation 7000 Mbps. This is 7 times faster than a SATA SSD drive and 35 times faster than a rotating hard drive! Which do you think is better?\nHello everyone, has many highlights, I will introduce you some of them. OceanStor Dorado 3000 V6 Huawei OceanStor Dorado all-flash storage systems are designed to carry mission-critical services of enterprises, financial institutions, and data centers. The storage systems have the following highlights: Zero service interruption : The industry's only storage product that ensures service continuity in the event of a single point, dual points, or multiple points of failure (failure of 7 out of 8 controllers across enclosures for high-end devices). Failover transparent to applications : In the event of a controller failure, services are switched over in seconds without interrupting host links. Upper-layer applications are not impacted. Global resource balancing: The end-to-end active-active architecture balances resources globally. Five chips for transmission, compute, storage, management, and intelligence lay the solid foundation for the industry's fastest storage. The intelligent chip and cache algorithm allow the storage system to deeply learn service I/O patterns and intelligently accelerate data processing. The industry's unique smart disk enclosure shares the computing load of the storage system, achieving linear expansion of performance and capacity. Intelligence throughout the service lifecycle: Intelligence participates in the end-to-end service process from resource provisioning to fault locating, allowing the system to predict the performance and capacity trend for the next 60 days, detect potential faulty disks 14 days in advance, and immediately provide solutions to 93% of problems upon detection.\nColleagues, good afternoon. We have prepared for you a new part of training materials on the HCIE Storage certification. This course is \"Storage DR Solution Application Practice\". This course guides you through the whole process of designing an active-standby disaster recovery (DR) solution based on cases to help you understand how to apply the solution to customers' environments. By scenario-based practices of the active-standby DR solution, you will be able to: Analyze the customer requirements. Design a DR solution. Deploy and implement the solution. Understand the O&M and test process. Understand the acceptance standards of the solution. To improve resource utilization and simplify infrastructure management, bank H virtualizes and consolidates service systems deployed on Huawei all-flash storage devices in the production data center. Upon service consolidation, a number of devices are set aside. Bank H established a disaster recovery data center in another city using these devices. The bank is considering deploying a DR solution at the two sites. Both sites use Huawei all-flash storage devices and have the HyperReplicationlicense. The bank holds a communication meeting for the project and delivered the Minutes of the Communication Meeting on the Requirements of Bank H for the Disaster Recovery Project and the Device Information for the Disaster Recovery Solution. Assume that you are a storage engineer of this project and need to complete the following tasks: Analyze the customer requirements. Design a DR solution. Deploy and implement the solution. Guide the acceptance of the project.\nSAN connectivity consists of hardware and software components that interconnect storage devices and servers. The Fibre Channel model for SANs is introduced. Networking is governed by adherence to standards and models. Data transfer is also governed by standards. By far the most common standard is . SCSI is an American National Standards Institute (ANSI) standard that is one of the leading I/O buses in the computer industry. An industry effort was started to create a stricter standard to allow devices from separate vendors to work together. This effort is recognized in the ANSI SCSI-1 standard. The SCSI-1 standard (circa 1985) is rapidly becoming obsolete. The current standard is SCSI-2. The SCSI-3 standard is in the production stage. The SCSI bus is a parallel bus, which comes in several variants (Figure a). Figure a. SCSI standards comparison table In addition to a physical interconnection standard, SCSI defines a logical (command set) standard to which disk devices must adhere. This standard is called the . It was developed more or less in parallel with ANSI SCSI-1. The SCSI bus not has data lines and also several control signals. An elaborate protocol is part of the standard to allow multiple devices to share the bus efficiently. In SCSI-3, even faster bus types are introduced, with serial SCSI buses that reduce the cabling overhead and allow a higher maximum bus length. As always, the demands and needs of the market push for new technologies.\nThe following uses an example to explain how to calculate the allowed expansion capacity. Three valid digits are retained after the decimal point. Assume that forty-eight 600 GB SAS disks will be added to the storage system, including four coffer disks and the hot spare policy and RAID policy are configured to Low and RAID 6 (8D + 2P) respectively. The allowed expansion capacity is calculated as follows: 600 GB is the nominal capacity provided by the disk vendor. Use the following method to convert this capacity to one that can be identified by the storage system: 600 GB x (1000/1024) x (1000/1024) x (1000/1024) = 572204.590 MB OceanStor5300 V3/5500 V3/5600 V3/5800 V3/6800 V3storage systems provide the DIF function for end-to-end data protection. This function takes 1% to 2% of storage space. The following uses 2% as an example. 572204.590 MB x (1 2%) = 560760.500 MB Minus the WriteHole capacity: 560760.500 MB 256 MB = 560504.500 MB Minus the reserved production space: 560504.500 MB 577 MB = 559927.500 MB Minus the metadata capacity: 559927.500 MB x (1 0.6%) = 556567.935 MB The storage system reserves 0.6% of each disk's space as metadata space. It dynamically allocates metadata space as services increase. The actual services prevail. The following uses 0.6% as an example.\nThe multimode and singlemode discussed in the SFP generally refer to the type of fiber that is connected to the SFP optical module. Let us first look at the two categories of fiber optic cable. Almost all multimode fibers are 50/125m or 62.5/125m in size, and the bandwidth (the amount of information transmitted by the fiber) is usually 200MHz to 2GHz. Multimode optical transceivers can carry up to 5 kilometers of transmission through multimode fiber. A light emitting diode or a laser is used as a light source. The pull ring or outer body is black in color. Single mode fiber has a size of 9-10/125 m and has an infinite bandwidth and lower loss characteristics than multimode fiber. Single-mode optical transceivers are often used for long-distance transmission, sometimes reaching 150 to 200 kilometers. LEDs with narrower LD or spectral lines are used as the light source. The color of the pull ring or outer body is blue, yellow or purple. Single-mode fiber is cheap, but single-mode devices are much more expensive than comparable multimode devices. Single-mode devices typically operate on both single-mode fibers and multimode fibers, while multimode devices are limited to operation on multimode fibers. The 10G module has undergone development from 300Pin, XENPAK, X2, and XFP, and finally realized the transmission of 10G signals in the same size as SFP. This is SFP+. With its advantages of miniaturization and low cost, SFP has met the high density of optical modules.\nHi there! This post enquires what is the difference between IOPS and throughput. Please see below. ISSUE DESCRIPTION Can someone answer me what is the difference between IOPSand throughput? And how it's working? Thanks in advance! Storage discussions often bring up terms such as IOPS and throughput as they help in gauging the caliber of a storage system. The other day, one such discussion churned out confusion among the readers. So, to clarify to those, StorageServers blog brings in the difference between Storage IOPS and Storage Throughput. Data transfer speed in megabytes per second is often termed as throughput. Earlier, it was measured in Kilobytes. But now the standard has become megabytes. The time taken for a storage system to perform an Input/Output operation per second from start to finish constitutes IOPS. Historically speaking, the performance of online transaction processing activity was entirely dependent on response time. The better the storage system IOPS was, the better was the online transaction processing rate. But, nowadays its a bit more complex, as some of todays database queries can depend as much on sequential database transfers (or throughput) as on individual IO response time. Thus, this endeavor is giving us a feeling that there are large components of response time critical workloads out there that perform much better with shorter response time.\nhas key components. In this lesson, we will focus on these in a . So, what are the ? These of are given below: Now, lets talk about these detailly. The first cokmponent of is Host Machines. A is the physical hardware that the virtual machines reside. It is the device that has physical resources like memory, storage, processor etc. These resources are used by virtual machines according to their configuration during the virtualization process. Host machines are the physical devices that runs virtualization software that create and manage virtual machines. The other component is Virtual Machine. A is the virtual device that resides in Host machine. It emulates s single physical device but as virtual. In other words, a Virtual Machine is created with the resources that it needs virtually in Host Machine. Each virtual device thinks that, it is the only device in the system. But there can be many different virtual devices in the host machine. The main aim of virtualization is already this multiple usage. A can be a PC, a server, a router, a firewall etc. According to your need, you can create a virtual machine in physical host machine and create small virtual machines in it. The communication between the host machine and virtual machines are done via . The last component is A is the key part of virtualization. In other words, Virtualization is commonly . It is also called .\nHello dear community, I'm going to compare the data and information to help you understand the basics of storage. SNIA (Storage Networking Industry Association) defines data as the digital representation of anything in any form. Structured data: Structured data is basically anything that can be put into relational databases and organized in such a way that it relates to other data via tables. Semi-structured data: Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. Unstructured data : Unstructured data is everything that cannot be put into relational databases and organized to relate to other data via tables, e.g. email messages, social media posts and recorded human speech etc.. Data processing is the reorganization or reordering of data by humans or machines to increase their specific value. A data processing cycle includes three basic steps: input, processing, and output. Information is processed, structured, or rendered in a given context to make it meaningful and useful. Information is processed data, including data with context, relevance, and purpose. It also involves the manipulation of raw data. Information lifecycle management (ILM) refers to a set of management theories and methods from the stage in which the information is generated and initially stored to the stage where the information is obsoletely deleted.\nHello Storage members, I will introduce you theStorage Management Initiative Specification (SMI-S) technology. The SMI-S, developed by the SMI organization of SNIA, is an open standard used to manage storage networks of multiple vendors. The SMI-S defines a set of secure and reliable interfaces that help the storage management system identify, classify, and monitor physical and logical resources in a storage area network(SAN). The CIM is developed by a distributed management task force (DMTF) to describe concept models of data. The CIM uses a layered object-oriented system structure to model for managed resources. Therefore, devices and components can be described in an object-oriented manner. The WBEM is an enterprise-class management system structure developed by a DMTF. The WBEM assembles management protocols and standard Internet technologies for unified management in a distributed operating environment, improving data exchange capabilities across technologies and platforms. The SLP is used to discover the SMI-S server and its functions in a storage network environment. As an open standard, the SMI-S expands general-purpose capabilities of the CIM, WBEM, and SLP to achieve interoperability in a storage network environment. For example, the WBEM provides security, resource lock management, and event notification. As an open standard, the SMI-S expands general-purpose capabilities of the CIM, WBEM, and SLP to achieve interoperability in a storage network environment. For example, the WBEM provides security, resource lock management, and event notification. SDK SMI-S Provider enables third-party storage management software to manage Huawei enterprise storage as a network element (NE). Figure 1 shows an application scenario.\nDear all, Now I want to share with you RTO and RPO in disaster recovery. RTO (recovery time objective) and RPO (recovery point objective) are two key metrics that organizations must consider in order to develop an appropriate disaster recovery plan that can maintain business continuity after an unexpected event. Recovery time objective refers to the time required for the production system to recover from a disaster. A smaller RTO means a shorter service interruption. Recovery point objective refers to the time when the data restored in the event of a disaster was backed up. It is used to measure the amount of data lost in a disaster. A smaller RPO means less data loss. RTO and RPO Share Several Characteristics * Recovery time and recovery point objectives differ according to application and data priority. Even the most deep-pocket corporation cannot afford to deliver near-zero RTO or RPO for all applications, nor should they. * The only way to assure 100% uptime (RTO) and no lost data (RPO) is by investing in failover virtual environments with continuous data replication. * IT prioritizes applications and data to match the expense of achieving RTO and RPO. Note that priority is not only guided by revenue but also by risk. A company may use an application infrequently, but if its data is regulated then data loss may result in big fines. * Both RTO and RPO are measured in units of time.\nA data storage plan concerns what gets saved, where it gets stored, when it gets stored, who is in charge of management, how much data an organization stores, what it keeps and destroys, and how it gets stored. A data storage strategy considers every part of the procedure. The data storage plan's requirements ensure the data and other information resources' security and availability. Data storage is mostly dictated by user requirements for data storage and retrieval. Data storage might happen immediately after processing or it can be queued for later. Customer transactions and updates to personal health data are examples of vital data that may require immediate storage following processing. More complex storage solutions, such as data mirroring or replication, may be required. These needs should be specified in the data storage plan. The stages of a storage plan have several steps First you need to Interview users to determine storage requirements. depending on the User it could be evaluating the type of storage pool storage capacity and efficiency of requirements After that, the data storage team creates the various processes outlined in the preceding section, which are then reviewed by IT management and confirmed by user management. Define the actions required for the storage procedures, storage medium, storage locations, data transport media, and other criteria after the plan has been approved. Complete the process in a multiphase sequence of activities if no storage operations are already in place.\nWith such a wide range of storage solutions, Huawei is able to meet the diverse needs of its customers to meet today's needs, including data centralization, clustering, data sharing, big data and cloud storage. (Cloud Storage). Huawei's integrated, block, and massive enterprise storage solutions focus on high reliability and performance. These solutions provide the ability to cluster (Clustering) with high efficiency and scalability while safe and cost-effective. This means that customers can customize their storage systems according to their needs, without the need for much investment. Huawei's data protection system complements Huawei's storage systems with advanced backup, disaster recovery and cloud storage backup. In addition, all of Huawei's solutions include storage backup and a series of Virtual Tape Libraries. Huawei also has its own data storage software that is powerful and easy to use, fully supported for resource management, data backup and recovery and other data storage needs. The following is an overview of Huawei's integrated storage equipment: OceanStor V3 Converged Storage A new lineage integrated storage device built specifically for enterprise applications using cloud-based operating systems. This equipment uses a new powerful hardware platform and a set of intelligent management software. Available OceanStor V3 models include the 6800, 5800, 5600, 5500 and 5300. Convergence to simplify SAN and NAS convergence, heterogeneous convergence, all-surface convergence, SSD and HDD convergence, and backup convergence. All of these contribute to a secure, reliable, simple, and efficient storage system tailored to the needs of users, thereby helping to make systems smaller as well as more agile for businesses.\nHello everyone! This post will be describingIntelligent Cloud-Networks. Please watch the video below to familiarize yourself with the topic. The cloud has enabled computing power and Artificial Intelligence (AI) to serve as inclusive technologies through online public services, becoming the foundation for the digital transformation of society, according to consulting firm Deloitte's Enterprise Cloud Adoption Best Practice - All in Cloud white paper. In short, cloudification is no longer optional, but imperative for digital development. As digital development continues to accelerate worldwide, cloudification across industries has also advanced substantially. Enterprises can't afford to question whether they need cloudification any longer. According to IDC a global market intelligence firm 80% of enterprises will accelerate their cloud migration by the end of 2021, with multi-cloud access including public, private, and hybrid clouds becoming the preferred choice. Enterprises will migrate their core services to the private cloud and office services to the public cloud. Meanwhile, customers will have access to an increasing number of digital services through the cloud. Just as the power grid supplied electricity to every household bringing about the Second Industrial Revolution the intelligent cloud-network will create new momentum for the digital economy by delivering computing power and intelligence to every industry, improving production efficiency for enterprises. The intelligent cloud-network has three unique features: Network digitalization : by detecting the network status through digital methods, the entire network's status can be recreated in the digital world for abstract modeling, with unified storage on the cloud.\nDear member, For scheduling the Huawei certificate examination, it is recommended that you schedule your HCIA/HCIP/HCIE with External Link Pearson VUE online. Step 1 . Visit the Step 2 . Sign in using the username and password (created when registering). Step 3 . After login, you will be asked to confirm the information and then redirected to the Pearson VUE exam platform to schedule an exam and pay for the exam. NOTE: If your personal name is inconsistent with your identity certificate, click Edit and return to Huawei official website for modification. Otherwise, you cannot take the exam. Step 4 . Supplement additional information for Huawei. This step is required only when you register on Pearson VUE for the first time. (Skip this step if you have completed additional information.) Request preference for date and time of testing at the preferred test center 1. Call 2. Supply name and ID, along with other information to verify identity. 3. Request preference for date and time of testing at the preferred test center. After you schedule your appointment, you will be sent a Confirmation of Appointment from Pearson VUE. First, verify that all information is correct. Then, call or go online to check that your appointment has been scheduled/rescheduled. If you do not receive a confirmation every time that you schedule or reschedule an appointment, contact Pearson VUE Huawei Candidate Services immediately to correct any errors to the appointment. An email address must be provided with your registration.\nKali Linux is an enterprise-ready security auditing Linux distribution based on Debian GNU/Linux. Kali is aimed at security professionals and IT administrators, enabling them to conduct advanced penetration testing, forensic analysis, and security auditing. When it comes to penetration testing, hacking, and offensive Linux distributions, one of the first things to be mentioned is Kali Linux. The software comes pre-packaged with a variety of different command line hacking tools geared towards various information security tasks, such as penetration testing, network security, computer forensics, and application security. Basically, Kali Linux is the ultimate OS for ethical hackers and is widely-recognized in all parts of the world, even among Windows users who may not even know what Linux is. Kali Linux is the most preferred operating system to perform various information security tasks due to the following reasons: It offers more than 600 penetration testing tools from various fields of security and forensics It's completely customizable, so if you are not comfortable with current Kali Linux features, you can customize Kali Linux the way you want Though penetration tools tend to be written in English, Kali provides multilingual support It supports a wide range of wireless devices Comes with the custom kernel, patched for injections Developed in a secure environment Its a free and open source software Minimum 20 GB of free space in your hard drive is recommended.\nAt least 4 GB of ram is recommended when using VMware or VirtualBox CD-DVD Drive / USB Support Step 1 : Install VMware In order to run Kali Linux, we will need some sort of virtualization software first. While there are many options to choose from, such as Oracles VirtualBox, I prefer using VMware. Once the installation is done, launch VMware from your applications folder. Step 2 : Download Kali Linux and Check Image Integrity To download Kali Linux, you can go to the official download page and select the one that best suits your needs from there. In addition, on the download page, you will find a bunch of hexadecimal numbers. And those are not there for fun. Kali Linux is intended to be used for security-related tasks. So, you need to check the integrity of the image you download. Step 3 : Launch a New Virtual Machine On the VMware Workstation Pro homepage, click on \"Create a New Virtual Machine,\" choose the Kali Linux iso file, select the guest operating system, and configure virtual machine details (here, Kali Linux). Start the virtual machine by selecting the Kali Linux VM, and then clicking on the green Power On button. Installation Procedure After the machine boots, you will be prompted to select the preferred installation mode in the GRUB menu. Choose graphical installation and continue. Step 1 Kali Lniux-install Kali Linux-Edureka The next few screens will ask you to select local information, such as your preferred language, your country/region, and keyboard layout.\nStep 2 Kali Linux-How to install Kali Linux-Edureka Once the local information is passed, the loader will automatically install some additional components and configure your network-related settings. The installer will then prompt for the hostname and domain of this installation. Provide the appropriate information for the environment and continue the installation. Step 3 Kali Linux-how to install Kali Linux-Edureka Set a password for your Kali Linux machine and click Continue. Don't forget this password. Step 4 Kali Linux-how to install Kali Linux-Edureka After setting the password, the installer will prompt you to set the time zone and then pause at the disk partition. The installer will now provide you with four choices regarding disk partitioning. The easiest option for you is to use \"Boot-Use Entire Disk\". Experienced users can use the \"manual\" partitioning method for more refined configuration options. Step 5 Kali Linux-how to install Kali Linux-Edureka Select the partition disk (recommend all files in one partition for new users), and then click \"Continue\". Step 6 Kali Linux-how to install Kali Linux-Edureka Confirm all changes to be made to the disks on the host. Please note that if you continue, it will erase the data on the disk. Step 7 Kali Linux-how to install Kali Linux-Edureka After confirming the partition change, the installer will run the process of installing files. Let it install the system automatically, this may take some time. After installing the necessary files, you will be asked if you want to set up a network mirror to obtain future software and updates.\nHello all, Many times, a person is unable to get access to the Web UI of the repeater. In such a case, you need to follow the below-written tips. If you are using the wifi, make sure it is connected well to the repeaters wifi network. If a computer is used, make sure that your computer is connected to the repeaters wifi network. You can set your computer to attain the IP address 192.168.188.1 automatically and obtain the DNS server address also. To regulate the login process of your Wi-Fi range repeater, you need to follow a definite procedure in an organized way. Here we are discussing the steps. Step 1: The very first thing is to plug the repeater into a reliable wall outlet that should function properly. Step 2: The next step is to open the wireless utility on your system and select the SSID of your respective network. Then, you need to enter the credentials like the password and the username. You can use the default credentials as mentioned in the manual that comes along with the extender itself. Step 3: Its time to open the browser of your choice. Enter the required IP address in the given column. You can use mywifiext.net or fill 192.168.188.1 directly to get access to the account settings. Step 4: In the setting portion, you need to type in the default username as Admin and leave the space blank in case of the password. Click login after doing that.\nHello all, Here is the good news, Huawei unveils the first 5G MEC-based industrial vision solution leveraging Cloud-Edge-Device Synergy. At PT Expo China 2020, Huawei launched the first 5G MEC-based Industrial Visual solution for the manufacturing industry. The solution encompasses the lossless compression-capable SDK, MEC platform (MEP), user plane functions (UPFs), lossless decompression service, and machine vision apps. The lossless compression-capable SDK is integrated with 5G industrial cameras that work with the lossless decompression service on the MEC node. With this backing, these cameras can compress HD images of one-third to one-sixth of their original size, greatly reducing uplink bandwidth usage. Liu Zhi, VP of Huawei Packet Core Network product line, explained that Huawei 5G MEC-based Industrial Vision Solution leverages the cloud-edge-device synergy and lossless compression technologies. These technologies are key to efficiently reducing the bandwidth required to transmit HD images in the manufacturing industry, realizing agile deployment and automatic O&M, and guaranteeing data security. The 5G MEC-based Industrial Vision Solution paves the way for wide application of the 5G MEC solution in the industrial vision industry. Huawei 5G MEC-based Industrial Vision Solution For example, Microview released the first 5G MEC-based industrial cameras embedded with the above-mentioned Huawei SDK and 5G modules. The cameras produce HD images with tens of millions of pixels, which it can then losslessly compress. This helps reduce bandwidth for transmission and therefore allows customers to install more cameras in their factory, helping them adhere to requirements for automated quality inspection in the manufacturing industry.\nhow data reads and writes in initial synchronization. HyperMetro uses dual-write and data change log (DCL) to synchronize data changes between the storage systems in two DCs, ensuring data consistency. The storage systems in both DCs provide services for hosts concurrently. Dual-write and locking mechanisms are essential for data consistency between storage systems. Dual-write and DCL technologies synchronize data changes while services are running. Dual-write enables hosts' I/O requests to be delivered to both local and remote caches, ensuring data consistency between the caches. If the storage system in one DC malfunctions, the DCL records data changes. After the storage system recovers, the data changes are synchronized to the storage system, ensuring data consistency across DCs. Two HyperMetro storage systems can process hosts' I/O requests concurrently. To prevent conflicts when different hosts access the same data on a storage system simultaneously, a locking mechanism is used to allow only one storage system to write data. The storage system denied by the locking mechanism must wait until the lock is released and then obtain the write permission. shows an example of the write I/O process in which a host delivers an I/O request to the local storage system and dual-write is used to write the data to the remote storage system. Figure 1 A host delivers a write I/O to the HyperMetro I/O processing module. The write I/O applies for write permission from the optimistic lock on the local storage system.\nAfter write permission is obtained, the system records the address information in the log but does not record the data content. The HyperMetro I/O processing module writes the data to the caches of both the local and remote LUNs concurrently. When data is written to the remote storage system, the write I/O applies for write permission from the optimistic lock before the data can be written to the cache. The local and remote caches return the write result to the HyperMetro I/O processing module. The system determines whether dual-write is successful. NOTE: In the background, the storage systems use the DCL to synchronize data between them. Once the data on the local and remote LUNs is identical, HyperMetro services are restored. If writing to both caches is successful, the log is deleted. If writing to either cache fails, the system: Converts the log into a DCL that records the differential data between the local and remote LUNs. After conversion, the original log is deleted. Suspends the HyperMetro pair. The status of the HyperMetro pair becomes To be synchronized . I/Os are only written to the storage system on which writing to its cache succeeded. The storage system on which writing to its cache failed stops providing services for the host. The HyperMetro I/O processing module returns the write result to the host. The data of LUNs on both storage systems is synchronized in real time. Both storage systems are accessible to hosts. If one storage system malfunctions, the other one continues providing services for hosts.\n[Shenzhen, China, September 22, 2021] Huawei, along with industry partners, held the Intelligent World 2030 Forum. David Wang, Executive Director and President of ICT Products & Solutions of Huawei, released the Intelligent World 2030 report with a keynote speech on . This is the first time that Huawei has used quantitative and qualitative methods to systematically describe the intelligent world in the next decade and forecast industry trends, helping industries identify new opportunities and discover new value. Over the past three years, Huawei has conducted in-depth exchanges with more than 1,000 academics, customers, and partners in the industry, organized more than 2,000 workshops, and drawn on data and methods from authoritative organizations, such as the United Nations, World Economic Forum, and World Health Organization. Huawei has derived insights from scientific journals such as Nature and IEEE, and drawn wisdom from relevant industry associations and consulting firms, as well as experts within and outside Huawei. Through these efforts, Huawei has developed the Intelligent World 2030 report, providing insights into ICT technology and application trends in the next decade. David Wang releases the Intelligent World 2030 report The report proposes eight cross-disciplinary and cross-domain directions for exploration at the macro level. It explains how ICT technologies can solve critical problems and challenges of human development, and what new opportunities can be brought to organizations and individuals. At the industry level, the report explores the future technologies and development directions of communications networks, computing, digital power, and intelligent automotive solutions.\nWang said, \"30 years ago, we decided to enrich life through communications. 10 years ago, we decided to connect every corner of the world, to build a better, connected world. Now, our vision and mission is to bring digital to every person, home, and organization for a fully connected, intelligent world. We firmly believe that a brilliant intelligent world is arriving at an accelerated pace.\" Many heavyweight guests were invited to speak at the forum, including renowned futurist Steven Johnson, founding and rotating chairman of the World Electric Vehicle Association Chen Qingquan, Co-President of Roland Berg Global Management Committee Denis Depoux, and Vice President of the China Academy of Information and Communications Technology (CAICT) Wang Zhiqin. They shared their insights on the intelligent world and discussed how ICT can better drive socio-economic development. As the prominent futurist and science author Steven Johnson said, we are entering an era of exponential growth. The coming decades will be characterized by a golden age of collaboration between human and machine intelligence, and algorithms will enhance human intelligence. As technology grows exponentially, all of society will benefit. The Intelligent World 2030 Forum is the first time that Huawei has systematically shared cutting-edge research and insights into the next decade. This sharing of knowledge will bring great value to social development, especially for global digital transformation and digital economy.\nImagination will determine how far we will go in the future, action will determine how quickly we will reach the future, and the best way to predict the future is to create it. There are still plenty of challenges to overcome on the road to the intelligent world. As David Wang said at the end of his speech, \"We believe, the greatest wisdom is found in shared ideas. Dreams are the key driver of social progress. Moving towards the next decade, let's work together to shape a better, intelligent world.\" We will live a better life in 2030, with more food, larger living spaces, renewable energy, digital services, and no traffic. We will be able to relinquish repetitive and dangerous work to machines, and have secure access to digital services. To meet these needs, we have set eight directions for exploration, including health, food, living, and transportation. In 2030, we will be able to identify potential health problems by computing and modeling public health and medical data, shifting the focus from treatment to prevention. Precise medical solutions powered by IoT and AI will become a reality. In 2030, vertical farms unaffected by climate will be applied on a large scale, so that we can provide green food for all. 3D printing will make it possible for us to create artificial meat to meet our nutritional needs. Our homes and offices will become zero-carbon buildings. Next-generation IoT technology will build adaptive home environments that understand our needs. New energy vehicles will become the mobile \"third space\".\nNew aircraft will make emergency services more efficient, reduce the cost of medical supplies, and change the way we commute. In addition to healthcare, food, living spaces, and transportation, Huawei has also explored the future of cities, energy, enterprises, and digital trust. We look forward to working with you to explore endless possibilities in 2030. In the next decade, the objects and boundaries of network connectivity will continue to expand. By 2030, as technologies such as XR, naked-eye 3D display, digital touch, and digital smell develop further, \"digital vision, digital touch, and digital smell\" will create an immersive and disruptive experience through next-generation networks. At the same time, as networks evolve from connecting billions of people to hundreds of billions of things, network design will change from focusing on human cognition to machine cognition. We will see an emergence of multi-level computing infrastructure for hundreds of billions of things and massive data, as well as computing power networks that provide connectivity. In addition, four future network scenarios will gradually become a reality. They are the networks that will deliver a consistent experience for homes, offices, and vehicles, satellite broadband Internet, industrial Internet, and computing power network. As part of the intelligent world, the communications network of 2030 will evolve towards cubic broadband networks, deterministic experience, AI-native, HCS, security and trustworthiness, and green and low-carbon networks. Huawei predicts that the total number of global connections will reach 200 billion by 2030.\nAt the same time, enterprise network access, home broadband access, and individual wireless access will exceed 10 Gbit/s, ushering in an era of 10 Gbit/s connectivity. By 2030, the digital and physical worlds will be seamlessly converged, allowing people and machines to interact perceptually and emotionally. AI will become ubiquitous and help us transcend human limitations. It will serve as microscopes and telescopes of scientists, enhancing our understanding of everything from the tiniest quarks to the largest cosmological phenomena. Industries already making extensive use of digital technology will become more intelligent with AI. Computing energy efficiency will increase dramatically, bringing us closer to zero-carbon computing. Digital technologies can become a tool for achieving the global goal of carbon neutrality. Computing is approaching its physical limits, so innovation in software, architecture, and systems are needed. More importantly, the entire industry needs to jointly explore a new foundation for computing, break through the physical limits of semiconductors, and make computing greener, more secure, and more intelligent. Huawei predicts that by 2030, humanity will enter the yottabyte data era, with general-purpose computing power increasing by 10 times and AI computing power by 500 times. In the next decade, humanity will enter the digital power era, striving towards low-carbon development, electrification, and intelligent transformation. New renewable energy sources, such as photovoltaic and wind power, will gradually replace fossil fuels. Power electronic technology and digital technology are being deeply converged to enable \"bit to manage watt\" throughout the energy system and realize various intelligent applications on the \"energy cloud\".\nHuawei predicts that by 2030, solar energy will become one of our main power sources, the proportion of renewables in global electricity generation will be 50%, the share of electricity in final energy consumption is expected to exceed 30%, electric vehicles as a proportion of new vehicles sold will exceed 50%, and renewable energy will power 80% of digital infrastructure. In the next decade, electrification and intelligence will be unstoppable, and ICT technologies will converge with the automotive industry. The automotive industry will witness the development of intelligent driving, intelligent spaces, intelligent services, and intelligent operations. Huawei hopes to use its ICT technologies to enable an intelligent automotive industry and help carmakers build better vehicles. The ultimate goal of intelligent driving is to use technologies such as autonomous driving to greatly reduce the incidence of traffic accidents, and deliver efficient and seamless travel experiences to users. Intelligent driving has so far been mainly limited to closed roads like high-speed roads and campus roads, but it will gradually see more application on public roads, such as those in urban areas. Vehicles will become a new intelligent space. With the support of ICT, technologies like AI, biometric recognition, in-vehicle optical sensors, and AR/VR will bring new features to the cockpit. Intelligent vehicles will truly transform from a flexible mobile space to an intelligent living space that integrates the virtual and physical worlds.\nHi all, I did an expansion evaluation check to add a disk enclosure to Oceanstor 5500 v3, and just 1 check didn't pass. The check is: The host is San Volume Controller of IBM I checked that information on OceanStor device manager, but no error is found.\nWhat is a PoE switch? A PoE switch is a network switch that can pass power on its ports in addition to data. This switch connects to PoE-Enabled devices via standard network cables (Cat6 or Cat5) and provides power to these devices. This feature is especially useful in environments where it is not possible to have separate wiring to conduct electricity. Surveillance cameras, cellular phones, and wireless access points are some of the most common devices connected to PoE switches. Equipment such as a PoE switch that transmits power through network cables is called Power Sourcing Equipment, or PSE for short. Devices that use the transmitted current in the network are also called Powered Device or PD for short. PoE switches on the market Most PoE switches on the market come with a Gigabit port speed (10/100/1000). In addition, SFP and SFP + ports are built into some PoE switch models to support fiber optic networks and provide higher switching capacity. The number of ports and their speed determine the capacity of the PoE switch. For small networks, it is usually sufficient to use a PoE switch with 8 ports, and for larger networks and enterprise networks, PoE switches with 24 ports or 48 ports are suitable. First: As stated in the article Selecting the right PoE, the first step in choosing the right PoE switch is to determine the \"number of ports\". Decide on the number of ports as well as the \"port speed\" (PoE Fast Ethernet switch, PoE Gig switch, etc.)\naccording to the network requirements (number of devices connected to the switch). Due to the high variety of PoE switch models, you will have no problem finding the desired switch. Huawei offers the following ports for PoE switches: PoE switch with 8 ports, PoE switch with 16 ports, PoE switch with 24 ports, PoE switch with 48 ports. For requirements higher than 48 ports, multiple switches or PoE chassis switches must be used. Second : One of the most important factors in choosing a PoE switch is the Power Budget value of the switch. Power Budget, or the power allocated to PoE, is equal to the maximum power that a PoE switch can distribute among all its connected devices (PDs). Another factor to consider is the amount of power that each PD device connected to the switch must receive from the switch's PoE port to turn on and operate. In short, to make sure you choose the right PoE switch that can provide power to all connected devices, you should check the maximum power of each PoE switch port as well as the number of PoE switch ports to which the PD is to be connected. To clarify, let's first review the following two standards for PoE switch output: According to the IEEE 802.3af standard (PoE standard), each PoE switch port can provide up to 15.4W of direct current to connected devices; in the newer IEEE 802.3at standard (PoE + standard) this number reaches 30W for each port.\nOf course, in practice, when you connect a network cable to a PoE port, some power is lost - the longer the cable, the more energy is wasted - so it is said that the minimum output power of each port in the PoE standard is 12.95W at PoE + standard is equal to 25.5W Finally, with an example: Consider a 24-port POE switch with a 370-watt power supply (similar to the Huawei S5720S-28P-PWR-LI-AC), in theory this PoE switch can be used for 24 IP cameras with IEEE 80.2.3af standard (ie 15.4W per port) provide the required power (24 = 370 / 15.4) while the same switch is able to provide only 12 IP cameras with IEEE 80.2.3at standard (ie 30W per port) ) (12 = 370/30). Third: The main difference between a PoE management switch and a PoE non-management switch is in the performance of the switch and how it is configured and, of course, their price. PoE management switch allows you to configure network protocols such as IGMP Snooping, NLAN, etc., while PoE non-management switch is a plug-and-play device and is installed without the need for special setup and can not be configured. For most small networks, a non-management switch is able to meet all the needs of users. What are the advantages of PoE switch? Once you have selected the PoE switch model you want, you can purchase the switch and install it on the network.\nAny device or medium which is either permanently attached or is movable/transportable, capable of storing information in an electronic form can be referred to as a storage device. Some examples are hard disk drive, CD ROM, flash media, DVD ROM, memory stick, etc. Devices such as iPod, PDA, mobile phones, etc. also contain storage devices as part of the hardware Two types of storage devices are used with computers. The RAM stores data that is directly accessible by the computers processor (CPU). RAM is commonly referred to as memory. The primary memory is temporary and is used to store program instructions and intermediate results of procedures. It is a device that is non-volatile. Though it may be located inside a computer, this type of storage is not considered primary because it cannot be directly accessed by the CPU. Data on a hard disk drive is organized in the form of files. This affords slower access and is cheaper. An optical disk is an example. This type of device includes a mechanism to locate specific data and transfer it to a drive when requested. Any computer user ends up using many different types of secondary storage devices. The internal or external hard disk drive is usually connected to the computer for retrieval of stored data using an interface such as a USB cable. Computers of today have the disk drives as externally connected devices. These can be easily removed and stored elsewhere.\nThese devices hold the stored data unless explicitly deleted or overwritten using instructions from the computer. Until a few years ago, floppy disks, magnetic tapes, and other magnetic media were used as secondary storage devices and were popular. These devices used the principle of a read/write head magnetizing material that was coated on the disk to store information. Once these became cumbersome to use, storage device manufacturers started looking at optical storage devices. Such a device is written onto and read from using a laser beam. The marked advantage that they have is the capability to store large amounts of data. The different types of optical memory are CD-ROM, DVD-ROM, CD-R/W, etc. With every new software or application that is being developed, the demand for storage is greater. Flash drives also referred to as thumb drives are the most popular portable storage devices of today. They are compact and connect with the help of a USB port. Memory cards are used in digital cameras or mobile phones are also secondary storage devices. These can be transferred to a computer using a reader that is connected through a USB port. Some computers have installed solid-state drives (SSD) in place of hard disks. An SSD is advantageous in that it has no movable parts like the hard disk. Their costs are now very competitive with the result that they are being used more in computers now. The cloud storage provider manages the devices and provides data backup too. Many users appreciate the managed services provided by cloud operators.\nObject Storage Object storage or object-based storage refers to the data storage architecture that manages stored data in the form of objects and not as files. The units of data storage called objects consist of: The stored data is organized into objects. These may be complete files or parts of a file (sub-file). They may also simply be a collection of bits and bytes that are related but not part of any file. The metadata is created by the one who creates the object storage. The metadata is made up of information that is contextual, i.e., what the data is all about, what it should be used for, any other information about how it is used to be used, and its confidentiality. This is a unique piece of information (usually 128 bits) that is tagged along to be able to find out this data over a distributed system. The physical location of the data becomes irrelevant. Once comprehensive metadata is added to the file, it is placed in a flat address space referred to as a storage pool. The comprehensive metadata is what makes object storage successful as it provides information about both the use and function of the data lying in the storage pool. Some of the benefits of object storage are as follows: As object storage is driven by metadata, and because metadata for every piece of data is available, the opportunity for analyzing data becomes much greater. You can keep on adding any amount of data to the pool.\nAs there is no folder hierarchy, data can be retrieved quickly. It is a cheaper method to store large amounts of data. Resources can be used at an optimal level with object storage of data. Object storage works best for cases such as web content, archives, and data backup. The flat address space facilitates ease of use. Objects that are protected are built into the object architecture by using multiple copies (at least three) of the data over a distributed system. If any node fails, the data can still be made available to the user by using the copies. Corruption of data is also overcome in much the same way. Object storage always is not the answer. You, as a user, have to decide as to which storage architecture suits your needs the best. Storing a large amount of data in a traditional environment or by a single user is expensive. There are many object storage providers that offer cloud storage services. We list some of them here. They are one of the pioneers in object storage technology. You can upload any amount of data and they will allow you to upload/download using API, the browser. They promise 99.99% durability of data. They offer the Standard, Standard IA, and Glacier types of storage. You can move your data from one type of storage to another. They allow for the storage of data on the basis of region and this means you can have faster content distribution when you keep your data near your customer.\nA user can create, get and modify objects and metadata using Object Storage API. This is implemented as a set of Representational State Transfer (REST) web services. The HTTPS (SSL) protocol can be used to interact with Object Storage. Standard HTTP calls can be used to perform API operations. For using APIs that are language-specific, you can use RESTful API. To change data, the user is required to authenticate oneself with a token. The token can be obtained from an authentication service after presenting the users credentials. On clearance, the service returns a token and an account URL. Object Storage API supports a non-serialized response format. The Object Storage system organizes data like accounts, containers, and objects through the service provider. The service provider creates your account and you become the owner of the account. It is made up of all the containers. The container is the namespace for objects. You can make use of an access control list (ACL) to control access to the objects in a container. The object stores the data content such as images and documents. Object Storage API allows a user to perform the following functions in addition to many others. Store as many objects as desired Upload and store objects of any size Compress files Manage object security Schedule deletion of objects This list is not exhaustive. In the object storage arena, Amazon Web Services (AWS) has many product offerings. It is useful to have a basic idea to know which product to use for what purpose.\nS3, EBS, and EFS are three products that work differently. Amazon S3 (Simple Storage Service) offers both high-level data scalability performance and security. Customers across different industries can use this service to store any amount of web content, mobile app, enterprise application, IoT device, big data, and archive data. The management features of S3 are simple and easy to use. This enables users to easily access stored data and tune according to their business needs. AWS S3 offers 99.99% data durability for millions of companies worldwide. File Storage Network storage devices are typified in the way they are interfaced on the client-side. Multiple clients are able to access a single shared folder in a traditional file-sharing system. The two popular protocols that enable this system are NFS and SMB/FICS. The file system offers the simplest architecture for storage systems. When the amount of data grows larger, the resource demands grow and cannot be compensated by simply adding storage space. The files are then organized into directories and sub-directories. Naming conventions make it easy for them to be organized. Most file storage systems allow for a centralized and easy retrievable system for the data. The cost is reasonable. The file system works best when the amount of data is small and stored on personal computers and servers used in medium to large enterprises or workplaces. They can typically be used in conditions such as an office where you need to store and share files.\nLocally archived files can use file types of storage systems with a NAS (Network Attached Storage) solution. The data center in such a case is typically small. This type of storage architecture can also be used to protect data. This is supported by the use of standard protocols, different drive technologies, and native replication methods. The data is stored in containers in the form of files, given a name, and tagged with metadata which consists of file creation date, modification date, and the size of the file. Data stored in files are retrieved using very little metadata. This metadata informs the computer where exactly the file is located. Every file is arranged following a specific hierarchy, by directories and sub-directories. This arrangement is well supported by NAS systems. In cloud file storage, data is stored in the cloud. Other applications and servers can access this data through shared file systems. Users can create, edit, delete read and write files or even organize them in a logical fashion using directory trees. Cloud file-systems allowing shared access controls security with user and group permissions. Block Storage Block storage can be considered as an alternative to file storage. A block is a volume that is filled with files that are further split into blocks of equal size. Usually, such blocks can be well managed by a server-based operating system. Each of these chunks of data can be managed as individual hard drives. There are many third-party applications used by organizations that help to manage data in block storage architectures.\nBlock storage is known to handle metadata very efficiently and the operating system allocates the storage for various applications and decides where the data is going to reside in the block. The control is efficient and accounts for the high performance that block storage architecture offers. Block storage is used for a wide range of database applications that require high-performance levels, email servers that do not support file systems or network-based systems, virtual machines that use guest operating systems, etc. Block-based architecture can be expanded as the volume of data grows; however, the integrity of such systems is likely to be tested when the volume grows into terabytes or petabytes of storage. Most of the major enterprise-level storage providers offer San products/block storage solutions. Top providers include Dell EMC, Hitachi Data Systems, IBM, HPE, NetApp, etc. In the cloud, AWS Elastic Block Store (AWS EBS) are providers of scalable block storage usable by EC2 (Elastic Cloud Compute) instances. Therefore, all those applications that run on SAN can place their databases, applications, and workloads on Amazons cloud. Block storage vendors include Huawei, DataDirect Networks, Nutanix, Oracle, etc., in addition to the largest storage providers. These vendors have several block storage platforms. Some of them provide unified arrays for both block and file storage. Amazon EBS (Elastic Block Store) is the block storage offered by Amazon Web Services. This is used to store persistent data. Highly available block storage volumes are provided for EC2 instances making it suitable for the same.\nDear all, When we do the SmartMigration, and still in the synchronization phase, Is it write i/o request from a server going to the source and target LUN? Can anyone explain to me the write I/O request work? Hello, friend! Data change synchronization During the synchronization, host services do not need to be interrupted. When data is changed on the host, the host sends an I/O write request to the storage system. Then, the storage system starts data change synchronization and writes the changed service data to both the source LUN and target LUN using the dual-write technology. If data fails to be written to the target LUN, the storage system records the data differences in the DCL and copies the data that fails to be written from the source LUN to the target LUN according to the DCL. After the copy is complete, the storage system returns a write success acknowledgment to the host. If data fails to be written to the source LUN, the storage system returns a write failure. Upon receiving the write failure, the host re-sends the data to the source LUN only, but not to the target LUN. This mechanism ensures data consistency on the source and target LUNs during migration. Process of synchronizing changed data during service migration: 1. The host delivers an I/O write request to the LM module of the storage system. 2. The LM module writes the data to the source LUN and target LUN and records this write operation to the log. 3.\nDear team, I'm looking through someone's SQL code for Oracle, and they use where 1=1 in a lot of places. What is this for? Thanks. Dear Axe, where 1 = 1 means it will always be true, so all records will be returned. ' 1=1 ' in where condition of SQL statement makes the where condition true. For example, consider the following example. ex1: create table emp1 as select * from emp where 1=1; This statement creates a table called 'emp1' with structure and data similar to table 'emp'.Because the where condition(1=1) is 'TRUE' for all rows of emp the select statement select all rows of 'emp' table.i.e This statement copies both table structure and data of 'emp' table into emp1 table. ex2: create table emp1 as select * from emp where 1=2; This statement creates a table called 'emp1' with only structure similar to table 'emp'.It does not select any row of 'emp' table , Because the where condition(1=2) is 'FALSE' for all rows of emp TABLE.i.e this statement copies only structure of emp table into emp1 table. Thank you. Dear Axe, where 1 = 1 means it will always be true, so all records will be returned. ' 1=1 ' in where condition of SQL statement makes the where condition true. For example, consider the following example.\nHello everyone, I have installed Oracle 12c and when I try to create a user I am getting errors as below ORA-65096: invalid common user or role name Forums say alter session set \"_oracle_script\"=true ; it works fine now. The problem for me is when I try to run the shell script it is not accepting the above session value. shell script contains multiple sql where it will drop users and create a user and create the role. I set up alter session set \"_oracle_script\"=true; inside the sql script. but I am facing this issue again and again. I want to make permanently this value into my DB. Kindly help. I have even changed the parameter value oracle_script\"=true in PFILE. still again and again I am getting the error ORA-65096: invalid common user or role name Thanks. Dear Axe, 99.9% of the time the error ORA-65096: invalid common user or role name means you are logged into the CDB when you should be logged into a PDB. For example, if you used the default 19c installation settings, you should login to ORCLPDB (the PDB) instead of ORCL (the CDB). DANGER - If you insist on creating users the wrong way, follow the steps below. Setting undocumented parameters like this (as indicated by the leading underscore) should only be done under the direction of Oracle Support. Changing such parameters without such guidance may invalidate your support contract. So do this at your own risk.\nHi, i have one question for dme three node deployment. we will deploy 2 vm in DC1 and 1 vm in DC2. DC1 and DC2 will use different IP segment. so, for communicate between 2vm in DC1 and 1vm in DC2, which port that we need to open? thank you. Hi, dear To ensure the normal running of DME Storage, check the installation environment to verify that it meets the recommended configuration requirements. After DME Storage is installed, obtain the IP addresses, ports, and protocols for DME Storage to communicate with managed objects and operation terminals. For thethree-node deployment, you can check the communication as follows: Firewall Access Source Access Target Port Protocol Function Firewall 1 IP address of a client Static IP addresses of all nodes 22 SSH Logs in to the OS. SFTP Uploads software packages. Management floating IP address of 31945 HTTPS Accesses the management portal. Firewall 2 IP address of a client Static IP addresses of all DME Storage nodes 22 SSH Logs in to the DME Storage node using SSH for routine O&M. Management floating IP address of 31943 HTTPS Accesses . 31945 HTTPS Accesses the management portal. Firewall 3 Static IP addresses of all nodes Management IP address of a storage device 8088 HTTPS Accesses a storage device. 22 SSH Collects the monitoring data of storage disks. Management IP address of a switch 22 SSH Accesses a switch. Management IP address of a host 22 SSH Accesses a host.\nHere, we divided the storage components into three sections according to the abstraction level to which they belong: Lower-level layers Middle-level layers Higher-level layers Higher-level layers The higher-level layers consists of the presentation and application layers. The earliest approach was to tightly couple the storage device with the server. This server-attached storage approach keeps performance overhead to a minimum. Storage is attached directly to the server bus by using an adapter, and the storage device is dedicated to a single server. The server itself controls the I/O to the device, issues the low-level device commands, and monitors device responses. Initially, disk and tape storage devices had no onboard intelligence. They merely ran the I/O requests of the server. The subsequent evolution led to the introduction of control units (CUs). These units are storage offload servers that contain a limited level of intelligence. The CUs can perform functions, such as I/O request caching for performance improvements or dual copying data (RAID 1) for availability. Many advanced storage functions are developed and implemented inside the CU. Network-attached storage (NAS) is basically a LAN-attached file server that serves files by using a network protocol, such as Network File System (NFS). NAS refers to storage elements that connect to a network and provide file access services to computer systems. An NAS storage element consists of an engine that implements the file services (by using access protocols, such as NFS or Common Internet File System (CIFS)) and one or more devices, on which data is stored.\nNAS elements might be attached to any type of network. From a SAN perspective, a SAN-attached NAS engine is treated just like any other server. However, NAS does not provide any of the activities that a server in a server-centric system typically provides, such as email, authentication, or file management. NAS allows more hard disk storage space to be added to a network that already uses servers, without shutting them down for maintenance and upgrades. With an NAS device, storage is not a part of the server. Instead, in this storage-centric design, the server still handles all of the processing of the data, but an NAS device delivers the data to the user. An NAS device does not need to be located within the server, but an NAS device can exist anywhere in the LAN. An NAS device can consist of multiple networked NAS devices. These units communicate to a host by using Ethernet and file-based protocols. This method is in contrast to the disk units that are already described, which use Fibre Channel Protocol (FCP) and block-based protocols to communicate. NAS storage provides acceptable performance and security, and it is often less expensive for servers to implement (for example, Ethernet adapters are less expensive than Fibre Channel adapters). To bridge the two worlds and open up new configuration options for clients, certain vendors, sell NAS units that act as a gateway between IP-based users and SAN-attached storage.\nSAN fiber Channel topologies. Fibre Channel-based networks support three types of base topologies: Point-to-point Arbitrated loop Switched fabric A switched fabric is the most commonly encountered topology today and it has subclassifications of topology. The below Figure 1 shows the various classifications of SAN topology. Switched fabric topology Our third topology, and the most useful topology that is used in SAN implementations, is Fiber Channel Switched Fabric (FC-SW). It applies to switches and directors that support the FC-SW standard; that is, it is not limited to switches as its name suggests. A Fiber Channel fabric is one or more fabric switches in a single, sometimes extended, configuration. Switched fabrics provide full bandwidth for each port that is compared to the shared bandwidth for each port in arbitrated loop implementations. One key differentiator is that if you add a device into the arbitrated loop, you further divide the shared bandwidth. However, in a switched fabric, adding a device or a new connection between existing devices or connections actually increases the bandwidth. For example, an 8-port switch (assume that it is based on 2 Gbps technology) with three initiators and three targets can support three concurrent 200 MBps conversations or a total of 600 MBps throughput. This total equates to 1,200 MBps if full-duplex applications are available. The following Figure 2 shows a switched fabric configuration. Figure 2. Sample switched fabric topology This configuration is one of the major reasons why arbitrated loop is considered a historical SAN topology.\nGlance is an image management service used by OpenStack. OS images can be stored in a backend storage, including local directories, NFS, and Swift. The configuration is specified by the parameters in the /etc/glance/glance-api.conf file. In this example, Glance is stored locally by default. In the test, Glance is stored on a volume of the storage device. Test procedure: 1. Manually create a 100 GB volume named glance on the storage device and map it to the controller node. 2. The controller node scans for LUNs and uses multipathing to merge LUNs. 3. Create the /var/lib/glance_huaweistorage directory, format the 100 GB LUN, and mount it to the directory. 4. Modify the directory permission. 5. Modify the configuration file. Change the value of filesystem_store_datadir in /etc/glance/glance-api.conf to /var/lib/glance_huaweistorage/images/ and set the image directory of the Glance to the block disk of Huawei storage. 6. Change the value of image_cache_dir in /etc/glance/glance-api.conf to /var/lib/glance_huaweistorage/image-cache and set the image cache directory of Glance to the block disk of Huawei storage. 7. Run the systemctl restart openstack-glance* command to restart Glance services. 8. Upload an ISO file of RHEL 7.6 and use openstack image create --container-format bare --disk-format iso --public --file /ISO_LOCATION IMAGE_NAME to manually create an image. In this case, the upload fails. Check the /var/log/glance/glance-api.log file. It is found that you do not have the permission to create the directory. Check the directory permission. The directory permission is the same as that of the Glance directory but cannot be imported.\nFirst the Term RAM is Random access memory, or what is known as Ram, is an internal memory for a personal computer, control unit, or even a smartphone. It is characterized by its rapid response to processor commands and its execution faster than the speed of the drive, as a large part of the operating system is loaded on it while the computer is running. It is also a link that does not retain data permanently, but it can temporarily store all the data that is being used at the present time so that the computer can process it as soon as possible. If we want to talk about the history of the development of Double data rate RAM, which is known for short as DDR or random memory with a double transfer rate, where random access memory has passed through many generations, including DDR1 and DDR2, and at the present time, motherboards have arrived to support DDR4, which appeared for the first time seven years ago, The secret behind this shift in the first place is the requirements for increased speeds and specifications, in addition to the commercial reasons that sometimes require you to re-purchase a new new generation motherboard with a processor, which is something that Intel is so skilled at in order to pay money for a small jump in performance sometimes . Important note: GDDR memory is used to store menus and graphic data and is not very power-intensive for graphics card processors.\nDDR5 Release Date The new generation of DDR5 RAM release date was in November 4, the first DDR5 RAM that will have capabilities and frequencies up to twice as much, with significant and revolutionary improvements in performance on future computers. DDR5 memory needs the new Intel Alder Lake processors with the Golden Cove architecture coming this year, which needs a new processor socket named LGA 1700, and AMD users will have to wait for the Zen 4 architecture in the future because Zen 3 will not support the new generation DDR5 memory in the near future during 2022. The release of DDR5 memory coincide with the launch of Intel's Alder Lake-S CPUs. DDR5 memories are characterized by significant improvements compared to the current generation, and it is not only the strength of the frequencies in DDR5 memories, as the capacities of the DIMM ports, which are the ram port on the motherboard to 128 GB/byte per slot, and currently the maximum DDR4 ports that they can handle is only 32 GB/byte. .\nStorage virtualization refers to the abstraction of storage systems from applications or computers. Storage virtualization is a foundation for the implementation of other technologies, such as thin provisioning, tiering, and data protection, which are transparent to the server. . Storage virtualization offers several advantages: Improved physical resource usage: By consolidating and virtualizing storage systems, previously wasted white space can be used. Improved responsiveness and flexibility: By decoupling physical storage from virtual storage, you can reallocate resources dynamically, as required by the applications or storage subsystems. Lower total cost of ownership (TCO): Virtualized storage offers more capability with the same or less storage. Several types of storage virtualization are available. Block-level storage virtualization Block-level storage virtualization refers to provisioning storage to your operating systems or applications in the form of virtual disks. Fibre Channel (FC) and Internet Small Computer System Interface (iSCSI) are examples of protocols that are used by this type of storage virtualization. Two types of block-level virtualization are available: Disk-level virtualization Disk-level virtualization is an abstraction process from a physical disk to a logical unit number (LUN) that is presented as a physical device. Storage-level virtualization Unlike disk-level virtualization, storage-level virtualization hides the physical layer of Redundant Array of Independent Disks (RAID) controllers and disks, and hides and virtualizes the entire storage system. File-level storage virtualization File-level storage virtualization refers to provisioning storage volumes to operating systems or applications in the form of files and directories.\noning. Figure a. Zoning based on device WWNs. An instance of WWN-based zoning is shown in Figure a. In this case, symbolic names are specified for each WWN in the SAN to apply the same port zoning rules as indicated in figure b below: Figure b. The number of switch ports determines zoning. Zone 1 is confined to only these devices and contains the identities alex, ben, and sam. Zone 2 is confined to only these devices and contains the aliases robyn and ellen. Zone 3 is confined to only these devices and contains the aliases matthew, max, and ellen. Software zoning can lead to the following security flaws: The simple name server (SNS) searches the software zoning table for authorized storage devices when a given host signs in to the fabric and requests them. Only the storage devices specified in the software zoning table are visible to the host. However, by employing device discovery, the host can directly connect to the storage device without having to ask SNS for the information. Rather than using the WWN assigned by the HBA manufacturer, a device might establish its own WWN. WWN spoofing is the term for this technique. An unknown server can impersonate a trusted server and obtain access to data stored on a specific storage device. By allowing the WWN to be connected to a specific port, certain fabric operating systems allow the fabric administrator to avoid this risk. Devices can be discovered and communicated with by any device that probes for WWNs in some way.\nAn unlisted phone number is a simple analogy. Despite the fact that the phone number is not public, nothing prevents someone from contacting it, whether on purpose or by accident. For the WWN, the same is true. Certain devices search for WWNs at random to see if they can initiate a conversation. Many switch vendors provide hardware-enforced WWN zoning, which might help to avoid this security risk. Instead than depending on the servers to obey the Fiber Channel Protocols, hardware-enforced zoning uses hardware methods to restrict access. When a device registers in to a software-enforced zone, it asks the name server for devices within the fabric, as in software zoning. Only devices in the same zone or zones are returned if zoning is enabled. The name server query response hides other devices. The switch does not manage data flow while using software-enforced zones, therefore there is no guarantee that data transferred from illegal zone members will be secure. Use software zoning, in which participating hosts ensure flexibility and security. Frame filtering Zoning is a fabric management service that may be used to divide a SAN into logical subsets. This service can also help with resource partitioning for administration and access control. Another feature that allows devices to give greater granularity zoning is frame filtering. Port-level zoning, WWN zoning, device-level zoning, protocol-level zoning, and logical unit number (LUN)-level zoning can all be accomplished with frame filtering. An application-specific integrated circuit is typically used to filter frames (ASIC).\noning. Hardware zoning is demonstrated in Figure b below. Another way to think of hardware zoning is as an array of connectors, as shown in this example. Figure b. Hardware zoning Device A can only use connection A to access storage device A in Figure b. Through connection B, Device B can only access Storage Device B. Switch hardware, usually at the application-specific integrated circuit (ASIC) level, ensures that no data is passed between illegal zone members in a hardware-enforced zone. Devices, on the other hand, can transport data between ports in the same zone. As a result, rigid zoning offers the highest level of protection. The switch hardware determines the availability of hardware-enforced zoning and the methods for creating hardware-enforced zones. One drawback of hardware zoning is that devices must be attached to a specific port, and if the device is linked to a different port, the entire zoning configuration becomes ineffective. The usage of software zoning is likely to simplify your setting in circumstances when device connections are not permanent. Hardware zoning has the benefit of being able to be put into a routing engine via filtering. As a result, this style of zoning has little impact on the routing process' performance. In a hardware zone, the designer can include as many unused ports as possible. As a result, if one port fails, such as due to a faulty gigabit interface converter (GBIC) or transceiver, the connection can be relocated to another port in the same zone.\nThis value is applicable to scenarios where the service load is relatively heavy. The speed is normally from 10 to 20 MB/s. High : When the speed is set to high, the data synchronization will take a short period. This value is applicable to scenarios where the service load is relatively light. The speed is normally from 50 to 70 MB/s. Highest : When the speed is set to highest, the data synchronization will take the shortest period. This value is applicable to scenarios where the service load is light. The speed is normally above 100 MB/s. [Example] Medium Recovery Policy In the event of a system problem such as a disk array fault or a LUN failure, all the pairs in a clone are interrupted. For synchronization or reverse synchronization tasks that are ongoing when a system problem occurs: If Recovery Policy is Automatic , suspended tasks automatically resume once the problem is resolved. If Recovery Policy is Manual , the value of Pair Running Status changes to To be recovered once the system problem is resolved. In this case, you can check the system before deciding whether to manually resume the suspended tasks. [Example] Automatic Enable initial synchronization Indicates whether to implement an initial synchronization of data from the primary LUN to the secondary LUN. If data on the primary LUN is inconsistent with that on the secondary LUN, you must select initial synchronization. Otherwise, data stays inconsistent.\nDear all, In HUAWEI OceanStor V3, which licensed feature is best described as: periodically detects hotspot data and promotes them from low-speed storage media to high-speed media? Thanks. Dear Axe, The SmartTier dynamic storage tieringhelps select an appropriate storage tier for each data block based on the data activity level. This feature improves the storage system performance and lowers the total cost of ownership. The SmartTier does not impose any adverse impact on service continuity or data availability. Because of the disk technology improvement, storage systems support a growing number and more types of disks. The disks supported by this storage system are solid state drives (SSDs), serial attached SCSI (SAS) disks, and nearline SAS (NL-SAS) disks. Each type of disks offers its unique advantages and disadvantages in performance and cost and it is difficult to keep balance between performance and cost: SSDs have a fast response time, a low request processing cost but a high capacity cost per gigabyte. NL-SAS disks have a slow response time, a high storage request processing cost but a low storage capacity cost per gigabyte. SAS disks fall in between the previous two types in the storage request processing cost and the storage capacity cost per gigabyte. The SmartTier is deployed to increase the performance and reduce cost of the whole storage system (By the cost we mean the cost of purchase of software and hardware, footprint, energy, and administration.). The SmartTier manages data storage intelligentlyl.\nHello all, NoF+ is a Huawei NVMe-oF solution. The RoCEv2 protocol runs the NVMe storage access protocol on the IP network, and Huawei's unique AI-Fabric and NOF Director technologies at the network layer provide efficient and reliable IP storage network services. The technology stack is shown below, and we achieve the best solution combination at all levels.The solution uses the NVMe over RoCE network protocol and combines three types of devices to achieve high reliability, low latency, and high-bandwidth and cost-effective transmission among data centers. This solution is known as NoF+. Why do we need Huawei NoF+? Here are 4 reasons for this. In traditional data centers, the end-to-end latency from servers to storage devices is similar to that shown in the preceding figure. Most of the time is spent on the processing of servers and storage devices. The latency of storage networks accounts for a small proportion. Therefore, all people focus on improving storage performance. With the development of flash storage media and heterogeneous computing capabilities (such as GPUs and NPUs), the performance of servers and storage devices is greatly improved. The new end-to-end latency becomes as follows: As storage and computing technologies advance rapidly, networks have become a new performance bottleneck. However, we need to find faster storage network solutions to further improve end-to-end storage access performance.\nIn traditional data centers, there are multiple types of storage networks, including Fibre Channel networks focusing on key core systems, IP networks for private clouds and virtualization platforms, and InfiniBand networks in HPC and some database appliance scenarios. However, the emerging public cloud and edge are basically IP-based. Among the top cloud vendors in the industry, almost all storage networks are IP-based. Even in HPC service scenarios, Azure insists on providing InfiniBand, and AWS, GCP, and Alibaba Cloud have all shifted to IP networks and provided high-speed RDMA network services using RoCE or similar technologies. In edge scenarios, HCI-like solutions are usually used. Due to the high cost of Fibre Channel and InfiniBand networks, all the solutions have been switched to IP networks. Considering that applications need to be seamlessly integrated, deployed, and migrated between the cloud, core, and edge, network normalization needs to be considered in advance to evolve to an all-IP data center. IP-based storage networks are inevitable. Traditionally, front-end interfaces, back-end interfaces of storage devices, and interconnection interfaces between controllers usually use different network technologies. For example, Dell EMC PowerMax uses Fibre Channel ports at the front end, PCIe ports at the back end, and InfiniBand network for controller interconnection. Due to historical technical limitations, different storage vendors use different storage ports in different scenarios. However, interface cards cannot be used in different scenarios, which inadvertently increases costs. In addition, in the next-generation NVMe storage, the PCIe technology commonly used in the back end has scalability limitations, which severely limits storage capacity expansion.\nDear All today we will learn about FusionStorage Troubleshooting process and Log Collection Troubleshooting Process Troubleshooting Principles Identify the fault in a timely manner by focusing on alarm reporting and performing periodic preventive maintenance inspection (PMI). Identify the fault impact scope by judging the number of users whose services are affected and checking whether two-point failures are caused. Take note of fault information, such as fault time period and symptoms. Collect logs by accessing the log path of each component based on certain log collection methods. Alarm Mechanism Alarms are reported by the alarm module to a database and presented to maintenance personnel on the portal if events that affect system reliability are detected when the system monitors each component in real time or services are processed. Alarms are classified into warning, minor, major, and critical alarms based on their severities. Alarm Page Introduction to Logs Log classification: FSM, FSA, MDC, OSD, VBS, and DI logs. Log level: DEBUG, INFO, WARNING, and ERROR. Historical log backup: backup based on different scenarios. Key process for logs: startup, initiation, volume creation, and CRB. Log exported by considering performance: Logs are exported from the memory to permanent hard disks every 15 minutes. In urgent scenarios, you can use a tool to export logs. Commonly-Used Log Paths When a log server is configured, logs generated by the CNA node are packetized per 15 minutes and uploaded to the log server.\nBitcoin is the world's first decentralized cryptocurrency a type of digital asset that uses public-key cryptography to record, sign and send transactions over the Bitcoin blockchain. Launched on Jan. 3, 2009, by an anonymous computer programmer under the pseudonym Satoshi Nakamoto, the Bitcoin network (with an uppercase B) is a peer-to-peer electronic payment system that uses a native cryptocurrency called bitcoin (lower case b) to transfer value over the internet or act as a store of value like gold and silver. Each bitcoin is made up of 100,000,000 satoshis (the smallest units of bitcoin), making individual bitcoin divisible up to 8 decimal places. This allows people to purchase fractions of a bitcoin with as little as one U.S. dollar. Bitcoin and other cryptocurrencies are like the email of the financial world. The currency does not exist in physical form, value is transacted directly between the sender and the receiver, and there is no need for banking intermediaries to facilitate the transaction. Everything is done publicly through a transparent, immutable, distributed ledger technology called blockchain. Launched in 2009, bitcoin is the world's largest cryptocurrency by market capitalization. Unlike fiat currency, bitcoin is created, distributed, traded, and stored with the use of a decentralized ledger system, known as a blockchain. Bitcoin's history as a store of value has been turbulent; it has gone through several cycles of boom and bust over its relatively short lifespan. As the earliest virtual currency to meet widespread popularity and success, bitcoin has inspired a host of other cryptocurrencies in its wake.\nBitcoin transactions are recorded on a public, distributed ledger known as a blockchain that anyone can download and help maintain. Transactions are sent directly from the sender to the receiver without any intermediaries. Holders who store their own bitcoins have complete control over them they cannot be accessed without the holders cryptographic key. Bitcoin does not exist in physical form. Bitcoin has a fixed supply of 21 million bitcoin. No more bitcoin can be created and units of bitcoin cannot be destroyed. Bitcoin is one of the first digital currencies to use peer-to-peer technology to facilitate instant payments. The independent individuals and companies who own the governing computing power and participate in the bitcoin networkbitcoin \"miners\"are in charge of processing the transactions on the blockchain and are motivated by rewards (the release of new bitcoin) and transaction fees paid in bitcoin. These miners can be thought of as the decentralized authority enforcing the credibility of the bitcoin network. New bitcoin are released to the miners at a fixed, but periodically declining rate. There are only 21 million bitcoin that can be mined in total. As of June 2021, there are over 18 million bitcoin in existence and less than 3 million bitcoin left to be mined.3 In this way, bitcoin and other cryptocurrencies operate differently from fiat currency; in centralized banking systems, the currency is released at a rate matching the growth in goods; this system is intended to maintain price stability.\nA decentralized system, like bitcoin, sets the release rate ahead of time and according to an algorithm. Bitcoin users send and receive coins over the network by inputting the public-key information attached to each persons digital wallet. In order to incentivize the distributed network of people verifying bitcoin transactions (miners), a fee is attached to each transaction. The fee is awarded to whichever miner adds the transaction to a new block. Fees work on a first-price auction system, where the higher the fee attached to the transaction, the more likely a miner will process that transaction first. Every single bitcoin transaction that takes place has to be permanently committed to the Bitcoin blockchain ledger through a process called mining. Bitcoin mining refers to the process where miners compete using specialized computer equipment known as Application-Specific Integrated Circuit (ASIC) chips to unlock the next block in the chain. One bitcoin is divisible to eight decimal places (100 millionths of one bitcoin), and this smallest unit is referred to as a Satoshi. If necessary, and if the participating miners accept the change, bitcoin could eventually be made divisible to even more decimal places. Crypto mining uses a system called cryptographic hashing. This function simply takes any input (messages, words or data of any kind) and turns it into a fixed length alphanumeric code known as a hash. Each input creates a completely unique hash and its nigh on impossible to predict what inputs will create certain hashes.\nEven changing one character of the input will result in a totally different fixed-length code. Each new block has a value called a target hash. In order to win the right to fill the next block, miners need to produce a hash that is lower than or equal to the numeric value of the target hash. Since hashes are completely random, its just a matter of trial and error until one miner is successful. This method of requiring miners to use machines and spend time and energy trying to achieve something is known as a Proof-of-Work system and is designed to deter malicious agents from spamming or disrupting the network. Whoever successfully unlocks the next block is rewarded with a set amount of bitcoin known as block rewards and gets to add a number of transactions to the new block. They also earn any transaction fees attached to the transactions they add to the new block. A new block is discovered roughly once every ten minutes. Bitcoin block rewards decrease over time. Every 210,000 blocks (or roughly four years), the number of bitcoins in each block reward is halved to gradually reduce the number of bitcoins entering the space over time. As of 2021, miners receive 6.25 bitcoins each time they mine a new block. The next bitcoin halving is expected to occur in 2024 and will see bitcoin block rewards drop to 3.125 bitcoins per block.\nAs the supply of new bitcoin entering the market gets smaller it will make buying bitcoin more competitive assuming demand for bitcoin remains high. This process of requiring network contributors to dedicate time and resources to creating new blocks ensures the network remains secure. But this security comes at a price. The Bitcoin network currently consumes around 93 Terawatt Hours (TWh) of electricity per year around the same energy consumed by the 34th largest country in the world. This appetite for electricity has drawn widespread criticism from celebrities such as Tesla CEO Elon Musk to government bodies such as China's State Council and the United States Senate over Bitcoin's impact on climate change. But while these figures are alarmingly high, its important to note that Bitcoin mining at most accounts for 1.29% of any single country's energy consumption. Not to mention, Bitcoin is a complete financial system whose energy consumption can be measured and tracked, unlike the fiat system which cannot be accurately measured and requires a range of additional layers to function, including ATMs, card machines, bank branches, security vehicles, storage facilities and huge data centers. There are also a number of initiatives including the Crypto Climate Accord and the Bitcoin Mining Council that aim to improve Bitcoins carbon footprint by encouraging miners to use renewable sources of energy. Aug. 18, 2008 The domain name bitcoin.org is registered. Today, at least, this domain is \"WhoisGuard Protected,\" meaning the identity of the person who registered it is not public information.\nOct. 31, 2008 A person or group using the name Satoshi Nakamoto makes an announcement to the Cryptography Mailing list at metzdowd.com: \"I've been working on a new electronic cash system that's fully peer-to-peer, with no trusted third party. This now-famous whitepaper published on bitcoin.org, entitled \"Bitcoin: A Peer-to-Peer Electronic Cash System,\" would become the Magna Carta for how bitcoin operates today. Jan. 3, 2009 The first bitcoin block is minedBlock 0. This is also known as the \"genesis block\" and contains the text: \"The Times 03/Jan/2009 Chancellor on brink of second bailout for banks,\" perhaps as proof that the block was mined on or after that date, and perhaps also as relevant political commentary.6 Jan. 8, 2009 The first version of the bitcoin software is announced to the Cryptography Mailing list. Jan. 9, 2009 Block 1 is mined, and bitcoin mining commences in earnest. Investing money into bitcoin in any of its many guises is not for the risk-averse. Bitcoin is a rival to government currency and may be used for black market transactions, money laundering, illegal activities, or tax evasion. As a result, governments may seek to regulate, restrict, or ban the use and sale of bitcoin (and some already have). Others are coming up with various rules. Most individuals who own and use bitcoin have not acquired their tokens through mining operations. Rather, they buy and sell bitcoin and other digital currencies on any of the popular online markets, known as bitcoin exchanges or cryptocurrency exchanges.\nFusionStorage Planning: MDC Deployment Rules Each storage host reserves 5 GB memory resources and CPU resources of an MDC process. One storage host can have only one MDC process, and at least three MDC processes are required for a FusionStorage system. One MDC can process services in a maximum of two resource pools and a maximum of 2000 OSDs. A FusionStorage system at most runs 64 MDCs and 32 spare MDCs to prevent MDC faults caused by rack-level faults. It at most supports 128 resource pools. In the phase of deployment planning, reserve two idle MDCs to ensure that a resource pool always belongs to one MDC. MDC must be deployed across racks in rack-level security . FusionStorage Planning: ZooKeeper Deployment Rules A FusionStorage system can run three or five ZooKeeper processes. ZooKeeper disk deployment: In standard FusionCube USB flash drive scenarios, the ZooKeeper disk occupies one hard disk. Slot 12 on an E9000 server is used by default. Occupying one hard disk and one slot will reduce FusionStorage capacity. ZooKeeper partition In cloud platform scenarios, the ZooKeeper disk occupies the same RAID group with the OS and uses partition 10 of the OS. A total of 50 GB storage space is used. The ZooKeeper disk cannot be deployed on the host to which the management VM belongs. The ZooKeeper disk shares the system disk and storage space with the OS. FusionStorage Planning: VBS Deployment Rules The VBS supports both SCSI and iSCSI ports.\nDear All, Today we are going to discuss FusionStorage VBS, OSD, and MDC FusionStorage VBS Module and Working Process The VBS module provides access services in FusionStorage and is used for: Volume and snapshot management I/O accessing and processing The VBM module is used for volume and snapshot management: creating a volume, attaching a volume, detaching a volume, querying a volume, deleting a volume, creating a snapshot, deleting a snapshot, and creating a volume using a snapshot. FusionStorage OSD Module and Working Process A physical hard disk requires an OSD process and the OSD process is responsible for Hard disk management I/O replication I/O data caching Disk Partition of the FusionStorage OSD Module: Each key is assigned to 1 MB disk space, and a certain number of keys form a chunk. Chunk: A partition consists of one or more chunks. VDB architecture for a raw disk FusionStorage MDC Module: MDC (Metadata Controller) is a highly reliable cluster, which can ensure high availability and reliability of the whole system using high availability (HA). Ensure reliability of metadata, such as topology, OSD view, partition view, and VBS view, using ZooKeeper clusters. Ensure data reliability using multiple data copies based on the partition allocation algorithm. Obtain the OSD or VBS status change or inform the OSD or VBS of the VBS or OSD status change based on message exchange between the OSD and VBS. Expand system capacity, query the system status, and perform maintenance on the system based on message exchange with agents.\nMonitor the OSD and VBS status using the heartbeat detection mechanism. ZooKeeper uses the distributed service architecture and is used for providing naming service, status synchronization service, cluster management, and distributed application configuration item management. The specific work is as follows: Managing leader and follower MDC nodes: MDC is deployed on three nodes, in which one is the leader MDC node, the others are follower MDC nodes. After the MDC processes are started, all the MDC processes will be registered on the ZooKeeper nodes. The MDC process that is registered early becomes the leader MDC. During the running process, ZooKeeper records information about the leader and follower MDC processes and monitors the health statuses of all MDC processes. If the leader MDC process is faulty, a new leader MDC process will be selected. Data store: During the MDC running process, control views, such as target, middle, and I/O views, are generated. ZooKeeper can provide interfaces used for saving, updating, querying, and deleting these control views. Data synchronization: When data is updated to the primary ZooKeeper disk, the primary ZooKeeper automatically synchronizes the data to the two secondary ZooKeeper disks, ensuring the real-time data synchronization. When the switchover occurs between the primary and secondary ZooKeeper nodes, services are not affected. FusionStorage View I/O view: indicates the mapping between the primary partition and the OSD node. Partition view: indicates the mappings between the primary and secondary partitions and the OSD node. The I/O view is a part of the partition view.\nThe MDC monitors the status of OSDs based on the heartbeat detection mechanism. The OSDs send specific message, such as OSD capacity, to the MDC. If the MDC does not receive any heartbeat messages from an OSD module for a period of time (5s in the current system), the OSD will be removed because the MDC considers that the OSD is faulty. For example, the OSD process disappears, or the network between the OSD and MDC is disconnected. In this case, the MDC updates the system OSD view and sends the view change message to each OSD. After receiving the message, the OSDs determine their follow-up work. Multi-copy replication is subject to the MDC view. In two-copy mode, when the client sends a write request to the primary OSD, the primary OSD copies the write request based on the MDC view to a secondary OSD of the partition. In multi-copy mode, the primary OSD module will copy the write request to multiple secondary OSDs. Relationship Between Main FusionStorage Modules When the system starts, MDCs interact with the ZooKeeper to decide a leader MDC. The leader MDC has monitoring heartbeat links with other MDCs and decides an MDC to handle the work of a faulty MDC. When the leader MDC is faulty, other MDCs interact with ZooKeeper and decide a new leader MDC. When an OSD starts, it queries the home MDC and reports the status to the home MDC. The home MDC sends the status change to a VBS.\nCoffer disk is used to temporarily store the raw data before they are actually written on the RAID. This is both a speed-up solution and a safety feature in the event of a blackout. To accomplish this task the coffer disks are helped by the BBU (backup battery unit). BBU unit is the backup battery that provides power to the storage controller to save all I/O into the coffer disk in the event of a power failure. Once the external power is restored, the cached data in the coffer disk will be flushed to the controller's cache an resume the I/O process to the physical disks in the external disk enclosures ensuring no data loss. Coffer disks are used to store three types of data: cache data requiring power failure protection, OceanStor OS system data, and system configuration information and logs. The storage system has two kinds of coffer disks: built-in coffer disk and external coffer disk: Built-in coffer disk Each controller houses one or more disks as coffer disks inside. They can be mSATA or SSD disks. External coffer disk In case of external coffer disk we have to scenarios: Disk and controller integration architecture If a storage system employs the disk and controller integration architecture, the first four disks in the storage system are configured as coffer disks. SAS, NL-SAS, or SSD disks can be used as coffer disks. The type of the four coffer disks must be the same. The coffer disks are labeled.\nStorage resource pool plane The storage resource pool plane is responsible for managing the allocation and management of all physical storage of the storage cluster nodes, and dividing the cluster nodes into multiple node pools by splitting the node pool. OceanStor DFS's InfoProtector technology provides N+M protection. N represents how many nodes the data is cut. M represents the number of nodes and disks that can be defended at the same time. Users can configure the M value. The N value is determined by the system itself. The size is determined, and as the number of cluster nodes grows, N grows, providing greater storage utilization without degrading data protection capabilities. When the data is configured as +M protection, data corruption will occur only if the M+1 nodes in the same node pool are faulty or greater than or equal to M+1 hard disk failures. The method of dividing the entire storage cluster node into multiple node pools greatly reduces the probability of data corruption. This protection allows files to be hashed across the entire cluster, providing higher data parallel access and re-parallelism. When a disk or node fails, the system can discover which parts of the file are affected and allow multiple The nodes participate in the reconstruction process, so that the number of disks and the number of CPUs participating in the reconstruction far exceed the traditional RAID technology, which makes the fault reconstruction time leap forward.\nWhen it comes to NAS, you usually think of a traditional NAS device. It has its own file system, has a large storage capacity, and has certain file management and service functions. The NAS device and the client are connected through an IP network, and files are shared between different platforms based on the NFS/CIFS protocol, and the data is transmitted by using the file as an organizational unit. Although NAS devices are often considered a storage architecture, the core of NAS devices is actually storage, which is file management services. Functionally, a traditional NAS device is a file server with DAS storage. From the IO path of the data, its data IO occurs inside the NAS device, and this architecture is no different from DAS. In fact, many NAS device internal file service modules and disks are connected through the SCSI bus. As for sharing files through NFS/CIFS, it is completely in the high-level protocol communication, and it is not in the data IO path at all, so the data transmission cannot be organized in blocks. Because of this overlapping of functions, after the emergence of SAN, NAS head devices (or NAS gateways) have gradually developed, and NAS over SAN solutions have become more and more, and NAS has returned to the essence of its file services. It can be seen that the location of the NAS and the general application host at the network level is the same. In order to store data on the disk, a file system must be established.\nStorage capacity planning allows IT admins to make the most of their budget and allocate resources where needed. Without proper storage capacity planning, IT departments may end up wasting money on extra storage space or failing to invest in the storage infrastructure that their firm requires. While the former is a waste of time and money, the latter can cause serious problems for end users. When it comes time to add a new, must-have application or program, an IT specialist may run into a wall due to a lack of storage capacity, halting staff productivity and preventing key projects from moving forward. Admins can avoid unexpected overruns, outages, and last-minute purchases by keeping a close eye on when capacity limitations will be reached. Effective capacity planning also aids administrators in avoiding disc bottlenecks and locating the source of performance issues. It's not enough to think about storage capacity planning as simply adding more space. Using strategies like these, it may be possible to reduce storage requirements: File compression: which can reduce the amount of space needed to store files and images Data deduplication: which eliminates extra copies of the same file, lowering the amount of storage capacity required Thin Provisioning: Storage administrators can use thin provisioning to virtually designate storage and only allocate it when it's needed. storage virtualization: a method of pooling and managing physical storage from a single storage device.\nHi Dear Members, When we talk about a particular technological category some basic concepts or important terms are needed to be learned first. Even if you are not specialized in that field these terms help you understand lots of important features and products related to IT, so today we are starting to explain some basic terms frequently used in Storage. RAID is a storage virtualization solution that stores and replicates data across several physical drives in a disc group (logical disc). The disc group outperforms a single disc in terms of storage performance and data redundancy. There are several RAID levels to choose from. RAID technology in a new form. Block virtualization divides discs into a defined number of chunks (CKs) and organizes them into chunk groups (CKGs). When a disc fails, the CKG discs that contain the CKs from the defective disc also participate in the reconstruction process. This greatly increases the number of discs used in the reconstruction, resulting in faster data reconstruction. Ck is a short term of Chunk. TheDisks are separated into fixed-size blocks (typically 4 MB). A CK is made of a number issued to each block. A CK is a RAID group's smallest unit. A CKG is short of Chunk Group IT isa logical collection of N+M CKs spread across many drives. The number of data blocks in a CKG varies depending on the number of drives used. In a CKG, M is the number of parity blocks.\nHello, everyone! You should consider several requirements for the storage infrastructures of today: Unlimited and just-in-time scalability: Businesses require the capability to flexibly adapt to the rapidly changing demands for storage resources without performance degradation. System simplification: Businesses require an easy-to-implement infrastructure with a minimum amount of management and maintenance. The more complex the enterprise environment, the more costs that are involved in terms of management. Simplifying the infrastructure can save costs and provide a greater return on investment (ROI). Flexible and heterogeneous connectivity : The storage resource must be able to support whatever platforms are within the IT environment. This resource is essentially an investment protection requirement that allows for the configuration of a storage resource for one set of systems. It later configures part of the capacity to other systems on an as-needed basis. Security : This requirement guarantees that data from one application or system does not become overlaid or corrupted by other applications or systems. Authorization also requires the ability to fence off the data of one system from other systems. Encryption : When sensitive data is stored, it must be read or written only from certain authorized systems. If for any reason the storage system is stolen, data must never be available to be read from the system. Hypervisors : This requirement is for the support of the server, application, and desktop virtualization hypervisor features for cloud computing.\nHello team, Does anyone know how to handle this? How do I lay out cabinets on eService-V2R1? Thanks. Dear Phany, Have a good day! Please follow the instruction below to lay out cabinets. Hardware Configuration Cabinet Planning Add Cabinet Table 1 Scenario Principle Plan the location of a cabinet 1. The distance between two adjacent cabinets should be at least 120 cm. The distance between a cabinet and the wall should be at least 100 cm. 2. It is recommended that you use 5 m of SAS cables to connect different cabinets. Plan the locations of storage devices in a cabinet 1. The bearable weight of the guide rails of a cabinet should be greater than 50 kg, and the bearable weight of the guide rails of a 6 U controller enclosure should be greater than 95 kg. 2. It is recommended that the controller enclosure be installed in the middle of the cabinet (from 19th U to 24th U). 3. The layout plan meets the requirement of power supply and cooling system of the equipment room (depending on the cooling effect of the air conditioner). It is recommended that you place up to 8 disk enclosures in a single cabinet. 4. You are advised to install disk enclosures from the bottom up in descending order of weight. 5. It is recommended that you reserve a 1 U (44.45 mm, 1.75 inches) space between two devices, if possible. Alternatively, you can set five disk enclosures as a group.\nData is encrypted as it is written to the disc on self-encrypting drives (SEDs). A disc encryption key (DEK) is set at the manufacturer and saved on the disc for each disc. The DEK is used by the disc to encrypt and decrypt data as it is written to and read from the disc. The disk's functionality, including encryption and decryption, is invisible to users reading and writing data. Secure erase mode is the name given to the default encryption and decryption mode. To decrypt and read data in secure erase mode, you don't need an authentication key or password. For cases when discs must be repurposed or returned, SEDs provide better capabilities for a straightforward and quick secure wipe. Data held on company systems can be a source of risk. Nearly every computer store customer credit card information, personal identification numbers, email lists, company policies, product roadmaps, and intellectual property, making these systems vulnerable to accidental loss, hackers, and data thieves. Data is never exposed even if equipment is lost All user data is always secured. The encryption function cannot be disabled. When drives should be retired, repurposed, or relinquished data is erased instantly Data can be destroyed in an instant, even if the drive is broken Keeping data safe while in flight After the drive has been unlocked, prevent unauthorized users from accessing it. SEDs encrypt and decrypt data at the endpoint. Once authorized, the encryption is fully transparent to the system. OASIS/KMIP will be utilized in data center key management.\nBy 2025, 60% of large organizations will use one or more privacy-enhancing computation techniques in analytics, business intelligence or cloud computing.- Gartner Gartner has identified privacy-enhancing computation as a key enterprise technology trend for 2022. I would like to share this technology. The real value of data exists not in simply having it, but in how its used for AI models, analytics, and insight. Privacy-enhancing computation (PEC) approaches allow data to be shared across ecosystems, creating value but preserving privacy. Approaches vary, but include encrypting, splitting or preprocessing sensitive data to allow it to be handled without compromising confidentiality. Securely provide access to private datasets Ability to conduct a joint analysis of private data held by multiple organizations Securely outsource private data to cloud computing parties Scatter services that depend on user data There is a variety of software and hardware-based methods to protect data in use. Some examples include: secure multi-party computation, homomorphic encryption, zero-knowledge proofs, and trusted execution environments (TEE). Each technique tackles the problem of how to securely protect data in use, with accompanying advantages and drawbacks. Confidential Computing stands out as a stable, scalable, and highly performant solution for a broad range of use cases. By performing the computation in a hardware-based TEE, it prevents unauthorized access or modification of applications and data while in use, thereby increasing the security assurances for organizations that manage sensitive and regulated data.\nPrivacy-Enhancing Technologies or PETs are a set of technologies (software or hardware solutions) that allows organizations to extract the value of big consumer data without compromising its security and privacy. Here are some of its examples: Communication anonymizers hide the real online identity and replace it with a non-traceable identity. They can be applied to email, Web browsing, P2P networking, VoIP, etc. Shared bogus online accounts . One person creates an account for MSN, providing bogus data for Name, address, phone number, preferences, life situation etc. They then publish their user-IDs and passwords on the Internet. Everybody can now use this account comfortably. Thereby the user is sure that there is no personal data about him or her in the account profile. Obfuscation refers to the many practices of adding distracting or misleading data to a log or profile, which may be especially useful for frustrating precision analytics after data has already been lost or disclosed. Its effectiveness against humans is questioned, but it has greater promise against shallow algorithms. Obfuscating also hides personal information or sensitive data through computer algorithms and masking techniques. This technique can also involve adding misleading or distracting data or information so it's harder for an attacker to obtain the needed data. Access to personal data : The service provider's infrastructure allows users to inspect, correct or delete all their data stored at the service provider. Enhanced privacy ID (EPID) is a digital signature algorithm supporting anonymity.\nUnlike traditional digital signature algorithms (e.g., PKI), in which each entity has a unique public verification key and a unique private signature key, EPID provides a common group public verification key associated with many of unique private signature keys. EPID was created so that a device could prove to an external party what kind of device it is (and optionally what software is running on the device) without needing to also reveal exact identity, i.e., to prove you are an authentic member of a group without revealing which member. It has been in use since 2008. Homomorphic encryption is a form of encryption that allows computation on ciphertexts. Zero-knowledge proof is a method by which one party (the prover) can prove to another party (the verifier) that they know a value x, without conveying any information apart from the fact that they know the value x. Secure multi-party computation is a method for parties to jointly compute a function over their inputs while keeping those inputs private. Non-interactive zero-knowledge proof (NIZKs) are zero-knowledge proofs that require no interaction between the prover and verifier. Format-preserving encryption (FPE) , refers to encrypting in such a way that the output is in the same format as the input. Blinding is a cryptography technique by which an agent can provide a service to a client in an encoded form without knowing either the real input or the real output.\nDear all, Windows 11 is coming, let's see what are the new features in this OS. Simplify the design and user experience to increase productivity and stimulate your creativity. From the new Start button and taskbar to every sound, font, and icon, its carefully designed. Putting Start in the center makes it easier to quickly find what you need. Start takes advantage of the power of the cloud and Microsoft 365, no matter what platform or device you used to view the most recent files, even on Android or iOS, it can also show you the most recent files. In Windows 11, new features of Snap Layouts, Snap Groups, and Desktops have been introduced to provide a more powerful multitasking method and keep abreast of the work that needs to be done. Introduce the chat function from Microsoft Teams in the taskbar. Now you can instantly connect with all your personal contacts via text, chat, voice or video anytime, anywhere, no matter which platform or device they are using, or across Windows, Android or iOS. DirectX 12 Ultimate, which can achieve stunning and immersive graphics at high frame rates; DirectStorage can achieve faster loading times and more detailed game worlds; and automatic HDR provides a wider and more vivid colors Range, bringing a truly fascinating visual experience. Hardware compatibility has not changed-Windows 11 supports your favorite PC gaming accessories and peripherals. Use Xbox Game Pass for PC or Ultimate.\nThe new Microsoft Store is the only trusted place to watch, create, play, work and learn apps and content. It has been rebuilt to increase speed and has a new design that is beautiful and easy to use. Not only will it bring you more apps than ever before, but we also make all content (apps, games, shows, movies) easier to search and discover through selected stories and collections. Leading first-party and third-party applications such as Microsoft Teams, Visual Studio, Disney+, Adobe Creative Cloud, Zoom, and Canva are welcome. Windows 11 has all the power and security of Windows 10 with a redesigned and refreshed look. It also comes with new tools, sounds, and apps. Every detail has been considered. All of it comes together to bring you a refreshing experience on your PC. Some Windows 10 features are not available in Windows 11. Compared with Windows 10, Windows 11 does have the potential to increase computer speed. Microsoft talked about the performance advantages and optimizations in Windows 11 in a YouTube video. Overall, the performance benefits in Windows 11 are largely dependent on the way newer operating systems handle the system processes that you usually see when you open Task Manager. According to Microsoft, Windows 11 does a lot of work on memory management to support the application windows that you open and run in the foreground. This should ensure that they get more CPU power than other system resources.\nDear Huawei team, I'm looking for a detailed user privilege information on storage products. For example: List of job that administrator account can do? And normal users can do? Hello, friend! Have a nice day! To prevent misoperations from compromising the storage system stability and service data security, the storage system defines user levels and roles to determine user permission and scope of permission. Level : determines whether a user has operation or access permission. The storage system defines three user levels. Level Description Super administrator A super administrator has full administrative permissions on the storage device, and is able to create users of all levels. Administrator An administrator has partial administrative permissions on the storage device but cannot manage users, upgrade the storage device, modify the system time, perform batch configuration, restart the device, or power off the device. Read-only user A read-only user has only the access permission on the storage device. After logging in to the storage device, read-only users can only query information about the storage device. Role : defines the scope of objects that can be operated or accessed by a user. The storage system provides both built-in and user-defined roles.\nThe provisioning of SAN-attached storage capacity to a server can be a time-consuming and cumbersome process. The task requires skilled storage administrators. And the complexity of the task can restrict the ability of an IT department to respond quickly to requests to provision new capacity. However, a solution to this issue is available through automation. An on-demand storage provisioning solution monitors the current disk usage of specified target host systems and applications and allocates more disk capacity for the period of the business need. End-to-end storage provisioning is the term that is applied to the whole set of steps that are required to provision usable storage capacity to a server. Provisioning covers the configuration of all of the elements in the chain. This process includes the steps from carving out a new volume on a storage subsystem, through creating a file system at the host and making it available to the users or applications. Typically, this process involves a storage administrator that uses several different tools, each focused on a specific task, or the tasks are spread across several people. This spreading of tasks and tools creates many inefficiencies in the provisioning process, which affect the ability of IT departments to respond quickly to changing business demands. The resulting complexity and high degree of coordination can also lead to errors and can possibly affect the systems and application availability. Automation of the end-to-end storage provisioning process by using workflow automation can significantly simplify this task of provisioning storage capacity.\nA data reduction strategy can decrease the required disk storage and network bandwidth, lower the TCO for storage infrastructures, optimize the use of existing storage assets, and improve data recovery infrastructure efficiency. Compression, data deduplication, and other forms of data reduction are features that are offered in multiple components of the information infrastructure. Compression immediately reduces the amount of required physical storage across all storage tiers. This solution, which supports the online compression of existing data, allows storage administrators to gain back free disk space in the existing storage system. You can compress data without changing administrative processes or forcing users to clean up or archive data. The benefits to the business are immediate because the capital expense of upgrading the storage system is delayed. Because data is stored in compressed format in the primary storage system, all other storage tiers and the transports in between realize the same benefits. Replicas, backup images, and replication links all require fewer expenditures after the implementation of compression at the source. After compression is applied to the stored data, the required power and cooling for each unit of storage are reduced. This reduction is possible because more logical data is stored on the same amount of physical storage. In addition, within a particular storage system, more data can be stored; therefore, fewer overall rack units are required. The exact compression ratio depends on the nature of the data. As always, compression ratios vary by data type and how the data is used.\nSymptom : Procedure : 1. Deploy vStore HyperMetro, create a file system HyperMetro pair, create an NFS share, and enable the NFSv4 protocol. 2. The CentOS 7.4 VM client uses NFSv4 to mount the NFS share and uses Vdbench to continuously read and write the mount point directory. 3. The Vdbench read and write model is small I/O, a large number of files (100 to 200 files in this case), and a large number of concurrent requests (IOPS is about 1 to 2000 in this case). 3. Perform a primary/secondary switchover or the preferred site is faulty to trigger IP address flapping. Symptom : After the Vdbench tool stops at zero for about 7s, I/Os are re-read. After 3 to 6s, I/Os are re-zeroed for the second time. This time lasts for about 25 to 40s before I/Os are normal. Cause : The protocol, client, and storage are incompatible. NFSv4 is a stateful protocol. Therefore, the server cache always records the open status of the client file. When IP address flapping occurs (in any fault scenario, IP address flapping occurs), NFSv4 has a quiet period, which is required by the protocol itself, the corresponding parameters are set for the storage device, but the minimum time is 30s. After the silent period, the server does not respond to new Open operations and only allows the client to regenerate Open connections. If a large number of open states exist on the server, the server does not determine whether these states are valid after IP address flapping occurs.\nDear team, I know Active-Activestorage architecture, and what other types of storage architectures? Thanks. Dear Axe, Common storage architectures in the industry include active-active architecture, ALUA architecture, and active-passive architecture. Active-active architecture Typical vendors: Huawei, EMC, HDS, and HPE. 1. LUNs are not owned. There are I/O paths between hosts and each controller. 2. Both controllers can process host I/O requests. I/Os of a LUN can be processed by two or more controllers. (The advantages and disadvantages lie in the performance of a single LUN. For example, OceanStor Dorado V6 provides 85% performance of a single LUN.) 3. Loads are always balanced among controllers. 4. Short switchover time. ALUA Architecture Typical vendors: IBM and MacroSAN. 1. A LUN is owned. The host has only one I/O path (solid line) to the owning controller. The other I/O path (dashed line) is activated only when the owning controller is faulty. 2. Both controllers can process host I/O requests, but the I/Os of the same LUN can be processed only by the controller to which the LUN belongs. 3. If LUN partitions are uneven or upper-layer service loads are uneven, a controller may become a bottleneck. Manual optimization is required. 4. The controller switchover time is longer than that in the A-A architecture. Active-passive architecture Typical vendors: NetApp and Pure. 1. A LUN is owned. The host has only one I/O path (solid line) to the owning controller. The other I/O path (dashed line) is activated only when the owning controller is faulty. 2.\nBoth controllers can process host I/O requests, but the I/Os of the same LUN can be processed only by the controller to which the LUN belongs. 3. Backend disks are also owned. Controller B processes I/O requests and forwards them to controller A. (The controller that does not own the disk needs to forward the data to the disk after processing the data.) The I/O overhead is high. 4. The controller switchover takes a long time. Generally, vendors of this type of architecture use more powerful CPUs to supplement the architecture. Thanks. Dear Axe, Common storage architectures in the industry include active-active architecture, ALUA architecture, and active-passive architecture. Active-active architecture Typical vendors: Huawei, EMC, HDS, and HPE. 1. LUNs are not owned. There are I/O paths between hosts and each controller. 2. Both controllers can process host I/O requests. I/Os of a LUN can be processed by two or more controllers. (The advantages and disadvantages lie in the performance of a single LUN. For example, OceanStor Dorado V6 provides 85% performance of a single LUN.) 3. Loads are always balanced among controllers. 4. Short switchover time. ALUA Architecture Typical vendors: IBM and MacroSAN. 1. A LUN is owned. The host has only one I/O path (solid line) to the owning controller. The other I/O path (dashed line) is activated only when the owning controller is faulty. 2. Both controllers can process host I/O requests, but the I/Os of the same LUN can be processed only by the controller to which the LUN belongs. 3.\nFully Interconnected Controllers Within a Controller Enclosure: On Huawei OceanStor Dorado 8000 V6 and Dorado 18000 V6 all-flash storage systems, acontroller enclosure contains four controllers, each of which is an independent hot-swappableservice processing unit. Each controller connects to the passive backplane through three pairsof high-speed RDMA links, thereby fully interconnecting with the other three controllers. Thanks to the full interconnection of controllers, data flows between controllers do not needto be forwarded by a third component, achieving balanced, fast, and efficient access. Noexternal cables or switches are required for the four controllers within a controller enclosure.This simplifies deployment and eliminates the risk of human errors. In addition, the passivebackplane uses only passive components, further improving reliability. On Huawei OceanStor Dorado 3000 V6, Dorado 5000 V6, and Dorado 6000 V6 all-flashstorage systems, a controller enclosure contains two controllers. Interface modules are notshared, but are owned by specific controllers. Fully Interconnected Controllers Across Controller Enclosures: To scale out to eight controllers for a Huawei OceanStor Dorado 8000 V6 or Dorado 18000V6 all-flash storage system, four shared scale-out interface modules can be used in eachcontroller enclosure to directly connect all controllers. Each shared scale-out interface moduleis directly connected to the four controllers within the local controller enclosure and canreceive and transmit data from/to the four controllers in the other controller enclosure. In thisway, the eight controllers are fully interconnected. Compared to switched networking, thisswitch-free networking mode saves half of cable connections and two switches, whichreduces the cost and simplifies management.\nHuawei OceanStor Dorado 8000 V6 and Dorado 18000 V6 all-flash storage systems supportfront-end interconnect I/O modules (FIMs). Each FIM connects to four controllers throughfour PCIe 3.0 links. Hosts can connect to any port on an FIM to access four controllers at thesame time. On Huawei OceanStor Dorado 8000 V6 and Dorado 18000 V6 all-flash storage systems: The 8 Gbit/s, 16 Gbit/s, and 32 Gbit/s Fibre Channel interface modules are FIMs. iSCSI networking does not support FIMs An FIM intelligently identifies host I/Os and distributes the I/Os based on specific rules. Inthis way, host I/Os are sent directly to the most appropriate controller without pretreatment ofthe controllers, preventing forwarding between controllers. Because each port of FIMs hasinternal connections to all of the four controllers, host I/Os can be delivered to the mostappropriate controller even if there is only one link between the host and the storage system.Moreover, services are not affected in the event of a controller failure. Figure below showsthe working principles of a Fibre Channel FIM. Each FIM provides fourphysical ports, each of which has only one WWN. A host sets up one external sessionconnection with each port, and each port establishes four internal connections with thecontrollers. From the perspective of the host, it establishes only one connection with thestorage system; from the perspective of the controllers, each controller has established aconnection with the host. The FIM performs protocol and connection processing anddistributes host I/Os to the four links by the intelligent distribution algorithm. FIMs simplify the system networking.\nData Center Interconnect (DCI) connects two or more isolated data centers for mutually reliable and efficient data transmission. DCI platforms share these common characteristics: Compact footprint in 1RU or 2RU chassis; IT equipment form factor with front-to-back airflow; Modular design support for low-cost pluggable line interfaces; Simple service modules, usually transponder or muxponder. The creation of DCI platforms was primarily the result of incessant growth of webscale cloud business and ICP content distribution traffic, which leads to greater requirements for high-capacity connections between cloud data centers. At the same time, a variety of industries are creating their own use cases and DCI networks. In Europe's top IXP, DCI devices with only one pair of dark fibers replace legacy devices with more than 200 dark fibers, reducing not only the fiber rental cost, but also laborious fiber connecting and maintenance. This reduces service provisioning time from 12 weeks or even 6 months to only 4 hours. In the financial industry, DCI platforms were built for disaster recovery or mission-critical services. The popularization of mobile banking and working remotely meant a national European bank replaced private lines with its own DCI network to handle a 250% increase in traffic. In the public sector, simplified and compact DCI platforms require less rack space and dark fiber, and eliminates the need for dedicated professional WDM O&M engineers. Furthermore, self-built DCI networks are gradually spreading in National Research and Education Network (NREN), healthcare and other industries.\nData center growth and distribution drive the enormous shipment of purpose-built DCI devices: According to a third party report, the number of data centers will quadruple by 2025, and DC capacity is beginning to be limited by real estate, power, or the physical connectivity available at chosen sites. Meanwhile, the applications and content become ever closer to end users. All these factors drive DCI equipment shipments, and moreover, customers will continue to compare solutions concerning capacity, footprint, power consumption, flexibility and scalability. More investment into DCI equipment to improve fiber utilization, lowering OPEX: DC operators need to continuously break the fiber transmission bottleneck without deploying or renting new fiber resources, either by improving attainable spectrum or by using higher modulation schemes. Low-cost pluggable modules for DCI are attracting more enterprise customers: The increased availability of low power consumption and low-cost pluggable modules, both on the client and line side, makes modular DCI solutions more appealing in smaller DC or enterprise WAN environments. Most vendors are developing one version of their Optical Digital Signal Processors (oDSPs) aimed at low-power, low-cost applications and another for high-capacity, high-performance use cases. What can IT directors learn from this report? DCI networks initially used point-to-point connections, with simple optical line systems, and later evolved to ring or mesh topologies with more complicated fixed optical add/drop multiplexers (FOADMs) or reconfigurable add-drop multiplexers (ROADMs). The most-concerning features in the market are capacity, simplicity, deployment ease, and pay-as-you grow scalability.\nCapacity Is King: Capacity is the top priority and customers tend to use overcapacity for resilience, achieving up to 9.6 Tbps of throughput capacity per chassis, and 4.8 Tbps throughput per RU. Line Interfaces and Transport Features : DCI platforms support a maximum per-wavelength capacity of 600G to 800G with 64QAM programmable coherent solutions, and up to 120 wavelengths in super C band using standard 50 GHz channel spacing. Meanwhile, Huawei's future-proof Super C+L band platform can support up to 76.8 Tbps (upgradable to 88 Tbps) on one single fiber, potentially doubling the per-fiber transport capacity. Client Port Capacities : The 100GbE port will remain the most important client-side interface in the foreseeable future, with 400GbE count growing in importance over 2020. The need for slower Ethernet interfaces is secondary, but the demand for OTN interfaces, and other interface types (like Fibre Channel and TDM) are increasing in financial industry and enterprise use cases. DCI Is Becoming Modular and Disaggregated: Most of the products in this class support varying combinations of client and line interfaces, or optical amplifiers and FOADMs. The modules usually support pluggable client interfaces, predominantly SFP+, QSFP+ or QSFP28. On the line side, however, MSA high-performance modules offer 800G maximum wavelength capacity, and low-cost, standard CFP2- Digital Coherent Optics (DCO) pluggable modules with up to 200G wavelength capacities. However, to avoid the complexity of the optical line system, some vendors also combined the traditional amplifier, optical supervisory channel unit, fiber interface unit, and optical performance monitor unit into one module.\nAutomation and Easy O&M: A third party report predicts that 30% of large enterprises will be using AI for IT and network operations by 2023. As DCI deployments increase in size and complexity, automation (both standalone and integrated across the network, tied in with other network domains using open API) has become increasingly important. Customers demand automation to simplify provisioning and change management, as well as introducing self-healing and self-optimization features. What are the recommendations for DCI Buyers? Think Beyond Pure Connectivity: The most important buying criteria for DCI platforms remains their throughput capacity per unit of rack space. Buyers should also look beyond headline wavelength capacities and examine the capacity/reach results that new DCI solutions can help achieve on their existing or planned fiber plant. Examine the Benefits of Automation : DCI buyers should now examine the benefits DCI solution automation and integrate automation into their own optical transport networks. Following this, the next step should be end-to-end automation spanning data center networks, IP networks, and optical transport. Consider Market Momentum : In DCI, adoption within large webscale client infrastructures can also serve as a catalyst for price cuts, as R&D costs dissolve in large volume platform shipments. This can boost the platform's competitiveness in the marketplace, creating a positive feedback loop. How is Huawei OptiXtrans DC908 performing? GlobalData report ranked Huawei OptiXtrans DC908 as a leader in architecture, performance, modularity, line interfaces, and network management. Despite being a first-time candidate, Huawei OptiXtrans DC908 has outperformed mainstream vendors.\nHello everyone! As the 'Best Huawei Enterprise Network' sub-series ended last week, it was time for us to continue with the same pace. Therefore, I now present to you the 'Best Huawei Enterprise IT' sub-series of the 'Best Huawei Enterprise' compilation series. This thus marks the beginning of this new sub-series. We will take each product category and then compile a list of the best Huawei Enterprise products available on the market that belong to that particular. Today we will be talking about the best Huawei Enterprise Storage. Let's get this IT 'party' started, shall we? CLOUD STORAGE As part of the best Huawei Enterprise Storage, it's worth mentioning the Cloud Storage category. FusionStorage Intelligent Distributed Storage is the name of the main product in this category, an in-house product that allows for state of the art elastic on-demand services and is engineered on a cloud-based infrastructure to cater for the needs even of the most demanding of enterprise clients. FusionStorage integrates local storage resources from general-purpose servers into a completely-distributed storage pool. Features like block, object, big data, and file storage for upper-layer applications are beautifully merged together and made available by Fusion Storage. And if were to talk facts: full-lifecycle intelligent system management; Proven performance of 4.5 million IOPS at a latency of less than 1 ms in SPC-1 V3; large-scale active-active clusters under a fully distributed architecture; Huawei 64-bit ARMv8 Kunpeng series processors for increased IOPS and reduced power consumption. Read more about FusionStorage !\nALL-FLASH STORAGE Another category of Huawei Enterprise Storage, All-Flash Storage, is also something worth analysing. The OceanStor Dorado and OceanStor F V5 series storage can yield a wide range of mission-critical products for many different businesses, regardless of their industry, scale, or scenarios to accelerate digital transformation. All-Flash Storage in specs: superior storage, with 78% lower TCO; ever-fast, with chip-powered architectures; Always-On applications with SmartMatrix-based five-layer reliable layouts; low OPEX, with AI-powered O&M. HYBRID FLASH STORAGE Moving further down the list of best Huawei Enterprise Storage, we will find the OceanStor 18000 V5 Series Mission Critical Hybrid Flash Storage Systems. A solution ready for tomorrow's technological advancements, the OceanStor 18000 V5 series is designed to ensure the highest level of data services for enterprise mission-critical business. It is built on a SmartMatrix system architecture, encompassing a flash-oriented optimization technology. Some specs can be found below: HyperMetro gateway-free active-active feature; up to 16 controllers; a cutting-edge hardware platform; up to 16 TB cache; unprecedented levels of performance; up to 9600 enterprise-class disk drives. In other words, an amazing match for the most demanding storage requirements of government, finance, telecommunications, manufacturing, and other sectors. More information on the OceanStor 18000 V5 Series can be found by clicking this . HYPER-CONVERGED INFRASTRUCTURE FusionCube for Cloud is the name to be mentioned here.\nThe write process itself is pretty simple and follows the flow demonstrated in the diagram above. The extent of the allocation unit determines the logic and the mapping of the physical address. In the case of the think LUN, the data is always written to the corresponding extent which HAS been allocated while creating it. This results in a considerably improved overall performance one that enterprises will surely benefit from. UNMAP unmap In the case of the ROW LUN, the deduplication determines that data with different logical address will correspond to the same physical address. The compression determines that the physical space is smaller than the logical space. So the IO mechanism for the existing LUN isnt suitable for the ROW LUN. DEDUPLICATION AND COMPRESSION The OceanStor V3&V5 reuses the file systems IO mechanism to achieve deduplication and compression. The OceanStor V3&V5 file system adopts the ROW write mechanism. For this reason, every write will result in allocating space/extent, so the metadata change is regular. The fragmented layout which stems from the regular allocation and release is serious. At the same time, the file system write mechanism takes into account the organization of the file system and should write on a transaction-by-transaction basis. This results in a decline in performance. NetApp storage has the same issue so that the SAN performance is not as good as NAS. OceanStor V5 is mainly designed for traditional hybrid storage. To increase the performance with SAS and NLSAS disks, a large amount of cache is reserved to read cache.\nInternet Small Computer Systems Interface is a transport layer protocol that works on top of the Transport Control Protocol (TCP) and enables block-level SCSI data transport between the iSCSI initiator and the storage target over TCP/IP networks. iSCSI stands for Internet Small Computer Systems Interface. iSCSI is a transport layer protocol that works on top of the Transport Control Protocol (TCP). It enables block-level SCSI data transport between the iSCSI initiator and the storage target over TCP/IP networks. iSCSI supports encrypting the network packets, and decrypts upon arrival at the target. SCSI is a block-based set of commands that connects computing devices to networked storage, including spinning up storage media and data reads/writes. The protocol uses initiators to send SCSI commands to storage device targets on remote servers. Storage targets may be SAN, NAS, tape, general-purpose servers both SSD and HDD LUNs, or others. The protocol allows admins to better utilize shared storage by allowing hosts to store data to remote networked storage, and virtualizes remote storage for applications that require direct attached storage. The iSCSI protocol plays an important play in many different network configurations. These technologies package SCSI commands into network packets and direct them to the storage target. The software-based iSCSI initiator is the least expensive of the options, and is often included in the operating system (OS). Host-based adapters (HBA) is hardware device. HBAs are more expensive than software, but have higher performance with more functionality.\nMaking it simple, FlashLink is a technology used by All Flash Huawei Storage Dorado. It combines different optimization technologies to make Dorado more faster and reliable. Below mentioned are the technologies used by FlashLink to make Dorado Storage more efficient. 1. Intelligent Multi-Core Technology It applies Core partitioning and core grouping, this makes full use of multiple cores of Huawei's chip, as a result it doubles CPU processing speed and reduces latency by 20%. (Performance is doubled & Latency is reduced by 20%). 2. Smart Disk DAE It means Huawei's Kunpeng chip is not only deployed in controller, but it is also deployed in SSD enclosure, so SSD enclosure and controller work together to offload the rebuilding from controller. In result data rebuild time is decreased. (1 TB rebuild time is decreased from 10 hours to 15 minutes.) 3. AI Cache It employs a deep learning AI algorithm which performs fast analysis, in-depth mining and cache pre-fetching for regular data I/O . So it improves read hit ratio by 50 % and batch application processing by 40%. (read hit ratio is improved by 50 % and batch application processing is improved by 40%) 4.Full Stripe Write It aggregates multiple discrete data blocks into a continuous data block in cache. Then, data block is written to SSDs in sequence so it prevents multiple operations caused by small discrete data, reducing the cpu utilization and pressure on SSDs. This results in less write amplification. (less write amplification) 5.\nThe SAN (Storage Area Network) uses Fibre Channel technology to connect storage arrays and server hosts through Fibre Channel switches to establish a regional network dedicated to data storage. IPSAN is generated after the SAN. The SAN defaults to FCSAN, which uses Fibre Channel to build a storage network. IPSAN builds a storage network with an IP network. Due to the high cost of FCSAN, many small and medium-sized storage networks are unacceptable, and some people are considering building a storage network based on Ethernet technology. But in SAN, the instructions that are transmitted are SCSI read and write instructions, not IP data packets. ISCSI (Internet Small Computer System Interface) is a standard for data block transmission over TCP/IP. ISCSI enables the SCSI protocol to run on IP networks, enabling fast data access, backup operations such as high-speed Gigabit Ethernet. In order to distinguish it from the previous-based FCSAN, this technology is called IPSAN. ISCSI inherits two of the most traditional technologies: SCSI and TCP/IP. This laid a solid foundation for the development of ISCSI. The based ISCSI storage systems require only a small investment to implement SAN storage and even leverage existing TCP/IP networks. Compared with the previous network storage technology, it solves the problems of openness, capacity, transmission speed, compatibility, security, etc. Its superior performance makes it attract attention and favor. After actual work, SCSI commands and data are encapsulated into TCP/IP packets and then transmitted over an IP network.\nIPSAN advantages: The cost is low, the purchased network cable and switch are all Ethernet, and even the existing network can be used to build the SAN; the deployment is simple and the management is difficult; the emergence of 10 Gigabit Ethernet makes the IP SAN no longer inferior to the transmission when competing with the FC SAN. Bandwidth; the inherent advantage of IP-based networks makes it easy for IP SANs to implement technologies such as off-site storage and remote disaster recovery through WAN. FCSAN: Early SANs used FC (Fibre Channel) technology. Therefore, the previous SAN refers to a Fibre Channel storage area network, which is called FCSAN in the industry. FCSAN Superiority: The transmission bandwidth is high. Currently, there are four standards of 1, 2, 4 and 8Gb/s. The mainstream is 4 and 8Gb/s. The performance is stable and reliable, and the technology is mature. It is the best choice for key application areas and large-scale storage networks. FCSAN Disadvantage: Cost extremely expensive, it is requiring fiber switches and a large amount of fiber cabling. Maintenance and configuration are complex and require training from a professional FC network administrator who is completely different from the LAN administrator. IPSANFCSAN The disk controller of the IPSAN storage device is not a hardware RAID chip and central processor structure in the FCSAN storage device, but it is divided into multiple disk groups in each disk cabinet, and each disk group is controlled by a micro processing chip.\nAll disk RAID operations (software computing, less efficient) and management operations for RAID groups. In this way, each disk I/O operation reads or writes data from a front-end host port through a switch-like device built into the IPSAN storage. These operations are based on the IP exchange protocol and the protocol itself. It is required that each micro-processing chip needs a large-capacity cache to support the queuing operation of the packet queue, so generally we see IP-SAN storage with tens of GB of cache. With this large buffer area, IP-SAN storage can achieve a high value of 600,000 IOPS or more when testing the maximum read bandwidth of the Cache, but this value does not really indicate that good performance can be obtained in practical applications. When there is mass storage, it is impossible for all data to be loaded into the system cache. At this time, a large number of disk I/O operations are required to find the data, and the SATA disk used for IPSAN storage is very weak in this block. And it also involves an efficiency loss problem of converting ISCSI data flowing to an ATA format data over an IP network. That is to say, IPSAN storage has a cache Cache to disk data I / O and data processing bottleneck. There is no such problem with FCSAN storage devices using FC disks.\nThrough two or even four redundant back-end fiber-optic disk channels, a very high disk read-write bandwidth can be obtained, and the FCP disk read-write protocol does not have a data format conversion problem because they use SCSI internally. Protocol transmission avoids loss of efficiency. Moreover, due to the high efficiency of fiber-optic switching and data transmission, FCSAN storage devices can obtain a good data hit rate and read-write performance without requiring a large cache. Generally, 2Gb or 4Gb can meet the requirements. In addition, due to the special hardware RAID parity control chip, the disk RAID performance will be much better than the software RAID performance, and the reliability is better. In FCSAN, there are flexible connection methods, which can select different connection topologies according to different application requirements. The main connection methods are as follows: Point-to-point: First, each component device establishes an initial connection by logging in, and then works with full bandwidth. The actual link utilization is determined by the Channel controller of each terminal and the buffer size that can be obtained by sending and receiving data. However, it is only suitable for small-scale storage devices and does not have sharing capabilities. Arbitrated loop: allows more than two devices to communicate and communicate through a shared bandwidth. In this topology, the creator of any process will first reach an agreement with the transmission medium on how to access the information before sending a message. Therefore, all devices can achieve orderly access to the communication medium through an arbitration protocol.\nFull exchange: Provides timely, multi-way point-to-point connections through link layer switching. Connected through dedicated, high-performance Fiber Channel switches, and peer-to-peer communication between multiple pairs of devices, so that the total bandwidth of the entire system increases with the increase of the number of devices, while increasing the number of devices without affecting the system performance. In Ethernet-based data transmission and access in IPSAN, although it can be physically represented as a bus or star connection, its essence is broadcast data transmission with collision detection multi-carrier sensing (CSMA/CD). The bus topology, so as the load and the number of communication clients in the network increase, the actual efficiency will decrease accordingly. FCSAN: Using dedicated Channel devices The use of fiber optic media in the link not only avoids all kinds of electromagnetic interference during transmission, but also effectively achieves long-distance I/O channel connections. The core switching devices used in FCSAN - fiber switches are designed with high reliability and high performance ASIC chips, making the entire process completely efficient based on hardware level. Also in the HBA design connected to the host, the vast majority of operations are handled independently, without any host processing resources. IPSAN: Using a common IP network and device In the transmission medium, a medium such as a copper cable, a twisted pair, or an optical fiber is used for signal transmission. However, in an ordinary inexpensive medium, there is a disadvantage that the signal attenuation is severe, and the use of the optical fiber also requires a unique photoelectric conversion device.\nOceanStor 5500 V3 SAN 5500V3 storage shows . Storage pool ID: 8 Storage pool 8 has 20 LUNs. LUN IDs range from 91 to 110. Check the reclaim pool capacity of LUN 91 to LUN 110. Total capacity: 7582.92 GB . Upgrade the storage system to the V300R003C20SPC200 version and install the V300R003C20SPH207 patch. In this version, the space in the reclaim pool is preferentially allocated. Therefore, as the space is released and re-allocated, the difference between the capacity displayed on the host and the storage is reduced. After the V300R006C10SPC100 version is released, upgrade the storage system to this version. In this version, the reclaim pool space can be sorted out. When the system traffic is low, the extents with a small amount of data are combined. After all the grains of an extent are released, the entire extent is returned to the storage pool. Therefore, after the VMware host reclaims space, the available capacity of the storage pool is increased. This version will be released in March 2018. Current Risk The total thin LUN capacity (20 x 2 TB thin LUN) configured for the storage is equal to storage pool capacity (40 TB). Metadata also needs 2% storage pool capacity. When the storage pool capacity is about to be used up, its capacity may fail to be allocated for metadata and write protection may be triggered.\ntorage consolidation and virtualization. As the need for data storage continues to grow rapidly, traditional physical approaches to storage management become increasingly problematic. Physically expanding the storage environment can be costly, time-consuming, and disruptive. These drawbacks are compounded when expansion must be done again and again in response to ever-growing storage demands. Yet, manually improving storage utilization to control growth can be challenging. Physical infrastructures can also be inflexible at a time when businesses must be able to make even more rapid changes to stay competitive. The alternative is a centralized, consolidated storage pool of disk devices that are easy to manage and are transparent to be provisioned to the target host systems. Going further, the consolidated or centralized storage can be virtualized, where storage virtualization software presents a view of storage resources to servers that is different from the actual physical hardware in use.Fig 1 shows the storage consolidation. Fig. 1 Storage consolidation This logical view can hide undesirable characteristics of storage while it presents storage in a more convenient manner for applications. For example, storage virtualization might present storage capacity as a consolidated whole, hiding the actual physical boxes that contain the storage. In this way, storage becomes a logical pool of resources that exist virtually, regardless of where the actual physical storage resources are in the larger information infrastructure. These software-defined virtual resources are easier and less disruptive to change and manage than hardware-based physical storage devices because they do not involve moving equipment or making physical connections.\nInformation lifecycle management (ILM) is a process for managing information through its lifecycle, from conception until disposal, in a manner that optimizes storage and access at the lowest cost. ILM is not just hardware or software; it includes processes and policies to manage the information. It is designed on the recognition that different types of information can have different values at different points in their lifecycles. Predicting storage needs and controlling costs can be especially challenging as the business grows. Although the total value of stored information increases overall, historically, not all data is created equal, and the value of that data to business operations fluctuates over time. Fig. 1 shows this trend, which is commonly referred to as the data lifecycle. The existence of the data lifecycle means that all data cannot be treated the same. Fig 1. Data lifecycle However, infrequently accessed or inactive data can become suddenly valuable again as events occur, or as new business initiatives or projects are taken on. Historically, the requirement to retain information results in a buy more storage mentality. However, this approach increases overall operational costs, complexity, and the demand for hard-to-find qualified personnel. Typically, only around 20% of the information is active and frequently accessed by users. The remaining 80% is either inactive or even obsolete (Fig 2). Fig 2. Usage of data The automated identification of the storage resources in an infrastructure and an analysis of how effectively those resources are used are the crucial functions of ILM.\nWhen the WORM file system is configured, the global security regulation clock is inconsistent with the local NTP time. The following problems are raised: Is that normal or abnormal? What is the cause? Whether the function is affected and how to rectify the fault or not. Q: Is this normal or abnormal? What is the cause? A: Yes. The global security compliance clock is initialized by the change system secure_compliance_clock date= command. Therefore, the global security compliance clock cannot be completely consistent with the local time during initialization. In addition, the global security regulation clock cannot be modified once it is initialized. Therefore, manually modifying the local time after initialization may cause inconsistency between the global security regulation clock and the local time. Q: Does the function be affected? How can I fix the problem or do not need to be rectified? A: This issue does not affect functions and does not need to be rectified. The time in the WORM file system does not depend on the local time. If the time is inconsistent, no problem occurs. Q: Why should the global security regulation clock be distinguished from the local time? Why not use the same time? A: The global security compliance clock cannot be changed once it is initialized. This ensures that the lock time set for WORM volumes is not affected by the local time modification. Therefore, the global security compliance clock must be manually initialized and the time is manually specified.\nDear community, I will share you theAI-and-5Gwhitepaper ofNVIDIA. Companies are using artificial intelligence in conjunction with network connectivity to automate operations and generate analytical insights to improve products and services. Many people are now adding 5G to connect devices to these systems because its high-speed, large-capacity, and ultra-reliable low-latency features can provide new use cases for artificial intelligence or enhance current products. So far, artificial intelligence and 5G infrastructure have been running on two separately constructed and managed systems. However, a better and more efficient method is to integrate artificial intelligence and 5G on a converged platform that can be deployed in corporate locations. This is NVIDIA artificial intelligence on 5G. Combining these technologies can simplify system deployment and management, and give enterprises more control over the infrastructure. Converged methods can also make better use of computing power and network to improve efficiency while reducing total cost of ownership (TCO). AI applications at the edge to gain the benefits of processing data. 5G offers extremely fast data speeds, high capacity, and ultrareliable, lowlatency communications needed for the most demanding applications. Connection density is also a significant distinction compared to WiFi. According to GSMA, WiFi can support just 256 to 1,024 devices on an access point with a range of 100 meters or less, depending on the WiFi version used. Research, cellular networks can be turned on in 26 minutes on average, as soon as the router is in place.\nDear team, What isUltraPath? Whatfunctions doesUltraPath provide? Thanks. Dear Axe, UltraPath is a Huawei-developed multipathing software. It can manage and process disk creation/deletion and I/O delivery of operating systems. UltraPath provides the following functions: Masking of redundant LUNs In a redundant storage network, an application server with no multipathing software detects a LUN on each path. Therefore, a LUN mapped through multiple paths is mistaken for two or more different LUNs. UltraPath installed on the application server masks redundant LUNs on the operating system driver layer to provide the application server with only one available LUN, the virtual LUN. In this case, the application server only needs to deliver data read and write operations to UltraPath. UltraPath then masks the redundant LUNs, and writes data into LUNs without damaging other data. Optimum path selection In a multipath environment, the owning controller of the LUN on the storage system mapped to an application server is the prior controller. With UltraPath, an application server accesses the LUN on the storage system through the prior controller, thereby obtaining the highest I/O speed. The path to the prior controller is the optimum path. Failover and failback Failover - When a path fails, UltraPath fails over its services to another functional path. Failback - UltraPath automatically delivers I/Os to the first path again after the path recovers from the fault. I/O Load balancing UltraPath provides load balancing within a controller and across controllers. For load balancing within a controller, I/Os poll among all the paths of the controller.\nHello team, What is ALUA? What does ALUA impact? Thank you. Dear Axe, Asymmetric Logical Unit Access (ALUA) is a multi-target port access model. In a multipathing state, the ALUA model provides a way of presenting active/passive LUNs to a host and offers a port status switching interface to switch over the working controller. For example, when a host multipathing program that supports ALUA detects a port status change (the port becomes unavailable) on a faulty controller, the program will automatically switch subsequent I/Os to the other controller. ALUA is applicable to a storage system that has only one prior LUN controller. All host I/Os can be routed through different controllers to the working controller for execution. ALUA will instruct the hosts to deliver I/Os preferentially from the LUN working controller, thereby reducing the I/O routing-consumed resources on the non-working controllers. If all I/O paths of the LUN working controller are disconnected, the host I/Os will be delivered only from a non-working controller and then routed to the working controller for execution. Thanks. Dear Axe, Asymmetric Logical Unit Access (ALUA) is a multi-target port access model. In a multipathing state, the ALUA model provides a way of presenting active/passive LUNs to a host and offers a port status switching interface to switch over the working controller. For example, when a host multipathing program that supports ALUA detects a port status change (the port becomes unavailable) on a faulty controller, the program will automatically switch subsequent I/Os to the other controller.\nDear team, What is MPIO, and what policy settings does MPIO have? Thanks. Dear Axe, Windows Microsoft Multi-Path IO (MPIO) allows storage vendors to develop multipathing solutions that contain the hardware-specific information needed to optimize connectivity with storage systems. MPIO can be used independently. This software helps balance loads among multiple paths, and implement path selection and failover between storage systems and hosts. MPIO supports the following policy settings: Failover Only This policy does not perform load balancing. This policy uses a single active path, and the rest of the paths are standby paths. The active path is used for sending all I/Os. If the active path fails, then one of the standby paths is used. When the failed path is reactivated or reconnected, the standby path that was activated returns to standby. Round Robin This load balancing policy allows the Device Specific Module (DSM) to use all available paths for MPIO in a balanced way. This is the default policy that is chosen when the storage controller follows the active-active model and the management application does not specifically choose a load balancing policy. Round Robin with Subset This load balancing policy allows the application to specify a set of paths to be used in a round robin fashion, and with a set of standby paths. The DSM uses paths from a primary path pool for processing requests as long as at least one of the paths is available. The DSM uses a standby path only when all the primary paths fail.\nFor example, given 4 paths: A, B, C, and D, paths A, B, and C are listed as primary paths and D is the standby path. The DSM chooses a path from A, B, and C in round robin fashion as long as at least one of them is available. If all three paths fail, the DSM uses D, the standby path. If paths A, B, or C become available, the DSM stops using path D and switches to the available primary paths. Least Queue Depth This load balancing policy sends I/O down the path with the fewest currently outstanding I/O requests. For example, consider that there is one I/O sent to LUN 1 on Path 1, and the other I/O is sent to LUN 2 on Path 1. The cumulative outstanding I/O on Path 1 is 2, and on Path 2 is 0. Therefore, the next I/O for either LUN will process on Path 2. Weighed Paths This load balancing policy assigns a weight to each path. The weight indicates the relative priority of a given path. The larger the number, the lower ranked the priority. The DSM chooses the least-weighted path from among the available paths. Least Blocks This load balancing policy sends I/O down the path with the least number of data blocks currently being processed. For example, consider that there are two I/Os: one is 10 bytes and the other is 20 bytes. Both are in process on Path 1, and there are no outstanding I/Os on Path 2.\nHello all, Today I will share with you the Internet of Vehicles Solution. As the automotive industry continues to accelerate towards connected cars, more Internet of Vehicles (IoV) solutions are emerging through advancements of networking, 5G, and the Internet of Things (IoT). Traditional car companies must undertake additional roles beyond car manufacturers and evolve into car service providers. With the IoV and increased connectivity, the development of innovative industry applications now feature management capabilities for data, interconnections, operations, and security, with openness for Application Platform Interfaces (APIs) to third-party solutions. Huaweis IoV Solution integrates data, devices, and operational management. Through the implementation of unified and secure network access, flexible device adaptation, and mass data collection and analysis, IoV contributes significantly to generate new potential revenue sources. A shared car experience maximizes the number of hours a car is being utilized, expands carpooling acceptance, reduces car idle times, improves usage rates, and increases travel efficiency. Vehicle networking achieves user scan-to-drive, promotes tracking of car driving status as well as driving behavior analysis, and other functions. Truck route planning, driving schedule and road conditions monitoring, as well as overall transport scheduling requirements by using vehicle networking to provide the corresponding truck driving and driver status information. Vehicle networking will enable efficient and accurate fleet management, with rapid response to unexpected events and business developments.\nDaer all, Can you tell the difference between LDAP and Active Directory?Today, I'll help you understand the difference between them. is an open and cross-platform protocol for directory service authentication. LDAP provides the communication language that applications use to communicate with other directory service servers. Directory services store users, passwords, and computer accounts and share this information with other entities on the network. Active Directory is a directory service implementation that provides functions such as authentication, group and user management, policy management, and so on. Active Directory (AD) supports both Kerberos and LDAP - Microsoft AD is the most common directory service system in use today. AD provides single sign-on (SSO) and works well in the office and VPN. AD and Kerberos are not cross-platform, which is one reason why companies implement access management software to manage logins from many different devices and platforms in a single location. AD does support LDAP, which means it can still be part of the overall access management scheme. Active Directory is just one example of an LDAP-enabled directory service. There are other flavors: Red Hat Directory Services, OpenLDAP, Apache Directory Server, etc. LDAP is a way to talk to Active Directory. LDAP is a protocol that can be understood by many different directory services and access management solutions. The relationship between AD and LDAP is very similar to the relationship between Apache and HTTP: HTTP is a web protocol. Apache is a web server that uses the HTTP protocol. LDAP is a directory service protocol.\nActive Directory is a directory server that uses the LDAP protocol. Occasionally, you'll hear someone say, \"We don't have Active Directory, but we do have LDAP. What they might mean is that they have another product, like OpenLDAP, which is an LDAP server. LDAP authentication in LDAP v3 has two options - simple and SASL (simple authentication and security layer). Simple authentication allows for three possible authentication mechanisms Anonymous authentication: Grants client anonymous status to LDAP. Unauthenticated authentication: For logging purposes only, access to clients should not be granted. Name/Password Authentication: Grants access to the server based on the credentials provided - Simple user/password authentication is not secure and is not suitable for authentication without confidentiality protection. SASL authentication binds the LDAP server to another authentication mechanism, such as Kerberos. The LDAP server uses the LDAP protocol to send LDAP messages to another authorization service. This initiates a series of challenge response messages that result in successful or failed authentication. Note that LDAP delivers all of these messages in clear text by default, so anyone with a network sniffer can read the packet. You need to add TLS encryption or similar encryption to secure your username and password. An LDAP query is a command that asks a directory service for certain information. For example, if you want to see which groups a particular user is part of, you can submit a query like this: Good syntax, huh? It's not as simple as typing a web address into a browser. Feels like LISP.\nFortunately, in most cases, you do not need to write LDAP queries. To stay sane, you'll perform all directory service tasks through a click-to-admin interface such as Varonis DatAdvantage, or use a command-line shell such as PowerShell, which abstracts the details of the original LDAP protocol. How does LDAP work with Active Directory? LDAP provides a way to manage users and group memberships stored in Active Directory. LDAP is a protocol that authenticates and authorizes granular access to IT resources, and Active Directory is a database of user and group information. What is LDAP Injection? LDAP injection occurs when the wrong actor uses manipulated LDAP code to modify or disclose sensitive user data in an LDAP server. LDAP client applications are authenticated and inspected through LDAP filters to prevent malicious injection. Where do we use LDAP? LDAP is used as the authentication protocol for the directory service. We use LDAP to authenticate users of local and web applications, NAS devices, and SAMBA file servers. Is LDAP Secure? To secure communications, you must use SSL/TLS connections to encrypt LDAP transactions. To set, use LDAPS on port 636 or StartTLS on standard LDAP port 389. What is the difference between Kerberos and LDAP? While both are network protocols for authentication (verifying user IDs), LDAP is different in that it can also authorize (determine access) clients and store user and group information. LDAP is the protocol and Active Directory is the server.\nHello all, We will talk about Ceph storage. I will introduce this concept. Ceph is a high-performance, scalable, and non-single-point distributed file storage system based on Sage A. Weil's paper. Sage Weil developed an open source project called Ceph in 2004 and opened Ceph in 2006 based on an open source protocol. Weil was the founder of Inktank Storage, a company that focused on Ceph until it was acquired by Red Hat, and in 2012 Ceph released its first stable version. Ceph provides the following three storage services: Object Storage (OBS) is compatible with Amazon S3 and OpenStack Swift by using Ceph's libraries, using C, C++, Java, Python, and PHP codes, or using RESTful gateways to access or store data in the form of objects. Block storage is directly mounted as a block device like a hard disk and supports thin provisioning, snapshot, and clone. File system is mounted like a network file system and is compatible with POSIX interfaces. High performance : The traditional centralized storage metadata addressing solution is abandoned. The CRUSH algorithm is used to achieve balanced data distribution and high parallelism. DR domain isolation is considered to implement copy placement rules for various workloads, such as cross-equipment room and rack awareness. High availability : The number of copies can be flexibly controlled. Fault domain isolation; Strong data consistency; Automatic recovery and self-healing in various fault scenarios; No single point of failure (SPOF) and automatic management. High scalability : decentralized and flexible expansion; The number of nodes increases linearly.\nSupports thousands of storage nodes and TB to PB-level data. Various features : supports three storage interfaces: block storage, file storage, and object storage. Supports user-defined interfaces and multiple language drivers. The object storage of Ceph is provided by LIBRADOS and RADOSGW, the block storage is provided by RBD, and the file system is provided by Ceph FS. The RADOSGW, RBD, and Ceph FS all need to call LIBRADOS interfaces. And ultimately they are stored as objects in the RADOS. 1, RAODS The bottom layer of Ceph is RADOS, which means \"A reliable,autonomous, distributed object storage\". Ceph object storage and Ceph block devices read and write data from the storage cluster of RADOS. The LIBRADOS programming interface is the basis of other client interfaces, and other interfaces are extended and implemented based on LIBRADOS. Nodes in the Ceph cluster have three roles: Monitor : maintains the global status of the entire Ceph cluster, monitors the cluster health status, and sends the latest CRUSH map (including the current network topology) to the client. OSD : provides storage resources, maintains objects on nodes, responds to client requests, and synchronizes data with other OSD nodes. MDS : Ceph Metadata Server, which is the metadata service on which Ceph FS depends. 2, Librados Librados is a library provided by the RADOS. The RBD, RGW, and CephFS at the upper layer are accessed through the Librados. Currently, the supports PHP, Ruby, Java, Python, C, and C++. 3.\nCeph client interface (Clients) LIBRADOS, RADOSGW, RBD, and Ceph FS are collectively called Ceph client interfaces. RADOSGW, RBD, and Ceph FS are developed based on the multi-programming language interfaces provided by LIBRADOS. RADIUS Gateway : RADIUS Gateway, which is an object storage service provided by Ceph. The underlying object storage interface is based on Librados and provides RESTful interfaces for clients. Supports Amazon's S3 and OpenStack's Swift APIs. Ceph FS : Ceph File System, a file system service provided by Ceph for external systems. Ceph storage clusters are used to store data. RBD : RADOS block device, which is a block device service provided by Ceph. In a Ceph cluster, Ceph block devices support thin provisioning, size adjustment, and data storage. The RBD interacts with the kernel module or the library of Librados through the RADOS protocol. The Ceph storage cluster receives files from clients. The client divides each file into one or more objects, groups these objects, and stores them to the OSD nodes of the cluster based on certain policies. Several concepts are explained as follows: File : files to be stored or accessed. Ojbect : object seen by the RADOS. The difference between an object and a file is that the maximum size of an object is limited by the RADIUS server (usually 2 MB or 4 MB) to implement the organization and management of underlying storage.\nDear all, I will introduceopen-sourceopenLooKeng. Open-source openLooKeng developed by Huawei uses Presto, anestablished engine for open-source SQL, to provide basic interactive queriesand analysis. openLooKeng continues to improve capabilities in convergedscenarios for queries, processing across data centers and clouds, data sourceexpansion, performance, reliability, and security. openLooKeng makes datagovernance and usage easier. Data Lake Insight(DLI) basic architecture Data Lake Insight (DLI) is a serverless data processing and analysis service fully compatible with Apache Spark, Flink, and openLooKeng (Presto-based) ecosystems. You can effortlessly perform stream, batch, and interactive analysis to query mainstream data formats without data ETL. You can also use standard SQL, and Spark and Flink applications for convergent data analysis. Unified SQL interfaces ensure data accessacross data centers and hybrid clouds. Unified SQL interfaces enable you to access multiple datasources openLooKeng allows you to effortlessly add connectors fornew data sources, removing the need for data migration. Table 1 openLooKeng features Highlights Function Description Millisecond-level query performance Index openLooKeng provides indexes based on Bitmap Index, Bloom Filter, and Min-max Index. Indexes are created based on existing data and index results are stored outside data sources. During query plan orchestration, unmatched files are filtered out based on index information, reducing the amount of data to be read and accelerating the query process. Cache openLooKeng provides various caches, including metadata cache, execution plan cache, and ORC-based row data cache. These diversified caches can speed up the response to the query of the same SQL or SQL of the same type for multiple times.\nDynamic filtering Dynamic filtering is an optimization method of applying the filtering information of one table to be joined to the filter of the other table during running (run time). openLooKeng provides the dynamic filtering optimization feature for multiple data sources and applies this feature to the Data Center Connector, in this way, the performance of associated query in different scenarios is improved. Operator pushdown When openLooKeng connects to data sources such as the RDBMS through the Connector framework, the operator is pushed down to the data sources for computing. RDBMS has a strong computing capability, allowing better performance. Currently, openLooKeng supports operator pushdown of multiple data sources, including Oracle and HANA. Especially, operator pushdown is implemented for the DC Connector to reduce the query latency. High availability HA active-active openLooKeng introduces the high availability active-active feature, ensuring load balancing among multiple coordinators and the availability of openLooKeng in the case of high concurrency. Auto-scaling The elastic scaling feature of openLooKeng allows smooth decommission of the service nodes that are executing tasks. In addition, inactive nodes can be started to accept new tasks. openLooKeng provides isolated and isolating interfaces for external resource managers (such as Yarn and Kubernetes) to implement elastic scaling of coordinator and worker nodes. Integrated analysis - Real-time analysis, offline analysis, and interactive analysis often coexist. DLI updates the openLooKeng engine to interconnect with the existing Spark engine at the metadata layer. You can implement offline analysis as well as interactive analysis without data migration.\nThe NAND flash has become a new favorite of mobile phone storage due to its high-density storage advantages. The eMMC/eMCP used by smartphones are storage solutions that encapsulate the NAND flash and control chip. In terms of application form, NAND flash products include USB flash drives (USBs), flash cards, and solid state drives (SSDs). DRAMs such as embedded storage (eMMC, eMCP, and UFS) are classified into DDR/LPDDR/GDDR and legacy (Legacy/SDR) DRAMs. DDR is short for Double Data Rate SDRAM. Double-rate synchronous dynamic random access memory (DRAM), which is mainly applied to personal computers and servers. Currently, the mainstream DDR standard is DDR4, but DDR5 has been released. It is expected that the penetration rate of LPDDR will increase in the future. LPDDR, also called mobile DDR (mDDR), is mainly used in mobile electronic products. GDDR refers to GraphicsDDR, which is mainly used in the image processing field. Low Power Double Data Rate SDRAM (Low Power Double Data Rate SDRAM), also called Mobile DDR SDRM (mDDR). LPDDR is a communications standard developed by the JEDEC Solid State Association for low-power memory and is mainly applicable to mobile electronic products. Compared with DDR, LPDDR has lower power consumption. LPDDR4X and LPDDR4 are used in the memory of mainstream flagship phones. Because they often appear at the same time and have similar names, it is easy to confuse people. In fact, the LPDDR4X is not the next-generation memory of the LPDDR4, but can be regarded as a power-saving optimized version of the LPDDR4.\nHello all, In this post, I will introduceHuawei FusionServer liquid cooling system forhigh performance computing. The Huawei FusionServer liquid cooling system uses a liquid-cooled solution to address the heat dissipation issues that cannot be tackled by traditional heat dissipation technologies, including precision air conditioning and thermal management systems (TMS), for certain high-power equipment. Liquid cooling has been gradually popularized in IT high-density equipment rooms, and distributed refrigerant cooling (DRC) in the central office (CO) high-density equipment rooms. For large-scale deployment of high-performance computing (HPC) servers with high power density, the Huawei FusionServer liquid cooling system provides end-to-end heat dissipation and cooling capabilities to greatly reduce the cooling power and operating expenses (OPEX). The FusionServer provides a maximum heat dissipation capability of 49 kW. In the IT field, especially HPC application scenarios, the evolution of servers toward higher speed and density also causes high power consumption. The power consumption of the traditional air cooling system accounts for more than 30% of the total data center power consumption. Concerns are being voiced about the air cooling capabilities and costs. Energy conservation and emission reduction are high on the agenda. The Huawei FusionServer liquid cooling system uses board-level liquid-cooled and provides a cooling power usage effectiveness (PUE) ratio of 1.1. It is ideal for heat dissipation of HPC servers in large-scale deployment. The Huawei FusionServer liquid cooling system solution consists of the cooler and chiller, air conditioning system, coolant distribution unit (CDU), FusionServer liquid-cooled cabinets, and the secondary loop connecting the CDU and the liquid-cooled cabinets. No.\nComponent Description 1 Cooler & chiller Cooling system (including the primary loop) of the equipment room. The cooler and chiller, provided by the customer, supply warm or chilled water below 35C to the CDU. Decide whether to use the chiller based on the ambient temperature. For details about requirements for the pressure and flow of supply and return water in the primary loop, see related documents of the CDU. 2 Air conditioning system Air conditioning system of the equipment room The air conditioning system, provided by the customer, dissipates heat for air-cooled components, such as switch modules, hard disks, and mezzanine cards. The air conditioning system must be configured to meet the dissipation requirements of air-cooled components. 3 CDU and liquid working medium CDU system and liquid working medium The CDU is purchased by an agent based on Huawei's reference architecture and installed by the agent. The liquid working medium is also purchased and used by the agent. For details about vendors that provide CDUs and liquid working media compatible with Huawei FusionServer liquid cooling system, see Appendix. 4 FusionServer liquid cooling system Huawei FusionServer liquid-cooled cabinet Huawei provides the FusionServer liquid-cooled cabinet, which consists of the integrated rack (with soft tubes, CCU, and PDUs), X6000 chassis, and liquid-cooled server nodes. The agent installs the cabinet and its components on site, connects it to the secondary loop, and commissions the water flow. 5 Secondary loop between the CDU and the FusionServer liquid-cooled cabinet The agent conducts a site survey and designs the secondary loop.\nHello all, Intel is lending early Alder Lake adopters a hand and releases a provisional list with validated DDR5-4800 DIMMs from several suppliers. This list should expand as more models become available. We are just a few weeks away from the official launch of Intels Alder Lake desktop platform that is the first to offer hybrid DDR4+DDR5 support. In order to help early adopters with the transition to the new RAM standard, Intel recently issued a provisional list with DDR5 DIMMs compatible with the upcoming Alder Lake Z690 motherboards. Intel notes that the list contains validation results from a small sample of DDR5 DIMMs, so the list should be expanded as RAM suppliers release more models. As mentioned in the validation document, all the DIMMs have been tested at the default DDR5-4800 speeds with VDDQ 1.1V and 40-39-39 timings on Automated Test Equipment. The FPS Review points out that the validation process was actually performed by a third-party entity called Advanced Validation Labs. Among the validated modules we see DIMMs with capacities ranging between 8 and 32 GB from RAM chip producers like SK Hynix, Samsung and Micron, but also from suppliers like Crucial and Kingston that have not yet officially announced specific DDR5 models. We already know that Alder Lake supports DDR5-6400 and even DDR5-8000 specs thanks to the Gear 2 and Gear 4 modes that lower the memory controller speeds. Intel probably did not want to test overclocked modules as these appear to introduce increased latencies that may affect certain workloads.\nHello, team! We need assistance regarding how to use SmartDedupe and SmartCompression. Do you know how to configure the effective capacity license? Can you help me configure it? Thanks in advance! Hello, friend! If the effective capacity license is used, SmartDedupe and SmartCompression are enabled by default when you create a LUN using CLI commands. You are advised not to disable them. Otherwise, the physical capacity may be insufficient. Some application types are preset for typical application scenarios of LUNs. For the preset application types, the application request size and the SmartDedupe and SmartCompression status have been configured. When creating a LUN on the CLI or DeviceManager, you only need to specify an appropriate application type. The preset application types include Default,Oracle_OLAP,Oracle_OLTP,Oracle_OLAP&OLTP,SQL_Server_OLAP,SQL_Server_OLTP,SQL_Server_OLAP&OLTP,SAP_HANA,Vmware_VDI,Hyper-V_VDI , and FusionAccess_VDI . When a LUN is created on DeviceManager, the default application type is Default . You can select an application type that matches your service I/O model by referring to the following application type details. For OceanStor Dorado 5000, OceanStor Dorado 6000,OceanStor Dorado 8000, and OceanStor Dorado 18000,Table 2-1lists the application request size and the SmartDedupe and SmartCompression status for each application type.\nPlease kindly explain the RAID 2 Plus as compared to other traditional RAID 2 Technology in HCIE.Please Elaborate. RAID (Redundant Array of Independent Disks) is a redundant array of independent disks. It  is proposed by the University of California, Berkeley in 1987. The basic idea is to combine multiple independent physical hard disks into a  virtual logical hard disk by using related algorithms, thereby providing a larger, higher, or higher data error tolerance function. Background  of block virtualization and reconstruction of traditional RAID: The overall performance of the application system deteriorates, and the long  reconstruction time and data loss risks increase. Traditional RAID is limited by the number of hard disks. When the data capacity increases sharply, enterprises cannot flexibly and flexibly allocate resources. Therefore, the underlying data distribution architecture must be highly flexible and scalable, with the increase of hard disk capacity, data management based on hard disks is becoming more and more difficult. Huawei  provides RAID 2.0+ technical support by designing and developing traditional RAID, LUN virtualization, and block virtualization. The following figure shows the principle of Huawei 2.0+ The following figure shows the logical objects of Huawei RAID2.0+ software Disk  Domain is a combination of disks (which can be all disks in the system). After the disks are combined and reserved for hot spare capacity, the provides storage resources for the storage pool. 1) and HVS series storage systems can have one or more disk domains. 2).\nMultiple storage pools (Storage Pool) can be created in a disk domain. 3). Disks in a disk domain can be SSDs, SAS disks, or NL-SAS disks. 4). Different disk domains are isolated from each other, including fault domains, performance, and storage resources. torage pool is a storage container. The  storage space used by all application servers comes from the storage pool. A storage pool is created based on a specified disk domain. The Chunk (CK) resources can be dynamically allocated from the disk domain, and the Chunk Group (CKG) is composed of each storage level (Tier). RAID  protected storage resources. Tier  is a storage tier, a collection of similar storage media in a storage pool, used to manage storage media of different performances to provide different storage space for applications with different performance requirements. The storage pool can be divided into multiple Tiers depending on the type of hard disk. 1). Creating a storage pool specifies the storage tier (Tier) type that the storage pool divides from the disk domain and the RAID policy and capacity for that type. 2). HVS series storage systems support RAID5, RAID6 and RAID10. 3). The capacity layer is composed of a large-capacity NL-SAS disk. The RAID policy recommends using a double-check mode RAID6. Disk  Group (DG) is a set of disks of the same type in a disk domain. The disk type can be SSD, SAS, or NL-SAS.\nThe HVS series automatically divides one or more Disk Group (DG) in each disk domain based on the number of disks of each type. 1). One Disk Group (DG) contains only one type of hard disk. 2). Multiple CKs of any CKG come from different hard disks of the same Disk Group (DG). Logical Drive (LD) is a logical disk that is managed by the HVS series storage system. It corresponds to a physical disk.  5. CK is short for CK. It divides the hard disk space in a storage pool into several fixed physical spaces. The size of each physical space is 64MB, which is the basic unit of the RAID.  6. The Chunk Group (CKG) is a logical storage unit that consists  of CKs from different disks in the same DG based on the RAID algorithm.  It is the minimum unit for allocating resources from a disk domain to a  storage pool. 1). CKs in a CKG come from disks in the same DG. 2). CKG have RAID properties (the RAID properties are configured on the tier). 3). CK and CKG are internal objects of the system and are automatically configured by the HVS storage system.  7. Extents are logical storage spaces of fixed sizes based on CKGs. The size of extents is adjustable. It is the minimum unit (data migration granularity) for hotspot data statistics and migration, and is  the minimum unit for applying for space and releasing space in a storage pool. 1).\nAn extent belongs to one volume or LUN. 2). The size of and extent can be set when a storage pool is created. The size cannot be changed after being created. 3). The extents of different storage pools can be different, but the extents of the same storage pool are of the same size.  8. In thin LUN mode, extents are further divided into fine-grained blocks based on the fixed size. These blocks are called Grain. Thin LUNs are allocated based on Grain. LBAs in Grain are consecutive.  9. A volume is an internal management object of the system. A volume object is used to organize all extents and Grain logical storage units of the same LUN. It can dynamically apply for and release extents to increase or decrease the actual space occupied by the volume. A LUN is a storage unit that can be directly mapped to a host for reading and writing. A LUN is a volume object. Huawei RAID2.0+ Technical Features: Automatic  load balancing reduces the overall failure rate. Quick thin reconstruction, improving the failure rate of dual disks; Quick thin reconstruction, improving the failure rate of dual disks; Fault self-detection and self-healing ensure system reliability. Huawei RAID2.0+ Reliability: Load sharing: RAID2.0+ enables hard disks to work more evenly, preventing hard disks from being overworked.\nRobust  reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. Fast  reconstruction: The RAID2.0+ greatly reduces the time window for reconstruction. In this way, the system can be restored to the fault tolerance state as soon as possible, thereby improving system reliability. Simplified  reconstruction: RAID2.0+ can detect the used space in the allocated space through metadata. Therefore, only used space is reconstructed during reconstruction, which reduces the amount of data to be reconstructed, shortens the reconstruction time, and reduces reconstruction risks. Self-detection and  self-healing: The RAID2.0+ uses the distributed hot spare space. When the system detects a fault, the system automatically starts reconstruction as long as the disk has free space (CK). This improves reliability and reduces management costs. Invalid  data volume: If a traditional RAID group fails, all data in the RAID group is affected. When multiple disks fail on the RAID2.0+, only the data associated with the failed disks becomes invalid. Most data can still be accessed. The amount of invalid data is reduced by order of magnitude compared with the traditional RAID. Huawei RAID2.0+ dual-disk failure analysis: RAID technology is the basis of storage data protection. It is still the fault tolerance capability of RAID. For  RAID 5, the number of faults that can be tolerated is 1. For traditional RAID, the unit is hard disk. For RAID2.0+, the unit is blocked.\nFor NAS applications: When the file system is mounted to the host, the specified inode size of the storage device is 4 KB. The number of inodes is equal to the size of the file system divided by 4 KB. You can run the df -i command on the host to query the exact value. The inode of the FS indicates the number of files that can be stored in the FS. If the file size is less than 3 KB (metadata occupies 1 KB), the FS may have free space but the inode is used up and files cannot be written. Although the larger the file system capacity, the larger the number of inodes. To improve access performance and efficient management, it is recommended that the number of files in the file system be less than or equal to 2 billion in the product documentation. For details about the file system specifications, see the software specifications in the product documentation of the corresponding version. When you run the show pal file_system_service_stat command to query the file system attributes, the Max File Inode Number field is displayed in the statistics. The meaning of this value is as follows: If the queried fs id is in the Quota Tree, this value indicates the hard quota of the number of files that are set when the Quota Tree is created. The number of files in the file system cannot exceed the hard quota.\n1. Supported RAID levels: RAID-5/RAID-6/RAID-TP 2. New disks are added. N in N+M can increase with the number of disks. N indicates the number of data columns, and M indicates the number of parity columns. M of RAID-5, RAID-6, and RAID-TP are 1, 2, and 3. 3. Dorado6000 V3 and Dorado5000 V3 SAS edition: The maximum value of N+M is 25. The number of disks is n. If the number of disks is less than 13, the value of N+M is n-1. If the number of disks is greater than or equal to 13 but less than 26, N+M is n-2. For 26 or 27 disks, N+M is n-3. The number of other disks is N+M. The maximum number is 25. For example, if 25 disk slots are used: RAID 5 uses 22+1, but not 23+1. The reason is that if one disk is damaged, the disk can be quickly reconstructed. After the reconstruction is complete, the faulty disk can be quickly reconstructed. The entire principle is to handle the fault based on the failure of two disks. RAID 6 uses 21+2, which accelerates the reconstruction time. RAID-TP uses 20+3 instead of 19+3. The reason is that there is a low probability that three disks are abnormal at the same time. Fast reconstruction when two disks are faulty is supported, which is the same as RAID-6. 4. Dorado5000 V3 NVMe edition: The maximum value of N+M is 23.\nDear sb, HyperMetro enables active-active block storage services. If the storage array in one data center malfunctions, host services are switched to the storage array in the other data center. If the link between two storage arrays in two data centers is down, only one storage array can be accessed by hosts. How quorum server determines which storage array continues providing services.? In this mode, an independent physical server or VM is used as the quorum server. You are advised to deploy the quorum server at a dedicated quorum site that is in a different fault domain from the two DCs. In  the event of a DC failure or disconnection between the storage systems,  each storage system sends an arbitration request to the quorum server, and only the winner continues providing services. The preferred site takes precedence in arbitration. The following example uses DC A as the preferred site and DC B as the non-preferred site. describes the arbitration mechanism in quorum server mode. In  the following table, if two or more faults occur in the system, the interval between the faults is less than or equal to 60 seconds. Table 1 Fault Diagram Fault Description Arbitration Result The quorum server malfunctions. The HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. HyperMetro automatically switches to static priority mode. The link between one storage system (for example, storage system in DC A) and the quorum server is down.\nThe HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. A storage system (for example, storage system in DC A) malfunctions. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services. The link between one storage system (for example, storage system in DC A) and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The link between two storage systems is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. A storage system (for example, storage system in DC A) and the quorum server malfunction. The HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services. The  link between the two storage systems and the link between a storage system (for example, storage system in DC A) and the quorum server are down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services. A  storage system malfunctions (for example, storage system in DC A) and the link between the other storage system and the quorum server is down.\nThe HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services. The  quorum server malfunctions and the link between the host and one storage system (for example, storage system in DC A) is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. HyperMetro automatically switches to static priority mode. The  link between a storage system (for example, storage system in DC A) and  the quorum server is down and the link between the storage system and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The  link between the storage systems is down and the link between the non-preferred site (for example, DC B) and the host is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. The  link between the storage systems is down. Then the links between a storage system (for example, storage system in DC A) and the quorum server as well as the host are down. The HyperMetro pair is in the To be synchronized state. The HyperMetro services are interrupted. To avoid misoperation or data loss, contact Huawei technical support for service recovery.\nBecause the local and heterogeneous storage systems use the same disk names, heterogeneous disks cannot be deleted by name. You must remove the logical paths of the heterogeneous disks to delete them. The procedure is as follows: Run the upadm show vlun type = migration command to query the names of the heterogeneous disks to be deleted. In this example, the disk name is hdisk0 , which represents both the local and heterogeneous disks. Run the lspath -l name -F command to query all logical paths of hdisk0 . The result includes the logical paths of both the local and heterogeneous disks. Note: In the preceding command, -l represents the disk name that is obtained by the upadm show vlun type = command. Run the upadm show phypath command to query the path of the heterogeneous storage system in Target Port. Note: Target Port indicates the WWN of the port on a storage system. In this example, the paths whose PhyPathID is 0 , 1 , 2 , and 3 belong to the heterogeneous storage system. According to the results in 2 and 3, the logical paths of hdisk0 on the heterogeneous storage system are the following: Run the rmpath -w connection -l -p -d command to delete the logical paths obtained in 4. Note: -w represents the path to be deleted. -l represents the disk name. -p represents the logical device name of the parent device. -d indicates deletion.\nWhen a remote device whose type is Replication is added or an iSCSI link is added to the remote device, error code 1073804034 may be reported or the link may fail to be added if the local storage system uses the TOE interface module for interconnection. When a TOE interface module on the local storage system connects to a non-TOE interface module on the remote storage system, the TOE and non-TOE interface modules process data in different ways. As a result, the two systems may use different connection methods, causing connection failure. Note: This problem does not occur when a non-TOE interface module on the local storage system connects to a TOE interface module on the remote storage system. Perform the following operations to locate and handle the problem: Confirm the cause of the connection failure. If you establish the connection between a TOE and a non-TOE interface module and fail to add links on the storage system that uses the TOE interface module, the problem is caused by the defect described above. For further confirmation, send the system logs to technical support engineers. After the confirmation, handle the problem with either of the following methods: Method 1: Set up the connection from the non-TOE interface module to the TOE interface module. Method 2: On the storage system that uses the non-TOE interface module, run the tcp_recycle_switch.sh command to disable the quick recycle function of the TIME_WAIT socket in the TCP connection. Enable this function after the connection is established.\nLoad balancing within a controller and load balancing between controllers are enabled by default. Users can modify the heterogeneous storage device whitelist to set the path selection algorithm to define the load balancing policy of the array. The detailed procedure is as follows. Note A heterogeneous array whitelist is a list of Huawei's or third-party vendors' devices that Huawei storage systems can take over. You can configure the whitelist to set the path selection algorithm used by the local storage system to take over heterogeneous storage systems and failover and failback policies. Run show remote_device general to obtain the ID of the remote array. Run show remote_lun general array_type=? remote_device_id=? to query basic information about the remote LUN. The Vendor and Model fields in the command output indicate the vendor and model of the remote storage device. Run show remote_device white_list to query the whitelist of heterogeneous arrays. The Path Selector field in the command output indicates the path selection algorithm of the heterogeneous array. Run change remote_device white_list to modify the heterogeneous array whitelist. Note: The path_selector field is used to set the path selection algorithm of the array. Its values include FIX , ROUND_ROBIN , and LEAST_QUEUE . FIX indicates that the specific path is selected, that is, the working controller is preferentially selected. The ROUND_ROBIN algorithm rotates through all paths, thus balancing I/Os. The LEAST_QUEUE algorithm selects the path that has the least number of I/Os. The ROUND_ROBIN and LEAST_QUEUE path selection algorithms support load balancing.\nHow to check whether the cables have been laid out properly: Labels are correctly attached to cables. Cables are laid out in an untangled and orderly fashion. Cables that pass the cable ladder are secured on the beam of the cable ladder. Cable troughs are used for cables laid outside a bay, and these cables are in the cable troughs. The bending radius of optical fibers is greater than or equal to 50 mm. Power cables and ground cables are bent smoothly. Power cables and ground cables are connected correctly and securely. Ground cables of a bay are connected correctly and reliably. The diameter of the power cables and ground cables meets the power distribution requirements. The external power cables and ground cables are bound with adequate slack. The cables are separately deployed with at least 150-mm space in between. Also, the cables are separately deployed from the signal cables with at least 30-mm in between. Cables are bound neatly in a rectangular shape (circular when the cables are single-cored). Cables are bent at a radius larger than 60 mm. Cable ties are not installed where a cable bends. Optical fibers are laid out without using force or having unnatural bends. OT terminals of the power cables and ground cables are welded or crimped firmly. Naked cables and OT terminal handles of the power cables and ground cables should be wrapped by protection tubes or insulation tapes. There should be no bare copper wires on OT terminals. Shock absorption sheets and washers should be installed properly.\nRemember the post from late July? The one about Huawei P50 series' lack of built-in 5G? Well, that post explained why P50 has no built-in 5G. And it was the truth. But you guys dismissed it, believing that the post doesn't make much sense at all for its words. Well, then you're wrong. And by the way... I am the one who posted it. I got continuously suspended for reasons that I won't explain. Also, I use this current account instead my other two accounts, the main and 2nd one, because both has problems that rendered almost all the posts and comments from there not being able to work properly. So yeah. Anyways, long post ahead. Back when Huawei P50 series released (July 29th), it made buzz across the world with its camera (camera system being innovative: XD Fusion, True Chroma, etc.) and premium things about the phones. And yes, Huawei hoped for the series to help them propel back to it full glory. After all, Huawei has so many troubles due to combined forces from US regulations and COVID-19 pandemic. The phones (especially P50 Pro) alternates between Kirin 9000 and Snapdragon 888, but that's due to the former chipset was on verge of running out. One odd thing is lack of (built-in) 5G. Yes, you know it. P50 doesn't have 5G built-in. Such, you has to add some 5G hardware to connect to your P50 to experience 5G, which would be awkward and potentially expensive.\nTLDR: My smartphone might be blocked because of a glitch. I need help with doing some tests to find out if all my attempts are useless or it might be some hope in the end. I was using fingerprint and pattern password to unlock my smartphone. Recently it asks me for a pin password which I don't remember adding. I have this smartphone for 3 years and now is the first time asking for pin. It doesn't let me use pattern password or fingerprint. I have read that it might be bug from the latest android security update. I have the notification \" Android System - User data locked Password required after you restart device Some features are disabled for security \" Did anyone else encounter this problem? Have you managed to solve it without factory resetting the smartphone? Right now I am entering all the passwords I have used in the past for different apps, in case one of the passwords was for the smartphone instead the app. Right now, I don't know how the smartphone should behave in case I have entered the correct pin. Could somebody restart their huawei mate 10 pro or similar smartphone and do the following tests? : enter the wrong pin several times until it says to wait x minutes. Then try to enter the correct pin very fast and add 1 extra digit. This way I can know if the smartphone prevents us from entering extra digits in case we already we wrote the correct pin.\nIn case it let's you add 1 extra wrong digit. Could you check how many seconds, do you need to wait for the smartphone to recognize the password and let you use the smartphone again? This one is a a bit more tricky. After you enter the wrong password several times, you will notice that when entering the pin, some digits might seems like they are getting doubled. Ex: you enter 1234567 and you see on smartphone this *******. When adding 8 you should see *******8 then ********. But it appears **8****8 then ********. In the end you see the same number of stars(*) but the last digit looked like it was doubled when you entered it. I saw this visual bug at different pin lengths, sometimes it appears also when entering only 3 digits. The bug disappears after restart we the smartphone again. The question is if this bug would prevent the smartphone from recognizing the pin even if you write the correct pin. In case something is not clear. I will add as many details are needed. I really wish I could unblock my smartphone without losing all my data. :( Thanks for the help! P.S. : My waiting time for entering new pins is 1 hour for every 3 attempts. But today I saw that the timer got reset and was asking for 1 min again.\nFiber Channel is an open, technical standard for networking that incorporates the channel transport characteristics of an I/O bus, with the flexible connectivity and distance characteristics of a traditional network. Because of Fiber Channels channel-like qualities, hosts and applications see storage devices that are attached to the SAN as though they are locally attached storage. Because of Channels network characteristics, Channel can support multiple protocols and a broad range of devices. And, Channel can be managed as a network. Channel can use either optical fiber (for distance) or copper cable links (for short distance at low cost). Channel is a multiple layer network that is based on a series of American National Standards Institute (ANSI) standards that define characteristics and functions for moving data across the network. These standards include the definitions of physical interfaces, for example: Cabling, distances, and signaling Data encoding and link controls Data delivery in terms of frames Flow control and classes of service Common services Protocol interfaces Like other networks, information is sent in structured packets or frames, and data is serialized before transmission. But, unlike other networks, the Channel architecture includes significant hardware processing to deliver high performance. Channel uses a serial data transport scheme that is similar to other computer networks, which stream packets (frames) of bits, one behind the other, in a single data line to achieve high data rates. Serial transfer does not suffer from the problem of skew, so speed and distance are not restricted in the same way that parallel data transfers are restricted.\nScalability limitations The amount of data that is available to the server is determined by the number of devices that can attach to the bus. The amount is also determined by the number of buses that are attached to the server. Up to 15 devices can be attached to a server on a single SCSI bus. In practice, because of performance limitations due to arbitration, commonly no more than four or five devices are attached in this way. This factor limits the scalability in terms of the number of devices that can connect to the server. Reliability and availability limitations SCSI shares aspects with bus and tag; for example, the cables and connectors are bulky, relatively expensive, and prone to failure. Access to data is lost in a failure of any of the SCSI connections to the disks. Data is also lost in the reconfiguration or servicing of a disk device that is attached to the SCSI bus because all of the devices in the string must be taken offline. In todays environment, when many applications need to be available continuously, this downtime is unacceptable. Speed and latency limitations The data rate of the SCSI bus is determined by the number of transferred bits, and the bus cycle time (measured in megahertz (MHz)). Decreasing the cycle time increases the transfer rate. However, because of limitations that are inherent in the bus architecture, decreasing the cycle time might also reduce the distance over which the data can be successfully transferred.\nThe physical transport was originally a parallel cable that consisted of eight data lines to transmit 8 bits in parallel, plus control lines. Later implementations widened the parallel data transfers to 16-bit paths (wide SCSI) to achieve higher bandwidths. A SCSI propagation delay in sending data in parallel along multiple lines leads to a phenomenon that is known as skew. Skew means that all bits might not arrive at the target device at the same time. Figure 1 shows this result. Figure 1. A SCSI propagation delay results in skew Arrival occurs during a small window of time, depending on the transmission speed and the physical length of the SCSI bus. The need to minimize the skew limits the distance that devices can be positioned away from the initiating server to 2 meters (6.5 ft) - 25 meters (82 ft). The distance depends on the cycle time. Faster speed means shorter distance. Distance limitations The distances refer to the maximum length of the SCSI bus, including all attached devices. Figure 2 shows the SCSI distance limitations. These limitations might severely restrict the total GB capacity of the disk storage that can be attached to an individual server. Figure 2. SCSI bus distance limitations Device sharing Many applications require the system to access several devices, or for several systems to share a single device. SCSI can enable this sharing by attaching multiple servers or devices to the same bus. This structure is known as a multi-drop configuration. Figure 3. shows a multi-drop bus structure configuration.\nWhat are the features of the oceanstor 5300 v3 that require a license? Function Requiring License Control or Not HyperSnap (Snapshot) Yes HyperClone (Clone) Yes HyperCopy (LUN Copy) Yes HyperReplication (Remote replication) Yes SmartQoS Yes SmartTier Yes SmartMotion Yes SmartThin Yes SmartPartition Yes SmartMigration Yes SmartErase Yes SmartMulti-Tenant Yes SmartVirtualization Yes HyperMirror Yes SmartCompression (for LUN) Yes SmartDedupe (for LUN) Yes SmartCompression (for FS) Yes SmartDedupe (for FS) Yes SmartQuota Yes CIFS Yes NFS Yes SmartCache Yes WORM (HyperLock) Yes NDMP Yes HyperMetro (for LUN) Yes HyperMetro (for FS) Yes HyperVault Yes a: HyperSnap for block and file services requires the same license. After importing the license file for the HyperSnap feature, a user can create snapshots for both block and file services. b: HyperReplication for block and file services requires the same license. After importing the license file for the HyperReplication feature, a user can create remote replications for both block and file services. A SAN basic software package provides the following functions: OceanStor DeviceManager, SmartThin, SmartMotion, SmartMigration, SmartErase, SmartMulti-Tenant, eService, and OceanStor SystemReporter. In addition to the functions of the SAN basic software package, the SAN+NAS basic software package also includes the following functions: CIFS, NFS, NDMP, SmartDedupe (for FS), SmartCompression (for FS), and SmartQuota. If a required function is not included in the basic software package, you need to purchase the license file for this function. For details, contact the dealer.\nHuawei is using SmartDedupe below is the working principal The basic concepts of SmartDedupe follows: Deduplication data block size: Specifies the granularity of data that will be deduplicated in a storage system. Similarity-based deduplication: The system divides data into blocks of a fixed size and analyzes the similarity among the blocks. Then, the system deduplicates the identical data blocks and performs delta compression on the similar data blocks. Fingerprint: The fingerprint is a fixed-length binary numeric value. OceanStor Dorado V6 series storage systems use the weak hash algorithm to calculate the fingerprints of data blocks. In a storage system, all the mappings between data block fingerprints and data storage locations are stored in the fingerprint table. Opportunity table: Saves data blocks' fingerprint and location information for identifying hot fingerprints. Byte-by-byte comparison: When a storage system searches for duplicate data blocks, it will compare fingerprints of data blocks. If the fingerprints are the same, the system compares the data blocks byte by byte. Deduplication metadata: Saves information about deduplication. For example, the metadata saves the fingerprint information about data blocks and the storage locations of data after deduplication is executed. shows the similarity-based deduplication process. Step 1: The storage system divides newly-written data into blocks. The default data block size is 8 KB. The storage system uses a similar fingerprint algorithm to calculate the SFPs of the newly-written data blocks. The storage system writes the data blocks to disks and records data blocks' fingerprint and location information in the opportunity table.\nStep 2: The storage system periodically checks whether SFPs exist in the opportunity table. If yes, go to 2. If no, continue the periodic check. The storage system checks whether similar data blocks are the same based on byte-by-byte comparison. If yes, the storage system considers that the new data block is the same as an existing data block, so it deletes the new data block and maps the fingerprint and storage location information to the existing data block in the fingerprint table. If no, the storage system performs delta compression on the new data block, records fingerprint information to the fingerprint table, updates fingerprint information to the metadata of the data block, and reclaims the space of the data block. For example, LUN 1, LUN 2, and LUN 4 in the storage system have the same attributes. lists the existing data blocks of LUN 1, LUN 2, and LUN 4 as well as the results of comparison between new data blocks J, K, and L on LUN 1 and the existing data blocks. LUN Name Existing Data Block Characteristic of New Data Block LUN 1 Data blocks A, B, and C The SFP of data block J is stored in the opportunity table. Data block J is the same as data block E in byte-by-byte comparison. The SFP of data block K is stored in the opportunity table. Data block K is different but similar to data block I in byte-by-byte comparison.\nHello, team! Storage: Oceanstor 5500 v5 I have a question regarding a Linux server (Debian 11 Bullseye) and multIPath. I have presented a LUN from our OceanStor to both ports of our Emulex FC card. I can see the disks twice on the linux server and I have configured the multIPath.conf with those parameters : devices { device { vendor \"HUAWEI\" product \"XSG1\" path_grouping_policy failover path_checker tur prio const path_selector \"round-robin 0\" failback immediate no_path_retry 15 dev_loss_tmo 30 fast_io_fail_tmo 5 } } My problem is when I create a Volume Group with LVM on this server, after a reboot, the multIPath seems to lose my devices and I can't access my volume group anymore. Could you please give me advice on the multiPath configuration on RedHat 7? Hello, friend! Have a nice day! Generally, multipathing software packages in Red Hat are rpm packages starting with device-mapper-multipath .If you did not install the multipathing software when installing the operating system, you can obtain the software package from the system image and use the rpm command to install it. DM-Multipath's most important configuration file is /etc/multipath.conf . Some operating systems have this file by default. If your operating system does not have this file, you can copy the multipath.conf.synthetic file to the /etc directory to generate one. Generating the multipathing configuration file If the system does not have a template, run the /sbin/mpathconf --enable command to manually generate /etc/multipath.conf .\nHow many kinds of load balancing policies does the OceanStor 9000 support if i use the infoEqualizor .  hello, have a nice day! The assigns load balancing policies based on zones. The supported load balancing policies include Round-robin , CPU usage , Node connections , Node throughput , and Comprehensive node load . shows the policies for reference. Name Description Advantages Disadvantages Round-robin When clients access the using domain names, nodes are selected in the sequence recorded in the cache to process client services. Data on which the domain name server (DNS) depends is cache data, ensuring accurate and reliable node selection. When the dynamic IP address of the selected node is switched, cache data is cleared. As a result, a new node will be selected from the start point.  If any DNS domain name requests (such as ping, nslookup, showmount commands) time out or authentication of service connections fails, load balancing is affected.  CPU usage When clients access the by using domain names, nodes with the lowest CPU usage are selected to process client services. Data on which the DNS depends is CPU usage. The nodes with the lowest CPU usage are selected to process client services. Within a performance data update period, all client services are allocated to the nodes with the lowest CPU usage.  CPU usage changes significantly only after nodes carry services, delaying load balancing. Mounted clients must run services first so that load balancing can be achieved based on the CPU usage.\n Node connections When clients access the by using domain names, nodes with the least number of connections are selected to process client services. Data on which the DNS depends is the number of node connections. Nodes with the least number of connections are selected to process client services. The number of node connections is not real-time performance data, affecting client load balancing.  If a mounted NFS service has no packet interaction within 6 minutes, the corresponding node clears the related connection information. However, the mount point still exists. New clients will be mounted to the same node based on node connections. As a result, the node has multiple mount points and carries multiple services, affecting load balancing.  Node throughput When clients access the by using domain names, nodes with the lowest throughput are selected to process client services. Data on which the DNS depends is the throughput. Nodes with the lowest throughput are selected to process client services. Within a performance data update period, all client services are allocated to the nodes with the lowest throughput.  The throughput changes significantly only after nodes carry services, delaying load balancing. Mounted clients must run services first so that load balancing can be achieved based on the throughput.  Comprehensive node load Comprehensive node loads are calculated based on CPU usage and throughput. The least loaded nodes are selected to carry client services.  Client services are not allocated only to the least loaded nodes.\nHello all, Topology layer: Fabrics provide better elastic scalability than PCIe. Here, we focus on the expansion capability of storage. There is no PCIe choice between servers and networks. The PCIe is designed for the system and implements the remote connection to external devices through extended hardware. However, the logical expansion capability of the PCIe is limited. For this reason, a controller can manage only two-digit SSDs, and the multi-controller sharing capability is greatly limited. Huawei Dorado storage uses the IP-based NVMe-oF technology to support more SSD devices and share SSD media among eight controllers. Topology Maximum of SSDs Shared Architecture DMA Engine PCIe 256 for PCIe bus total, no more than 100 for SSDs Can be shared by 2 controllers Data Channel No DMA CPU dependent NVMe-oF No limited Can by shared by 8 controllers, even 32 controllers with switch. DMA enabled CPU independent RoCEv2 technology is used to implement all-IP networks. Simplify: All-IP Data Center using RoCE technology Before introducing the advantages of RoCE, let's see what RoCE is. The core positioning of RoCE is to run the RDMA protocol on the Ethernet. See the following figure. In RoCEv2, the InfiniBand transport layer protocol (IBTA) packets are encapsulated into UDP/IP packets to support RDMA. The core of this protocol stack is RDMA. Remote Direct Memory Access (RDMA) allows applications to directly access the memory of a remote node, bypassing CPU intervention. Therefore, data transmission interaction efficiency can be greatly improved, and load on a CPU can be reduced.\nRDMA is really a good thing, but there are big limitations. RDMA is implemented based on the assumption that the following dependent network is absolutely reliable and zero packet loss occurs. If network packet loss occurs, the RDMA transmission efficiency is seriously affected. On a traditional TCP/IP network, if the packet loss rate is 2%, the performance decreases by about 37%. However, the performance can be tolerated. On an RMDA network, the performance deteriorates severely and even the traffic is zero. Generally, RDMA requires that the packet loss rate of the network be lower than 0.001%. RDMA requires a reliable network, but IP networks usually rely on Ethernet networks that adopt the best-effort policy and cannot ensure high reliability with zero packet loss. How? The priority-based flow control (PFC) is implemented based on the priorities of data flows. If the destination node detects that the receive queue exceeds the threshold (that is, the packet cannot be processed and may be discarded), the destination node sends a signal to the source end to request the source end to stop sending the packet and retransmit the packet when the queue is idle. It seems that the packet loss problem is solved and priority-based control is implemented. Unfortunately, PFC is based on the link between two points. It transmits data in a segment of layers. Once PFC takes effect, traffic on the entire network is affected and other data is affected.\nHello all, What is EC Redundancy Ratio? Thanks. Dear Axe, This table shows the Redundancy ratios. Thanks.\nNormally, fiber-optic cabling is referred to by mode or the frequencies of lightwaves that are carried by a particular cable type. Fiber cables come in two distinct types (Below Figure 1). Figure 1. Cable types The cable types are described as follows: Multi-mode fiber for shorter distances Multi-mode cabling is used with shortwave laser light and has either a 50-micron or a 62.5-micron core with a cladding of 125 micron. The 50-micron or 562.5-micron diameter is sufficiently large for injected light waves to be reflected off the core interior. Multi-mode fiber (MMF) allows more than one mode of light. Common multi-mode core sizes are 50 micron and 62.5 micron. MMF fiber is better suited for shorter-distance applications. Where costly electronics are heavily concentrated, the primary cost of the system is not the cable. In this case, MMF is more economical because it can be used with inexpensive connectors and laser devices, therefore reducing the total system cost. Single-mode fiber for longer distances Single-mode fiber (SMF) allows only one pathway, or mode, of light to travel within the fiber. The core size is typically 8.3 micron. SMFs are used in applications where low signal loss and high data rates are required. An example of this type of application is on long spans between two system devices or network devices, where repeater and amplifier spacing needs to be maximized.\nWhat is Quorum? How does it work? What is the benefit of Quorum? In  this mode, an independent physical server or VM is used as the quorum server. You are advised to deploy the quorum server at a dedicated quorum site that is in a different fault domain from the two DCs. In  the event of a DC failure or disconnection between the storage systems,  each storage system sends an arbitration request to the quorum server, and only the winner continues providing services. The preferred site takes precedence in arbitration. The following example uses DC A as the preferred site and DC B as the non-preferred site. describes the arbitration mechanism in quorum server mode. In  the following table, if two or more faults occur in the system, the interval between the faults is less than or equal to 60 seconds. Fault Diagram Fault Description Arbitration Result The quorum server malfunctions. The HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. HyperMetro automatically switches to static priority mode. The link between one storage system (for example, storage system in DC A) and the quorum server is down. The HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. A storage system (for example, storage system in DC A) malfunctions. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services.\nThe link between one storage system (for example, storage system in DC A) and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The link between two storage systems is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. A storage system (for example, storage system in DC A) and the quorum server malfunction. The HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services. The  link between the two storage systems and the link between a storage system (for example, storage system in DC A) and the quorum server are down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services. A  storage system malfunctions (for example, storage system in DC A) and the link between the other storage system and the quorum server is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services.\nThe  quorum server malfunctions and the link between the host and one storage system (for example, storage system in DC A) is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. HyperMetro automatically switches to static priority mode. The  link between a storage system (for example, storage system in DC A) and  the quorum server is down and the link between the storage system and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The  link between the storage systems is down and the link between the non-preferred site (for example, DC B) and the host is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. The  link between the storage systems is down. Then the links between a storage system (for example, storage system in DC A) and the quorum server as well as the host are down. The HyperMetro pair is in the To be synchronized state. The HyperMetro services are interrupted. To avoid misoperation or data loss, contact Huawei technical support for service recovery. One  storage system (for example, storage system in DC A) malfunctions and the links between the storage system and the host as well as between the  other storage system and the quorum server are down. The HyperMetro pair is in the To be synchronized state.\nDear team, Can anyone show me what isElastic EC? And how does it work? Thanks. Dear Axe, In traditional disk-level RAID mode, data is stored on different disks of a single node and cannot be restored when the entire node fails. To prevent data loss, the storage system needs to provide redundancy protection for data among nodes. Erasure coding (EC) is a redundancy protection mechanism that implements data redundancy protection by calculating parity fragments. When writing data, the distributed storage system divides the data into N data fragments (N is an even number) and calculates M parity fragments (M can be 2, 3, or 4) by using the EC encoding algorithm. Server-level security: N data fragments and M parity fragments are stored on different nodes. If M nodes or M disks are faulty, the system can still properly read and write data, services are not interrupted, and data is not lost. Cabinet-level security: N data fragments and M parity fragments are stored in different cabinets. If M cabinets or M nodes or M disks of different cabinets are faulty, the system can still read and write data properly, services are not interrupted, and data is not lost. The space utilization with the EC redundancy mode used is about N/(N + M). A larger value of N indicates higher space utilization. The data reliability is determined by the value of M. A larger value of M indicates higher reliability. For details about the redundancy ratios, see the EC redundancy ratios table at the end of this section.\nHello all, Automatic management: AI technology is used to implement intelligent network management and continuously maximize performance. Traditional traffic management depends on traffic police. Traffic congestion control methods, PFC and ECN, have similar problems. The ECN is used as an example. To ensure that the ECN functions effectively and prevent PFC or network packet loss, the administrator needs to manually configure an ECN threshold. If the threshold is too low, the network throughput will be low. If the threshold is too high, PFC or network packet loss may occur, resulting in a long network delay. See the following figure. Unfortunately, there is no best practice for setting ECN thresholds. Even in the same scenario, different traffic pressures require different thresholds. Static thresholds are always regrettable. Historical experience has proved that it is unreliable to assign such trivial tasks to people, which require dynamic adjustment, because after all, human energy is limited. The best idea is to be handled by the program. Huawei develops the AI Fabric technology to implement intelligent dynamic adjustment of network parameters, continuously providing the most appropriate parameter combinations, and maximize network performance. AI Fabric is an intelligent traffic navigation platform. It collects various indicator data in real time, senses various traffic loads on the network, learns various traffic models, and provides optimal configuration suggestions in real time based on AI capabilities, providing optimal traffic paths for data packets. In the AI deployment model, the AI Fabric uses the central+edge dual-layer model.\nHello all, Here is theData Sheet of Huawei OceanStor 2600 V3 Storage System. Huawei OceanStor 2600 V3 storage system are flash-oriented storage products specifically designed for enterprise-class applications. Employing a storage operating system built on a cloud-oriented architecture, a powerful new hardware platform, and a suite of intelligent management software, the OceanStor 2600 V3 storage system deliver industry-leading functionality, performance, efficiency, reliability, and ease-of-use. The 2600 V3 storage system are ideal for applications such as middle-size/large database Online Transaction Processing (OLTP)/Online Analytical Processing (OLAP), file sharing, and cloud computing. Further, these systems offer a wide range of efficient backup and disaster recovery solutions. With a versatile set of capabilities, the 2600 V3 storage system can be widely applied in industries ranging from government, finance, telecommunications, to energy. Provides elastic storage, simplifies service deployment, improves storage resource utilization, and reduces Total Cost of Ownership (TCO). Underlying storage resource pools provide both block and file services and shorten storage resource access paths to ensure that the two services are equally efficient. The design purpose-built for all flash architecture gives full play to the performance of SSDs and maintains a low latency of 1 ms or less. Combining the advantages of solid-state storage and traditional HDD storage, the systems enable the media performance to reach the full potential in different scenarios. A built-in virtualization function efficiently manages storage systems from multiple vendors and unifies resource pools for flexible, centralized resource allocation and protects their data.\nHi team, In the VMware scenario where background host I/Os and VAAI (which are of different LUNs) are running for a long time, VM migration task times out and fails when the foreground host I/O pressure is heavy. Thanks. Dear Mumammad, Have a nice day. The root cause maybe is that the QoS flow control mechanism of the storage system preferentially ensures foreground I/Os when both the foreground and background host I/O pressures are high. The implementation mechanism is as follows: 1. For the background added to the work service scheduling mechanism, the bandwidth or concurrency of background tasks is restricted. When the service module performs background tasks in the WS scenario, the bandwidth or concurrency adjustment range [Min., Max.] is set. Based on the foreground pressure (such as host read/write latency, QoS bucket queuing latency, and OLC rating), QoS determines to increase or decrease the background bandwidth or concurrency. The adjustment range does not exceed [Min., Max.]. 2. For I/Os that are written to the cache, both the foreground and background need to obtain tokens from the QoS token bucket. When the foreground's workload is heavy, foreground I/Os are preferentially allocated with tokens. Most of the tokens are allocated to foreground I/Os based on the I/O priority, and only a small part of the tokens are allocated to background I/Os. I/Os that fail to obtain tokens are put into the end of the QoS queue.\nIf an I/O does not obtain tokens within 3 seconds, a timeout error is returned, and the service module continues to retry. Therefore, the flow control mechanism of the storage system may cause the failure of background VM migration tasks. Thanks. Dear Mumammad, Have a nice day. The root cause maybe is that the QoS flow control mechanism of the storage system preferentially ensures foreground I/Os when both the foreground and background host I/O pressures are high. The implementation mechanism is as follows: 1. For the background added to the work service scheduling mechanism, the bandwidth or concurrency of background tasks is restricted. When the service module performs background tasks in the WS scenario, the bandwidth or concurrency adjustment range [Min., Max.] is set. Based on the foreground pressure (such as host read/write latency, QoS bucket queuing latency, and OLC rating), QoS determines to increase or decrease the background bandwidth or concurrency. The adjustment range does not exceed [Min., Max.]. 2. For I/Os that are written to the cache, both the foreground and background need to obtain tokens from the QoS token bucket. When the foreground's workload is heavy, foreground I/Os are preferentially allocated with tokens. Most of the tokens are allocated to foreground I/Os based on the I/O priority, and only a small part of the tokens are allocated to background I/Os. I/Os that fail to obtain tokens are put into the end of the QoS queue.\nHello everyone, I believe you can learn some basic concepts of storage in this post. Block virtualization A new type of RAID technology. Block virtualization divides disks into multiple chunks (CKs) of a fixed size and organizes them into multiple chunk groups (CKGs). When a disk fails, the disks of the CKG where the CKs in the faulty disk reside also participate in reconstruction. This significantly increases the number of disks involved in the reconstruction, improving the data reconstruction speed. CK Disks are divided into fixed-size blocks. Each block is assigned a number and constitutes a CK. A CK is the smallest unit of a RAID group. CKG A logical collection of N+M CKs on different disks. N is the number of data blocks in a CKG and changes with the number of disks involved. M is the number of parity blocks in a CKG. A CKG has the properties of a RAID group. Grain CKGs are divided into small, fixed-size blocks called grains (the default size of a grain is 8 KB). Grains are the basic units that constitute a thin LUN. Logical Block Addresses (LBAs) in a grain are consecutive. Dynamic RAID A new RAID algorithm that dynamically adjusts the number of CKs in CKGs to ensure system reliability and storage capacity. If a CK is faulty and no CK is available from disks outside the disk domain, the system dynamically reconstructs the original N+M CKs to (N-1)+M CKs.\nWhen an extra SSD is inserted, the system then migrates data from (N-1)+M CKs to the newly constructed N+M CKs for efficient disk usage. Storage pool A storage resource container, which consists of multiple disks. The storage resources used by application servers are all from storage pools. Hot spare space Space used for data reconstruction of faulty blocks in block virtualization. When a CK is faulty, the system lets a CK of the hot spare space take over and instructs other CKs in the CKG to perform data reconstruction using the hot spare space. This ensures data integrity and read/write performance. LUN Storage space in a storage pool is divided into logical units called LUNs. A host can use storage space provided by LUNs after LUNs are mapped to it. LUN group A collection of multiple LUNs. If the data of an application is stored on multiple LUNs, you can create a LUN group for these LUNs. Operations on a LUN group apply to all its member LUNs. A LUN group can contain one or more LUNs. Host A physical or virtual machine that can access a storage system. Host group A collection of multiple hosts. If an application is deployed on a cluster consisting of multiple hosts, these hosts will access the data volumes of the application at the same time. In this case, you can create a host group for these hosts. Port Endpoint of a connection. For physical connections, ports are physical interfaces, such as Fibre Channel ports and iSCSI ports.\nSir, How many maximum number of Ports can be bounded in HyperReplication Please review below for Network Planning. This chapter describes the networks and data you must plan before HyperReplication configuration. HyperReplication  involves a primary storage device and a secondary storage device. You must plan the networking mode for HyperReplication and replication links  between the storage devices before configuring HyperReplication. HyperReplication supports Fibre Channel and IP networks as well as Fibre Channel and iSCSI ports for data replication. Based on application scenarios, the following three networking modes are available: direct connection using iSCSI front-end ports, direct connection using Fiber Channel front-end ports (multi-mode optical fibers), and connection through remote transfer devices (switches or routers). If  two storage devices are connected using a switch through Fibre Channel ports, you are advised to zone their access ports in one-to-one relationship. This ensures isolation between host service ports and inter-array replication service ports, minimizing the impact on each other. When planning replication links between storage devices, comply with the following rules: Ensure  that each controller of the primary storage device has a replication link to the secondary storage device, reducing I/Os forwarded between controllers. Otherwise, ensure that each controller enclosure of the primary storage device has a replication link to the secondary storage device. A  maximum of 256 available links are supported between a controller and a  remote device, and the HyperReplication feature uses at most eight of them as the replication links between the local and remote devices.\nCapacity planning Plan  the same available capacity that can be actually used for both primary and secondary file systems. If the available capacity of the secondary file system is smaller than that of the primary file system, remote replication will be interrupted due to insufficient capacity of the secondary file system during data synchronization. If the available capacity of the secondary file system is larger than that of the primary  file system, the capacity utilization of the secondary file system is low. In the following two scenarios, however, the available capacity that can be actually used for  the primary file system and that for the secondary file system are different: A file system is being created. The reserved snapshot space and the deduplication or compression feature are configured for primary and secondary file systems. Bandwidth planning For  asynchronous remote replication, the write latency of applications is irrelevant to the distance between the primary and secondary sites. Therefore, asynchronous remote replication is applicable to DR scenarios  where the primary and secondary sites are far away from each other, or the network bandwidth is limited. Generally, there is no explicit limit on the WAN distance between the primary and secondary sites. It is recommended that the link bandwidth should be 10 Mbit/s larger than Bandwidth at peak hours/Bandwidth utilization and unidirectional transfer delay should be smaller than 50 ms.\nA  maximum of 256 available links are supported between a controller and a  remote device, and the HyperReplication feature uses at most eight of them as the replication links between the local and remote devices. Capacity planning Plan  the same available capacity that can be actually used for both primary and secondary file systems. If the available capacity of the secondary file system is smaller than that of the primary file system, remote replication will be interrupted due to insufficient capacity of the secondary file system during data synchronization. If the available capacity of the secondary file system is larger than that of the primary  file system, the capacity utilization of the secondary file system is low. In the following two scenarios, however, the available capacity that can be actually used for  the primary file system and that for the secondary file system are different: A file system is being created. The reserved snapshot space and the deduplication or compression feature are configured for primary and secondary file systems. Bandwidth planning For  asynchronous remote replication, the write latency of applications is irrelevant to the distance between the primary and secondary sites. Therefore, asynchronous remote replication is applicable to DR scenarios  where the primary and secondary sites are far away from each other, or the network bandwidth is limited. Generally, there is no explicit limit on the WAN distance between the primary and secondary sites.\nHello there guys! As one of the most important Enterprise IT branches, storage has always been the center of technical discussions in the Telecom world. The way organizations store their data or how they manage their storage facilities have always represented interesting talking points among IT engineers, especially given the recent rise in the utilization of Cloud technologies. In this post we are going to touch base on how IT storage and the Cloud intertwine in order to provide us with one of the storage out there - Huawei FusionStorage. Let us discover more about Huawei Fusion Storage in the paragraphs to come! BACKGROUND INFORMATION As previously mentioned, Huawei FusionStorage combines cloud infrastructure and storage elements that generate an elastic, on-demand service. This translates into Huaweis most recent development in terms of intelligent, distributed storage. Huawei FusionStorage enables many essential features. Worth mentioning are: block, object, Big Data and file storage for upper-layer applications; easy to be scaled-out to support the enterprise; ultimate performance; bound with Huawei 64-bit ARMv8 Kunpeng series processors to increase IOPS and reduce power consumption. Huawei FusionStorage is ready for the industrial challenges of tomorrow, contributing towards the 'cloudification' of enterprises via a smart, robust intelligent storage service platform.\nHuawei storage solutions are widely used across the globe so what is the reason of its success? Here are few points which surely are some of the important aspects: Huawei's extensive storage portfolio guarantees that you have the tools you need for the job. Huawei storage covers everything from database and virtualization to big data, mobile, and cloud. Huawei OceanStor Storage has helped over 12,000 global customers maximize the value of their data with storage services, and has been named a leader in the Magic Quadrant for General-Purpose Storage Arrays for the past four years, confirming its position as one of the world's leading storage vendors. Through innovation initiatives and labs with customers, partners, research institutions, and renowned universities, Huawei continues to raise the bar in storage solutions. Huawei storage solutions can help your company innovate. Efficient Maximum number of discs and ports, as well as SPC-1 verified performance, are 30% greater than the industry standard; cache capacity is four times the standard. Smart - SmartQoS guarantees crucial service performance while doubling resource utilization. - Smart Tier: Accurate hotspot data and relocation, resulting in a threefold increase in performance. Smart Motion: Dynamic deployment adjustment based on scenarios, tripling disc utilization With a 60% reduction in O&M cost, Simple Deploying 100 LUNs in one minute, resulting in a tenfold increase in efficiency; all storage devices are managed by a single management system. Huawei storage has definitely secured a leading position in consumer world but there is lot more to come in future.\nHello everyone!!! I share with you this small guide to perform a Health Check on a Huawei Storage using the SmartKit tool. 1. Open theSmartKit tool. 2. In the STORAGE part, select the Health Check option located in the Routine Maintenance box. NOTE: It will probably ask you to do an UPGRADE which will take about 20 minutes to complete, if you need to have a good Internet connection. If the UPGRADE cannot be performed, the procedure can be continued without problems. 3. Start the Health Check . When selecting the \"DEVICE SELECTION\" option, it will ask you to add a device. When adding the device, it will ask for your credentials (username and password) if everything is correct, it will appear as in the following figure. NOTE: Add the path to save the inspection result where it says \"RESULT FOLDER\" . 4. Subsequently, the Health Check inspection process will start. To do this, you can select which characteristics you want it to analyze of the equipment, in this case all the options were selected. 5. End of the Health Check process . Since the Health Check has finished, , you must select the Open the result folder box , which will take you to the path where it was saved. NOTE: It is necessary to make sure that everything is well connected before performing the Health Check, especially the electrical cables, since that may be a cause that prevents the Health Check from being successful.\nHello, everyone! I know DR includes host-layer DR, network-layer DR, and array-layer DR. What are their advantages and disadvantages? Thanks in advance! Hello, friend! Have a nice day! Host layer (typical replication software such as Symantec VVR, Oracle DataGuard, DSG and Quest). Network layer (typically IBM SVC, EMC VPLEX, and Huawei VIS). Array layer (arrays that support mirroring or replication, such as Huawei OceanStor series). DR type Advantages Disadvantages Host layer This function is implemented on hosts, and the compatibility between underlying devices does not need to be considered. During database replication, the DR center can take over part of the work of the production center. Database replication can be implemented only for the corresponding database. Host-layer replication occupies certain host resources and affects the application system. Implemented on hosts, which is complex and usually requires system reconstruction. Network layer Broad compatibility with different back-end heterogeneous SAN storage resources. Simultaneous disaster recovery for multiple SAN arrays without a one-to-one relationship. Extendable disaster recovery platform. No extra investment required as the number of hosts and arrays increases. High initial investment because few vendors can provide such a solution. Array layer Data replication does not affect the host application system. When the production array is faulty, applications can be switched to the DR array in a short time. Data replication is implemented based on lower-layer arrays, and users are not charged based on host licenses. Does not support heterogeneous storage arrays. Storage arrays at the production center and the disaster recovery center must be from the same vendor.\nHello, everyone! key benefits of using SAN i.e,Infrastructure simplification concept. The key benefits that a SAN might bring to a highly data-dependent business infrastructure can be summarized into three concepts: Infrastructure simplification Information lifecycle management Business continuity These benefits are strong arguments for the adoption of SANs. The infrastructure simplificationconcepts is briefly described as follows. Four major methods exist by which infrastructure simplification can be achieved. An overview is provided for each of the major methods of infrastructure simplification: 1. Concentrating the systems and resources into locations with fewer, but more powerful, servers and storage pools can help increase IT efficiency and simplify the infrastructure. Additionally, centralized storage management tools can help improve scalability, availability, and disaster tolerance. 2. Storage virtualization helps to make complexity nearly transparent. At the same time, storage virtualization can offer a composite view of storage assets. This feature might help reduce capital and administrative costs, and it provides users with better service and availability. Virtualization is designed to help make the IT infrastructure more responsive, scalable, and available. 3. Choosing storage components with autonomic capabilities can improve availability and responsiveness, and can help protect data as storage needs grow. As soon as day-to-day tasks are automated, storage administrators might be able to spend more time on critical, higher-level tasks that are unique to the companys business mission. 4. Integrated storage environments simplify system management tasks and improve security.\nHello, everyone! The following describes the major types of storage devices that are available in the market. By being contained in a single box, a storage system (hard disk drive (HDD), solid-state drive (SSD), or Flash) typically has a central control unit that manages all of the I/O. This configuration simplifies the integration of the system with other devices, such as other disk systems or servers. Depending on the specific functionality that is offered by a particular storage system, you can make a storage system behave as a small, midsize, or enterprise solution. The decision about the type of storage system that is more suitable for a SAN implementation depends on the performance capacity and availability requirements for the particular SAN. Tape systems, similar to disk systems, are devices that consist of all of the necessary apparatus to manage the use of tapes for storage. In this case, however, the serial nature of a tape makes it impossible for them to be treated in parallel. This treatment is because Redundant Array of Independent Disks (RAID) devices are leading to a simpler architecture to manage and use. Three types of tape systems exist: Drives Autoloaders Libraries As with disk drives, tape drives are the means by which tapes can connect to other devices. They provide the physical and logical structure for reading from, and writing to tapes. Tape autoloaders are autonomous tape drives that can manage tapes and perform automatic backup operations. They are typically connected to high-throughput devices that require constant data backup.\nThe following uses SUSE Linux Enterprise Server 10 and BIND 9 as examples to describe how to interconnect with a BIND-based external DNS server in the Linux operating system. Prerequisites You have enabled InfoEqualizer and configured basic functions and dynamic front-end service IP addresses. Background Information When connecting to an external DNS server, you can select the forwarding or delegation mode as follows: If the customer provides an external DNS server and the dynamic or static domain name of the OceanStor 9000 is not a subdomain of the customer's existing domain, the customer's DNS server is connected in forwarding mode. This section describes how to enable the DNS server to forward the InfoEqualizer domain name request from the client to the OceanStor 9000. Then the OceanStor 9000 returns the front-end service IP address of the node to the client. If the customer provides an external DNS server and a domain has been configured, and the dynamic or static domain name of the OceanStor 9000 is a subdomain of the customer's existing domain, the customer's DNS server needs to be connected in delegation mode. For example, the customer has configured the domain test.com, the dynamic domain name of the OceanStor 9000 must be example.d.test.com or the static domain name must be example.s.test.com. In this case, you need to connect to the external DNS server in delegation mode. Procedure Interconnection with an external DNS server in forwarding mode The parameters in the following operations are only examples. Set them based on the site requirements.\nInterconnection with an external DNS server in delegation mode The parameters in the following operations are only examples. Set them based on the site requirements. a. Log in to the DNS server as an administrator. b. Run the cat /path/named.conf command to check whether forwarders are configured in options in the configuration file. Yes: Confirm with the DNS maintenance administrator of the customer whether blocking forwarders in the zone affects the resolution of the original domain name in the zone. If not, go to c to shield the forwarders. If not, contact technical support. If no, go to d. c. Run the vim /path/named.conf command to edit the configuration file, press i to enter the editing mode, and enter the following content: Enter :wq and press Enter to save the settings and exit. zone \"test.com\" in { type master; file \"test.zone\"; forwarders{}; }; Parameter description: test.com: configured DNS domain name. test.zone: name of the configured DNS zone. /path: indicates the path of the named.conf configuration file. The path varies with the operating environment. Generally, the path is /etc. Shielding forwarders prevents domain name resolution failures caused by information being forwarded to other DNS servers instead of OceanStor 9000 DNS servers during domain name resolution. d. Run the vim /path/named.conf command to edit the configuration file, press i to enter the editing mode, and enter the following content: Enter :wq and press Enter to save the settings and exit. Parameter description: test.com: configured DNS domain name. test.zone: name of the configured DNS zone.\nWhen a remote device whose type is Replication is added, or the iSCSI link is added to the remote device, if error code 1073804034 occurs or the connection fails to be created, the cause may be that the storage system uses the TOE interface module to connect. When the local storage system uses the TOE interface module to connect to the remote storage system, since protocols of the TOE and non-TOE interface modules process data in different ways, the two systems may have different connection methods, causing connection failure. If the remote storage system uses the TOE interface module but the local storage system uses non-TOE interface module, the above problem does not occur. Perform the following operations to locate and handle the problem: On a network where the TOE interface module connects the non-TOE interface module, if the link fails to be created on the storage system where the TOE interface module resides, and a communication exception error is returned, then the problem is what has been described above. If you need to further confirm the problem, contact technical support engineers to the system log. After the confirmation, handle the problem with any of the following methods. Method 1: Select the non-TOE interface module as the end for starting the connection. The TOE interface module is the receiving end of the connection. Establish the connection oppositely. This method helps establish the link.\n[Symptom Description] The OceanStor 5600 V3 takes over other storage devices using the heterogeneous feature and creates a volume mirror on the heterogeneous LUN. The customer reported that an alarm was generated on the OceanStor V3 storage system, indicating that the volume mirror copy was disconnected abnormally and some directories in the cluster were inaccessible. [Alarm Information] 2016-04-24 01:41:00 0xF02470010 Critical None The mirror copy (mirror LUN ID 85, mirror copy ID 130) is abnormally interrupted. 2016-04-23 23:42:41 0xF02470010 Critical 2016-04-24 10:56:06 The mirror copy (mirror LUN ID 98, mirror copy ID 157) is abnormally interrupted. [Cause Description] External LUNs have been mapped to clusters before being mapped to V3 storage. When the LUNs are mapped to V3 storage, the cluster reservation information of the external LUNs is not cleared. As a result, when the V3 storage array delivers write I/Os to the external LUNs, a reservation conflict error is returned, as a result, the volume mirror copy is disconnected unexpectedly and some directories in the cluster cannot be accessed. [Diagnosis Method] According to the analysis of event records and array logs, the data volume of one copy is on the external storage, and the other is on the V3 storage. According to the message log, the mirror LUN whose ID is 85 is disconnected from the copy whose ID is 130 because an error is returned when the copy whose ID is 130 is written, but the volume mirror copy whose ID is 130 is an external LUN.\nSearch for the function printing (keyword: initLunObjForCreate) for creating a mirror copy and find that the logical ID (ldid) of the mirror copy with ID 130 is 641. Search for the keyword ldProcNewTmpDisk and find that the sid of the disk whose ID is 641 is 176. The following information about the external LUN is displayed in the log: The error code 0x00000018 indicates that cluster reservation information exists on the external LUN and a reservation conflict error is returned for write I/Os delivered by OceanStor V3 storage. Root cause: During service migration at the site, after LUNs of external storage are demapped from the cluster, the LUNs are directly mapped to V3 storage without checking whether there are residual cluster reservation information on the LUNs. As a result, subsequent write I/Os fail. [Solution] Perform the following steps to rectify the fault: 1. Clear the cluster reservation information of external LUNs. 2. Clear the fault page of OceanStor V3 storage. Step 1 Clear the cluster reservation information of the external LUN. Note: After the reserved external LUNs are cleared, if the heterogeneous takeover is canceled, the original cluster may fail to use the external LUNs. In this case, contact upper-layer cluster software engineers for assistance. Evaluate the risks before performing this operation. The method of clearing the reservation information varies according to storage versions. The following describes how to clear the reservation information of external LUNs on OceanStor V3 storage systems. (1) V300R002C10SPC200 and later versions A.\nThe disk space of the Linux root directory is insufficient. When the VM template is modified to increase the disk size or a new hard disk is inserted, the disk space in the system remains unchanged. The cause is that the disk is not formatted and no partition is added. After using VMware to increase the system disk size, you need to expand the root directory. 1. Run the df -h command to check the disk space. It is found that the capacity of /dev/mapper/vg_node003-lv_root to which the root directory node is mounted is only 7.1 GB. 2. Increase the disk space. For example, use VMware vCenter to add a system disk. 3. Run the fdisk -l command to check the disk information. If the first line Disk /dev/sda: 53.7 GB is different from the actual output of df -h, the disk is added successfully. 4. Run the fdisk /dev/sda command to create a partition. Note: Disks are named in different formats, for example, /dev/vda. For details, see the first line of the fdisk -l command in step 3. If Disk /dev/sda: 53.7 GB, 53687091200 bytes is displayed in the first line, run the fdisk /dev/sda command. Perform the following operations to partition the disk: Command (enter m for help): Command Operation Command (enter m for help): Command (enter m for help): t #Change the partition format. Partition ID (1-5): 4 #Change the partition ID.\nCause Analysis (1) When a heterogeneous LUN is connected, I/Os are sent to the remote end to query reservation information. The I/O fails to be read. (2) If the remote reservation information fails to be read, the registration reservation failure event is reported to the upper-layer ADM module. (3) After receiving the registration reservation failure event, ADM reports an alarm indicating that the path from the local storage array to the heterogeneous storage array fails to be registered. (4) At 23:56:39 on July 16, 2020, LUNs were scanned again. (5) After LUNs are scanned again, I/Os are sent to the remote end to query reservation information. The query is successful. (6) Although the local storage array successfully registers the path to the heterogeneous storage array after LUNs are scanned again, the alarm suppression is not cleared because the recovery event of the successful path registration is not sent to the BDM_ADM module. Root Cause 1. When a heterogeneous LUN is connected, I/Os are sent to the remote end to query reservation information. The I/O fails and the remote reservation information fails to be read. According to the recurrence test in the lab, there is no risk of the query failure when the NETAPP FAS8020 is in the NO_RESERVE disk mode on the live network. 2. When the NETAPP FAS8020 has no reservation, an I/O failure message is returned when the reservation information is queried (the I/O should be successful). After the LUN scanning operation is triggered again, the reservation information I/O success message is returned.\nRun the vxlicrep command to check whether the license is activated and install the license vxlicinst. 2. Check whether the VxVM status is normal (vxdctl mode). 3. Scan the disks mounted to the system. 4. Displaying Disk Information: 5. Initialize the disk: 6. Run the command to check whether the disk is initialized successfully. 7. Creating a DG 8. Adding Disk to the DG 9. Create LVs. 10. Run the command to check the DG information. 11. Export DG information: 12. Import DG information: 13. Restoring the in the DG 14. Run the command to activate the lvs in the DG. 15. Modifying the LV Attribute: 16. Run the command to view the DG information. 17. Run the command to check the available space of the DG. 18. Delete the DG 19. Creating the Active RVG 20. Adding a slave RVG 21. Creating a SRL 22. Querying SRL Information: 23. Adding an DCM 24. Deleting the LV 25. Deleting an DCM 26. Deleting an RVG 27. Rename diskname 28. Rename DG 29. Renaming the LV 30. Expand the logical volume. 31. Shrinking the LV 32. Checking the RLink Status: 33. Run the command to check the RVG status. 33. Synchronizing Data Between the PM and RM: 34. Enabling the FastResync Function of a Volume: 35. Create the Snapshot 36. Checking Whether the Snapshot of the LV Is Successfully Created: 37. Change the communication protocol used by the RLink to TCP 38. Checking the Read/Write Status of Volumes and Disks: 39.\nData, algorithms, scenarios all are continuously interacting, in turn integrating more closely and driving new developments in Artificial Intelligence (AI). Simulating human behaviors and outcomes, AI is now capable of rapid processing andself-learning. With the development of the technology, AI applications have not only penetrated the financial services industry they have matured.In doing so, they are proving to be disruptive forces across banking and insurance, as well as capital markets. For example, surveys suggest that approximately 23% of jobs in the financial sector in China where AI financial practices are active will be replaced by AI. Furthermore, of the remaining 77% of jobs, working hours will reduce by approximately 27% due to the influence of AI. That figure is equivalent to an astonishing 38% increase in efficiency. 1. Enhanced processing efficiency and user experience Perception technologies such as computer vision, voice recognition, and Natural Language Processing (NLP) are maturing and being applied to financial service processes. As a result, service processing automation in the industry has reached new levels, in turn improving customer experiences. Typical examples of AI-powered automation include chatbots, automatic identity authentication, and intelligent bill entry via Optical Character Recognition (OCR). A chatbot follows the standard path of the customer journey, analyzes conversations, and understands the intentions of these conversations using machine learning algorithms. When encountering difficulties, the chatbot sends any issues to live personnel to resolve, learning from the manual replies that are sent back in return all improving the quality of customer service as well as reducing costs.\nAutomatic identity authentication verifies a customer's identity by analyzing voice, eyes, or facial features. Compared with the traditional approaches of security questions and passwords, this solution delivers a higher verification efficiency and a superb user experience busy users are not required to remember trivial details. An intelligent bill entry solution greatly improves error control and entry efficiency, compared with manual record keeping. 2. More accurate intelligent analysis and decision-making Advancing data analysis and deep learning technologies greatly improve the accuracy of intelligent analysis and decision-making, creating or improving the business value of financial products and services. In the past, business intelligence and traditional analysis only covered trend analysis, cause analysis, data mining, and prediction. Now, AI can enhance the relevance and specificity of suggestions through continuous learning and improvement, enabling personalized analysis. Targeted analysis and decision-making are available in the fields of risk management, marketing, and services. For instance, AI offers credit ratings based on social networking, optimizing existing rating mechanisms and becoming capable of generating scores, even for people without credit records. AI also supports personalized marketing based on individual customer preferences and specific product DNA, recommending only the most suitable products. In addition, AI is capable of dynamic fraud detection, detecting frauds in real-time from complex transaction modes. Customer Service and Marketing In their marketing efforts, it is vital for financial enterprises to precisely identify real customer needs. AI is a perfect fit for this task, using user profiles and big data models to enable precision marketing.\nChatbots can also discover potential customer needs, improving sales conversion rates, customer service efficiency, and the overall user experience all while significantly lowering labor costs. Investment Decision-Making Investment institutions and investment banking departments spend most of their time on trivial tasks, from data collection and analysis to report writing. AI systems offer unique advantages in these areas, since they are capable of processing massive volumes of data at speed. The technology is able to use NLP to uncover the hidden patterns governing market changes for example, which companies' share prices will be affected by the launch of HarmonyOS, Huaweis latest Operating System (OS). Robo-Advisor An AI-powered robo-advisor considers the unique risk preferences and financial status of investors, harnesses big data and quantitative models (in particular, portfolio theory), then provides each investor with individualized asset allocation strategies and wealth management services centering on index funds. The solution also supports position tracking and dynamic adjustment, as the market changes. Investment advice generated by AI combines investor preferences with Modern Portfolio Theory (MPT), offering transparent information with low commission fees. The solution makes private banking services both accessible and intelligent, allowing ordinary investors to enjoy a level of service once reserved for the high-end. Intelligent Risk Control Intelligent risk control, such as transaction fraud prevention, is also empowered by AI. By learning user behavior patterns and analyzing fraud blacklists, the anti-fraud model can identify any abnormal behavior of special users during loan application and approval processes, effectively detecting fraud.\nIn addition, AI systems are able to obtain knowledge and learn rules from massive amounts of transaction data, to detect and block exceptions such as malicious cash-out, card theft, spam registration, other illicit activities (in marketing), and fake transactions. A secure and reliable financial environment emerges as a result. Know Your Customer (KYC) is the foundation of all financial services. Voice, fingerprint, facial, and iris recognition technologies drastically reduce both the time to recognition as well as error rates. Moreover, AI systems can draw 360-degree customer profiles using massive volumes of historical transaction data, allied to collaborating external data, helping to improve business risk control capabilities. As a leading ICT technology provider, Huawei believes that sufficient data, powerful computing power, and quality algorithms lie at the foundation of AI. Based on these building blocks, Huawei is launching a full-stack, all-scenario AI platform solution dedicated to the financial services sector. Together with AI applications, this solution will significantly improve financial transaction service experiences, transaction efficiency, and risk prevention capabilities. Three Ways Huawei Helps Financial Institutions Develop AI Service Capabilities 1. Huawei provides Ascend series AI chips with extremely high computing power (up to 256 PFLOPS for a single core) to help financial enterprises build a powerful Atlas server computing platform (up to 1024-core cluster networking). What's more, the platform also comes with a chipset operator library and a highly automated operator development toolkit, to optimize the AI inference framework and computing capabilities, using chip core technologies. The platform is compatible with major inference platforms in the industry.\n2. Adhering to the openness principle, Huawei helps financial enterprises build FusionInsight ModelArts, an AI platform based on MindSpore (an efficient deep learning-based inference framework) and ModelArts (an AI application development platform). ModelArts contains universal inference capability development with an invoking interface, enhanced inference capability development with an invoking interface, and a pre-integrated AI inference capability set. Thanks to these modules, the platform maximizes AI application development efficiency and simplifies development processes. Huawei ModelArts is also fully compatible with other inference frameworks in the industry. 3. Data plays a vital role in smart finance. To support AI's data management, storage, and processing, Huawei provides the FusionInsight HD big data platform and the GaussDB200 distributed data warehouse platform. In addition, Huawei works with ecosystem partners to provide AI modules for financial services based on Huawei-developed AI applications. The regulation-compliant modules include biometric recognition capability, bill recognition capability, financial semantic library, financial semantic emotion knowledge base, financial transaction risk model library, and financial credit rating model library modules. Huawei provides full-stack AI solutions for large financial enterprises, helping them build a controllable AI platform and develop differentiated financial services and product capabilities. Using Huawei's solutions, banks can build a a next -generation, intelligent risk control platform to monitor transactions in real time and block abnormal transactions: one Chinese Bank prevented 83% of risky transactions and avoided the loss of more than CNY 100 million (over USD 14 million) within half a year.\nSymptom The storage system is connected to the source storage HDS VSP through the SNS5384 switch for data migration. After LUNs are mapped from the HDS VSP to the 18500 V3, the LUNs mapped from the source storage HDS VSP cannot be detected after multiple full scans are performed on the 18500 V3, in addition, the initiator of the 18500 V3 on the HDS VSP is offline. Cause Analysis 1. Log in to the storage compatibility assistant website and check whether the operations related to heterogeneous takeover are the same as the recommended operations. 2. Log in to the SNS switch and check the connection status of the corresponding port. Check the optical power of the optical module on the port to check whether the physical connection is normal. 3. Log in to the SNS switch and check the zone/alias configuration of the switch. Check whether the zone/alias is correctly added and whether the zone is added to the activated configuration file. 4. On the HDS VSP, disable and enable the heterogeneous ports again. 5. Log in to the SNS switch, remove the zone configuration from the activation file, save the file, and activate the file again. Add the zone to the activation file and activate the corresponding file. After LUNs are scanned on the OceanStor 18500 V3, the LUNs of the source storage can be discovered. Root Cause HDS storage has high security restrictions. It does not connect to the host before LUN mapping. Therefore, the initiator is offline on the storage.\nWhat is the difference between a management port, a maintenance port, and a serial port on the OceanStor V3 device? All three ports are used for managing and maintaining storage systems. However: A management port is connected using a network cable to a network port on a maintenance terminal. A maintenance port can only be used by Huawei engineers in emergencies. In addition, it cannot be connected to the same network as a management port; otherwise, network reciprocating may occur, causing network storms. Therefore, do not connect a management port and a maintenance port to the same switch. A serial port is connected using a serial cable to its peer serial port on a maintenance terminal. By a serial port: After connecting a maintenance terminal to a controller enclosure, you can use a terminal-emulation program (such as PuTTY) to log in to the command-line interface (CLI) of a storage system. A pair of user name and password is required by the login. By a management port: 1. After connecting a maintenance terminal to a controller enclosure, you can use a terminal-emulation program (such as PuTTY) to log in to the CLI of a storage system. The IP address of the management port and a pair of user name and password are required by the login. 2. Alternatively, you can access the IP address of the management port from a browser to log in to DeviceManager. The IP address of the management port and a pair of user name and password are required by the login.\nHello all, The distance between the front mounting bar and the inner side of the front door varies according to the cabinet type. Because the system enclosure is fixed on the front mountingbars, a longer distance between the front mounting bars and the inner side of the front door can result in less cabling space at the rear of the device. In such a case, you can install U-shaped brackets to move the system enclosure forward for more cabling space. A U-shaped bracket is a mechanical part installed between the mounting ear of the enclosure and mounting bar of the cabinet to adjust the relative position between the enclosure and the cabinet. A maximum of two U-shaped brackets can be stacked. U-shaped brackets are classified into three types: 25 mm, 50 mm, and 75 mm. If U-shaped brackets are not installed, the system enclosure is fixed to the front mounting bars by using captive screws. The distance between the front panel of the system enclosure and the inner side of the front door is fixed. Install the U-shaped brackets on the front mountingbars, and then secure the system enclosure to U-shaped brackets of the respective size. This allows you to adjust the distance between the front panel of the system enclosure and the inner side of the front door within a certain range. 1. On thefront of the cabinet, align the bottom edge of the adjustable guide rail withthe U scale on the front mounting bar, and fully insert the front positioningpin into the square hole.\n1. Test the performance in the scenario where the storage is connected through iSCSI. If the storage is connected through switches, the bandwidth cannot reach the normal level of the storage, which may be caused by packet loss. Run the ifconfig command in the minisystem of the storage system to check whether packet loss occurs on the service port. If packet loss occurs, run the ifconfig command twice when services are running. The overunns value changes. To resolve packet loss during network transmission, enable flow control on the switch port. 2. Traffic control Traffic control is a technology that prevents packet loss due to network congestion. After flow control is configured on both ends of a link, if congestion occurs on the local device, the local device sends a message to instruct the peer device to temporarily reduce the rate at which packets are sent. After receiving the message, the peer device stops sending packets to the local device regardless of the working rate of the interface. This prevents congestion. On the network shown in Figure 1, SwitchA communicates with SwitchC through SwitchB. The flow control mechanism is as follows: 1. 10GE1/0/1 on SwitchA is connected to 10GE1/0/2 on SwitchB, and the auto-negotiation rate is 10000 Mbit/s. Data packets are transmitted between interfaces at a rate of 10000 Mbit/s. 2. The maximum transmission rate of GE1/0/1 on SwitchB is 1000 Mbit/s. When congestion occurs during packet forwarding, SwitchB buffers received packets.\nRAID (Redundant Array of Independent Disks) is a redundant array of independent disks. It is proposed by the University of California, Berkeley in 1987. The basic idea is to combine multiple independent physical hard disks into a virtual logical hard disk by using related algorithms, thereby providing a larger, higher, or higher data error tolerance function. Background of block virtualization and reconstruction of traditional RAID: The overall performance of the application system deteriorates, and the long reconstruction time and data loss risks increase. Traditional RAID is limited by the number of hard disks. When the data capacity increases sharply, enterprises cannot flexibly and flexibly allocate resources. Therefore, the underlying data distribution architecture must be highly flexible and scalable, with the increase of hard disk capacity, data management based on hard disks is becoming more and more difficult. Huawei provides RAID 2.0+ technical support by designing and developing traditional RAID, LUN virtualization, and block virtualization. The following figure shows the principle of Huawei RAID2.0+ The following figure shows the logical objects of Huawei RAID2.0+ software 1. Disk Domain is a combination of disks (which can be all disks in the system). After the disks are combined and reserved for hot spare capacity, the provides storage resources for the storage pool. 1) and HVS series storage systems can have one or more disk domains. 2). Multiple storage pools (Storage Pool) can be created in a disk domain. 3). Disks in a disk domain can be SSDs, SAS disks, or NL-SAS disks. 4).\nDifferent disk domains are isolated from each other, including fault domains, performance, and storage resources. 2. Storage pool is a storage container. The storage space used by all application servers comes from the storage pool. A storage pool is created based on a specified disk domain. The Chunk (CK) resources can be dynamically allocated from the disk domain, and the Chunk Group (CKG) is composed of each storage level (Tier). RAID protected storage resources. Tier is a storage tier, a collection of similar storage media in a storage pool, used to manage storage media of different performances to provide different storage space for applications with different performance requirements. The storage pool can be divided into multiple Tiers depending on the type of hard disk. 1). Creating a storage pool specifies the storage tier (Tier) type that the storage pool divides from the disk domain and the RAID policy and capacity for that type. 2). HVS series storage systems support RAID5, RAID6 and RAID10. 3). The capacity layer is composed of a large-capacity NL-SAS disk. The RAID policy recommends using a double-check mode RAID6. 3. Disk Group (DG) is a set of disks of the same type in a disk domain. The disk type can be SSD, SAS, or NL-SAS. The HVS series automatically divides one or more Disk Group (DG) in each disk domain based on the number of disks of each type. 1). One Disk Group (DG) contains only one type of hard disk. 2).\nMultiple CKs of any CKG come from different hard disks of the same Disk Group (DG). 4. Logical Drive (LD) is a logical disk that is managed by the HVS series storage system. It corresponds to a physical disk. 5. CK is short for CK. It divides the hard disk space in a storage pool into several fixed physical spaces. The size of each physical space is 64MB, which is the basic unit of the RAID. 6. The Chunk Group (CKG) is a logical storage unit that consists of CKs from different disks in the same DG based on the RAID algorithm. It is the minimum unit for allocating resources from a disk domain to a storage pool. 1). CKs in a CKG come from disks in the same DG. 2). CKG has RAID properties (the RAID properties are configured on the tier). 3). CK and CKG are internal objects of the system and are automatically configured by the HVS storage system. 7. Extents are logical storage spaces of fixed sizes based on CKGs. The size of extents is adjustable. It is the minimum unit (data migration granularity) for hotspot data statistics and migration, and is the minimum unit for applying for space and releasing space in a storage pool. 1). An extent belongs to one volume or LUN. 2). The size of and extent can be set when a storage pool is created. The size cannot be changed after being created. 3).\nThe extents of different storage pools can be different, but the extents of the same storage pool are of the same size. 8. In thin LUN mode, extents are further divided into fine-grained blocks based on the fixed size. These blocks are called Grain. Thin LUNs are allocated based on Grain. LBAs in Grain are consecutive. Thin LUNs are mapped to LUNs in the unit of Grain. For Thick LUN, this object does not exist. 9. A volume is an internal management object of the system. A volume object is used to organize all extents and Grain logical storage units of the same LUN. It can dynamically apply for and release extents to increase or decrease the actual space occupied by the volume. A LUN is a storage unit that can be directly mapped to a host for reading and writing. A LUN is a volume object. Automatic load balancing reduces the overall failure rate. Quick thin reconstruction, improving the failure rate of dual disks; Quick thin reconstruction, improving the failure rate of dual disks; Fault self-detection and self-healing ensure system reliability. Load sharing: RAID2.0+ enables hard disks to work more evenly, preventing hard disks from being overworked. Robust reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. Fast reconstruction: The RAID2.0+ greatly reduces the time window for reconstruction.\nIn this way, the system can be restored to the fault tolerance state as soon as possible, thereby improving system reliability. Simplified reconstruction: RAID2.0+ can detect the used space in the allocated space through metadata. Therefore, only used space is reconstructed during reconstruction, which reduces the amount of data to be reconstructed, shortens the reconstruction time, and reduces reconstruction risks. Self-detection and self-healing: The RAID2.0+ uses the distributed hot spare space. When the system detects a fault, the system automatically starts reconstruction as long as the disk has free space (CK). This improves reliability and reduces management costs. Invalid data volume: If a traditional RAID group fails, all data in the RAID group is affected. When multiple disks fail on the RAID2.0+, only the data associated with the failed disks becomes invalid. Most data can still be accessed. The amount of invalid data is reduced by order of magnitude compared with the traditional RAID. RAID technology is the basis of storage data protection. It is still the fault tolerance capability of RAID. For RAID 5, the number of faults that can be tolerated is 1. For traditional RAID, the unit is hard disk. For RAID2.0+, the unit is blocked. For RAID 6, the number of faults that can be tolerated is 2. Therefore, if a dual-parity protection type, such as RAID 6, is used, both traditional RAID and block-based RAID2.0+ will not be lost when a dual-disk failure occurs.\ni have OceanStor9000 (3 Node Added) i need one more node delete Can we delete without losing data? How ? Hi dear Abdussamed, Prerequisites: Before deleting a node, ensure that at least four functioning nodes exist in the node pool; otherwise, system reliability deteriorates. You can delete only one node at a time to ensure data security. Ensure that data balancing policy is enabled. You can delete a node from a cluster using Toolkit or command-line interface (CLI) commands. I recommend you delete a node using Toolkit. 1. Log in to Toolkit as the super administrator. 2. Click Node Deletion . In the Node Deletion window that is displayed, find the row containing the node that you want to delete based on the node serial number. In the row, click Delete . Carefully read the content in the Danger dialog box and select I have read and understood the consequences associated with performing this operation. Then click OK to delete the node. If the node is offline, the system deletes it directly. After that, data distribution is balanced in the background. If the node is online, the system migrates the data from the node to other nodes in the background before deleting the node. Estimate the time required for the deletion based on the to-be-migrated data amount. In Task Management in the lower right corner on DeviceManager, view the Execution Result of the deletion task. If the execution result is Succeeded , the node has been deleted.\nHello, everyone! key benefits of using SAN i.e, concept. The key benefits that a SAN might bring to a highly data-dependent business infrastructure can be summarized into three concepts: Infrastructure simplification Information lifecycle management Business continuity These benefits are strong arguments for the adoption of SANs. The i concepts is briefly described as follows. Information lifecycle management Information is an increasingly valuable asset, but as the amount of information grows, it becomes increasingly costly and complex to store and manage it. Information lifecycle management (ILM) is a process for managing information through its lifecycle, from conception until intentional disposal. The ILM process manages this information in a manner that optimizes storage and maintains a high level of access at the lowest cost. A SAN implementation makes it easier to manage the information lifecycle because it integrates applications and data into a single-view system, in which the information resides. This single-view location can be managed more efficiently. Business continuity The business climate in todays on-demand era is highly competitive. Clients, employees, suppliers, and IBM Business Partners expect to be able to tap into their information at any hour of the day, from any corner of the globe. Continuous business operations are no longer optional; they are a business imperative to becoming successful and maintaining a competitive advantage. Businesses must also be increasingly sensitive to issues of client privacy and data security so that vital information assets are not compromised.\nHello, everyone! Distributed clients and servers are frequently chosen to meet specific application needs. They might, therefore, run different operating systems, such as Windows Server, various UNIX offerings, VMware vSphere, or VMS. They might also run different database software, for example, Oracle, or SQL Server. Therefore, they have different file systems and different data formats. The management of this multi-platform, multivendor, networked environment is increasingly complex and costly. Software tools for multiple vendors and appropriately skilled human resources must be maintained to handle data and storage resource management on the many different systems in the enterprise. Surveys that are published by industry analysts consistently show that management costs that are associated with distributed storage are much greater. The costs are shown to be much greater than the cost of managing consolidated or centralized storage. This comparison includes the costs of backup, recovery, space management, performance management, and disaster recovery planning. Disk storage is often purchased from the processor vendor as an integral feature. It is difficult to establish whether the price you pay per gigabyte (GB) is competitive, compared to the market price of disk storage. Disks and tape drives, which are directly attached to one client or server, cannot be used by other systems, leading to inefficient use of hardware resources. Organizations often discover that they need to purchase more storage capacity, even though free capacity is available in other platforms.\nAdditionally, it is difficult to scale capacity and performance to meet rapidly changing requirements, such as the explosive growth in server, application, and desktop virtualization. You also need to manage information over its entire lifecycle, from conception to intentional destruction. Information that is stored on one system cannot readily be made available to other users. One exception is to create duplicate copies and move the copy to the storage that is attached to another server. Movement of large files of data might result in significant degradation of performance of the LAN and wide area network (WAN), causing conflicts with mission-critical applications. Multiple copies of the same data might lead to inconsistencies between one copy and another copy. Data that is spread on multiple small systems is difficult to coordinate and share for enterprise-wide applications. Examples of enterprise-wide applications include Internet commerce, enterprise resource planning (ERP), data warehouse, and business intelligence (BI). Backup and recovery operations across a LAN might also cause serious disruption to normal application traffic. Even when you use fast Gigabit Ethernet transport, the sustained throughput from a single server to tape is about 25 GB per hour. It takes approximately 12 hours to fully back up a relatively moderate departmental database of 300 GBs. This time frame might exceed the available window of time in which the backup must be completed. And, it might not be a practical solution if business operations span multiple time zones.\nHello all, Deduplication is a data reduction technique that optimizes storage capacity. It eliminates redundant data by deleting duplicate data in the data set, leaving only one of them. Dedupe technology can effectively improve storage efficiency and utilization, and the data can be reduced to 1/20 to 1/50. This technology can greatly reduce the need for physical storage space, reduce network bandwidth during transmission, and effectively save equipment procurement and maintenance costs. At the same time, it is also a green storage technology that can effectively reduce energy consumption. Dedupe can be divided into file-level and data block-level according to the granularity of deduplication. The file-level dedupe technology, also known as Single Instance Store (SIS), is block-level deduplication with a smaller granularity and can reach between 4-24KB. Obviously, the data block level can provide a higher data deduplication rate, so the current mainstream dedupe products are block level. Dedupe divides the file into fixed-length or variable-length data blocks, and uses the Hash algorithm such as MD5/SHA1 to calculate the fingerprint (FP, Fingerprint) for the data block. Two or more hash algorithms can be used to calculate the data fingerprint at the same time to obtain a very small data collision probability. A block of data with the same fingerprint can be considered to be the same block of data, and only one copy is required in the storage system. Thus, a physical file corresponds to a logical representation in the storage system, consisting of a set of FP metadata.\nAt some time points where the I/O latency is abnormal, the IOPS of the LUN is 0 or the IOPS is a single digit. After the IOPS increases, the latency statistics are restored. This problem occurs only when the IOPS is extremely low but the I/O latency is high. The host latency analysis focuses on the average latency of a large number of IOPS in a period of time. I/O glitches do not need to be considered. Note: The IOPS is 0 because the number of I/Os in the statistical period (one minute by default) is very small, for example, one or two. As a result, the calculated IOPS in one minute is too small, and the statistical result is 0.If only a few I/Os have long latency, services will not be affected. A single I/O passes through links, switches, storage hardware components, protocol processing, and cache processing from the host HBA. Each phase may involve queuing. The time when the next module obtains an I/O depends on the current queue status. The I/O requests received by the storage system are not evenly distributed. If an I/O occurs in the previous internal task or other burst I/Os occupy thread or queue resources, the I/O processing delay increases due to queuing. The long I/O latency occurs occasionally in the processing of a few I/O requests. The I/O latency accounts for a small proportion in a certain number of I/O requests and does not affect upper-layer services.\nHello Dears Can you tell or share me how to install esight on bare 2288HV5 all the procedures step by step ? Thank you Dear Berakia, Before installing eSight, ensure that the installation plan has been configured and checked, including the network connectivity, host names, IP addresses, routes, time zones and time, user passwords, and installation paths of the active and standby servers. Perform the operations in this section only on the active server. Automatic synchronization will be performed on the standby server. Here is the procedure: 1. Log in to the active eSight server as the root user. 2. Run the following command to cancel the timeout interval: 3. Run the following commands to install eSight: 4. Select the installation language and set the installation scenario to remote HA as prompted. 5. Enter the system and heartbeat IP addresses of the active server as prompted. 6. Configure the southbound and northbound service isolation as prompted. 7. Enter the system and heartbeat IP addresses of the standby server as prompted. 8. Confirm the configuration. 9. Enter the root user password of the standby server as prompted. 10. If the following information is displayed, the installation is successful: 11. Delete the patch installation package and temporary files from the server after eSight is installed. More about Installing eSight: Thank you.\nHello, everyone! I would like to share with you an article about the need for Redundant Array of Independent Disks. For this basic example, we use the HDD as our building block. Data is stored on HDDs on which data can be read and written. Depending on the methods that are used to run those tasks, and the HDD technology on which the HDDs were built, the read and write function can be faster or slower. The evolution of HDDs is incredible. We can store hundreds of gigabytes on a single HDD, which allows us to keep all of the data we can ever imagine. Even though this approach seems to bring us only advantages so far, one question might be what happens if for any reason we are unable to access the HDD? The first solution might be to have a secondary HDD where we can manually copy our primary HDD to our secondary HDD. Immediately, we can see that our data is safe. But, how often must we run those manual copies if we expect not to lose data and to keep it as up-to-date as possible? To keep it as current as possible, every time we change something, we must make another copy. But, must we copy the entire amount of data from one HDD to the other, or must we copy only what changes? Fortunately, technology exists that can help us. That technology is the concept, which presents a possible solution to our problem.\nHello, everyone! I would like to share with you an article about RAID using disk pools. When a logical storage volume needs to be provisioned to servers, first the storage RAID needs to be created. To create the RAID, select the available HDDs and group them together for a single purpose. The number of grouped HDDs depends on the RAID type that we choose and the space that is required for provisioning. To understand what is meant, a basic example is shown that uses the following assumptions: Ten HDDs, which are named A, B, C, D, E, F, G, H, I, and J, are included. Two RAID controllers, which are named RC1 and RC2, support any RAID level. Each RAID controller can manage any HDD. Each RAID controller can act as a backup of the other controller, at any time. The following tasks can be performed: Select HDDs A, B, and F, and create a RAID 5 array that is managed by RC1. We call it G1. Select HDDs E, I, and J, and create a RAID 5 array that is managed by RC2. We call it G2. Disk pool creation By issuing these simple steps, we create disk pools. These pools consist of grouping disks together for a single purpose, such as creating a RAID level, in our case, RAID 5. Nested (hybrid) RAIDs solutions, such as 5+0 are used when the amount of storage data is significant and important for business continuity. RAID 50 consists of RAID 0 striping across lower-level RAID 5 arrays.\nHello, everyone! I would like to share with you a brief introduction about storage area networks (SAN). The defines the as a network whose primary purpose is the transfer of data between computer systems and storage elements. A SAN consists of a communication infrastructure, which provides physical connections. It also includes a management layer, which organizes the connections, storage elements, and computer systems so that data transfer is secure and robust. The term SAN is typically (but not necessarily) identified with block I/O services rather than file access services. In simple terms, a SAN is a specialized, high-speed network that attaches servers and storage devices. The SAN is sometimes referred to as the network behind the servers. A SAN allows an any-to-any connection across the network, by using interconnect elements, such as switches and directors. The SAN eliminates the traditional dedicated connection between a server and storage, and the concept that the server effectively owns and manages the storage devices. The SAN also eliminates any restriction to the amount of data that a server can access. Traditionally, a server is limited by the number of storage devices that attach to the individual server. Instead, a SAN introduces the flexibility of networking to enable one server or many heterogeneous servers to share a common storage utility. A network might include many storage devices, including disk, tape, and optical storage. Additionally, the storage utility might be located far from the servers that it uses. The SAN can be viewed as an extension to the storage bus concept.\nThis concept enables storage devices and servers to interconnect by using similar elements, such as LANs and wide area networks (WANs). SANs create new methods of attaching storage to servers. These new methods can enable great improvements in both availability and performance. The SANs of today are used to connect shared storage arrays and tape libraries to multiple servers, and they are used by clustered servers for failover. A SAN can be used to bypass traditional network bottlenecks. A SAN facilitates direct, high-speed data transfers between servers and storage devices, potentially in any of the following three ways: This method is the traditional model of interaction with storage devices. The advantage is that the same storage device might be accessed serially or concurrently by multiple servers. A SAN might be used for high-speed, high-volume communications between servers. This outboard data movement capability enables data to be moved without server intervention, therefore freeing up server processor cycles for other activities, such as application processing. Examples include a disk device that backs up its data to a tape device without server intervention, or a remote device mirroring across the SAN. SANs allow applications that move data to perform better, for example, by sending data directly from the source device to the target device with minimal server intervention. SANs also enable new network architectures where multiple hosts access multiple storage devices that connect to the same network.\nThe use of a SAN can potentially offer the following benefits: : Storage is independent of applications and accessible through multiple data paths for better reliability, availability, and serviceability. : Storage processing is offloaded from servers and moved onto a separate network. : Simpler management, scalability, flexibility, and availability are possible. : A remote copy of data is enabled for disaster protection and against malicious attacks. : A single image of storage media simplifies management. Fibre Channel (FC) is the predominant architecture on which most SAN implementations are built. Storage area network connectivity The first element to consider in any SAN implementation is the connectivity of the storage and server components, which typically use FC. SANs, such as LANs, interconnect the storage interfaces together into many network configurations and across longer distances. Much of the terminology that is used for SAN has its origins in Internet Protocol (IP) network terminology. Storage area network storage The SAN liberates the storage device so that the storage device is not on a particular server bus and attaches it directly to the network. Storage is externalized. It can be functionally distributed across the organization. The SAN also enables the centralization of storage devices and the clustering of servers, which potentially can help you achieve easier and less expensive centralized administration that lowers the total cost of ownership (TCO). The storage infrastructure is the foundation on which information relies. Therefore, the storage infrastructure must support the business objectives and business model of a company.\nIn this mode, an independent physical server or VM is used as the quorum server. You are advised to deploy the quorum server at a dedicated quorum site that is in a different fault domain from the two DCs. In  the event of a DC failure or disconnection between the storage systems,  each storage system sends an arbitration request to the quorum server, and only the winner continues providing services. The preferred site takes precedence in arbitration. The following example uses DC A as the preferred site and DC B as the non-preferred site. describes the arbitration mechanism in quorum server mode. In  the following table, if two or more faults occur in the system, the interval between the faults is less than or equal to 60 seconds. Table 1 Fault Diagram Fault Description Arbitration Result The quorum server malfunctions. The HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. HyperMetro automatically switches to static priority mode. The link between one storage system (for example, storage system in DC A) and the quorum server is down. The HyperMetro pair is in the Normal state. The LUNs in both DCs A and B continue providing services. A storage system (for example, storage system in DC A) malfunctions. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services.\nThe link between one storage system (for example, storage system in DC A) and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The link between two storage systems is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. A storage system (for example, storage system in DC A) and the quorum server malfunction. The HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services. The  link between the two storage systems and the link between a storage system (for example, storage system in DC A) and the quorum server are down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A stops while the LUN in DC B continues providing services. A  storage system malfunctions (for example, storage system in DC A) and the link between the other storage system and the quorum server is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A is inaccessible while the LUN in DC B stops. You must forcibly start the HyperMetro pair to enable the LUN in DC B to provide services.\nThe  quorum server malfunctions and the link between the host and one storage system (for example, storage system in DC A) is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. HyperMetro automatically switches to static priority mode. The  link between a storage system (for example, storage system in DC A) and  the quorum server is down and the link between the storage system and the host is down. The HyperMetro pair is in the Normal state. The LUN in DC B provides services. The  link between the storage systems is down and the link between the non-preferred site (for example, DC B) and the host is down. The HyperMetro pair is in the To be synchronized state. The LUN in DC A continues providing services while the LUN in DC B stops. The  link between the storage systems is down. Then the links between a storage system (for example, storage system in DC A) and the quorum server as well as the host are down. The HyperMetro pair is in the To be synchronized state. The HyperMetro services are interrupted. To avoid misoperation or data loss, contact Huawei technical support for service recovery. One  storage system (for example, storage system in DC A) malfunctions and the links between the storage system and the host as well as between the  other storage system and the quorum server are down. The HyperMetro pair is in the To be synchronized state.\nWhat is smart QoS and what are its key features? Diverse  services result in a sharp increase in network traffic, which may cause  network congestion, increase forwarding delay, or even cause packet loss. Any of these situations will cause service quality deterioration or even service interruption. Therefore, real-time services require a solution to prevent network congestion. The best solution is to increase  network bandwidth, but increasing network bandwidth is costly. The most  cost-effective way is to use a \"guarantee\" policy to manage traffic congestion. QoS  guarantees end-to-end service quality based on the requirements of different services. It helps improve utilization of network resources and allows different types of traffic to preempt network resources based  on their priorities; for example, voice, video, and important data applications can be processed preferentially on network devices. The  factors that affect the network service quality need to be learned to improve network quality. Traditionally, factors that affect network quality include link bandwidth, packet transmission delay, jitter, and packet loss rate. To improve the network service quality, ensure the bandwidth of transmission links, and reduce packet transmission delay, jitter, and packet loss rate. These factors that affect the network service quality become QoS indicators. Bandwidth The  bandwidth, also called throughput, refers to the maximum number of transmitted data bits between two ends within a specified period (1 second) or the average rate at which specified data flows are transmitted between two network nodes. Bandwidth is expressed in bit/s.\nGenerally,  data transmission capability and network service quality are accompanied by the bandwidth. In other words, a lane is positive to the traffic flow capacity with low traffic jam in a highway. Network users all expect higher bandwidth; however, the O&M costs are higher. Therefore, bandwidth becomes a serious bottleneck as the Internet develops rapidly and services become increasingly diversified. Delay The  delay refers to the time required to transmit a packet or a group of packets from the transmit end to the receive end. It consists of the transmission delay and processing delay. Voice  transmission is used as an example. A delay refers to the period during  which words are spoken and then heard. Generally, people are insensitive to a delay of less than 100 ms. If a delay ranging from 100 ms to 300 ms occurs, a speaker can sense slight pauses in the responder's reply, which can seem annoying to both. If a delay longer than 300 ms occurs, both the speaker and responder obviously sense the delay and have to wait for responses. If the speaker cannot wait but repeats what has been said, voices overlap and the quality of the conversation deteriorates severely. Jitter If  network congestion occurs, the delays of packets over the same connection are different. The jitter is used to describe the degree of delay change, that is, the time difference between the maximum delay and  the minimum delay.\nJitter  is an important parameter for real-time transmission, especially for real-time services, such as voice and video, which are zero-tolerant of jitters because jitters will cause voice or video interruptions. Jitters  also affect protocol packet transmission. Specific protocol packets are  transmitted at a fixed interval. High jitters may cause flapping of the  protocols. Jitters  exist on networks but the service quality will not be affected if jitters do not exceed a specific tolerance. The buffer can alleviate excess jitters but prolongs delays. Packet Loss Rate The  packet loss rate refers to the ratio of lost packets to total packets. Slight packet loss does not affect services. For example, users are unaware of the loss of a bit or a packet in voice transmission. The loss  of a bit or a packet in video transmission may cause the image on the screen to become garbled instantly, but the image can be restored quickly. TCP  is used to transmit data to handle slight packet loss because TCP instantly retransmits the packets that have been lost. If severe packet loss does occur, the packet transmission efficiency is affected. QoS focuses on the packet loss rate. The network packet loss rate must be controlled within a certain range during transmission. How  are QoS indicators defined within proper ranges to improve network service quality? The QoS model is involved. The QoS model is not a specific function, but an E2E QoS scheme.\nFor example, intermediate devices may be deployed between two connected hosts. E2E service quality  guarantee can be implemented only when all devices on a network use the  same QoS service model. International organizations such as the IETF and ITU-T designed QoS models for their concerned services. The following describes three main QoS service models. Best-Effort Best-Effort  is the default service model for the Internet and applies to various network applications, such as the File Transfer Protocol (FTP) and email. It is the simplest service model, in which an application can send any number of packets at any time without notifying the network. The network then tries its best to transmit the packets but provides no guarantee of performance in terms of delay and reliability. The Best-Effort model is suitable for services that have low requirements for delay and packet loss rate. Integrated Service (IntServ) In  the IntServ model, an application uses a signaling protocol to notify the network of its traffic parameters and apply for a specific level of QoS before sending packets. The network reserves resources for the application based on the traffic parameters. After the application receives an acknowledgement message and confirms that sufficient resources have been reserved, it starts to send packets within the range  specified by the traffic parameters. The network maintains a state for each packet flow and performs QoS behaviors based on this state to guarantee application performance. The  IntServ model uses the Resource Reservation Protocol (RSVP) for signaling.\nThe RSVP protocol reserves resources such as bandwidth and priority on a known path, and each network element along the path must reserve required resources for data flows requiring QoS guarantee. That is, each network element maintains a soft state for each data flow. A soft state is a temporary state that is periodically updated through RSVP messages. Each network element checks whether sufficient resources can be reserved based on these RSVP messages. The path is available only  if all involved network elements can provide sufficient resources. Differentiated Service (DiffServ) The  DiffServ model classifies packets on a network into multiple classes and takes different actions for each class. When network congestion occurs, packets of different classes are processed based on their priorities, resulting in different packet loss rates, delay, and jitter.  Packets of the same class are aggregated and sent as a whole to ensure consistent delay, jitter, and packet loss rate. Unlike  the IntServ model, the DiffServ model does not require a signaling protocol. In this model, an application does not need to apply for network resources before sending packets. Instead, the application sets QoS parameters in the packets, through which the network can learn the QoS requirements of the application. The network provides differentiated  services based on the QoS parameters of each data flow and does not need to maintain a state for each data flow. DiffServ takes full advantage of IP networks' flexibility and extensibility and transforms information in packets into per-hop behaviors (PHBs), greatly reducing signaling operations.\nDiffServ is the most commonly used QoS model on current networks. QoS implementation described in the subsequent sections is based on this model. QoS  services based on the DiffServ model are supported on Huawei data communications products, including switches, routers, WLAN products, and  firewalls. The DiffServ model involves the following QoS mechanisms: Traffic classification and marking Traffic  classification and marking are prerequisites for differentiated services. Traffic classification divides packets into different classes or sets different priorities, and can be implemented using traffic classifiers configured on the Modular QoS Command-Line Interface (MQC). Traffic marking sets different priorities for packets and can be implemented through priority mapping and re-marking. Traffic policing, traffic shaping, and interface-based rate limiting Traffic  policing and traffic shaping control the traffic rate within a bandwidth limit. Traffic policing drops excess traffic when the traffic rate exceeds the limit, whereas traffic shaping buffers excess traffic. Traffic policing and traffic shaping can be performed on an interface to  implement interface-based rate limiting. Congestion management and congestion avoidance Congestion  management buffers packets in queues upon network congestion and uses a  scheduling algorithm to determine the forwarding order. Congestion avoidance monitors network resource usage and drops packets to mitigate network overload if congestion worsens. Traffic  classification and marking are the basis of differentiated services. Traffic policing, traffic shaping, interface-based rate limiting, congestion management, and congestion avoidance control network traffic and resource allocation to implement differentiated services.\nDiverse  services result in a sharp increase in network traffic, which may cause  network congestion, increase forwarding delay, or even cause packet loss. Any of these situations will cause service quality deterioration or even service interruption. Therefore, real-time services require a solution to prevent network congestion. The best solution is to increase  network bandwidth, but increasing network bandwidth is costly. The most  cost-effective way is to use a \"guarantee\" policy to manage traffic congestion. QoS  guarantees end-to-end service quality based on the requirements of different services. It helps improve utilization of network resources and allows different types of traffic to preempt network resources based  on their priorities; for example, voice, video, and important data applications can be processed preferentially on network devices. The  factors that affect the network service quality need to be learned to improve network quality. Traditionally, factors that affect network quality include link bandwidth, packet transmission delay, jitter, and packet loss rate. To improve the network service quality, ensure the bandwidth of transmission links, and reduce packet transmission delay, jitter, and packet loss rate. These factors that affect the network service quality become QoS indicators. Bandwidth The  bandwidth, also called throughput, refers to the maximum number of transmitted data bits between two ends within a specified period (1 second) or the average rate at which specified data flows are transmitted between two network nodes. Bandwidth is expressed in bit/s. Generally,  data transmission capability and network service quality are accompanied by the bandwidth.\nIn other words, a lane is positive to the traffic flow capacity with low traffic jam in a highway. Network users all expect higher bandwidth; however, the O&M costs are higher. Therefore, bandwidth becomes a serious bottleneck as the Internet develops rapidly and services become increasingly diversified. Delay The  delay refers to the time required to transmit a packet or a group of packets from the transmit end to the receive end. It consists of the transmission delay and processing delay. Voice  transmission is used as an example. A delay refers to the period during  which words are spoken and then heard. Generally, people are insensitive to a delay of less than 100 ms. If a delay ranging from 100 ms to 300 ms occurs, a speaker can sense slight pauses in the responder's reply, which can seem annoying to both. If a delay longer than 300 ms occurs, both the speaker and responder obviously sense the delay and have to wait for responses. If the speaker cannot wait but repeats what has been said, voices overlap and the quality of the conversation deteriorates severely. Jitter If  network congestion occurs, the delays of packets over the same connection are different. The jitter is used to describe the degree of delay change, that is, the time difference between the maximum delay and  the minimum delay.\nJitter  is an important parameter for real-time transmission, especially for real-time services, such as voice and video, which are zero-tolerant of jitters because jitters will cause voice or video interruptions. Jitters  also affect protocol packet transmission. Specific protocol packets are  transmitted at a fixed interval. High jitters may cause flapping of the  protocols. Jitters  exist on networks but the service quality will not be affected if jitters do not exceed a specific tolerance. The buffer can alleviate excess jitters but prolongs delays. Packet Loss Rate The  packet loss rate refers to the ratio of lost packets to total packets. Slight packet loss does not affect services. For example, users are unaware of the loss of a bit or a packet in voice transmission. The loss  of a bit or a packet in video transmission may cause the image on the screen to become garbled instantly, but the image can be restored quickly. TCP  is used to transmit data to handle slight packet loss because TCP instantly retransmits the packets that have been lost. If severe packet loss does occur, the packet transmission efficiency is affected. QoS focuses on the packet loss rate. The network packet loss rate must be controlled within a certain range during transmission. How  are QoS indicators defined within proper ranges to improve network service quality? The QoS model is involved. The QoS model is not a specific function, but an E2E QoS scheme.\nFor example, intermediate devices may be deployed between two connected hosts. E2E service quality  guarantee can be implemented only when all devices on a network use the  same QoS service model. International organizations such as the IETF and ITU-T designed QoS models for their concerned services. The following describes three main QoS service models. Best-Effort Best-Effort  is the default service model for the Internet and applies to various network applications, such as the File Transfer Protocol (FTP) and email. It is the simplest service model, in which an application can send any number of packets at any time without notifying the network. The network then tries its best to transmit the packets but provides no guarantee of performance in terms of delay and reliability. The Best-Effort model is suitable for services that have low requirements for delay and packet loss rate. Integrated Service (IntServ) In  the IntServ model, an application uses a signaling protocol to notify the network of its traffic parameters and apply for a specific level of QoS before sending packets. The network reserves resources for the application based on the traffic parameters. After the application receives an acknowledgement message and confirms that sufficient resources have been reserved, it starts to send packets within the range  specified by the traffic parameters. The network maintains a state for each packet flow and performs QoS behaviors based on this state to guarantee application performance. The  IntServ model uses the Resource Reservation Protocol (RSVP) for signaling.\nThe RSVP protocol reserves resources such as bandwidth and priority on a known path, and each network element along the path must reserve required resources for data flows requiring QoS guarantee. That is, each network element maintains a soft state for each data flow. A soft state is a temporary state that is periodically updated through RSVP messages. Each network element checks whether sufficient resources can be reserved based on these RSVP messages. The path is available only  if all involved network elements can provide sufficient resources. Differentiated Service (DiffServ) The  DiffServ model classifies packets on a network into multiple classes and takes different actions for each class. When network congestion occurs, packets of different classes are processed based on their priorities, resulting in different packet loss rates, delay, and jitter.  Packets of the same class are aggregated and sent as a whole to ensure consistent delay, jitter, and packet loss rate. Unlike  the IntServ model, the DiffServ model does not require a signaling protocol. In this model, an application does not need to apply for network resources before sending packets. Instead, the application sets QoS parameters in the packets, through which the network can learn the QoS requirements of the application. The network provides differentiated  services based on the QoS parameters of each data flow and does not need to maintain a state for each data flow. DiffServ takes full advantage of IP networks' flexibility and extensibility and transforms information in packets into per-hop behaviors (PHBs), greatly reducing signaling operations.\nThe Network Data Management Protocol is an open protocol based on enterprise-level data management. The NDMP protocol defines a network-based control mechanism that controls the backup and recovery of data and controls the transfer of data between Huawei storage systems and tape libraries. NDMP allows the storage system to send data to be backed up directly to the tape library it is connected to or to the backup server located on the network. This process does not require any backup client participation and does not consume a lot of network resources and server resources. The V3 storage system NDMP needs to purchase a license to use it. The NDMP protocol supported by V3 is V4. The NDMP backup service needs to use the file system read and write function, which affects the current service performance and reduces system performance. The more backup streams that are enabled, the greater the impact on system performance. Therefore, it is recommended to set a backup window when the traffic is low, and it does not conflict with the peak of the business. NDMP client : A server that deploys backup software, also known as a backup server. The NDMP client is responsible for initiating and controlling NDMP backup, restoring services, creating and controlling NDMP sessions, and is responsible for reading, storing, and managing all session states. NDMP server : A storage system that provides files to clients using HTTP, FTP, CIFS, or NFS, also known as NDMP hosts.\nDear all, What are the different types of SSD Health Check Tools? Thanks. Hello Muhammad, Not every piece of SSD health check software has all of the following features included. Before you buy, read the softwares description carefully to see if the features you need are included. Be aware that different vendors may have slightly different terminology for these tools. SSD tools enable users to vary when the SSD controller carries out garbage collection and other parameters. This can have a marked effect on the drives performance. Many tools allow users to optimize or tune their SSD for different requirements, so that, for example, they can increase performance while losing some storage capacity, or maximizing storage capacity at the cost of performance and reliability. During garbage collection, data can be transferred to new areas of the SSD to ensure that the individual NAND cells are all used as evenly as possible. This is called wear leveling, and is carried out to prevent some cells being used so frequently that they wear out and become unusable early in the life of the SSD. SSD health tools can monitor the amount of wear that the NAND cells are experiencing and the amount of error correction that needs to be applied. Health tools monitor the overall health of the SSD and predict when it is nearing the end of its life. Since data stored on an SSD is frequently moved around for wear leveling and garbage collection purposed, the SSD itself is overprovisioned.\nIs there any question related RAID 2.0+ in HCIE interview. please guide. Yes there is a question about Raid 2 + please prapare it they will ask working principal. please review the follwing. RAID (Redundant Array of Independent Disks) is a redundant array of independent disks. It  is proposed by the University of California, Berkeley in 1987. The basic idea is to combine multiple independent physical hard disks into a  virtual logical hard disk by using related algorithms, thereby providing a larger, higher, or higher data error tolerance function. Background  of block virtualization and reconstruction of traditional RAID: The overall performance of the application system deteriorates, and the long  reconstruction time and data loss risks increase. Traditional RAID is limited by the number of hard disks. When the data capacity increases sharply, enterprises cannot flexibly and flexibly allocate resources. Therefore, the underlying data distribution architecture must be highly flexible and scalable, with the increase of hard disk capacity, data management based on hard disks is becoming more and more difficult. Huawei  provides RAID 2.0+ technical support by designing and developing traditional RAID, LUN virtualization, and block virtualization. The following figure shows the principle of Huawei 2.0+ The following figure shows the logical objects of Huawei RAID2.0+ software Disk  Domain is a combination of disks (which can be all disks in the system). After the disks are combined and reserved for hot spare capacity, the provides storage resources for the storage pool.\n1) and HVS series storage systems can have one or more disk domains. 2). Multiple storage pools (Storage Pool) can be created in a disk domain. 3). Disks in a disk domain can be SSDs, SAS disks, or NL-SAS disks. 4). Different disk domains are isolated from each other, including fault domains, performance, and storage resources. torage pool is a storage container. The  storage space used by all application servers comes from the storage pool. A storage pool is created based on a specified disk domain. The Chunk (CK) resources can be dynamically allocated from the disk domain, and the Chunk Group (CKG) is composed of each storage level (Tier). RAID  protected storage resources. Tier  is a storage tier, a collection of similar storage media in a storage pool, used to manage storage media of different performances to provide different storage space for applications with different performance requirements. The storage pool can be divided into multiple Tiers depending on the type of hard disk. 1). Creating a storage pool specifies the storage tier (Tier) type that the storage pool divides from the disk domain and the RAID policy and capacity for that type. 2). HVS series storage systems support RAID5, RAID6 and RAID10. 3). The capacity layer is composed of a large-capacity NL-SAS disk. The RAID policy recommends using a double-check mode RAID6. Disk  Group (DG) is a set of disks of the same type in a disk domain. The disk type can be SSD, SAS, or NL-SAS.\nThe HVS series automatically divides one or more Disk Group (DG) in each disk domain based on the number of disks of each type. 1). One Disk Group (DG) contains only one type of hard disk. 2). Multiple CKs of any CKG come from different hard disks of the same Disk Group (DG). Logical Drive (LD) is a logical disk that is managed by the HVS series storage system. It corresponds to a physical disk.  5. CK is short for CK. It divides the hard disk space in a storage pool into several fixed physical spaces. The size of each physical space is 64MB, which is the basic unit of the RAID.  6. The Chunk Group (CKG) is a logical storage unit that consists  of CKs from different disks in the same DG based on the RAID algorithm.  It is the minimum unit for allocating resources from a disk domain to a  storage pool. 1). CKs in a CKG come from disks in the same DG. 2). CKG have RAID properties (the RAID properties are configured on the tier). 3). CK and CKG are internal objects of the system and are automatically configured by the HVS storage system.  7. Extents are logical storage spaces of fixed sizes based on CKGs. The size of extents is adjustable. It is the minimum unit (data migration granularity) for hotspot data statistics and migration, and is  the minimum unit for applying for space and releasing space in a storage pool. 1).\nAn extent belongs to one volume or LUN. 2). The size of and extent can be set when a storage pool is created. The size cannot be changed after being created. 3). The extents of different storage pools can be different, but the extents of the same storage pool are of the same size.  8. In thin LUN mode, extents are further divided into fine-grained blocks based on the fixed size. These blocks are called Grain. Thin LUNs are allocated based on Grain. LBAs in Grain are consecutive.  9. A volume is an internal management object of the system. A volume object is used to organize all extents and Grain logical storage units of the same LUN. It can dynamically apply for and release extents to increase or decrease the actual space occupied by the volume. A LUN is a storage unit that can be directly mapped to a host for reading and writing. A LUN is a volume object. Huawei RAID2.0+ Technical Features: Automatic  load balancing reduces the overall failure rate. Quick thin reconstruction, improving the failure rate of dual disks; Quick thin reconstruction, improving the failure rate of dual disks; Fault self-detection and self-healing ensure system reliability. Huawei RAID2.0+ Reliability: Load sharing: RAID2.0+ enables hard disks to work more evenly, preventing hard disks from being overworked.\nRobust  reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. Fast  reconstruction: The RAID2.0+ greatly reduces the time window for reconstruction. In this way, the system can be restored to the fault tolerance state as soon as possible, thereby improving system reliability. Simplified  reconstruction: RAID2.0+ can detect the used space in the allocated space through metadata. Therefore, only used space is reconstructed during reconstruction, which reduces the amount of data to be reconstructed, shortens the reconstruction time, and reduces reconstruction risks. Self-detection and  self-healing: The RAID2.0+ uses the distributed hot spare space. When the system detects a fault, the system automatically starts reconstruction as long as the disk has free space (CK). This improves reliability and reduces management costs. Invalid  data volume: If a traditional RAID group fails, all data in the RAID group is affected. When multiple disks fail on the RAID2.0+, only the data associated with the failed disks becomes invalid. Most data can still be accessed. The amount of invalid data is reduced by order of magnitude compared with the traditional RAID. Huawei RAID2.0+ dual-disk failure analysis: RAID technology is the basis of storage data protection. It is still the fault tolerance capability of RAID. For  RAID 5, the number of faults that can be tolerated is 1. For traditional RAID, the unit is hard disk. For RAID2.0+, the unit is blocked.\nFor RAID 6, the number of faults that can be tolerated is 2. Therefore,  if a dual-parity protection type, such as RAID 6, is used, both traditional RAID and block-based RAID2.0+ will not be lost when a dual-disk failure occurs. If  RAID5 is used, for traditional RAID, double disk failure will result in  data loss, while HVS series storage system adopting RAID2.0+ technology  will not have two failed blocks in each CKG as long as the dual disk fails. (CK), then the data will not be lost. Yes there is a question about Raid 2 + please prapare it they will ask working principal. please review the follwing. RAID (Redundant Array of Independent Disks) is a redundant array of independent disks. It  is proposed by the University of California, Berkeley in 1987. The basic idea is to combine multiple independent physical hard disks into a  virtual logical hard disk by using related algorithms, thereby providing a larger, higher, or higher data error tolerance function. Background  of block virtualization and reconstruction of traditional RAID: The overall performance of the application system deteriorates, and the long  reconstruction time and data loss risks increase. Traditional RAID is limited by the number of hard disks. When the data capacity increases sharply, enterprises cannot flexibly and flexibly allocate resources. Therefore, the underlying data distribution architecture must be highly flexible and scalable, with the increase of hard disk capacity, data management based on hard disks is becoming more and more difficult.\nHuawei  provides RAID 2.0+ technical support by designing and developing traditional RAID, LUN virtualization, and block virtualization. The following figure shows the principle of Huawei 2.0+ The following figure shows the logical objects of Huawei RAID2.0+ software Disk  Domain is a combination of disks (which can be all disks in the system). After the disks are combined and reserved for hot spare capacity, the provides storage resources for the storage pool. 1) and HVS series storage systems can have one or more disk domains. 2). Multiple storage pools (Storage Pool) can be created in a disk domain. 3). Disks in a disk domain can be SSDs, SAS disks, or NL-SAS disks. 4). Different disk domains are isolated from each other, including fault domains, performance, and storage resources. torage pool is a storage container. The  storage space used by all application servers comes from the storage pool. A storage pool is created based on a specified disk domain. The Chunk (CK) resources can be dynamically allocated from the disk domain, and the Chunk Group (CKG) is composed of each storage level (Tier). RAID  protected storage resources. Tier  is a storage tier, a collection of similar storage media in a storage pool, used to manage storage media of different performances to provide different storage space for applications with different performance requirements. The storage pool can be divided into multiple Tiers depending on the type of hard disk. 1).\nCreating a storage pool specifies the storage tier (Tier) type that the storage pool divides from the disk domain and the RAID policy and capacity for that type. 2). HVS series storage systems support RAID5, RAID6 and RAID10. 3). The capacity layer is composed of a large-capacity NL-SAS disk. The RAID policy recommends using a double-check mode RAID6. Disk  Group (DG) is a set of disks of the same type in a disk domain. The disk type can be SSD, SAS, or NL-SAS. The HVS series automatically divides one or more Disk Group (DG) in each disk domain based on the number of disks of each type. 1). One Disk Group (DG) contains only one type of hard disk. 2). Multiple CKs of any CKG come from different hard disks of the same Disk Group (DG). Logical Drive (LD) is a logical disk that is managed by the HVS series storage system. It corresponds to a physical disk.  5. CK is short for CK. It divides the hard disk space in a storage pool into several fixed physical spaces. The size of each physical space is 64MB, which is the basic unit of the RAID.  6. The Chunk Group (CKG) is a logical storage unit that consists  of CKs from different disks in the same DG based on the RAID algorithm.  It is the minimum unit for allocating resources from a disk domain to a  storage pool. 1). CKs in a CKG come from disks in the same DG. 2).\nCKG have RAID properties (the RAID properties are configured on the tier). 3). CK and CKG are internal objects of the system and are automatically configured by the HVS storage system.  7. Extents are logical storage spaces of fixed sizes based on CKGs. The size of extents is adjustable. It is the minimum unit (data migration granularity) for hotspot data statistics and migration, and is  the minimum unit for applying for space and releasing space in a storage pool. 1). An extent belongs to one volume or LUN. 2). The size of and extent can be set when a storage pool is created. The size cannot be changed after being created. 3). The extents of different storage pools can be different, but the extents of the same storage pool are of the same size.  8. In thin LUN mode, extents are further divided into fine-grained blocks based on the fixed size. These blocks are called Grain. Thin LUNs are allocated based on Grain. LBAs in Grain are consecutive.  9. A volume is an internal management object of the system. A volume object is used to organize all extents and Grain logical storage units of the same LUN. It can dynamically apply for and release extents to increase or decrease the actual space occupied by the volume. A LUN is a storage unit that can be directly mapped to a host for reading and writing. A LUN is a volume object.\nHuawei RAID2.0+ Technical Features: Automatic  load balancing reduces the overall failure rate. Quick thin reconstruction, improving the failure rate of dual disks; Quick thin reconstruction, improving the failure rate of dual disks; Fault self-detection and self-healing ensure system reliability. Huawei RAID2.0+ Reliability: Load sharing: RAID2.0+ enables hard disks to work more evenly, preventing hard disks from being overworked. Robust  reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. Fast  reconstruction: The RAID2.0+ greatly reduces the time window for reconstruction. In this way, the system can be restored to the fault tolerance state as soon as possible, thereby improving system reliability. Simplified  reconstruction: RAID2.0+ can detect the used space in the allocated space through metadata. Therefore, only used space is reconstructed during reconstruction, which reduces the amount of data to be reconstructed, shortens the reconstruction time, and reduces reconstruction risks. Self-detection and  self-healing: The RAID2.0+ uses the distributed hot spare space. When the system detects a fault, the system automatically starts reconstruction as long as the disk has free space (CK). This improves reliability and reduces management costs. Invalid  data volume: If a traditional RAID group fails, all data in the RAID group is affected. When multiple disks fail on the RAID2.0+, only the data associated with the failed disks becomes invalid. Most data can still be accessed.\nDears; I need a configuration example on how to assign my storage resources to the Microsoft exchange users? starting from on how to create LUN and RAID group? Hello, how to configure storage resources for Microsoft Exchange. The system has sufficient storage space. Provisioning Application Microsoft Exchange Create The Create Microsoft Exchange Storage Resource Wizard dialog box is displayed. Table 1 Parameter Description Value Name Name of a Microsoft Exchange instance. The name: [Example] ExchangeApp_001 Description Description of a Microsoft Exchange instance. Description can be left blank or contain up to 255 characters. [Example] - Must be unique. Can contain only letters, digits, underscores (_), periods (. ), and hyphens (-). Must contain 1 to 22 characters. Next describes related parameters. Table 2 Parameter Description Value Exchange Version Version of the Microsoft Exchange software. [Value range] The value can be Exchange 2007 , Exchange 2007 LCR , Exchange 2007 local LCR , Exchange 2007 remote LCR , Exchange 2010 , or Exchange 2010 with DAG . The default value is Exchange 2010 . [Example] Exchange 2010 Mailbox Quota Maximum capacity of a Microsoft Exchange email box. The unit of the parameter value can be MB or GB . [Value range] The value ranges from 512 MB to 5 GB. The default value is 2 GB . [Example] 512 MB Number of Emails Per Day Maximum number of emails that a Microsoft Exchange email box can send and receive per day. [Value range] The value ranges from 5 to 500. The default value is 25 .\n[Example] 25 Log Retention Period (days) Retention period of logs generated by the Microsoft Exchange email server. [Value range] The value ranges from 1 to 30 days. The default value is 14 . [Example] 14 Number of Mailboxes Maximum number of email boxes supported by the Microsoft Exchange email server. [Value range] The value of this parameter is subject to the number of emails per day, log retention period (days), and remaining capacities of storage pools. The value dynamically changes. [Example] 152 Optional: Click OK . You are returned to the Set Exchange Application Parameters page. Click Advanced . The Advanced dialog box is displayed. Set advanced properties for Microsoft Exchange. describes related parameters. Table 3 Parameter Description Value Allocate public folders When Exchange Version is Exchange 2007 or related versions, this parameter is available. If Allocate public folders is enabled, the system will allocate one more LUN to Microsoft Exchange instances. This parameter is designed for the share access function. This parameter provides an easy and effective way for you to share information. If Outlook 2003 or earlier versions or Microsoft Entourage are installed on clients, those clients can connect to the Exchange server using this public folder only. [Example] Allocate public folders Capacity Storage capacity of the public folder. After Allocate public folders is enabled, this parameter is available. [Example] 15 GB Enable SmartThin After this option is selected, the system creates thin LUNs.\nA thin LUN is allocated an initial capacity (30% of the total capacity) when created and is dynamically allocated required storage resources when its available capacity is insufficient. [Example] Enable SmartThin Reserve snapshot space After this option is selected, the system automatically selects a storage pool whose capacity is 130% of the required capacity. When you use the storage pool to create storage resources for applications, reserve sufficient space to create snapshots for applications. [Example] - Next The system will allocate optimal storage resources based on preset Microsoft Exchange parameters. describes storage resource parameters. Table 4 Parameter Description Value LUN Capacity LUN storage space allocated to store emails or logs. [Example] 300 GB LUN Quantity Number of LUNs allocated to store emails, folders, or logs. [Example] 7 (log) Storage Pool Policy The system sets storage pool allocation policies based on preset Microsoft Exchange application parameters. The value can be High performance , Performance/Cost balance , Low cost , or Manual selection . The four values are described as follows: [Example] Performance/Cost balance Selected Storage Pool Name of the storage pool automatically allocated by the system to a Microsoft Exchange instance. [Example] StoragePool001 High performance : The system automatically selects a RAID 6 storage pool containing SAS disks only. If such a storage pool does not exist in the system, create one. Performance/Cost balance : The system automatically selects a RAID 6 storage pool containing SAS and NL-SAS disks only. If such a storage pool does not exist in the system, create one.\nHi team, What is the difference between groups and Organizational Units in active directory? Thank you. Hi Lucas, Active Directory groups are used to . As a best practice, you place users into groups and then apply the groups to an access control list (ACL). Its quite typical to have your AD groups mirror your company hierarchy (e.g., a group for Finance, Marketing, Legal, etc.). Organizational Units are useful when you want to to a subset of users, groups, and computers within your domain. For example, a domain may have 2 sub-organizations (e.g., consumer and enterprise) with 2 separate IT teams managing them. Creating 2 OUs lets each IT team administer their own policies that affect only the users, computers, etc. that fall within their unit. Organizational Units also allow you to of the directory. Heres an example: lets assume that you have an organizational unit structure such that the top level OU is named Employees and the child OUs are Departments and HRUsers. Departments also includes child OUs such as SalesUsers, EngineeringUsers, FinanceUsers, and ExecutiveUsers. If you wanted someone from the IT department to have the ability to reset the password for all employees in all departments, you would establish that delegation of administration at the Departments OU level. If, however, you wanted a manager from the HR department to be able to reset the passwords for only the HR users, you would configure the delegation of administration on the HRUsers OU, giving them the ability to reset passwords exclusively for these users.\nI need to know about requirement of Hyper Replication Hello, friend! HyperReplication is a value-added feature that requires a software license in both the primary and secondary storage systems. The remote replication feature of this storage series does not support direct data replication with a storage device from another vendor. A maximum of 256 available links are supported between a controller and a remote device, and the remote replication feature uses at most eight of them as the replication links between the local and remote devices. Because of the LUN identification mechanism used by hosts' operating systems and volume managers, do not map the primary and secondary LUNs of a remote replication pair to the same host. For example, in Oracle ASM, hosts distinguish between LUNs by ASM disk headers and may wrongly identify the primary and secondary LUNs as the same LUN, resulting in data loss on the two LUNs. During the creation of a remote replication pair, a LUN mapped to a host cannot be used as the secondary LUN in the pair. The ports for the replication links between storage devices cannot be the same as those for the connections between hosts and storage devices or be used as bond ports. The maximum data transmission distance supported by synchronous remote replication is 300 km, and the maximum data transmission distance supported by asynchronous remote replication is not restricted.\nIf you want to know more information, please check the following post: If you want to know more information about storage feature, please check the following link: Have a nice day! Hello, friend! HyperReplication is a value-added feature that requires a software license in both the primary and secondary storage systems. The remote replication feature of this storage series does not support direct data replication with a storage device from another vendor. A maximum of 256 available links are supported between a controller and a remote device, and the remote replication feature uses at most eight of them as the replication links between the local and remote devices. Because of the LUN identification mechanism used by hosts' operating systems and volume managers, do not map the primary and secondary LUNs of a remote replication pair to the same host. For example, in Oracle ASM, hosts distinguish between LUNs by ASM disk headers and may wrongly identify the primary and secondary LUNs as the same LUN, resulting in data loss on the two LUNs. During the creation of a remote replication pair, a LUN mapped to a host cannot be used as the secondary LUN in the pair. The ports for the replication links between storage devices cannot be the same as those for the connections between hosts and storage devices or be used as bond ports. The maximum data transmission distance supported by synchronous remote replication is 300 km, and the maximum data transmission distance supported by asynchronous remote replication is not restricted.\nHello, friend! When a hard disk in use in the storage system fails, the storage system restores the data in the failed hard disk to a reserved free space, the method is called hot backup, and this part of free space is called hot backup space, the hot standby space does not store any user data. The hot standby space can be a physical hard drive, or it can be the space scattered through the Intel Virtualization Technology on each member's disk at the same level of the hard disk domain. The traditional hot standby space, that is, the global hot standby disk, is reserved by specifying several spare hard disks as the hot standby disk. Huawei's OceanStor t V1 storage system uses this approach to reserve space for hot standby. And only global hot spare is supported, you can not specify a hot spare for a particular RAID group, that is, a hot spare created can be used for any RAID group. When a hard disk in a RAID group fails, the data on the failed disk is reconfigured on the hot standby disk, and when the failed disk is replaced, the data on the hot standby disk is automatically restored to the new hard disk. Hot backup disk will not scan the disk, is the controller to monitor the disk, if found disk failure, will let the hot backup disk to work. But for now it is recommended that you use RAID 2.0 + technology.\nOrganizations top data priorities over the next two years fall into three areas, all supported by wider adoption of cloud platforms: improving data management, enhancing data analytics and ML, and expanding the use of all types of enterprise data, including streaming and unstructured data. CxOs and boards recognize that their organizations ability to generate actionable insights from data, often in real-time, is of the highest strategic importance. If there were any doubts on this score, consumers accelerated flight to digital in this past crisis year have dispelled them. To help them become data-driven, companies are deploying increasingly advanced cloud-based technologies, including analytics tools with machine learning (ML) capabilities. However, what these tools deliver will be of limited value without abundant, high-quality, and easily accessible data. In this context, effective data management is one of the foundations of a data-driven organization. But managing data in an enterprise is highly complex. As new data technologies come on stream, the burden of legacy systems and data silos grows, unless they can be integrated or ring-fenced. Fragmentation of architecture is a headache for many a chief data officer (CDO), due not just to silos but also to the variety of on-premise and cloud-based tools many organizations use. Along with poor data quality, these issues combine to deprive organizations data platformsand the machine learning and analytics models they supportof the speed and scale needed to deliver the desired business results.\nTo understand how data management and the technologies it relies on are evolving amid such challenges, MIT Technology Review Insights surveyed 351 CDOs, chief analytics officers (CAOs; we refer to these and CDOs as data leaders at various points in the report) as well as chief information officers (CIOs), chief technology officers (CTOs), and other senior technology leaders. We also conducted in-depth interviews with several other senior technology leaders. Following are the key findings of this research: This select group of high-achievers deliver measurable business results across the enterprise. They are succeeding thanks to their attention to the foundations of sound data management and architecture, which enable them to democratize data and derive value from machine learning. The foundations ensure reduced data duplication, easy access to relevant data, the ability to process large amounts of data at high speeds, and improved data quality. The high-achievers are also advanced cloud adopters, with 74% running half or more of their data services or infrastructure in a cloud environment. The CDOs interviewed for the study ascribe great importance to democratizing analytics and ML capabilities. Pushing these to the edge with advanced data technologies will help end-users to make more informed business decisions the hallmarks of a strong data culture. This is only possible with a modern data architecture. One CDO sums it up by saying that successful data management is achieved when the right users have access to the right data to quickly generate insights that drive business value. Scaling ML use cases is exceedingly complex for many organizations.\nThe differences are as follows: 1. During the installation of the database in 2016, the Management Tool installation option is not available. Instead, you need to download and install the (SSMS-Setup-ENU.exe) https://go.microsoft.com/fwlink/?LinkId=531355 management tool from the official website. This tool does not need to be installed on each server cluster node. It can be installed on a third-party VM for remote management. 2. During the installation, a message is displayed indicating that you do not have the permission to add remote nodes when creating a cluster on node A. It is preliminarily determined that the fault is caused by the DCOM, but it is difficult to locate the fault. Then, prepare the AD domain environment and the network environment of the two hosts in the following sequence to rectify the fault. 1) Configure the AD domain and DNS, especially check whether the DNS is available. 2. Disable unused NICs on the two Windows 2016 OSs and retain only management NICs and heartbeat NICs. 3) Disable the firewall. 4) Disable the NETBIOS of the heartbeat network adapter. 5) Disable the IPv6 addresses of the two NICs and configure the IPv4 addresses. When a message is displayed asking you whether to share the two NICs, select NO. Then, the location of the two NICs is public. 6. Check the IP addresses. The heartbeat and management network between the two nodes are normal. 7.\nSymptom Map HDS LUNs to host A, configure PVs, VGs, and LVs using the LVM, and mount them. Create a mirror or snapshot for an HDS LUN and migrate the mirror or snapshot data to the LUN using SmartVirtualization and SmartMigration of Dorado V3 (offline). When LUNs of DoradoV3 are mapped to host A, the OS and multipathing software scan for disks and then run the pvs command. It is found that LUNs of DoradoV3 directly overwrite (replace) LUNs of HDS in VGs. Before DoradoV3 LUNs are mapped to hosts, the VGs in the PVs are HDS LUNs. After LUNs of DoradoV3 are mapped to hosts, the VGs on the PVS are LUNs of DoradoV3. That is, in the same VG, DoradoV3 LUNs directly replace HDS LUNs. Cause HDS LUNs are mapped to hosts and managed by LVMs. LUNs have LVM metadata. After LUNs and their mirrors or snapshots are migrated to Dorado V3, Dorado V3 LUNs have the same LVM metadata. If different LUNs with the same metadata are mapped to the same host, a conflict occurs. Solution 1. Multiple LUNs with the same LVM metadata cannot be mapped to the same host. 2. To map multiple LUNs with the same LVM metadata to the same host, run the vgimportclone command of the LVM (tested in Red Hat 6.8) to import VGs. A. Command for importing VGs: vgimportclone -n VG_NEW_NAME PV_NAME1 PV_NAME2 In this case, the PVS check is performed. The HDS LUNs are not overwritten by DoradoV3 LUNs and belong to different VGs.\nTCP Offload Engine (TOE) is a concept that has been used for many years. It aims to use a dedicated processor on a network adapter to process some or all data packets. In other words, using a dedicated NIC equipped with a TOE chip, four-layer processing requests, including TCP, can be transferred from the host processor to the NIC. The final result is to improve the server performance while accelerating the network response. As more processor resources are released, server hosts can better respond to various network applications, such as file services, network attached storage (NAS), high-performance computing, high-end backup and recovery, IP storage, and video editing. Even with increased network traffic, the server can still have a short response time with TOE. However, TOE has its own drawbacks, and in fact, the TOE solution has been limited to sending large chunks in an environment of 8 KB or more. These large payloads require less interaction with the platform, so the TOE has plenty of time to deal with memory problems before the next payload arrives. Typically, storage backup and retrieval systems and enterprise databases use large data payloads. In addition, the TOE application has many restrictions, such as modifying the operating system and depending on specific TOE NICs. The main difference between the TOE and standard network driver is that user data is directly copied to the driver at the data link layer for processing.\nThe concept of a containerThe container technology in IT is a literal translation of the English word Linux Container. The word \"container\" has the meaning of \"container\" and \"container\" (mainly \"container\"). Its core feature is that it has a uniform format and can be overlapped layer by layer. In the early days, it was thought that the hardware abstraction layer based on hypervisor virtualization could maximize the flexibility of virtualization management. VMs running different OSs can be derived, run, and destroyed using hypervisors (such as KVM and Xen). However, over time, users find that the hypervisor approach has become more troublesome. Why? For a hypervisor environment, each virtual machine needs to run a complete operating system and a large number of applications installed in it. But in the actual production development environment, we focus more on the applications we deploy. If I have to build a complete operating system and the dependent environment for each deployment release, that makes the task and performance heavy and low. Based on this, people wondered, is there any other way to focus more on the application itself, where the underlying redundant operating system and environment can be shared and reused? In other words, after I deploy a service and run it, I want to migrate it to another place, so I don't have to install an operating system and a dependent environment.\nIt's like container shipping, where I pack my cargo in a Lamborghini sports car (like a developed app) in a container that can easily be transported by ship from the Shanghai dock (CentOS 7.2 environment) to the New York dock. (Ubuntu 14.04 environment). Also during transportation, my Lamborghini (APP) was not damaged (document not lost) and after unloading at another dock, I could still run a perfect run (startup normal). Linux Container technology (2008) solves the \"container transportation\" problem in the IT world. Linux Container (LXC) is a lightweight kernel-level OS-layer virtualization technology. The Linux container is implemented by the namespace and Cgroup mechanisms. What are namespaces and Cgroups? As we mentioned above, containers can be used to pack and isolate the goods of Company A and Company B. Otherwise, it will be difficult to distinguish the goods unloaded. Namespaces have the same function for isolation. Isolation is not enough. We need to manage resources for goods. Similarly, shipping terminals also have such management mechanisms: container size, container size, container quantity, priority, suspension of transportation services in extreme weather, and channel change. Commonly, the corresponding Cgroup is responsible for resource management and control, for example, the CPU/MEM usage of a process group, the priority control of a process group, and the suspension and resumption of a process group. Features of container technology Compared with the hardware abstraction layer virtualization hypervisor technology, container features are clear.\nAnalyze the fast write function in the case of Fibre Channel remote replication links. 1. Check the disk array logs. The FAST WRITE switch is disabled. If the standard link latency is 2 ms, the backend monitoring latency reaches 4 ms. The log detection FST function is disabled. Data_OceanStorDorado18000V6_20200214111054\\DataCollect\\System_log\\log_controller_0B\\msg_other\\Other\ss_info (requires decompression) 2. Cause: If fast write is disabled, an I/O needs to be completed twice, which is a common implementation of the FCP protocol. If the fast write function is enabled for the optimization of the long-distance FCP protocol, one I/O needs to be returned and returned for each communication interaction. The delay is two times the delay. 3. The immediate data switch is used on the replication link. Therefore, if there are multiple replication links, the immediate data switch must be enabled on both ports of the replication link. Other Fibre Channel ports connected to hosts are not adjusted. 4. Run the following command to check whether the instant data switch of remote replication links is turned on: show remote_device link link_type=FC link_id=0: Check the number of replication links. If the last field is NO, the replication link is disabled. 5. Best practice: 6. Enable the ports connected to the remote replication link immediately. Enable the ports at both ends of each link. Check whether the ports are enabled by running the port and the preceding link commands. 7. Check whether the switches connected to the two storage arrays have the remote license.\nUltraPath supports two boot modes: boot from local disks and boot from SAN storage. You can select a boot mode when installing install.sh. Boot from local disk: The UltraPath software is installed on the local disk of the server operating system and the application server starts from the local disk. Boot from SAN: A multipathing image is generated on the server during the UltraPath software installation, and the system is started using the multipathing image. Advantages and disadvantages: 1. In boot from local mode, the system driver is loaded before the multipathing software. As a result, the system driver generates a system drive letter for each LUN based on the number of links after the server restarts, When UltraPath is loaded, all drive letter aggregation paths are deleted and a new drive letter is generated. In some operating systems, LUN drive letters are generated in an incorrect sequence. In boot from SAN mode, Huawei UltraPath loads the driver before the HBA card during system startup. All links of the LUN are aggregated and then drive letters are generated. Drive letters are not generated and deleted for multiple times. The system drive letters generated for the LUN are consecutive. 2. In boot from Local mode, system services and third-party services are loaded after the system driver. This may conflict with the loading sequence of Huawei UltraPath, causing unknown exceptions. 3. Install UltraPath in boot from Local mode. Restart the UltraPath service for the upgrade to take effect.\nThe storage system uses the system load awareness mechanism. Different scheduling modes are used based on the concurrent load of the system. The scheduling mode remains unchanged in high-concurrency load scenarios (normal mode). In low-concurrency load scenarios, some CPU cores are reserved for polling. When the thread queue is empty, the system proactively schedules the thread to reduce the latency overhead of the thread to be woken up and improve the performance in low-concurrency load scenarios (low-load mode). The system checks the load information of the current system every 5 seconds, checks the concurrent information in the last 5 seconds, records the concurrent information of QoS sampling, and summarizes the total concurrent information of each service registration point, determine the load and switch the scheduling mode based on the total number of concurrent sampling records and the IOD service running time ratio in the latest five concurrent samplings. If the number of concurrent requests is less than the threshold of the low-concurrency load for five consecutive times (the low concurrency threshold is 10 based on the actual performance test case) and the actual service running time of the IOD is less than 60%, the scheduling mode is switched to the low-load scheduling mode. Dynamically calculate the number of reserved CPUs (initialize 30%) based on the current load and execution ratio and perform polling.\nHello, friend! 1. In the upper right corner of FusionCompute, click the name of the current login user. 2. Click Change Password . The Change Password dialog box is displayed. 3. Enter the old password and the new password. 4. Click OK . The password is changed. Change a password as follows: 1. Use an SSH tool to log in to a management node as user dsware . 2. Run the sh /opt/dsware/client/bin/dswareTool.sh --op modpwd command, enter user name cmdadmin , the old password, and a new password, and confirm the new password as prompted. 1. Log in to the FusionCube Center WebUI. 2. In the upper right corner, click the drop-down arrow next to , and select Change Password . The Change Password dialog box is displayed. 3. On the Change Password page, enter Old password,Password , and Confirm password. 4. Click Modify . The Information dialog box is displayed. 5. Click OK . 6. Use the new password to log in to FusionCube Center WebUI. If the login is successful, the password is changed successfully. If the login fails, contact the system administrator. 1. If the FusionCompute login password is changed after FusionCompute is connected, synchronize the new password to FusionCube Center in a timely manner. Otherwise, FusionCompute management is affected. 2. If the password of the fc2Rest machine-machine account is changed after FusionStorage is added, synchronize the new password to FusionCube Center in a timely manner. Otherwise, FusionCube cannot manage FusionStorage. Hope this helps! Hello, friend! 1.\nHello, I need do troubleshooting. How can iCollecting Live Network Information? Thank you Hi, The information to be collected includes the basic information, fault information, storage device information, network information, and application server information. Collect the types of information specified in and send the collected information to maintenance engineers. Information Type Item Action Basic information Device serial number and version Provide the serial number and version of the storage device. You can log in to the DeviceManager and query the serial number and version of the storage device on the home page. Customer information Provide the customer's contact person and contact means. Fault information Occurrence time Record the time when a fault occurs. Symptom Record the symptom when a fault occurs, for example, an error dialog box or an event notification. Operations performed before a fault occurs Record the operations performed before a fault occurs. Operations performed after a fault occurs Record operations that are performed before reporting the fault to maintenance personnel. Storage device information Hardware module configuration Record the configuration of the hardware modules in the storage device. Indicator status Record status of the storage device indicators, especially the indicators in orange or red. For details about indicator status, see the of the corresponding product model. System data Manually export the operating data, and system logs of the storage device. Alarms and events Manually export the alarms and events of the storage device.\nDear errajarajan, Here are several reasons to consider using network attached storage. 1. Central File Storage and Access Using NAS for file storage allows you to access any files you need either from your desktop workstation or when youre away from the office remotely. This means no more laborious emailing files back and forth to yourself that you want to work on from home. And if youre on the road at a client meeting, you can pull up any sales collateral you need remotely. Centralizing your file storage also improves security and reduces the chance that files will go missing. You can easily set up file access permissions on NAS that will ensure only those that have proper authority can access a given file or folder. 2. Automated Local Cloud Backup Network attached storage devices give you affordable backup solutions that can act as a cloud backup for mobile devices and PCs that are located onsite or off. For office computers, you can choose to backup via ethernet cable connection or wirelessly, which gives you more options for ensuring data is backed up safely and accessible when you need it. 3. Easy to Set Up and Use NAS devices are designed to be simple to set up and easy to use. You wont have to go through hours of complicated setup instructions and to make it even easier, Quantum PC Services can set up your NAS and connections for you. 4.\nHi Mate, could share to me the step for replace the disk of oceanstor S5600T in \"single-link\" disk state? currently the RAID group status in normal state. thank you. Hi, dear You can do as follows to replace the Hard disk: 1. Check status before replacement. 2. Wear an ESD wrist strap. 3. Press the latch on the disk module to release the handle and pull out the disk module, 4. Put the removed disk module into an ESD bag. 5. Take the replacement disk module out of its ESD bag. 6. Open the latch of the replacement disk module, insert the disk module into the empty slot, and close the latch, 7. Wait about two minutes and check the status of the Running and Alarm/Location indicators on the disk module to determine whether the disk module is successfully installed. 8. Confirm the replacement For more details, please see Hi, dear You can do as follows to replace the Hard disk: 1. Check status before replacement. 2. Wear an ESD wrist strap. 3. Press the latch on the disk module to release the handle and pull out the disk module, 4. Put the removed disk module into an ESD bag. 5. Take the replacement disk module out of its ESD bag. 6. Open the latch of the replacement disk module, insert the disk module into the empty slot, and close the latch, 7.\nHi, when create/modify the logic port, the system remind below error message \"The specified IP address and the management network port's IP address reside on the same network segment.\" What's the solution? thanks. Dear Abdussamed, As required by the storage system, service IP address and management IP address can't be in the same network segment. Option 1 . Check the management network segment in the CLI of storage(can get it in the running data log as well), if the service IP has the same network segment with management, please change another IP address with a different network segment. Option 2 . usually service IP and management IP in same network segment is not recommended, if insist and accept the problem \"This operation may cause services that actively connect to external servers unavailable. Such services include SNMP, syslog, Email, AD, NIS, DNS, FTP, LDAP.\", can proceed below steps, then can configure service IP and management IP in the same network segment. Thanks. Dear Abdussamed, As required by the storage system, service IP address and management IP address can't be in the same network segment. Option 1 . Check the management network segment in the CLI of storage(can get it in the running data log as well), if the service IP has the same network segment with management, please change another IP address with a different network segment. Option 2 .\nHello, What's Fibre Channel or ISCSI? Thank you Hi, dear Internet Small Computer System Interface (iSCSI) is an Internet Protocol (IP)-based storage networking standard for linking data storage facilities. It provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network. Used over the IP-based SAN, the iSCSI protocol provides quick, cost-effective, and long-distance storage solutions. iSCSI encapsulates SCSI commands into a TCP or IP packet, enabling I/O data blocks to be transferred over the IP network. What is FC Channel? Fibre Channel (FC)is a technology for transmitting data between computer devices at data rates of up to 20 Gbps at present time and more in the near future. Fibre Channel began in the late 1980s as part of the IPI (Intelligent Peripheral Interface) Enhanced Physical Project to increase the capabilities of the IPI protocol. That effort widened to investigate other interface protocols as candidates for augmentation. In 1998, Fiber Channel was approved as a project and now have become and industry standard. Hi, dear Internet Small Computer System Interface (iSCSI) is an Internet Protocol (IP)-based storage networking standard for linking data storage facilities. It provides block-level access to storage devices by carrying SCSI commands over a TCP/IP network. Used over the IP-based SAN, the iSCSI protocol provides quick, cost-effective, and long-distance storage solutions. iSCSI encapsulates SCSI commands into a TCP or IP packet, enabling I/O data blocks to be transferred over the IP network. What is FC Channel?\nBased on Huawei Kunpeng 920 processor, how many cores can the system provide? Hello, I'm glad to be able to help you answer this question. The 5180 storage type is a 4U1 storage server based on the Huawei Kunpeng 920 processor, which can provide 64-core, 2.6GHz main frequency computing power and up to 560TB local storage capacity. The 5180 storage type has the characteristics of mass storage, high performance, low power consumption and easy scalability, and is suitable for efficient acceleration of the workload for large data analysis, software-defined storage and other application scenarios. The Huawei XA320 High Density Node is a 6000 High Density 1U half-width 2-way server node. Based on the Huawei Pang 920 processor, the system can provide up to 128 cores. 7nm process: top level. 2.6G/3.0G, up to 64C: Based on ARM V8, the computing performance is considerable. Supports up to 4 CPU s, Hydra bus interconnection, 240Gbit/port bandwidth, which is approximately equivalent to 16x PCIe Gen 4 bandwidth, no worse than Intel UPI. Intel Whitley UPI 20lane*11.2GbT/s=224Gbit/s. 40x PCIe Gen4: This is also a leading level. I don't know if Huawei's IO card supports Gen4 anymore. There is still room for improvement in the number of channels. 2x 100GE: Huawei's traditional advantage is beyond words. 16x SAS: This is a little unexpected. Did Huawei buy someone's IP? Cow! Intel integrated SAS failed before you said that.\nHi , Is there any rest api to query on hyper cdp snapshots capacity and consumed capacity? I am using Sorry, friend! I can't find the API command to check the HyperCDP snapshot capacity information. You can check the following document, all hyperCDP interfaces in REST. And you can check the HyperCDP capacity planning: In order to ensure the normal operation of the business, HyperCDP needs to reserve space.The reserved space of HyperCDP is used to store the data at the moment when the HyperCDP is created.Its function is: when customer data goes wrong, the data at the creation time of HyperCDP needs to be used to roll back. The calculation method of the reserved capacity of a single HyperCDP is: Multiply the write bandwidth of the LUN by the time.For example, the customer's write bandwidth to the LUN is 5MB/s, and HyperCDP retains it for 1 hour without deleting it. The maximum capacity required is 5MB*60*60=1800MB. It does not exceed the capacity of the LUN. The calculation method of scheduled HyperCDP reserved capacity is: The influencing factors are: write bandwidth, reserved number, and reserved interval.For example, the customer's write bandwidth to the LUN is 5MB/s, the retention interval is 1 minute, and the number of reservations is 1440 (reserved for one day). Each CDP occupies a maximum of 5MB*60=300MB, and all CDPs occupies a maximum of 1440*300MB=421GB. No matter how long the cycle is, the capacity occupied by each CDP does not exceed the capacity of the LUN.\nHello all, We talk about the 3-2-1 backup, it is a reliable recovery methodology for ensuring that data is protected adequately and backup copies of the data are available when needed. The basic concept of the 3-2-1 backup strategy is that three copies are made of the data to be protected, the copies are stored on two different types of storage media and one copy of the data is sent offsite. In the classic 3-2-1 backup scenario, backup software makes a copy of the companys critical data and saves the copy to another on-premises storage device. During that process or immediately afterward, two more copies of the data are saved to two other devicestraditionally at least one of those devices was a tape library. Tape was a standard part of the process because it made it easy to create a portable copy of the data in the form of a tape cartridge that could easily be sent offsite. Although the 3-2-1 backup approach has been around since the early days of data protection, it is a concept that is still embraced by most backup software and hardware vendors as a best practice for using their products effectively. They recognize that the general concept is still valid regardless of how or where a company stores its data, even as new requirements and voluminous data have made the 3-2-1 equation a bit more complicated.\nThe 3-2-1 backup strategy is made up of three rules, they are as follows: Three copies of data- This includes the original data and at least two backups. Two different storage types- Both copies of the backed up data should be kept on two separate storage types to minimize the chance of failure. Storage types could include an internal hard drive, external hard drive, removable storage drive or cloud backup environment. One copy offsite- At least one data copy should be stored in an offsite or remote location to ensure that natural or geographical disasters cannot affect all data copies. The 3-2-1 backup strategy is recognized as a best practice for information security professionals and government authorities. While it does not guarantee all data will never be compromised, this strategy eliminates the most risk. The 3-2-1 methodology is important in ensuring that there is no single point of failure for data. Not only is an organization covered if one copy is corrupted or a technology fails, but also if a natural disaster or theft occurs that wipes out the physical storage types. There are a number of ways of achieving a workable 3-2-1 backup system, and the variations on the basic theme will depend largely on the amount of data to be protected, the installed storage equipment and the type of offsite repository available. For example, the first step of making the three copies of the backup data can be accomplished in a variety of ways.\nThe simplest method would be for the backup software to create the master backup copy and then that software or a replication utility would make the two additional copies, storing one on a different media type. Alternatively, mirroring could be used to create the first two copies simultaneously with the third spun off from one of those copies. Because one copy has to reside on a different medium, the process of making that copy is often the final step in the process as copying to a different media type is likely to occur at a different rate than that of making the first two disk or solid-state drive-based copies For the second copy that is maintained in-house for quick or operational recoveries, companies should store that copy on another server or storage system separate from the originating equipment. The target gear for the second copy should allow easy access to the backup data in case it is needed, such as if the original data is lost or damaged. Storing copy number two on equipment similar to the original system should facilitate recoveries. However, the in-house copy does not necessarily have to be stored on the same or similar media. In some cases, an organization may make the first master copy and then copy that data to two tape drives, simultaneously or sequentially. One tape cartridge would be retained onsite and the other would be sent to the offsite facility.\nThe drawback to this approach is that recovering data from the onsite tape might take some time, certainly longer than recovering from a hard-disk or solid-state drive. This would also be true if other types of removable media were used, such as optical disks or removable drives, although recovery times will vary. Some backup software applications will handle the multiple-copy aspect automatically, possibly even creating the second or third copy. Data duplicating or replication applications can also handle this chore. In very small environments, the second and third copies may be triggered manually, but that method quickly becomes untenable as data stores grow. In the traditional version of 3-2-1 backup, satisfying the requirement that the 1 copy of data must go offsite was typically accomplished by sending a tape to another location. Usually, a company would contract with a tape vaulting service, which would pick up and store the tape cartridge. Data copied to non-tape portable media would similarly be handled by an offsite vaulting service. Today, many companies still rely on tape and outside vaulting services, such as Iron Mountain. Some companies that have their own remote facilities such as secondary data centers or disaster recovery installations may use ordinary courier services to pick up and deliver their offsite copies. Vaulting or other offsite scenarios come with their own caveats. Too often, tapes get lost between the loading dock, the delivery van or the offsite service. And natural elementsheat, cold, rain or snowcan wreak havoc with tape cartridges, causing data loss.\nFor these reasons, offsite copies on any medium should be tested to ensure their integrity soon after they arrive at the remote facility. Increasingly, however, companies are trying to avoid the pitfalls of manually handling copies of their data by transmitting their offsite copies electronically. So instead of physically shipping tapes, disks or drives, offsite copies can be sent over the internet or private communications lines to a cloud storage service. Often, the backup software or hardware will handle transmitting the offsite copy automatically. If the offsite copy also represented the second media type copy (the 2 of 3-2-1), sending it to a cloud service adds a wrinkle to the 3-2-1 backup method as the service is likely to store the copy on the same type of media that is used in house. Although this deserves some consideration, it may not be an issue, as cloud storage is often considered more of a storage medium than an alternative storage location. Data protection technologies and techniques have evolved considerably since the 3-2-1 backup methodology was first conceived, but it can still be applied to the various forms of modern data backup and recovery. Snapshotting and replication are two widely used data protection technologies as they help overcome some of the obstacles of adequately backing up very large data stores. But using snapshots plus replication fits nicely within the 3-2-1 model as the methods used to make the original copy of the data as well as the two duplicates.\nBackup appliances are relatively new fixtures in some data centers that combine backup software with dedicated hardware. These devices simplify initial backups and can usually connect seamlessly with a cloud backup service to stash an offsite copy of the data. Similarly, data protection processes like continuous data protection (CDP) strain the 3-2-1 model a bit, but with proper management these newer approaches can support 3-2-1 effectively. There are other developments in modern data protection that do not eliminate the possibility of using a 3-2-1 backup scheme, but they may make it a little more complicated to manage. Two of the most impactful developments involve using backup data for other purposes and the shifting ideals of data integrity. Today, backups are not just insurance policies, tucked away until something goes awry. Companies are extracting more value from their backup data stores by using that data for things like developing and testing new applications. Contemporary approaches to programming, such as DevOps, require easy access to data that is as close to the real thing as possible to ensure that applications are developed properly. Backup data fits that bill very nicely. Analytical applications may also need access to large amounts of current data. By using fresh backup data, the results of the analytic process are likely to be more reliable and accurate. Tighter controls and management of data companies is required to ensure that these applications get the best data possible while maintaining the primary concepts of 3-2-1 backup.\nData integrity has always been a key concern of data protection activities. It is not enough to simply back up data and lock away the copies, it is imperative to ensure that backups are complete, uncorrupted and recoverable. Recovery testing helps in this regard, as well as employing some of the more advanced features that backup apps offer to detect ransomware and other threats. Again, these concerns do not necessarily derail a 3-2-1 backup approach, but they may add some steps to the process. The GDPRGeneral Data Protection Regulationis another contemporary wrinkle that could affect data protection practices. The GDPR is a European Union enactment that defines the steps that organizations must take to protect the data of its users. For 3-2-1 backup practitioners, this means ensuring that wherever those three copies of data are stored, security measures are adequate enough to ensure against data loss and to avoid potentially crippling EU fines. Another directive of the GDPR says that a companys customers or users have the right to have their names and all data related to them expunged from the companys storage systems and media. In a 3-2-1 environment, this means that the information must be removed from all three copies of the data. As an effective data protection scheme, 3-2-1 backup has stood the test of time, but when in the context of contemporary storage systems and services, some of the steps and practices may need some adjustments.\nKeeping track of 3-2-1 backup data copies can get complicated depending on the type of backup an organization is using in its 3-2-1 setup. For most companies, doing a full backup daily is not feasible, so other approaches are likely being used, such as incremental or differential backups. Because those types of backups require some interim actions to produce a full backup copy, the 3 data copies part may require some added management to ensure that all available copies are up to date and easily accessible. If a cloud storage service is being used to store the offsite copy, the service provider should be asked to provide details related to its data protection processes. Also, many cloud storage providers will charge customers extra to recover any data that the customer may have accidentally deleted. To avoid over charges and to add a level of data safety, it is a good idea to copy the cloud-based backup to another cloud storage service. As noted earlier, using a cloud storage service or transmitting a backup copy to a remote facility requires telecommunications services with ample bandwidth to handle the volume of data that is being sent or retrieved. These costs may also escalate as the amount of data a company has grows. And even with speedy communication lines, it takes time to retrieve data, so that should be considered when developing recovery plans.\nLog on to the computer as an Administrator or as a member of the Administrator group on that computer. Run from the installation package. The installation wizard opens. On the welcome page, select the checkbox, and then proceed to the next page. On the page, click , and then proceed to the next page. On the page, select the checkbox, and then proceed to the next page. Caution: If you are going to use an existing CommServe database, select the same packages that are present in the old CommServe environment, or else the installer will fail. Follow the instructions in the installation wizard. If you want to use an existing CommServe database, complete the following steps on the page: If you want the client name to be the same as the production CommServe hostname instead of the one that is provided during installation, select . If you want to use the client name and the hostname provided during installation, select . Click , select the checkbox, and then proceed to the next page. Note: If you are installing other server packages with the CommServe software, such as the Web Server, and you want to use an existing database, select the checkbox that corresponds to the database. On the page, specify the path to the CommServe database dump file (and to any other database that you selected). The dump files are located in your Disaster Recovery (DR) folder, along with other DR backup files.\nDear all, What is a VSS snapshot? How does VSS snapshot work? Thanks. Dear Axe, Volume Shadow Copy Service(VSS) , is a technology included in Microsoft Windows that can create backup copies or snapshots of computer files or volumes, even when they are in use. It is implemented as a Windows service called the Volume Shadow Copy service. Volume Shadow Copy Service (VSS) Provider to facilitate management. Microsoft's VSS operates by taking what is called a copy on write snapshot of your system. This allocates a small temporary storage space. Then, every time you write to a part of your disk, the information on the disk is first copied to the snapshot before allowing the write to take place. Figure VSS in use This technique makes VSS quite efficient. A snapshot only contains as much data as has changed since the snapshot started you do not need an entire copy of your disk. Also, writes are only affected for used space free space does not need to copy anything, as there is no original to preserve.\nHello, I would need clarification on what information can be read from Local WWN and NGUID on Dorado V6 systems. We know that on OceanStor V3 the Local WWN contains the ESN number, the creation time and the LUN ID. Is it the same in a Dorado V6? Hello, friend! The LUN WWN is the format from 2 parts: Part 1 is the WWN info of the storage system, but it's not 100% the same as the storage system, it comes from conversion and this part is 64 bit. Part 2 consists of the LUN creation time (32 bit) and the LUN ID (32 bit). For NGUID, it only exists on 6.1.0 version storage, and this ID is also is composed of two parts: The first part is also from the storagesystem WWN info, but in the conversion uses a different way to calculate and the result will be sorted as BigEndianin the end The second part contains also the LUN creation time and LUN ID, but again it will be sorted as BigEndian in the end. Bellow, you can find the method that is used to generate WWNs and NGUIDs and also the standard definition.\nHello, team! Have a nice day! I want to check the LUN capacity, do you know how to check the physical used for LUN? Thanks in advance! Hello, friend! Run the \" show lun generallun_name=? \" command to query information about the specified LUN based onthe LUN name. Capacity is the total capacity for this LUN. The space showed on the application/OS side. Can be over-provisioned. Subscribed Capacity is actual used capacity quota or capacity used/write by applications If you create a LUN with 1T provisioning within and 260G application data write in it. Your reduction ratio is 2.6:1. So Subscribed Capacity is 260G. 260G/2.6=100G. 100G physical capacity used in the storage pool. It means 100G physical space can store 260G application data by deduplication and compression Capacity 1T LUNsize Subscribed Capacity 260G application data written from OSside Reduction Ratio: 2.6:1 reduction ratio is 2.6:1 show lun general show lun reduction_info You should mention available capacity for the storage pool which describes how much physical capacity is available. Subscribed capacity for storage pool describes how much application data is used and how many application data can be written. The alarm will be reported if available capacity for storage pool is below 20%. Please consider expansion DAE or move some data off current storage if available capacity for storage pool is below 10%. For storage pool Available capacity: physical capacity for storage Subscribed capacity: application data can be written and how much application data used. Hope this helps! Hello, friend! Run the \" show lun generallun_name=?\nThe architecture of enterprise storage applications includes three modes: DAS, NAS, and SAN. The three modes are logically different from each other. Enterprise storage has the following requirements: performance, security, scalability, ease of use, the total cost of ownership, and services. Enterprise users' storage systems cannot be built overnight. They will evolve from single-server storage to networked storage. Therefore, three storage solutions are available for enterprise users to choose from. 1. Select a DAS solution DAS direct-attached storage depends on the operating system of the server host for data I/O, storage maintenance, and management. Data backup and restoration require server host resources (including CPU and system I/O). Data flows need to flow back to the host and then to the tape drive (library) connected to the server. Data backup usually occupies 20% to 30% of server host resources. Therefore, the daily data backup of many enterprise users is performed at night or when the service system is not busy to avoid affecting the normal running of the service system. The larger the amount of data stored in direct-attached storage, the longer backup and recovery times, and the greater the dependency and impact on server hardware. This solution is mainly used on early computers and servers. At that time, data storage requirements are not large, and the storage capability required by a single server can meet daily data storage requirements. Therefore, this solution is widely used in low-end network applications. 2.\nSelect a NAS solution As a network-attached storage device, the NAS device has an optimized independent storage operating system, which can effectively and tightly release system bus resources and fully support I/O storage. In addition, the NAS device integrates local backup software to back up important data on the NAS device. In addition, NAS devices provide hard disk RAID, redundant power supplies and fans, and redundant controllers to ensure stable NAS applications. NAS devices are used to implement file-sharing applications on different operating systems. Compared with traditional servers or DAS storage devices, NAS devices are easy to install, commission, use, and manage. NAS devices can save device management and maintenance costs. The NAS device provides an RJ-45 interface and an independent IP address. The NAS device can be directly mounted to the switch on the backbone network or the hub of other LANs. The NAS device can be used in plug-and-play mode through simple settings (for example, setting the IP address of the machine). In addition, online network data expansion does not require a pause, ensuring smooth data storage. The NAS data storage solution is designed based on the local area network (LAN). It communicates based on the traditional TCP/IP protocol. It is message-oriented and transmits data in file I/O mode. In the LAN environment, NAS can implement data sharing between heterogeneous platforms, such as Windows NT, Linux, and UNIX. For these reasons, the NAS storage solution has low usage and maintenance costs for enterprises and can be fully assumed by existing network administrators. 3.\nDear all, Whats the difference between SIEM, SIM, and SEM? Thank. SIM Security information management (SIM) refers to the collection of log files and storage in a central repository for later analysis. SIM is therefore also referred to as log management. SIM solutions are often agent-based with software running on the servers and computers being monitored. These relay log and other security-related information to a central SIM server. Here, system administrators can log into a console and run security reports, graphs and charts in real time. Some SIM systems have local filters that normalize, interrogate and clean up logs before they send the information to the central server. This reduces the amount of data sent across the network (which could cause network bandwidth congestion) and stored on the SIM server (which could quickly gobble up disk space). Such filtering has to be done in a way that doesnt inhibit the ability to recreate the system state that triggered a security incident. SEM Security event management (SEM) is the identifying, gathering, monitoring, evaluating, correlating and monitoring of system events and alerts. In a sense, SEM is an improvement of SIM, though the two are seen as distinct areas of security management. Just like SIM, the data is usually relayed from the host computer to a central repository using SNMP, syslog and other communication protocols. The centralized repository ensures events and alerts are kept in a forensically sound and secure storage. The information is then analyzed with security algorithms and statistical calculations to identify threats, vulnerabilities and risks.\nThe SEM can parse entries for significance as they come in and immediately notify the responsible persons whenever an entry warrants attention. Centralization also makes it easier to pick up events that affect multiple systems. The primary purpose of a SEM tool is to identify alerts or events worth investigating such as administrator logons that occur outside working hours. SIEM As the acronym suggests, security information and event management tools combine SIM and SEM capabilities. A SIEM collects, organizes and analyzes security-related activity from numerous hardware and software sources across an organizations technology infrastructure. A SIEM aggregates real-time and historical data from routers, switches, servers, desktop computers, antivirus software, firewalls, intrusion prevention/detection systems (IPS/IDS), enterprise applications, databases and more. It applies pre-defined analytical rules to the data in order to pick up threats, patterns and suspicious activity that call for a system administrators action or investigation. While the primary purpose of a SIEM is security, many enterprises are using their SIEM to demonstrate to regulators and auditors their compliance with data protection laws and standards such as GDPR, HIPAA, PCI-DSS and SOX. A SIEM can also come in handy for resource capacity management. You can keep track of data growth and bandwidth user over time and thereby proactively manage and budget for future capacity needs. Dear Phany, Security Information and Event Management (SIEM) is an umbrella term for security software packages ranging from Log Management Systems to Security Log / Event Management, Security Information Management, and Security Event correlation.\nHello, friend! I want to migrate data from the Huawei OceanStor 5300 V3 to Huawei OceanStor Dorado 3000 V6. I have some questions about Huawei SmartVirtualization: 1. Do I need to buy a SmartVirtualization license to migrate data from the old storage device to the new storage device? Or can I use Smartvirtualization out of the box? 2. Do I need to set up a dedicated ISCSI connection between the old and new storage devices? Or can I use the ISCSI network to present the LUN from the old storage device to the new storage device? 3. If I use a dedicated ISCSI connection, I can see the port initiator of the old storage device on the new storage device. How does it work, when I want to migrate the data over the existing ISCSI network? Now, I do not see the port initiators of the old storage device logging into the new storage device. Do I need to modify something on the new storage device so that I can see the initiators of the old storage device? Can you solve this annoying issue for me, please? Hello, friend! Have a nice day! 1. OceanStor 5300 V3 and Dorado 3000 V6 are Huawei storage, so SmartVirtualization doesn't need a license. 2. Suggest to setup a dedicated ISCSI connection between two storages to prevent migration tasks consuming too much bandwidth to affect the existing service. 3. For the detailed configuration procedure, you can refer to the guide.\nHi, dear Question: What is the difference between S5500T and S5600T controllers and disk enclosure cascading methods? Answer: The capacity of each controller enclosure in the storage system is limited. Therefore, when the storage capacity exceeds a certain range, controller enclosures need to be added. As a result, controller enclosures are cascaded. Similarly, when the storage capacity required by services reaches the bottleneck, you need to add disk enclosures to increase the storage capacity. Question: How to connect the positive connection and reverse connection ? Answer: you can connect as the following picture. Question: When do you need a positive connection and when do you need a reverse connection? Answer: Positive connection Assume that three disk enclosures are cascaded to controller enclosures 1, 2, and 3. Expansion ports P0 and P1 on controller A are connected to expansion ports PRI 0 and PRI 1 on expansion module A of disk enclosure 1. The EXP expansion port on expansion module A in disk enclosure 1 is connected to the PRI expansion port on expansion module A in disk enclosure 2. The method of connecting controller A to expansion ports on expansion modules A of all disk enclosures is called forward connection c. Reverse connection Assume that controller enclosures 1, 2, and 3 are cascaded, and the expansion ports on controller A and expansion modules A of all disk enclosures are connected forwardly.\nHi, dear Huawei RAID2.0+ Technical Features: Automatic load balancing reduces the overall failure rate. Quick thin reconstruction, improving the failure rate of dual disks; Quick thin reconstruction, improving the failure rate of dual disks; Fault self-detection and self-healing ensure system reliability. Huawei RAID2.0+ Reliability: 1. Load sharing: RAID2.0+ enables hard disks to work more evenly, preventing hard disks from being overworked. 2. Robust reconstruction: With the RAID2.0+ technology, more disks are used to share the reconstruction load during reconstruction, reducing the reconstruction workload of each hard disk and reducing the risk of hard disk faults during reconstruction. 3. Fast reconstruction: The RAID2.0+ greatly reduces the time window for reconstruction. In this way, the system can be restored to the fault tolerance state as soon as possible, thereby improving system reliability. 4. Simplified reconstruction: RAID2.0+ can detect the used space in the allocated space through metadata. Therefore, only used space is reconstructed during reconstruction, which reduces the amount of data to be reconstructed, shortens the reconstruction time, and reduces reconstruction risks. 5. Self-detection and self-healing: The RAID2.0+ uses the distributed hot spare space. When the system detects a fault, the system automatically starts reconstruction as long as the disk has free space (CK). This improves reliability and reduces management costs. 6. Invalid data volume: If a traditional RAID group fails, all data in the RAID group is affected. When multiple disks fail on the RAID2.0+, only the data associated with the failed disks becomes invalid. Most data can still be accessed.\nSolution Description This solution describes how to deploy VMs using VVOL for VMware ESX6.0 on Huawei storage arrays that support VVOL. The following information is included: VVOL is a new function of VMware ESXi 6.0. VVols can provide VM-based storage resources and deliver more VM-based storage operations to storage systems, making full use of storage resources. The VVol technology lays a foundation for VM I/O isolation and reverse storage awareness of VMs. OceanStor DJ is service-driven storage control software that centrally manages storage resources and provides service-driven and automated data services. OceanStor DJ works with VASA Provider to implement the VVol function. OceanStor eSDK VASA is a software developed by Huawei and complies with VASA standards. It is used to manage Huawei storage devices through the vSphere client. VASA Provider supports the Virtual Volume (VVVol) function of VMware. The concepts related to VVol are as follows: Virtual Volumes A virtual volume is an object exported from a storage system and managed by the storage system. It is uniquely identified by a GUID and is used to encapsulate VM files, virtual disks, and derivative files. The storage system can sense the content related to the storage system by using dedicated APIs for Storage Awareness (VASA). Storage Container Storage containers are the original storage capacity pools or storage function sets provided by the storage system. They are containers for virtual volumes. Virtual volumes can only be allocated space from storage containers. Storage containers are defined by the storage array administrator.\nHello, friend! Item 18500 V3 18800 V3 Processors per controller 2 x 6-core processor 2 x 10-core processor (enhanced ) 2 x 10-core processor 2 x 12-core processor (enhanced) Maximum number of engines 4 Memory capacity of a controller 256 GB 512 GB 1 TB 256 GB 512 GB 256 GB 512 GB 1 TB Enhanced: 512 GB Maximum number of controllers per engine 4 Maximum number of disks per engine 2.5-inch disk: 6400 3.5-inch disk: 3072 2.5-inch disk: 9600 3.5-inch disk: 4608 Engine configuration 6 U engine without disks Supported disk enclosure types 2 U SAS disk enclosure with 25 x 2.5-inch disks 4 U SAS disk enclosure with 24 x 3.5-inch disks Maximum number of disk enclosures 2.5-inch disk: 256 3.5-inch disk: 128 2.5-inch disk: 384 3.5-inch disk: 192 Maximum number of disk enclosures that can be connected to back-end channels (ports) A maximum of four 2 U SAS disk enclosures (each houses 25 x 2.5-inch disk) can be connected to a pair of SAS ports. Two is recommended. A maximum of two 4 U SAS disk enclosures (each houses 24 x 3.5-inch disk) can be connected to a pair of SAS ports. One is recommended 2 U SAS disk enclosures (each houses 25 x 2.5-inch disk) and 4 U SAS disk enclosures (each houses 24 x 3.5-inch disk) cannot connect to the same back-end loop of a SAS interface module.\nHello, friend! You can use the Huawei Ultrapath. The UltraPath improves data transfer reliability, ensures security of paths between an application server and a storage system, and provides customers with an easy-to-use and highly efficient path management solution to bring the performance of application servers and storage systems into full play, maximizing return on investment (ROI). Functions and Features of the UltraPath The UltraPath has the following functions and features: Selection of paths between an application server and a storage system The UltraPath is loaded to a Linux operating system as a kernel module and registered as a virtual device internally. The UltraPath enables an application server and a storage system to communicate with each other over the optimal path. Failover A failover is a service trespass upon a failure. Multiple paths can be set up between an application server and a storage system to ensure highly reliable data transfer. When the primary path fails, the failover function enables the UltraPath to automatically switch services to a secondary path, preventing service interruption caused by a single point of failure. An owning controller is assigned to each LUN created on a storage system. In the event that a LUN is mapped to an application server, the path between the application server and the owning controller of the LUN is the primary path, and other redundant paths are secondary paths. Using the primary path as the working path maximizes the I/O performance.\nHello, friend! An asynchronous remote replication session periodically replicates data from the primary storage system to the secondary storage system. The characteristics of asynchronous remote replication are as follows: Asynchronous remote replication relies on the snapshot technology. A snapshot is a point-in-time copy of source data. When a host writes data to a primary LUN, the primary storage system returns a response indicating a successful write to the host, as soon as the primary LUN returns a response indicating a successful write. Data synchronization is triggered by a user manually or by the system periodically to keep data consistent between the primary LUN and the secondary LUN. After an asynchronous remote replication relationship is set up between a primary LUN and a secondary LUN, initial synchronization is performed to copy all of the data from the primary LUN to the secondary LUN so that the two LUNs have consistent data. After the initial synchronization is complete, the storage system processes host writes as follows: When receiving a host write, the primary storage system sends the data to the primary LUN. As soon as the primary LUN returns a response indicating a successful write, the primary storage system returns a response indicating a successful write to the host. At the scheduled synchronization time, new data on the primary LUN is copied to the secondary LUN. Writing process in asynchronous remote replication mode: 1. The host sends a write I/O request to the RM, and the RM directly writes data to the primary cache. 2.\nHello, friend! Have a nice day! 1. In COW snapshots without performance optimization, the protected data must be backed up before host data can be delivered. It takes a certain amount of time for backup data to be read and written to disks, resulting in a sharp decrease. If a data area on a LUN that has been copied before write is written again, the data area on the source LUN is directly overwritten. This has no impact on host performance. The curve recovery is closely related to the host I/O model. During random I/Os, the performance is gradually restored as the number of I/Os that need to be performed for COW decreases. Huawei COW is optimized for performance and implemented under the cache. Host I/Os are returned after being written to the cache. Therefore, the LUN performance does not deteriorate sharply after Huawei snapshots are used. 2. After copy on write is performed for all data blocks in a data LUN, the write performance is restored to the previous one because copy-on-write is not required. Hope this helps! Hello, friend! Have a nice day! 1. In COW snapshots without performance optimization, the protected data must be backed up before host data can be delivered. It takes a certain amount of time for backup data to be read and written to disks, resulting in a sharp decrease. If a data area on a LUN that has been copied before write is written again, the data area on the source LUN is directly overwritten.\nDear wissal, The performance of RAID 5 is determined by the number of disks and stripe depth: Assume that the stripe depth is 128 KB. RAID 5 uses 9 disks, because there are eight data disks and one parity disk. Eight data disks bring 1 MB stripes, but CHUNSIZE is 1 MB. In this case, optimal performance can be achieved. Assume that the number of member disks is five, I should set the stripe size to 256 KB. The principle is the same as that above. Then let's talk about the sequential write performance. When the number of member disks is fixed, RAID 5 has N-1 disks working, whereas RAID 10 has only N/2 disks working. However, RAID 5 has parity data, but RAID 10 does not. Therefore, the performance of RAID 5 is better than that of RAID 10 when the front-end data volume is small. Sequential read: RAID 5: In addition to the parity disk, N-1 disks provide read data for stripes. In RAID 10, N disks provide data, but the data is mirrored. Therefore, only N/2 disks provide data for a stripe. Therefore, the read performance of RAID 5 is slightly higher. Let's talk about how data is flushed: The data is flushed to the high and low watermarks. If the high watermark is higher than the high watermark, data is continuously flushed from the cache to the hard disk until the data is below the low watermark.\nHi there! I have a Huawei P9 Lite phone and I m always in trouble with storage. Phone just wants me to delete stuff from internal storage. I have an SD card which is 16 GB but it was small so i have decided to buy a new one (Samsung 128 GB). The problem is that the internal storage of the phone is full and I can t use my smartwatch,GuitarTune apps and thing like that beacuse i cant download them from Google Play and i can t move apps to the SD card. I have watched videos on the internet but that doesn t solve my problem. My mobile didn t give me the option to format my SD card and in the Developer settings i can t set it as a default for an apps because it simply doesnt allows that. Can someone help m e to solve this issue ? I want to set it as an Internal to save there apps and thing like that beacuse the internal storage of the phone is just to small.https://19216811.cam/ Dear user, Move the large file to the memory card for storage. If a phone supports an external memory card, you are advised to move the images, audio, and video files from the phone's internal storage to the memory card. Downloading too many applications will cause insufficient memory. You are advised to uninstall infrequent applications to release memory. You can use the cleanup function in Phone Manager or File Management to clear junk files.\nHello, community! I want to know how to check the SFP information and whether the working status of the SFP is normal. Thanks! Dear! For how to check whether the working status of the SFP is normal, if the absolute value of TX Power is larger than 6, the light-emitting power of the SFP is low and the SFP has a problem. If the absolute value of TX Power is larger than 9, the link attenuation value exceeds 3 dBm and the link quality has a problem. Run the sfpshow portnum command to check the SFP information about a port.\nHello all, Have a good day. Today we talk aboutContainer Attached Storage. Container Attached Storage (CAS) provides a new paradigm by leveraging the container environment itself to provide persistent storage, mapping data to applications. As containers (especially K8s) become an important application delivery platform, CAS is coming to the public's attention. In the past five years, application containerization has changed rapidly as K8s becomes a container orchestration platform. When containers were first introduced, we thought that persistent storage was not required. Persistence was achieved through application-based elasticity, including data replication and mirroring at the application layer. With the development of container environments, applications such as traditional database software are containerized, which promotes the requirement for long-term data storage in the lifecycle of a single container. For many reasons, this change is inevitable. First, application-based resilient usage leads to considerable overhead, requiring data replication and host-based I/O removal around the container infrastructure. Second, many application platforms may not have the replication function. If only one mirror copy is maintained, data is at great risk. Third, businesses need data persistence as part of compliance and audit requirements. The persistence of the storage tier provides the ability to implement data protection and security control. Initially, persistent storage is mapped to a container by a volume attached to the server where the container is running, LUNs or directories. This approach is extremely inefficient and inflexible.\nOver time, the container storage interface (CSI) has become a standard approach, allowing storage vendors to develop plug-ins for mapping storage to containers, allowing the container ecosystem itself to request storage dynamically through a process, a process that obscures the specific platform steps required to provide persistent volumes. Container-Added Storage (CAS) is a software platform that uses the container ecosystem to provide storage for containers. In the HCI environment, VMs run on each server node for storage, or implement scale-out storage tiers in hypervisors running on nodes. As with previous HCI, CAS eliminates the need for dedicated SANs, or at least no longer shared storage in what we think it is in its current form. If the container platform is delivered on a virtual machine, that would be great because each virtual machine can use additional storage (whether it is ultimately provided by SAN or not). This storage is abstracted and divided into volumes different from the CAS data. In a bare-metal environment, local disk resources are abstracted as container volumes, and CAS maintains metadata and status information about how to divide physical storage capacity. Metadata storage becomes critical. Most vendors recommend separating metadata storage from container clusters where applications are running. Maturity - The most obvious evolution may be the development of new features. CAS solutions have a long way to go compared to existing mature storage solutions, and many solutions have gaps in data services such as data protection. CAS products also need to take advantage of new media such as persistent storage.\nOLTP is an application type of the database service where a large number of users perform online transaction operations. OLTP (online transaction processing) is a class of software programs capable of supporting transaction-oriented applications on the Internet. In computing, a transaction is a sequence of discrete information exchanges that are treated as a unit. Many everyday acts involve OLTP, including online banking, online shopping and even in-store shopping when the point of sale (POS) terminal is tied to inventory management software. Two important characteristics of an OLTP system are concurrency and atomicity. Atomicity guarantees that if one step is incomplete or fails during the transaction, the entirety will not continue. Concurrency prevents multiple users from altering the same data at the same time. In order for a transaction to be completed successfully, all database changes must be permanent, a condition known in computing as atomic statefulness. To avoid single points of failure, OLTP systems are often decentralized. For example, Google Cloud Spanner is a distributed relational database service that runs on Google Cloud. It is designed to support global online transaction processing. The OLTP application has the following load characteristics: From the perspective of the database: 1. The data volume involved in each transaction is very small. 2. The database data must be up to date. Therefore, the database availability is high. 3. A large number of users access the network at the same time. 4. The database is required to respond quickly. Generally, a transaction needs to be completed within several seconds.\nFrom the perspective of storage: 1. Each I/O is very small, usually 2 KB to 8 KB. 2. Access to hard disk data is very random. 3. At least 30% of the data is random write operations. 4. REDO logs (redo log files) are written frequently. Backup and recovery, as part of a high availability strategy, can be performed on a low level of granularity to efficiently manage the size of the database. OLTP systems usually remain online during backups and users may continue to access the system while the backup is running. The backup process should not introduce major performance degradation for the online users. Partitioning helps to reduce the space requirements for the OLTP system because part of a database object can be stored compressed while other parts can remain uncompressed. Update transactions against uncompressed rows are more efficient than updates on compressed data. Partitioning can store data transparently on different storage tiers to lower the cost of retaining vast amounts of data. For data maintenance operations (purging being the most common operation), you can leverage partition maintenance operations with the Oracle Database capability of online index maintenance. A partition management operation generates less redo than the equivalent DML operations. A common scenario for OLTP environments is to have monotonically increasing index values that are used to enforce primary key constraints, thus creating areas of high concurrency and potential contention: every new insert tries to update the same set of index blocks. Partitioned indexes, in particular hash partitioned indexes, can help alleviate this situation.\nIn VMware vSphere VMFS, many operations must establish a lock on the volume when updating a resource. Because VMFS is a clustered file system, many ESXi hosts can share the volume. When one host must make an update to the VMFS metadata, a locking mechanism is required to maintain file system integrity and prevent another host from coming in and updating the same metadata. The following operations require this lock: 1. Acquire on-disk locks 2. Upgrade an optimistic lock to an exclusive/physical lock 3. Unlock a read-only/multiwriter lock 4. Acquire a heartbeat 5. Clear a heartbeat 6. Replay a heartbeat 7. Reclaim a heartbeat 8. Acquire on-disk lock with dead owner It is not essential to understand all of these operations in the context of this whitepaper. It is sufficient to understand that various VMFS metadata operations require a lock. ATS is an enhanced locking mechanism designed to replace the use of SCSI reservations on VMFS volumes when doing metadata updates. A SCSI reservation locks a whole LUN and prevents other hosts from doing metadata updates of a VMFS volume when one host sharing the volume has a lock. This can lead to various contention issues when many virtual machines are using the same datastore. It is a limiting factor for scaling to very large VMFS volumes. ATS is a lock mechanism that must modify only a disk sector on the VMFS volume. When successful, it enables an ESXi host to perform a metadata update on the volume.\nHello team, I would like to know about the Global Server Load Balancing? How does it work? Thank you. Hi Lily, Have a good day. The Global Server Load Balancing (GSLB) can adjust the traffic between servers in different locations on wide area networks (WANs), including the external/public network, to ensure that each user can enjoy the services provided by the nearest server and guarantee the best service quality. Based on data loads such as the CPU usage and bandwidth usage, the GSLB can determine the link between the user (visitor) and the server, and select the server with the best link. Common GSLB policies include the DNS-based policy, redirection-based policy, and routing protocol-based policy. This section describes the DNS-based GSLB policy. Most applications using the load balancing technology access the target host using domain names. After an application connection request is sent, the DNS obtains the server IP address and then returns the service IP address of the most appropriate server based on intelligent decision-making, ensuring the proper running of services. The service processing capability is the concern of the GSLB service redundancy disaster recovery (DR) solution, that is, the GSLB service redundancy DR solution ensures the continuous service operation after a disaster occurs. In other words, a DR center equivalent to the production center is constructed in a DR site (these two centers can work in the load sharing mode) to ensure that services provided by the information system can run continuously.\nHello all, This is the best practice for Huawei OceanStor Dorado V6 series storage systems optimal configuration. Front-End and Internal Networking Each host has at least one link to each controller (entrylevel and mid-range storage) or each quadrant (highend storage), using standard dual-switch networking. Replication links between storage systems: For entrylevel and mid-range storage, each controller of the local storage system is connected to each controller of the peer storage system. For high-end storage, the quadrant of each engine on the local storage system is linked to that of the same engine on the peer storage system. Front-end service links and replication links between storage systems use different physical ports. Disk Enclosure Expansion Maximum expansion depth: Level 2/Loop Expansion method (see the networking assistant): All disk enclosures are connected in the forward direction of the expansion loops. Disk enclosures use single-uplink networking. Port P0 on a disk enclosure is connected to the upper-level enclosure, and port P1 is connected to the lower-level disk enclosure. On the HyperMetro-Inner network of high-end storage systems, port P2 on a disk enclosure is connected to an expansion port on controller enclosure 1. Storage System Software Software version Upgrade the storage system software to the version recommended by support.huawei.com/e. Multipathing Software 1. Software selection For host operating systems that are compatible with UltraPath, UltraPath is strongly recommended. 2. UltraPath version Use UltraPath 21.6.2 or later for Dorado V6. Upgrade UltraPath to the version recommended by support.huawei.com/e. 4. OS native multipathing software See OceanStor Dorado V6 Host Connectivity Guide for XXX.\nStorage Pool Number of disks Storage system with one engine: 8 normal SSDs Storage system with multiple engines: at least 8 x number of engines RAID policy Use RAID 6 or RAID TP. Number of hot spare disks If a faulty disk cannot be replaced within 30 days, it is advised to configure at least two or more hot spare disks. Space Reservation The storage pool to which a LUN belongs reserves x% of the LUN capacity. SmartDedupe and SmartCompression When creating a LUN, select a proper service type based on the committed data reduction ratio and the I/O model of the LUN. HyperMetro 1. Networking Both storage systems must have the same front-end networking modes (for example, Fibre Channel). 2. Model and version Both storage systems must be of the same model and have the same software or patch version. 3. Mapping HyperMetro LUNs from both storage systems are mapped to the same host. If a host runs VMware, the two LUNs (from different storage systems) in a HyperMetro pair must use the same host LUN ID when being mapped to the host. 4. HyperMetro working mode If the distance between two sites is less than 10 km, the load balancing mode is recommended. If the distance between two sites exceeds 10 km, the local preferred mode is recommended. Site Deployment Inspection After the storage system's initial configuration is complete, use OceanStor SmartKit to conduct a site deployment inspection. Ensure that the inspection is passed.\nHello everyone, today I'm going to introduce how to configure FC-SAN with Huawei 5300 V3. Let's learn about it together. Divided into the following three types: Direct Single exchange Double exchange WWNN node name WWPN port name To query the WWPN number on Linux: Specifically, the common operating system FC HBA card port WWN view method such as under Windows, In Windows system, you can use the management software provided by the FC HBA card manufacturer to view the WWN number of the fiber adapter, as follows: For other operating systems, refer to the following information . The F_Port : F port is also known as the fiber-optic network port and is used to connect servers and storage devices to switches. The E_Port : E port is also known as the extension port and is used for connections between switches. FL_PORT : A switch port of an FC switch can be used as part of a loop, and data can be transferred from the switch to the ring. A switch port that works well in a loop environment is called \"FL_port\"; G_Port : universal port G; Zone is similar to VLAN; A Zone is a collection of port or device names that can be interchanged; Devices in a Zone can only communicate with devices in a Zone; A device can be in more than one Zone at a time. When the switch plans the zones, it divides them into small zones, that is, each Zone contains only one starter and one target.\nCyber-attacks are on a rise and they will continue to keep on increasing. This is mainly due to the amount of success rate. Cyber-attackers target the vulnerabilities in the systems of the enterprises and make them the target, and generate a lot of money as ransom money from them. To make things even worse, cyber-attackers are now targeting backup storage in an attempt to generate even more ransom money as most valuable information is in the backups and companies are left with no option but to pay the ransom demand, no matter how ridiculously high it may be. Cloud Backup Storage Has Become a New Target Cyber-attackers are launching ransomware attacks on organizations cloud backup storage. A successful attack on cloud backup storage means that the organization cannot recover from backups in an event of a disaster and hence would leave the organization no option but to pay the ransom demand. Having said this, this does not guarantee that the organization will get the decryption key. It is common to see cyber-attackers bully the organizations even more and ask for more ransom as they cannot be traced (as payment is made in Bitcoin) so nothing would be linked to them. Well then what might be a solution? Well, it is simple, make sure your cloud backup storage is secure. Create Backups for Your Cloud Backups One can never be too sure about its data and with these ever-evolving cyber threats, it is best to be extra careful about these threats.\nHey guys, can you tell me the difference between IPSAN and FCSAN storage structure? Hello, in my opinion, there are the following differences. First, there are three storage structures that are common today DAS: Direct storage NAS: Network-attached storage SAN: Storage Area Network FC SAN Advantages: high transmission bandwidth, currently there are 1,2,4 and 8Gb/s four standards, the mainstream is 4 and 8Gb/s, stable and reliable performance, mature technology, is the key application field and large-scale storage network choice. IP SAN Since the high cost of FC SAN makes many small and medium sized storage networks unacceptable, some people begin to consider building storage networks based on Ethernet technology. But in SAN, the instructions transmitted are SCSI read and write instructions, not IP packets. ISCSI (Internet Small Computer System Interface) is a standard for transferring data blocks over TCP/IP[1]. ISCSI can run the SCSI protocol over IP networks, enabling it to perform fast data access backup operations on high speed Gigabit Ethernet, for example. This technology is called IP SAN in order to distinguish it from the previous FC SAN based on fiber optic technology. ISCSI inherits two of the most traditional technologies: SCSI and TCP/IP. This has laid a solid foundation for the development of iSCSI. ISCSI-based storage systems can implement SAN storage capabilities with minimal investment, even directly utilizing existing TCP/IP networks. Compared with the previous network storage technology, it has solved the problems of openness, capacity, transmission speed, compatibility, security and so on, and its superior performance has attracted much attention and favor.\nIn practice, SCSI commands and data are encapsulated into TCP/IP packets and transmitted over IP networks. Low cost, the purchase of network cables and switches are Ethernet, and can even use the existing network to form a SAN; Easy deployment and low management difficulty; With the advent of 10 gigabit Ethernet, IP SAN is no longer inferior to the transmission bandwidth when competing with FC SAN. The natural advantage of IP-based network makes IP SAN easy to realize remote storage, remote disaster recovery and other technologies across the WAN time; References [1] Huawei.OceanStor S2600T, S5500T, S5600T, S5800T, and S6800T Storage System V200R002C30 Basic Storage Service Guide (SAN Volume) 10[EB/OL]. Hello Wooo, An IP SAN is a dedicated storage area network (SAN) that allows multiple servers to access pools of shared block storage devices using storage protocols that depend on the Internet Engineering Taskforce standard Internet Protocol suite. The storage protocols designed to move block-based data between a host server and storage array include the Internet Small Computer Systems Interface (iSCSI), Fibre Channel over IP (FCIP) and Internet Fibre Channel Protocol (iFCP). The most common type of IP SAN uses iSCSI to encapsulate SCSI commands and assemble data into packets for transfer between the host servers and storage devices. IP SAN protocols typically run over a standard Ethernet network and use the Transmission Control Protocol/Internet Protocol (TCP/IP) for communication. An IP SAN for block-based data is often referred to as an iSCSI SAN.\nAn IP SAN is generally viewed as lower cost, less complex and simpler to manage than an FC SAN. An FC SAN requires special hardware such as host bus adapters and FC switches, whereas the IP SAN can use commodity Ethernet networking hardware. One potential disadvantage of an IP SAN is higher latency than an FC SAN, which uses deterministic layer 2 switching technology. A fiber channel storage area network (FC SAN) is a system that enables multiple servers to access network storage devices. A storage area network enables high-performance data transmission between multiple storage devices and servers. FC technology is essential for SAN implementation and establishes connectivity according to requirements. The following are basic FC SAN components: Platform devices Fabric-attached end devices, including servers, hosts and storage subsystems Devices connected to other SAN platforms via fabric facilities One or more FC nodes, including host adapters, host bus adapters, storage controllers and one or more node ports controlled at levels of FC-2 or higher Interconnected device fabric, including switches, hubs and bridges, and generic node port fabric references FC SAN has logical and physical features, as follows: Topology Views: Identify internal fabric configuration for management applications requiring device data interconnection type. Display operational data, such as assigned fabric path routing. Management applications use topology views to construct maps with interconnected platform routes, connections and devices. Logical and Physical Topologies: Include end-to-end views. Logical topology enables interdependent platform device management, regardless of connection. Physical topology enables different device paths for performance and availability.\nHi, Is it a must to connect EXP port of oceanstor storage to PRI ports? Or is there a way around it? Dear umaryaqub, Have a good day. All EXP expansion ports on a storage system can only connect to PRI expansion ports. Otherwise, services are interrupted. Before connecting disk enclosures, read the following principles: The controller enclosure and disk enclosures can be cascaded using 1 m and 3 m mini SAS HD electrical cables. Disk enclosures can be cascaded using 1 m, 3 m, and 5 m mini SAS HD electrical cables or 15 m mini SAS HD optical cables. Connect the expansion module on controller A to expansion module A on each disk enclosure and the expansion module on controller B to expansion module B on each disk enclosure. All EXP expansion ports on a storage system can only connect to PRI expansion ports. Otherwise, services are interrupted. If two or more disk enclosures are to be connected, connect them in multiple loops based on the number of expansion ports on the controller enclosure, and ensure that the disk enclosures are evenly distributed to each loop. A maximum of eight standard SAS disk enclosures can be connected to one SAS port. Two is recommended. When a storage system has more than eight 4 U disk enclosures, the disk enclosures must be installed in two adjacent cabinets. Links between controller enclosures and disk enclosures as well as links between disk enclosures must be connected forward.\nHi, What's the difference between RAID Level and Storage Tier? and how do they compliment storage system services? Have a good day. Storage Tiers A storage pool is a logical combination of one or more storage tiers. The storage pool of the storage system supports a maximum of three storage tiers. A storage tier is a set of storage media that has the same performance and uses the same RAID level. Each storage tier provides different performance at different costs. You can configure storage tiers based on your requirements. Table 1 Specifications of each storage tier Storage Tier Storage Medium Response Speed Capacity Cost Per Gigabyte Request Processing Cost Per Gigabyte Tier 0 (high-performance tier) SSD Fast High High Tier 1 (performance tier) SAS Medium Medium Medium Tier 2 (capacity tier) NL-SAS Slow Low Low Functions of different storage tiers are as follows: High-performance tier: delivers the highest performance among the three tiers. As the cost of SSDs is high and the capacity of a single SSD is small, the high-performance tier is suitable for applications that require high random read/write performance, for example, database indexes. Performance tier: delivers high-performance. As the cost of SAS disks is moderate and the capacity of a single SAS disk is large, the performance tier has good reliability and is suitable for general online applications. Capacity tier: delivers the lowest performance among the three tiers.\nAs the cost of NL-SAS disks is the lowest and the capacity of a single NL-SAS disk is large, the capacity tier is suitable for non-critical services, for example, backup. RAID Levels Consider the following when selecting RAID levels: Reliability Read/Write performance Disk utilization Different RAID levels provide different reliability, read/write performance, and disk utilization, as described in Table 2. Table 2 RAID levels RAID Level Redundancy and Data Recovery Capability Read Performance Write Performance Disk Utilization Maximum Number of Allowed Faulty Disks RAID 0 No data redundancy is provided and damaged data can not be recovered. High High The disk utilization is 100%. 0 RAID 1 High. RAID 1 provides completely redundancy. When a CK fails, the mirror CK can be used for data recovery. Relatively high Relatively low 2D : The disk utilization is about 50%. 4D: The disk utilization is about 25%. A maximum of N-1 disks can fail at the same time (in a RAID 1 disk array with N disks). RAID 3 Relatively high. Each CKG has one CK as the parity CK. Data on any data CK can be recovered using the parity CK. If two or more CKs fail, the RAID level fails. High Low RAID 3 supports flexible configurations. Specifically, a RAID 3 policy allows data block and parity block policies ranging from 2D+1P to 13D+1P. The following examples show disk utilization rates of several configurations commonly used by RAID 3: 4D + 1P : The disk utilization is about 80%.\n2D + 1P: The disk utilization is about 66.67%. 8D + 1P: The disk utilization is about 88.89%. 1 RAID 5 Relatively high. The parity data is distributed on different CKs. In each CKG, the parity data occupies space of a CK. RAID 5 allows the failure of only one CK. If two or more CKs fail, the RAID level fails. Relatively high Relatively high RAID 5 supports flexible configurations. Specifically, a RAID 5 policy allows data block and parity block policies ranging from 2D+1P to 13D+1P. The following examples show disk utilization rates of several configurations commonly used by RAID 5: 2D + 1P: The disk utilization is about 66.67%. 4D + 1P: The disk utilization is about 80%. 8D + 1P: The disk utilization is about 88.89%. 1 RAID 6 Relatively high. Two groups of parity data are distributed on different CKs. In each CKG, the parity data occupies space of two CKs. RAID 6 allows two CKs to fail simultaneously. If three or more CKs fail, the RAID level fails. Medium Medium RAID 6 supports flexible configurations. Specifically, a RAID 6 policy allows data block and parity block policies ranging from 2D+2P to 26D+2P. The following examples show disk utilization rates of several configurations commonly used by RAID 6: 2D + 2P: The disk utilization is about 50%. 4D + 2P: The disk utilization is about 66.67%. 8D + 2P: The disk utilization is about 80%. 16D + 2P: The disk utilization is about 88.89%. 2 RAID 10 High.\nDear all, Is cloud storage distributed? What is the difference between cloud storage and distributed storage? Thanks. Hello Phany, A distributed storage system is the infrastructure that can split data across multiple physical servers, and often across more than one data center. It typically takes the form of a cluster of storage units, with a mechanism for data synchronization and coordination between cluster nodes. Distributed storage systems can store several types of data: Files a distributed file system allows devices to mount a virtual drive, with the actual files distributed across several machines. Block storage a block storage system stores data in volumes known as blocks. This is an alternative to a file-based structure that provides higher performance. A common distributed block storage system is a Storage Area Network (SAN). Objects a distributed object storage system wraps data into objects, identified by a unique ID or hash. Distributed storage systems have several advantages: Scalability the primary motivation for distributing storage is to scale horizontally, adding more storage space by adding more storage nodes to the cluster. Redundancy distributed storage systems can store more than one copy of the same data, for high availability, backup, and disaster recovery purposes. Cost distributed storage makes it possible to use cheaper, commodity hardware to store large volumes of data at low cost. Performance distributed storage can offer better performance than a single server in some scenarios, for example, it can store data closer to its consumers, or enable massively parallel access to large files.\nHope this help: The terms \"data\" and \"information\" are sometimes misinterpreted as referring to the same thing. However, they are not the same. Data is a collection of . Those values can be , numbers, or any other . If those values are not processed, they have little meaning to a human. Information is data that was processed so a human can read, understand, and use it. The \"P\" in stands for \"processing,\" specifically, data processing. Processing data into information is the fundamental purpose of a computer. Information you of something. It answers a specific question. It represents a specific truth or fact. Data is the collection of recorded values from which information can be ascertained. For example, consider the question, \"what is the temperature outside?\" Data provides the basis for an answer to that question. If the data is \"25.6\" and \"Celsius,\" the answer is, \"Outside, the temperature is 25.6 degrees Celsius.\" You must know what \"temperature\" is, and what \"degrees Celsius\" are, to process the data into information. Some data is not relevant or informational. This irrelevant data is called . For example, if you create an audio recording of a piano concert, you might hear people in the audience coughing, or the sound of a ceiling fan. These noises are irrelevant to the purpose of the audio recording, which is to record the sound of the piano. Information is analogous to a . In the example above, the relevant data is the sound of the piano.\nColleagues, good afternoon. We have prepared for you a new part of training materials on the HCIE Storage certification. This course is \"Storage Data Migration Solution Application Practice\". This document describes the background, practice mode, scenario, questions, tasks and suggested answers for Storage Data Migration. Five years ago, group company M purchased a Huawei storage device to store its enterprise business data and management system data. As its business expands and its data volume soars, the original storage device is becoming increasingly unable to meet the needs of its business growth in terms of capacity and performance. Group company M now has purchased the latest Huawei storage devices and needs to migrate data from the original storage device to the latest storage devices. Currently, the old storage system stores a large amount of data. In the entire migration process, group company M hopes that services can only be temporarily interrupted and the host performance is not affected. Some data in the OA system needs to be migrated to the public cloud. Note: Cases in this course are examples only. The actual configurations may vary according to actual environments. For details, see the corresponding product documentation. The study material will be attached to the post and will be broken down into parts so that you can study the materials sequentially. We draw your attention to the fact that the material is in English, this is very useful since the HCIE level certification will also take place in English.\nHello, everyone! What are the basic concepts of storage protocols?In this post, we systematically describe some basic concepts of storage. The term RAID was invented by David Patterson, Garth A. Gibson, and Randy Katz atthe University of California, Berkeley in 1987. It combines multiple independent physicaldisks into a virtual logical disk using related algorithms to provide larger capacity, higherperformance, and better error tolerance capabilities. RAID Related Posts: RAID 0, RAID 1, RAID 5, RAID 6 and RAID 10 Basic Knowledge of RAID What is RAID 2.0+ One or multiple logical volumes can be created for RAID based on the specifiedcapacity. A logical volume is identified by logical unit number (LUN). To prevent single points offailure, the high-reliability systemprovides redundancy backup fordevices that may encountersingle points of failure. Pathredundancy is also included. The multipathing technology canbe used to ensure reliable use ofredundant paths. Thistechnology automatically andtransparently transfers I/O flowsto other available paths, ensuringeffective and reliabletransmission of I/O flows. File system: refers to a data structure and adata management mode when files arestored on disks.Therefore, it is necessary to correlatesectors so that data on disks can beaccessed. In other words, a logical datastorage structure must be established. A filesystem is used to establish such datastorage structure. Generally, the process ofcreating file systems on disks is calledformatting. File systems and application programs are on a same server. NFS and CIFS are universalnetwork file systems. Thesesystems can be used toimplement file sharingbetween heterogeneousplatforms.\nThe Common Internet File System (CIFS) is a mainstreamshare file system developed by Microsoft for servingheterogeneous platforms and is mainly applied in Windows.Client systems use the TCP or IP protocol to request fileaccess services from server systems over a network.CIFS share authentication provides two types of shared fileaccess permissions: user and Active Directory Server(ADS). The CIFS normal share means that the file system is sharedas a directory and all users can access the directory.The CIFS homedir share is a file sharing mode provided byfile engines. The CIFS homedir share only allows a user toaccess the directory named with the user name and eachuser can only access a directory that belongs to the user'sdirectory. Network File System (NFS) is a distributed filesystem protocol. It allows a user on a client computer to accessfiles over a computer network much like localstorage is accessed. NFS, like many otherprotocols, builds on the Open NetworkComputing Remote Procedure Call (ONCRPC) system. The NFS is an open standarddefined in a Request for Comments (RFC),allowing anyone to implement the protocol. Input/output operations per second(IOPS), that is, read and writeoperations (I/Os) per second, is aperformance index in evaluating therandom access performance ofdatabases.The IOPS is a standard formeasuring the performance of a SANstorage system. A larger IOPSindicates better performance. SPC is an internationally recognizedauthoritative, third-party, and nonprofit storage performance testorganization. Currently, vendors in thestorage industry, such as Huawei,IBM, HP, Sun, HDS, and Dell, areimportant members of SPC. SPC-1 isan industry-recognized storageperformance benchmark test standardlaunched by SPC.\nHello all, This post is about fully interconnected disk enclosures in dorado V6. The storage system supports three types of disk enclosures, which are SAS, smart SAS, and smart NVMe disk enclosures. Currently, they cannot be used together on one storage system. Smart SAS and smart NVMe disk enclosures use the same networking mode. In this networking, a controller enclosure uses the shared 2-port 100 Gbit/s RDMA interface modules to connect to a disk enclosure. Each interface module connects to the four controllers in the controller enclosure through PCIe 3.0 x16. In this way, each disk enclosure can be simultaneously accessed by all four controllers, achieving full interconnection between the disk enclosure and the four controllers. A smart disk enclosure has two groups of uplink ports and can connect to two controller enclosures at the same time. This allows the two controller enclosures (eight controllers) to simultaneously access a disk enclosure, implementing full interconnection between the disk enclosure and eight controllers. Figure 1 Full interconnection between disk enclosures and eight controllers When full interconnection between disk enclosures and eight controllers is implemented and the HyperMetro-Inner feature (which creates three cache copies and performs continuous mirroring) is used, the system can tolerate failure of 7 out of 8 controllers. When SAS disk enclosures are used, they connect to controller enclosures using SAS shared interface modules. Each SAS shared interface module has a built-in SAS switch with 32 protocol channels, which provides eight 12 Gbit/s SAS ports (each port provides 4x12 Gbit/s rate).\nTo disable some of the VAAI primitivesfor troubleshooting purposes, for examplesee the following CLI commands, which detail how to enable and disable VAAI primitives. ATS To check the status of the ATS primitive and to turn it on and off at the command line, the following commands can be used: # esxcli system settings advanced list --option /VMFS3/HardwareAcceleratedLocking Value of HardwareAcceleratedLocking is 1 # esxcli system settings advanced set --int-value 0 --option /VMFS3/ HardwareAcceleratedLocking # esxcli system settings advanced list --option /VMFS3/HardwareAcceleratedLocking Value of HardwareAcceleratedLocking is 0 XCOPY To turn the Extended Copy primitive for cloning on and off, use the previous command with the following advanced setting: /DataMover/HardwareAcceleratedMove WRITE_SAME To turn the Write Same primitive for zeroing blocks on and off, use the following advanced setting: /DataMover/HardwareAcceleratedInit UNMAP To turn the UNMAP primitive for space reclamation on and off, use the following advanced setting: /VMFS3/EnableBlockDelete In vSphere 4.1, it was possible to define how often an ESXi host verified whether hardware acceleration was supported on the storage array. # esxcfg-advcfg -g /DataMover/HardwareAcceleratedMoveFrequency Value of HardwareAcceleratedMoveFrequency is 16384 This means that if at initial deployment, an array does not support the offload primitives, but at a later date the firmware on the arrays is upgraded and the offload primitives become supported, nothing must be done regarding ESXiit automatically will start to use the offload primitives. The value relates to the number of I/O requests that occur before another offload is attempted.\nHello everyone, Have a good day! As economies come to grips with the pandemic and emerge from it, they need to consider how to transform their industries to be competitive in the new normal. ICT will be at the forefront of any organizations strategy from now on. It is imperative that policy makers understand how the competitive landscape has changed and how to build a national competitive advantage. American Economist Michael Porter argues that an economys competitive advantagexi is driven by a differentiating combination of basic and more advanced national factor endowments nurtured in the context of local demand and related local suppliers, all working together for the national good. Basic factor endowments are typically hard to develop or acquire. For example: Natural resources, such as oil and gas in Brunei and Saudi Arabia, copper in Chile, and sand in Egypt Climate that makes certain industries attractive, such as agriculture in Canada and tourism in tropical countries Geographical locations that influence the movement of goods and people, such as Singapore and the UAE Demographics, such as the large populations of China and India or the highly skilled population of the Czech Republic Basic factors are inherited and require little or no new investment to be utilized in the production process. Factor endowments are not static. With education and training, for example, the characteristics of the labor force can change. Basic factors alone do not explain how countries such as Singapore, Japan, and Ireland can grow their economies beyond the advantages that basic factors confer.\nColleagues, good afternoon. We have prepared for you a new part of training materials on the HCIE Storage certification. This course is \"Storage Backup Solution Application Practice\". This document describes the background, practice mode, scenario, questions, tasks and suggested answers for Storage Backup. The HQ data center of group X, which is located in area A, uses a hybrid data center architecture that combines physical hardware and virtual resources. Currently, group X has two branches. Each branch has its own small-sized data center, which also uses the hybrid data center architecture that combines physical hardware and virtual resources. Currently, the group headquarters uses the old backup software to perform simple data backup, and the branches do not plan more specific data backup protection measures. After conducting thorough discussion and market research, the group decides to purchase a new set of IT devices and backup solution to upgrade the business system. The initial product list includes Huawei all-flash storage devices and backup software. Some R&D and test applications want to use HyperSnap and HyperCDP of Huawei storage systems to test file system backup. In the future, the backup software will be used to centrally back up key business systems so that the backup and recovery for key services needs to be completed within seconds. Note: Cases in this course are examples only. The actual configuration may vary according to operating environments. For details, see the corresponding product documentation.\nDear team, What is SMB Protocol? How does it work? Thank you. Dear Lucas, Have a good day. The Server Message Block Protocol (SMB Protocol) is a client-server communication protocol used for sharing access to files, printers, serial ports, and data on a network. It can also carry transaction protocols for authenticated inter-process communication. In short, the SMB protocol is a way for computers to talk to each other. SMB works through a client-server approach, where a client makes specific requests and the server responds accordingly. This is known as a response-request protocol. Once connected, it enables users or applications to make requests to a file server and access resources like printer sharing, mail slots, and named pipes on the remote server. This means a user of application can open, read, move, create, and update files on the remote server. Thank you. Dear Lucas, Have a good day. The Server Message Block Protocol (SMB Protocol) is a client-server communication protocol used for sharing access to files, printers, serial ports, and data on a network. It can also carry transaction protocols for authenticated inter-process communication. In short, the SMB protocol is a way for computers to talk to each other. SMB works through a client-server approach, where a client makes specific requests and the server responds accordingly. This is known as a response-request protocol. Once connected, it enables users or applications to make requests to a file server and access resources like printer sharing, mail slots, and named pipes on the remote server.\nHello, everyone! What are the basic concepts of storage protocols?In this post, we systematically describe some basic concepts of storage. Small Computer System Interface (SCSI) isthe most common method for connectingstorage devices to servers.SCSI was first developed in 1979 and is aninterface technology for mid-range computers.With the development of computertechnologies, SCSI is now completelytransplanted to ordinary PCs.SCSI-3 is the basis of all storage protocols,because all storage protocols use the SCSIinstruction set. Internet Small Computer System Interface(iSCSI) is an Internet Protocol (IP)-basedstorage networking standard for linking datastorage facilities. It provides block-level accessto storage devices by carrying SCSIcommands over a TCP/IP network.Used over the IP-based SAN, the iSCSIprotocol provides quick, cost-effective, andlong-distance storage solutions.iSCSI encapsulates SCSI commands into aTCP or IP packet, enabling I/O data blocks tobe transferred over the IP network. Fibre Channel is a high-speed data transfer protocolproviding in-order, lossless delivery of raw block data.Fibre Channel is primarily used to connect computer datastorage to servers in storage area networks (SAN) incommercial data centers. Fibre Channel networks form aswitched fabric because the switches in a network operatein unison as one big switch. Fibre Channel typically runson optical fiber cables within and between data centers,but can also run on copper cabling.Fibre Channel is a high-performance serial connectionstandard. The interface transmission rate can be 16 Gbit/sor 32 Gbit/s. The transmission media can be copper cablesor optical fibers. The transmission distance is long andmultiple interconnection topologies are supported.\nHello team, The available and used storage pool capacities queried on DeviceManager or the CLI are different when the global deduplication and compression function is enabled and disabled. In the preceding figures, the first figure shows the storage pool capacity information displayed on DeviceManager when global deduplication and compression is enabled, and the second figure shows that when global deduplication and compression is disabled. Thanks. Dear Mumammad, The storage pool capacities displayed on the Home page of DeviceManager are calculated based on the storage pool capacity data reported by the EDS process. However, the calculation methods are different when global deduplication and compression is enabled and disabled. As a result, the storage pool capacity information displayed on DeviceManager varies with the enablement status of global deduplication and compression. The storage pool capacity information reported by the EDS process contains the following five values: Total available capacity of the storage pool (E1) Writable capacity of the storage pool before deduplication and compression (E2) Writable capacity of the storage pool after deduplication (E3) Writable capacity of the storage pool after compression (E4) Metadata capacity of the storage pool (E5) Among the preceding five values, E1 is fixed after the storage pool is created, and E2, E3, E4, and E5 increase as the amount of written data increases.\nHello all, What are front-end interconnect I/O modules(FIMs), and how do they work? This post will answer it. Huawei OceanStor Dorado 8000 V6 and Dorado 18000 V6 all-flash storage systems support front-end interconnect I/O modules (FIMs). Each FIM connects to four controllers through four PCIe 3.0 links. Hosts can connect to any port on an FIM to access four controllers at the same time. On Huawei OceanStor Dorado 8000 V6 and Dorado 18000 V6 all-flash storage systems: The 8 Gbit/s, 16 Gbit/s, and 32 Gbit/s Fibre Channel interface modules are FIMs. iSCSI networking does not support FIMs. An FIM intelligently identifies host I/Os and distributes the I/Os based on specific rules. In this way, host I/Os are sent directly to the most appropriate controller without pretreatment of the controllers, preventing forwarding between controllers. Because each port of FIMs has internal connections to all of the four controllers, host I/Os can be delivered to the most appropriate controller even if there is only one link between the host and the storage system. Moreover, services are not affected in the event of a controller failure. Figure 1 shows the working principles of a Fibre Channel FIM. Each FIM provides four physical ports, each of which has only one WWN. A host sets up one external session connection with each port, and each port establishes four internal connections with the controllers.\nFrom the perspective of the host, it establishes only one connection with the storage system; from the perspective of the controllers, each controller has established a connection with the host. The FIM performs protocol and connection processing and distributes host I/Os to the four links by the intelligent distribution algorithm. Figure 1 Working principles of a Fibre Channel FIM FIMs simplify the system networking. Without FIMs, at least four connections are required between a host and a four-controller system. With an FIM, only one connection is required for the host to connect to all of the four controllers (two or more connections are recommended for redundancy). When FIMs are used, failure of a controller will not disconnect front-end ports from hosts, and the hosts are unaware of the controller failure, ensuring high availability. If a controller fails, the FIM port chip detects that the PCIe link between the port and the controller is disconnected. Then the FIM redistributes host I/Os to other controllers, achieving within-asecond service switchover without any impact on host services. Without FIMs, a link switchover must be performed by the host's multipathing software in the event of a controller failure, which takes a longer time (10 to 30 seconds) and reduces reliability. In Figure 2 , if controller 1 is faulty, the FIM redistributes host I/Os to other functioning controllers to complete service switchover.\nHello all, Have a good day. The good news is that OceanStor Pacific mass data storage wins interop best of show award grand prize. The 2021 Interop Tokyo Conference, Japan's largest ICT exhibition, announced the latest news: After a number of rigorous reviews by leading IT experts, the Interop Organizing Committee announced the Best of Show Award for outstanding showcased products at the conference. Huawei OceanStor Pacific series won the Best of Show Award Gold Award in the server and storage field for its unique technical advantages such as efficiency and performance. Huawei OceanStor Pacific series builds next-generation high-performance data analytics (HPDA) storage based on multi-protocol interworking, hybrid-load-oriented, and ultra-high-density design. Through continuous innovation, the OceanStor Pacific series helps users cope with industry upgrades and unlock data potential. Huawei OceanStor Pacific Series Storage Wins Interop Tokyo 2021 Best of Show Award for Server and Storage A series of forums and seminars will be launched at the same time to track the cutting edge of science and technology development and industry trends. At the seminar on OceanStor Pacific series, Huawei introduced the features of OceanStor Pacific series storage products. It uses the industry's first lossless protocol interworking technology and allows multiple protocols, such as POSIX/MPI/NFS/CIFS/HDFS/S3, to access the same file, implementing zero data copy in the entire process and improving process and storage efficiency. It also supports multiple service loads. One set of storage can provide high bandwidth for large files and high IOPS for small files, bearing full-process data processing.\nHi all, Do you knowWhat is FC switch? How it works? Thanks. Dear Axe, A Fibre Channel (FC) SAN is a specialized high-speed network that connects host servers to storage systems. The FC SAN components include HBAs in the host servers, switches that help route storage traffic, cables, storage processors (SPs), and storage disk arrays. To transfer traffic from host servers to shared storage, the FC SAN uses the Fibre Channel protocol that packages SCSI commands into Fibre Channel frames. Ports in FC SAN Each node in the SAN, such as a host, a storage device, or a fabric component has one or more ports that connect it to the SAN. Ports are identified in a number of ways, such as by: World Wide Port Name (WWPN) A globally unique identifier for a port that allows certain applications to access the port. The FC switches discover the WWPN of a device or host and assign a port address to the device. Port_ID (or port address) Within a SAN, each port has a unique port ID that serves as the FC address for the port. This unique ID enables routing of data through the SAN to that port. The FC switches assign the port ID when the device logs in to the fabric. The port ID is valid only when the device is logged on. Zoning Zoning provides access control in the SAN topology. Zoning defines which HBAs can connect to which targets.\nHi there, Community! This post talks about the storage IOPS and throughput calculation. Please read further down for more details on the topic. The bottleneck of the storage system is mainly reflected in two aspects: throughput and IOPS. Throughput English: throughput - the amount of data read or written per unit time. IOPS English full spell: Input / Output Operations Per Second - the number of read / write (I / O) operations per second, mostly used in databases (and other occasions) to measure the performance of random access. The IOPS performance of the storage side is different from that of the host side. IOPS refers to how many times the host can receive accesses per second. The host's IO needs to access the storage multiple times to complete. For example, if a host writes a minimum data block, it also has to go through three steps: 'send write request, write data, receive write acknowledgement'; that is, three memory accesses. The throughput and IOPS distribution are analyzed below. 1. Throughput (throughput) The throughput depends primarily on the architecture of the array, the size of the fiber (FC SAN) or network (IP SAN) channel and the number of hard disks. The architecture of the array is different from that of each array. They also have internal bandwidth (similar to the system bus of pc), but in general, the internal bandwidth is designed enough, not the bottleneck. The impact of Fibre Channel is still relatively large.\nFor example, in a data warehouse environment, the traffic demand for data is very large. The maximum traffic that can be supported by a 2Gb fiber card should be 2Gb/8 (small B) = 250MB/s ( The actual flow rate of the big B), when 4 fiber cards can reach the actual flow of 1GB / s, so the data warehouse environment can consider changing the 4Gb fiber card. Finally, let's talk about the limitations of the hard disk. Here is the most important. When the current bottleneck no longer exists, it depends on the number of hard disks. I will list the traffic volume that different hard disks can support: 10 K rpm 15 K rpm ATA 10M/s 13M/s 8M/s Then, assuming that a set of 120 15K rpm fiber-optic hard disks is stored, the maximum flow that the storage can support is 120*13=1560MB/s. If it is a 2Gb fiber-optic card, it may take 6 pieces to be able to use 4Gb fiber. Card, 3-4 blocks is enough. 2. IOPS (Input / Output Operations Per Second) The decision on IOPS depends mainly on the algorithm of the array, the cache hit ratio, and the number of disks. The algorithm of the array is different because of different arrays. For example, we have recently encountered hds usp. Maybe because ldev (lun) has queue or resource limitation, the single ldev iops will not go up, so before using this storage. It is necessary to understand some of the algorithm rules and restrictions of this storage.\nThe hit rate of the cache depends on the distribution of the data, the size of the cache size, the rules of data access, and the algorithm of the cache. If it is completely discussed, it will become very complicated and can be discussed one day. I only emphasize the hit rate of a cache here. If an array, the higher the hit rate of the read cache, the better, generally indicating that it can support more IOPS. Why do you say this? This is related to the hard disk IOPS we will discuss below. Relationship. The limitation of the hard disk, the IOPS that each physical hard disk can handle is limited, such as: 10 K rpm 15 K rpm ATA 100 150 50 Similarly, if an array has 120 15K rpm fiber-optic hard drives, then the maximum IOPS it can support is 120*150=18000. This is the theoretical value of the hardware limit. If this value is exceeded, the response of the hard disk may become very slow and unable to provide business normally. On raid5 and raid10, there is no difference in reading iops, but the same business writes IOPS and the IOPS that fall on the disk are different. We evaluate the IOPS of the disk. If the disk limit is reached, the performance must be gone.\nThen we assume a case, the business IOPS is 10000, the read cache hit rate is 30%, the read IOPS is 60%, the write IOPS is 40%, the number of disks is 120, then the calculation is in the case of raid5 and raid10, respectively. What is the IOPS of the disks? Raid5: Iops of a single disc = (10000*(1-0.3)*0.6 + 4 * (10000*0.4))/120 = (4200 + 16000)/120 = 168 Here 10000*(1-0.3)*0.6 means that it is a read IOPS, the ratio is 0.6, except for the cache hit, there are actually only 4200 IOPS. And 4 * (10000 * 0.4) means write IOPS, because every write, in raid5, actually happened 4 io, so the IOPS written is 16000. In order to consider the raid5 in the write operation, the two read operations may also be in life, so the more accurate calculation is: Iops of a single disc = (10000*(1-0.3)*0.6 + 2 * (10000*0.4)*(1-0.3) + 2 * (10000*0.4))/120 = (4200 + 5600 + 8000)/120 = 148 Calculated the IOPS of a single disk to 148, basically reaching the disk limit: Raid10 Iops of a single disc = (10000*(1-0.3)*0.6 + 2 * (10000*0.4))/120 = (4200 + 8000)/120 = 102 As you can see, because raid10 only takes 2 times for a write operation, the same pressure, the same disk, only 102 IOPS per disk, is far below the limit of the disk IOPS.\nHello everyone, This post is talking about the eGovernment Cloud in West Africa. Based on Huaweis eGovernment cloud, NOSi developed more than 150 websites and 77 types of eGovernment software, covering social security, electronic elections, budget management, distance education and healthcare, and Enterprise Resource PlanningERPfor all government departments, schools, hospitals, and state-owned enterprises in Cape Verde. NOSi also provided eGovernment applications and data center hosting services for surrounding countries, including Equatorial Guinea, Mozambique, Burkina Faso, Guinea-Bissau, So Tome, and Principe. Major NOSi eGovernment applications and websites included the following: Government Resource Integration and Planning Framework (Integrated Government Resource Planning, IGRP) Financial Information System (SIGOF) Free Network Access Service (Konekta) Social Welfare System (SIPS) Medical Information System (SIS) Geographic Information System (GIS) Portal (Porton dinos ilha) Online Certificate System (Online-Certification) National System of Identity and Civil Identification (SNIAC) Land Registration Special Management System Municipal Information System (MIS) Student Information Management System Take the IGRP as an example. Developers can use a variety of pre-integrated application modules and components to quickly build upper-layer application software, improve the efficiency of the governments public departments, avoid duplicate resource investment, minimize public management costs, and maximize Return On Investment (ROI). With these capabilities, the IGRP earned the title eGov Software Maker from NOSis President. Another example is the Medical Information System (SIS). It is a connection module used to manage hospitals, monitor the population status, and improve institutions functional capabilities.\nThe SIS manages pharmaceuticals, clinical equipment, materials, laboratory diagnosis, and reservations (analyzing a hospitals appointment information through the Internet and making schedules for doctors based on the results), and collects statistics on hospitalizations, appointments, and deaths. Antonio Joaquim Fernandes, NOSis President, said, Huawei provides valuable support for the national data center, data transmission network, and eGovernment construction in Cape Verde. It provides data, voice, and videoconferencing services for government departments and public institutions and delivers an innovative digital platform to help NOSi build an eGovernment platform. Based on the digital platform, we will develop the business center, enterprise incubation center, and training center to build a leading information service platform in Africa for Cape Verde. According to the 2017 International Telecommunication Union (ITU) report, the ICT Development Index (IDI) of Cape Verde ranked No. 4 in Africa, far higher than that of coastal countries such as Nigeria, Angola, Gambia, and Mozambique. Under the regional ICT hub strategy of Cape Verde, NOSi has delivered eGovernment applications and services to neighboring countries in West Africa based on its ICT infrastructure and capabilities and attracted government delegations from more than 40 countries. Currently, every organization, including each government, is in a critical period of digital transformation. Huawei is looking forward to bringing digital to every organization for a fully connected, intelligent world. It is evident that the construction of Cape Verdes eGovernment cloud is a necessary step for government, education, medical institutions, and enterprises in Cape Verde to enter a smart world.\nHello everyone, This post is talking about the digital world to ten volcanic islands. The second phase of the eGovernment project further upgraded the ICT infrastructure based on the achievements of the first phase. To be specific, Huawei performed the following: Deployed new IT devices and system software and transformed the old data center into the disaster recovery center, providing secure and reliable IT leasing services for government agencies and enterprises in Cape Verde through an active-active data center. Deployed internal office networks and videoconferencing systems for the government, schools, and hospitals in Cape Verde to expand the office informatization coverage in those places and improve the efficiency and quality of government administration, education, and medical services. Jointly developed the integrated ICT training system WebLab with the Cape Verde Ministry of Education to support ICT talent cultivation in Cape Verde and promote social information sharing and development. In terms of cloud data center capacity expansion, Huawei built 1,000 VMs for customers and upgraded the system from 480-core CPUs with 400 TB of storage capacity to 1,656-core CPUs with 1,000 TB of storage capacity. If the national data centers demands for VMs continues to grow at the same annual rate (60 percent) as that from 2011 to 2015, the capacity expansion implemented this time could meet the business development requirements in the next five years.\nIn addition, Huawei provided FusionCloud desktop cloud systems for government agencies and national informatization training centers, solving key government administration problems such as incomplete information protection, low-efficiency maintenance, insufficient resource usage, and difficult network isolation and switchover. Based on the one cloud, one lake, and one platform architecture, the Huawei eGovernment Cloud solution provides the NOSi with shared basic resources, open data support platforms, rich smart government administration applications, comprehensive eGovernment services, strong security assurance, and efficient O&M service assurance. Those services helped remove data barriers between departments, build cloud platform-based and cross-department data sharing and exchange platforms, and deliver ICT infrastructure to enable the proactive and efficient one-stop work mode of government agencies and enterprises in Cape Verde. Similar to many African countries, Cape Verde suffered from unevenly distributed public resources, with one third of the countrys schools in three cities (the capital Praia, the port city of Mindelo, and Santa Catarina) and 58.6 percent of the hospitals on two islands (Santiago and Santo Anto). The Cape Verde eGovernment network was dedicated to connecting 1,142 organizations across the country through the same network. It used 530 routers and 669 switches provided by Huawei to expand the network built in phase one and allowed access from schools, medical institutions, government agencies, and enterprises in small and medium-sized cities and towns, and built data transmission pipelines for upper-layer applications. The network infrastructure broke geographical separation and brought network and eGovernment benefits to people in remote areas.\nHello all, This post is talking about the constructor of Cloud-Pipe-Device infrastructure in Cape Verde. In the first phase of the eGovernment project, Huawei completed the following: Delivered a national data center with 54 IT standard cabinets covering 200 square meters to the government of Cape Verde, providing information services for not only the government, enterprises, and institutions of Cape Verde, but also surrounding countries. Built intra- and inter-island backbone networks, metropolitan area networks, and wireless broadband access networks; constructed a fiber backbone ring using Dense Wavelength-Division Multiplexing (DWDM) technology on six major islands to upgrade Synchronous Digital Hierarchy (SDH) capacity from 622 MB to 20 GB; and provided broadband access service through the construction of Worldwide Interoperability for Microwave Access (WiMAX) to achieve the network coverage for some organizations throughout the country. Established 21 telepresence videoconferencing systems, giving the government the convenience of remote conferences. The phase-1 project construction effectively improved the national information and communication technology level of Cape Verde, which was a solid step towards eGovernment and social informatization. Huaweis continuous innovation in the cloud data center domain also impressed NOSi. Huawei employed the one cloud, one lake, and one platform architecture to assist customers in various industries in accelerating information system integration and sharing, thereby creating business value: One cloud : A converged , which implements unified delivery, management, and services of the infrastructure through intensive construction.\nHello al, This post is talking abouteGovernment project in Cape Verde. At the westernmost edge of the world map, there is a small dot Cape Verde (Portuguese: Repblica de Cabo Verde) in the Atlantic between the edge of the African continent and the map frame. Cape Verde, a volcano archipelago located in the mid-Atlantic Ocean, is composed of 10 volcanic islands and has a coastline of 965 kilometers. Cape Verde suffers from poor industry and agriculture due to its unique geographical location; however, the service industry is extremely robust, accounting for more than 70 percent of the countrys GDP and proposing strong demands for information technology development. With informatization as a national strategy, the Cape Verde government is committed to building a more people-oriented government, creating more business opportunities to improve the competitiveness of Cape Verde, developing an open economy to promote economic development, and alleviating poverty through information communication and network technologies. In recent years, many West African countries have built national data centers for informatization technology advances. However, due to lack of application software development capabilities, ICT talent, and an ICT ecosystem, many data centers have no load. The government of Cape Verde expects to change this situation. Through the implementation of the eGovernment project, the government of Cape Verde is attempting to build a nationwide eGovernment office network and a national data center.\nThe purpose is to greatly improve government office efficiency, promote the sharing of education, medical care, and other types of resources, improve Cape Verdes informatization level, and build the country into an information hub for West Africas coastal countries, as well as a lighthouse in West Africa. NOSi, Cape Verdes Operational Information Society Nucleus, initiated and implemented the eGovernment project and was responsible for service development and O&M after the eGovernment system was built. With 19 years of experience in eGovernment operations and development, NOSi has strong capabilities in eGovernment application software development and ICT technologies. The first phase of the eGovernment project was initiated in 2010 and delivered in 2014, which mainly included the construction of a national data center and an upgrade of the government communications network. This project phase completed the preliminary establishment of the national government network system platform and island interconnection network platform. Based on these achievements, NOSi initiated the deployment of the government informatization system. With the gradual emergence of new eGovernment applications in Cape Verde and the rapid growth of service leasing to third parties, the national data center, with only 200 Virtual Machines (VMs) built in the first phase, was fully loaded, leaving no available space for new applications or services. Organizations in areas that were not connected to the network were still using a paper-based working mode, leading to poor archival management, low work efficiency, and great difficulties in statistics collection and management.\nHello all, I want to share the datasheet ofOceanStor 9000 V5 Scale-Out NAS. Huawei OceanStor 9000 V5 scale-out NAS storage system features a fully symmetric distributed architecture and extensive scale-out capabilities that deliver superior performance and provide a superlarge single file system for shared storage of unstructured data. OceanStor 9000 V5 is ideal for diverse applications and storage resource sharing fields, such as the video surveillance, entertainment, education, and energy industries, as well as research fields covering satellite mapping, gene sequencing, backup and archiving. High-performance read/write access : Achieves up to 2.8 GB/s of bandwidth per node and industryleading performance on a single disk. When flash disks are configured, the bandwidth of a single nodereaches up to 5 GB/s. Network acceleration : Supports 10GE, 25GE, 40GE, InfiniBand and a variety of other networkingmodels; supports Remote Direct Memory Access (RDMA) and TCP Offload Engine (TOE) to improvetransmission performance. Linear scalability : Linear increase in system performance as nodes are added, with up to 700 GB/sof bandwidth. When flash disks are configured, the system bandwidth can up to TB/s level. Mixed storage of videos and pictures : The A single node supports 1,200-channel video recording at2 Mbit/s or 800-channel video recording at 4 Mbit/s, stores up to 7,000 pictures per second, and enablestens of millions of pictures to be retrieved within seconds. It supports Huawei or third-party streamingmedia software. Single file system : A single file system of more than 140 PB simplifies management andmaintenance while eliminating data silos caused by multiple namespaces.\nImpressive expansion capabilities : Seamless expansion from 3 to 288 nodes enables linearexpansion of capacity and performance. Even data distribution : The shared-nothing symmetric distributed architecture evenly distributesdata and metadata to all nodes, eliminating system bottlenecks. Ultra-high utilization : Besides support for high reliability, EC redundancy permits a maximum diskutilization of 95%. Support for multiple types of interfaces : NFS, CIFS, NDMP, FTP, OpenStack Manila and otherinterfaces enable the system to support diverse applications and implement data managementthroughout the entire lifecycle. Support for varied node types : To suit different applications, various types of nodes are supported. Integrated management : One set of software centrally manages IT devices, provides analysisreports, simplifies management, and improves operational efficiency. Flexible configuration : Directory-based redundancy ratio policies provide multiple data protectionlevels. Automatic statistics collection and analysis : Automatic performance statistics collection andanalysis help customers use resources efficiently. Automatic deployment : The software platform is automatically deployed and configured, and theone-click capacity expansion feature enables customers to add a single node in just 60 seconds. Rights management : Access control over IP addresses, users, and user groups ensure that storagepools are secure and mutually isolated. Client connection loads are balanced across nodes and automatic balancing between capacity andperformance is implemented to optimize cluster resources. Intelligent unified management is implemented with support for node failovers and failbacks. Load balancing is implemented based on domain names, and a variety of load-balancing policies aresupported.\nInfoTier dynamically stores data on different nodes based on data access frequency and implementsintelligent migration of frequently accessed data (hot data). The software fully leverages theadvantages of different types of storage media and reduces the Total Cost of Ownership (TCO). A variety of data migration policies and migration priorities are supported to accommodate everchanging service needs. Space quota management is implemented based on users, user groups, and directories, to meetdifferent customer requirements. The software allows flexible and easy access to storage space, with quota nesting management. Erasure Coding (EC) technology implements N+M data protection and protects data from aconcurrent failure of four nodes. Automatic reconstruction is available, allowing multiple nodes to concurrently reconstruct data at aspeed of up to 2 TB/hour. InfoLocker offers protection against data loss, malicious modification, and deletion. The software allows users to set a WORM clock and protection period. InfoReplicator shortens the system recovery time and is applicable to disaster recovery, data backup,and long-distance data migration scenarios. The software supports 1:N and N:1 replication for different types of directories. Restoration capabilities ensure that the failure of multiple disks does not affect video streaming. Onlydata on the failed or damaged disks is lost. In video surveillance scenarios, video and image streams can be directly written into the storagesystem without passing through other transit servers. Single-node deployment supports streaming media access. Video buffering is available. That is, the front-end device can send the videos stored on the storagemedium to the platform after the connection is restored.\nHi everyone, Do you know how to show the IO performance for Linux? Dont worry, I will tell you how to deal with this. The iostat command is used for monitoring system input/output device loading by observing the running time of the devices and their average transfer rates. The iostat command generates reports that can be used to change system configuration to better balance the input/output load between physical disks. The first report generated by the iostat command provides statistics concerning the time since the system was booted. Each subsequent report covers the time since the previous report. All statistics are reported each time the iostat command is run. The report consists of a CPU header row followed by a row of CPU statistics. On multiprocessor systems, CPU statistics are calculated system-wide as averages among all processors. A device header row is displayed followed by a line of statistics for each device that is configured. When option -n is used, an NFS header row is displayed followed by a line of statistics for each network filesystem that is mounted. Theintervalparameter specifies the amount of time in seconds between each report. The first report contains statistics for the time since system startup (boot). Each subsequent report contains statistics collected during the interval since the previous report. Thecountparameter can be specified in conjunction with theintervalparameter. If thecountparameter is specified, the value ofcountdetermines the number of reports generated atintervalseconds apart. If theintervalparameter is specified without thecountparameter, the iostat command generates reports continuously.\nDear all, Follow those steps to change the WiFi password and username in Huawei B315s-607. 1. Connect to Wi-Fi using the default password on the modem. Your default Wi-Fi password is the Wi-Fi Key, which is at the bottom of the modem as shown below. 2. Once connected you will need to open the internet browser, and enter IP Link: 192.168.8.1 in the address box. 3. Enter the user name and password to log in to the web management page. Note: The default User name is admin The default password is admin 4. Once logged in, it will directly go to Quick Setup Configure WLAN Settings. Please note if you want to change your Wi-Fi Name Select the SSID box as highlighted below and Rename it. 5. Changing Wi-Fi Password: In Order to change your Wi-Fi password, Select Modify Password and enter your password in the WLAN Key box as shown below. Once you have entered the password click on next. It will move on to Update Configuration leave it on Auto-Update and click on Next. 6. The last step on Quick Setup requires you to Modify Password for your modem login. Note: The modem password that you had entered before was admin . The current Password is admin . New Password you will enter your new modem login. (Please note your new modem password in case you want to login again to change the password). Also, note that your new password strength should be high. 7. Once done click on Finish and Restart your modem.\nHello all, This post will give you a view of OceanStor 9000 V5 product. As a big data storage product, OceanStor 9000 V5 provides advantages such as large capacity, high performance, and flexible scalability as well as a range of value-added features. It can be applied to various fields such as broadcasting, media, high-performance computing (HPC), Internet operation, data centers, and large-sized enterprises. Big data is high-volume, high-velocity, and high-variety information assets that demand an effective information processing mode for enhanced insight, decision making, and process optimization. Features of big data are: volume, velocity, and variety. The arrival of the big data era places the following requirements on storage systems: Volume includes capacity and quantity and requires flexible storage scalability. Velocity requests distributed processing and storage, a scale-out architecture, common hardware, and high performance with controllable costs. Variety requires storage to provide a variety of interfaces to address different storage requirements. OceanStor 9000 V5 is a big data storage platform developed based on big data features. OceanStor 9000 V5 adopts a fully symmetric architecture, provides seamless scale-out of 3 to 288 nodes, and supports the following functions: Provides Huawei-developed distributed file system OceanStor DFS to store massive unstructured data and support global unified namespace. Interconnects with FusionInsight Hadoop and Cloudera Hadoop consisting of open source Hadoop components to help users build enterprise-class big data analysis platforms easily. OceanStor 9000 V5 employs a networking mechanism that features full mesh and redundancy, allowing any nodes to concurrently access any data.\nMultiple nodes can concurrently access different areas of the same data, achieving high-performance data read and write. OceanStor 9000 V5 provides standard NFS, CIFS, and FTP sharing services. Figure 1 shows the system architecture of OceanStor 9000 V5. Figure 1 Converged system architecture OceanStor 9000 V5 delivers industry-leading read/write performance that fully addresses the performance requirements of different fields such as media editing and HPC. OceanStor 9000 V5 provides the following key technologies for file system applications: Traditional NAS systems consist of engines and storage units. All concurrent access requests are processed by engines, which are likely to become performance bottlenecks. OceanStor 9000 V5 employs a fully symmetric distributed architecture where each node can provide external service access. Based on the load balancing design, data access requests are evenly distributed to all nodes in the cluster, greatly improve the system concurrent access capability. The read/write speed of data in cache is much higher than that on disks, but each node has limited cache space. If cache space on nodes is limited and independent from each other, it is difficult to increase the cache hit ratio and ensure data consistency. OceanStor 9000 V5 provides the global cache mechanism to resolve these problems. A cache pool is provided to integrate the cache on all nodes. The cache pool stores only one copy of a file and is accessible to any node. The global cache mechanism elevates system performance and improves the data hit ratio while shortening the latency. OceanStor 9000 V5 supports 10GE high-speed networks.\nEach node provides four 10GE ports. OceanStor 9000 V5 also supports node interconnection over an InfiniBand (IB) network to meet more demanding performance requirements. With high-speed node interconnection, OceanStor 9000 V5 meets different networking requirements and delivers lower internal latency and better performance. High-performance SSDs help eliminate read/write performance bottlenecks, lower latency, and increase throughput. OceanStor 9000 V5 can automatically identify hotspot data and small files, and migrate data between different storage tiers based on data access frequencies to improve resource utilization. OceanStor 9000 V5 supports the SMB3 Multichannel function. By fully utilizing multi-core CPUs and bandwidth resources of clients, SMB3 Multichannel greatly improves service performance and reliability. In addition, if one channel fails, SMB3 Multichannel transmits data over another channel to prevent services from being affected. Internal file replication is rapidly implemented within OceanStor 9000 V5 without involving clients. The traditional NFS protocol is limited by the bandwidth of a single network port and block size. A client supports a maximum bandwidth of 1 GB/s. OceanStor 9000 V5 enhances the NFS protocol. Multiple network ports and the NFS protocol plug-in DFSClient are configured on NFS clients for concurrent connections of multiple network ports and cache optimization to improve performance of the clients. Bandwidth of a Mac OS X client is 1.5 GB/s. Bandwidth of an RHEL client is 2.5 GB/s. OceanStor 9000 V5 provides the disk defragmentation function. In a specified period of time, periodically executing disk defragmentation tasks ensures satisfactory system performance for a long time.\nWith a distributed architecture, OceanStor 9000 V5 supports seamless scale-out of 3 to 288 nodes. All nodes are symmetrically arranged in OceanStor 9000 V5, and all data and metadata are evenly distributed on each node to eliminate system bottlenecks. OceanStor 9000 V5 can be smoothly scaled out in line with service growth, enabling linear increase of capacity and performance. OceanStor 9000 V5 offers up to 100 PB capacity for a single file system to help minimize the CAPEX and OPEX, adapting to changing storage requirements. OceanStor 9000 V5 provides the following node types to fit into different application scenarios: P12E/P12X/P12A, P25E/P25X/S25X, and P36E/P36X/P36A/C36A nodes are high-performance storage nodes. They are applied to OPS-intensive and high-bandwidth application scenarios. C36E or C36X is a large-capacity archiving node, which is used for large-capacity application scenarios. OceanStor 9000 V5 employs a distributed cluster architecture where all components are fully redundant, eliminating single points of failure (SPOFs). Key components of storage nodes, such as fan modules, power modules, and port modules, adopt a redundant design to improve hardware reliability. The reliability design of OceanStor 9000 V5 also includes: N+M data protection level Erasure Coding (EC) enables inter-node RAID function, ensuring data integrity of a node even when the node fails. Multiple protection levels are supported, such as N+1, N+2, N+3, N+4, N+2:1, and N+3:1. You can specify a protection level for any empty directory in a file system. Mirroring Mirroring helps store multiple copies of metadata onto different storage nodes for improved data protection.\nData reconstruction Automatic node fault detection and automatic data recovery are supported. When a disk or node fails, data can be retrieved, preventing data loss upon node or disk failures. Physical domain division Physical domain division is an efficient way to isolate faulty domains from functioning ones. When a disk or node in a domain (node pool or node pool group) fails, physical domain division ensures that reliability and security of data in other domains are not affected, so is the read/write performance of other domains during data recovery. Data consistency scanning Data consistency scanning, also called data scrub, periodically checks data in the background without affecting services, thereby discovering disk errors in advance to prevent silent corruption from causing data loss. OceanStor 9000 V5 also provides a variety of advanced data protection technologies that ensure data availability even in a catastrophe, thereby protecting service continuity. InfoReplicator : A remote replication feature. You can set an automatic replication cycle or manually start a data replication task to replicate specified directories or files from one storage system to other storage systems over IP links (either a LAN or a WAN). In this case, the disaster recovery (DR) capability is maximized. Network Data Management Protocol (NDMP) : OceanStor DFS acts as a data source, and data is backed up to back up media such as tape libraries for data backup and recovery purposes. InfoStamper : A directory-level snapshot feature.\nHello all, Huawei has released new smartphone Mate x2 , let's watch the video!!! Behold, the new foldable HUAWEI Mate X2 arrives to impress. Folded, it fits perfectly in your pocket and palm as a 6.45-inch flagship smartphone, ready for use on the go. Unfolded, its 8-inch FullView Display offers an eye-opening viewing experience. The up-to-90 Hz refresh rate of both screens delivers fluid flow, keeping up with your moves. Enjoy a smoother unfolding experience for longer, thanks to the new and improved multi-dimensional hinge made using innovative materials and forging techniques. The discreetly concealed hinge gives HUAWEI Mate X2 a flawless look, and creates a virtually seamless fit. Aiming to impress, HUAWEI Mate X2 unfolds four select coloursrefreshing Crystal Blue inspired by the crisp blue sky on a sunny winter morning, dreamy Crystal Pink for a soft, romantic touch, plus classic Black and White. I like the pink one! Packed with over 15 billion transistors, the ultra-compact 5nm Kirin 9000 5G SoC is streets ahead when it comes to high-speed data processing. Consisting of two big cores plus a tiny core, the reimagined NPU pushes AI processing to a new level, while the 24-core Mali-G78 GPU offers heightened image processing. This chip brings greater performance for whatever task you ask of it, from intensive gaming to complex film editing. However you hold the HUAWEI Mate X2, it will automatically detect signal changes.\nDear, Dear, I have observed Disk array fault alarm as below. Location Information: Disk Array Name=storage000, Disk Array Fault Alarm Title=0xE02040063 The coin battery fails, Disk Array Fault Location Parameter=[S3900-M200, 210235G6KT10G5000036] CtrlID=B Additional Information: Disk Array Fault Cause=The coin battery of controller (B) fails. After the storage system is powered off, the system clock may be lost. -Is there any serve impact for this \"Disk array fault\" alarm. -As per alarm description, system clock may be lost. So can we do manual clock setting to cover from impacting. And I am much not understand clearly for this issue. Kindly help pls. Hi, friend. As per the alarm description, this alarm impacts your storage system time, and some features that depend on the system time, such asremotereplication. If you do not handle the alarm, the displayed time may be different from the actual time when other problems or alarms occur. And as you said, you also can manually clock setting to cover from impacting. But you can just replace a coin battery to solve this alarminstead of manually setting the time every time. Hi, friend. As per the alarm description, this alarm impacts your storage system time, and some features that depend on the system time, such asremotereplication. If you do not handle the alarm, the displayed time may be different from the actual time when other problems or alarms occur. And as you said, you also can manually clock setting to cover from impacting.\nHello there, Traffic is communication in the physical world, and communication is traffic in the digital world. As 5G breaks through the limitation of distance and AI breaks through the bottleneck of capabilities, the development of new ICT technologies will benefit the comprehensive digital transformation of the transportation industry. For example, expressways use ETC to replace manual tolls, which improves traffic efficiency and relieves congestion; subway passengers entering the station can understand which carriage of the next train has the least people, which improves the passenger experience. Through the digitization of infrastructure and business processes, smart transportation digitizes everyone involved in transportation, each piece of goods, each vehicle, and each business process. Through cross-departmental information exchange and data integration, it is oriented to planning, construction and operation. , Operate full-cycle business, use computing power to drive transportation capacity, demand productivity from data, and more comprehensively improve the safety, efficiency and experience of transportation, and achieve a people-oriented harmony in transportation. Huawei believes that the digitalization of the industry is the collection of digitalization of various scenarios. The goal of digital transformation of the transportation industry is to build a comprehensive transportation that \"pleases people to walk, and optimises the flow of things\", from the whole process, the whole architecture, and the whole life cycle. Practice transportation digitization to bring about the improvement of production efficiency, the improvement of operation management, the innovation of business model, and the improvement of public service capabilities. Scene digitization requires technology, understanding of the industry, and real practice.\nHello team, What is the difference between SFTP and FTPS? Which one is more reliable? Thank you. In the 1990s concern about internet security was growing, and in response Netscape created the Secure Sockets Layer (SSL, now known as TLS) protocol to protect communications over a network. SSL was applied to FTP to create FTPS. Like FTP, FTPS uses two connections, a command channel and a data channel. You can choose to encrypt both connections or only the data channel. FTPS authenticates your connection using either a user ID and password, a certificate, or both. When connecting to a trading partner's FTPS server, your FTPS client will first check if the server's certificate is trusted.The certificate is considered trusted if either the certificate was signed by a known certificate authority (CA), or if the certificate was self-signed by your partner and you have a copy of their public certificate in your trusted key store. Your partner may also require that you supply a certificate when you connect to them. If your certificate isnt signed by a third-party CA, your partner may allow you to self-sign your certificate, sending them the public portion beforehand to load into their trusted key store. User ID authentication can be used with any combination of certificate and/or password authentication. While FTPS adds a layer to the FTP protocol, SFTP is an entirely different protocol based on the network protocol SSH (Secure Shell) rather than FTP.\nUnlike both FTP and FTPS, SFTP uses only one connection and encrypts both authentication information and data files being transferred. SFTP provides two methods for authenticating connections. Like FTP, you can simply use a user ID and password. However, with SFTP these credentials are encrypted, which gives it a major security advantage over FTP. The other authentication method you can use with SFTP is SSH keys. This involves first generating a SSH private key and public key. You then send your SSH public key to your trading partner and they load it onto their server and associate it with your account. When they connect to your SFTP server, their client software will transmit your public key to the server for authentication. If the public key matches your private key, along with any user or password supplied, then the authentication will be successful. User ID authentication can be used with any combination of key and/or password authentication. Weve established thatboth FTPS and SFTPoffer strong protection through authentication options that FTP cant provide. So why should you choose one over the other? One major difference between FTPS and SFTP is that FTPS uses multiple port numbers. The first port, for the command channel, is used for authentication and passing commands. However, every time a file transfer request or directory listing request is made, another port number needs to be opened for the data channel.\nInternal transactions refer to operations performed by the storage system background based on user operations. For example, the storage system reconstructs a disk failure, balances the disk capacity, and collects global garbage. Internal transactions and value-added features require a large amount of metadata operations. On the other hand, a large number of non-host I/Os may be generated, which affects host services of the storage system. Before troubleshooting performance problems, check whether internal transactions or value-added features exist. Commonly used items include: Pre-copy and reconstruction Copying: The storage system uses the disk pre-copy technology to periodically check the hardware status. Once a disk is found faulty, the storage system proactively migrates its data to reduce the risk of data loss. Reconstruction: When a hard disk fails, the reconstruction function restores data to the newly allocated hot spare space through RAID redundancy, resulting in a large amount of RAID computing and data copy. Both pre-copy and reconstruction tasks generate data copies. If a large number of data copies exist, the current service performance will be affected. Therefore, before performing a performance test, you need to check whether a pre-copy or reconstruction task occurs in the current storage system. You can run CLI commands to view the information. migrating SmartMigration supports LUN migration within and between storage devices. The migration involves a large number of read and write copies of data, which affects the performance to a certain extent. The migration speed can be set.\nBefore analyzing the performance of front-end host ports, you need to check the location, number of connections, working status, and rate of the port. You can use DeviceManager or CLI to view the information. Use DeviceManager to view front-end host port information. To query information about front-end ports, run show port general on the CLI.\nID Health Status Running Status Type Working Rate(Mbps) WWN --------------- ------------- -------------- --------- ------------------ ---------------- CTE0.IOM.H13.P0 Normal Link Up Host Port 16000 CTE0.IOM.H13.P1 Normal Link Down Host Port -- CTE0.IOM.H13.P2 Normal Link Down Host Port -- CTE0.IOM.H13.P3 Normal Link Up Host Port 16000 CTE0.IOM.L13.P0 Normal Link Up Host Port 16000 CTE0.IOM.L13.P1 Normal Link Down Host Port -- CTE0.IOM.L13.P2 Normal Link Down Host Port -- CTE0.IOM.L13.P3 Normal Link Up Host Port 16000 Role Working Mode Configured Mode Enabled Max Speed(Mbps) Number Of Initiators Protocol ----------- ------------ --------------- ------- --------------- -------------------- -------- INI and TGT Fabric Auto-Adapt Yes 16000 4 FC-SCSI INI and TGT -- Auto-Adapt Yes 16000 0 FC-SCSI INI and TGT -- Auto-Adapt Yes 16000 0 FC-SCSI INI and TGT Fabric Auto-Adapt Yes 16000 2 FC-SCSI INI and TGT Fabric Auto-Adapt Yes 16000 4 FC-SCSI INI and TGT -- Auto-Adapt Yes 16000 0 FC-SCSI INI and TGT -- Auto-Adapt Yes 16000 0 FC-SCSI INI and TGT Fabric Auto-Adapt Yes 16000 2 FC-SCSI After confirming the position, number of connections, working status, and rate of the front-end host port interface module, you are advised to check the following items: The number of front-end host ports connected to the two controllers must be the same. Ports must be evenly distributed on multiple front-end interface modules to balance loads between controllers and front-end interface modules.\nWhen the UltraPath is not installed on a host, perform the following operations to query the mapping between host disks and LUNs. Querying the Mapping Between Host Disks and LUNs (Windows) On the storage system, obtain the WWN of the LUN mapped to the host. 1. Log in to the command-line interface (CLI) of the storage system as a super administrator. 2. Run show initiator initiator_type=? [ wwn=? | iscsi_iqn_name=? ] to show the host corresponding to the WWN or iSCSI IQN. In the preceding command output, the value of Host ID is the host corresponding to the WWN. 3. Run show host lun host_id= command to view all LUNs mapped to the host. host_id= represents the ID of a host. The value of LUN ID is the ID of a LUN mapped to the host in the storage system. 4. Run the show lun general lun_id= command to view the WWN of the LUN mapped to the host. On the host, obtain the WWN of the LUN corresponding to a disk. 1. Log in to the Windows application server as an administrator. 2. Press Windows+R (if the operation is performed remotely, perform it in full screen mode) to open the Run dialog box. 3. Enter diskmgmt.msc and press Enter . 4. In the displayed Disk Management window, right-click the disk you want to query, and choose Properties . 5. On the Details tab page, set Property to Device Instance Path . Value below Property is the serial number of the disk.\nThe concurrency capability of an HBA indicates the maximum number of I/Os that each LUN can transmit at a time. If this is insufficiently low for a high concurrency scenario, it typically leads to poor performance. Windows Hosts In Windows, the concurrency capability of most HBAs is 128. However, in certain Windows versions, this value may be relatively small. For example, in Windows Server 2012 R2, the corresponding value of an Emulex HBA is 32. Insufficient concurrency capability leads to the issue of the host pressure not being fully transferred to the storage side. If the difference between the latency on both sides is large, you can use the management software provided by the HBA vendor to query the concurrency capability of the HBA and set it to an appropriate value as necessary. Linux Hosts In Linux, the queue parameter settings of an HBA vary depending on its type and driver. For details, see the specifications provided by the HBA vendor. For example, the QLogic 8 Gbit/s dual-port Fibre Channel HBA allows the maximum queue depth of each LUN to be 32. If the latency difference between the host side and the storage side is large, run iostat to check whether the concurrency bottleneck is reached. In the above figure, the highlighted box shows the entries under avgqu-sz , which represent the average queue depth of the block device corresponding to a LUN.\nInsufficient host memory or high CPU usage affects the host's service delivery capability, resulting in performance deterioration. Run the lsattr -El mem0 command on an AIX host to check the memory usage. Run the vmstat command to check the usage of the virtual memory and the actual memory. page indicates the number of page transfer-in and transfer-out requests. If pi and po are always non-zero values, note that the memory may be insufficient. wa indicates the I/O waiting time. In normal cases, the waiting time is not too high. Avm indicates the number of active virtual pages that are allocated to work segments during process running. The unit is 4 KB. The smaller the number of pages, the better. Note that avm does not include the file system cache. fre indicates the number of free lists. Generally, the value of free is greater than or equal to 120. When the value of free is less than 120, the system automatically stops the process to release the idle list. Checking CPUs Run the lsdev -Cc processor command to check the number of CPU cores. Run the sar parm1 parm2 command to check the CPU usage. parm1 indicates the printing frequency, and parm2 indicates the number of printing times. When the sum of \"%usr\" and \"%sys\" is greater than 80%, the CPU will be a bottleneck. You can run the ps aux | head -4 command to view the first three processes that occupy the CPU.\nHello all, This post is talking about the product overview of DME Storage. DME Storage is a self-driving management system for Huawei storage data lifecycle (planning provisioning protection E2E O&M optimization) in data center scenarios. It uses automation and AI engines to build automatic and intelligent O&M management, aiming at providing better O&M management for Huawei storage and improving the O&M efficiency. DME Storage uses a southbound addition and collection framework. Based on the two engines, a self-driving management system is built for the storage data lifecycle to interconnect with N ecosystems and provide users with automatic storage provisioning and intelligent storage O&M. DME Storage is a self-driving management platform for Huawei storage data lifecycle in customer data centers. It faces the following security threats and challenges: Portal access security, as shown in (1) of the figure O&M management is provided by web services. Therefore, their security risks are web attacks, such as cross-site scripting (XSS) attacks, cross-site request forgery (CSRF) attacks, and structured query language (SQL) injection. Other security threats related to O&M services include: unauthorized access, overstepping, and sensitive data leaks. Southbound device access security, as shown in (2) of the figure DME Storage needs to authenticate southbound devices, cloud services, and systems to prevent malicious systems from accessing DME Storage without being authenticated. Secure transmission of southbound collected data is also a key security challenge for southbound device access. Northbound openness security, as shown in (3) of the figure DME Storage provides open APIs.\n1. Private cloud : Cloud resources can be used only by users in one organization, which is the core feature of the private cloud. However, there is no strict regulation on the ownership, schedule management, and operation of the cloud. The ownership, schedule management, and operation of the cloud may be the company, a third-party organization, or a combination of the two. The cloud may be located within the organization or hosted elsewhere. 2. Community cloud : Cloud resources are dedicated to users in several fixed units, which have the same requirements on the cloud (such as security requirements, cloud mission, rules, and compliance requirements). The ownership and routine management of the cloud may be one or more units in the community, it could be a third party outside the community, or a combination of the two. The cloud may be deployed locally or elsewhere. 3. Public cloud : Develop cloud resources for public use. The ownership, day-to-day management and operation of the cloud can be a business organization, academic institution, government department, or a combination of them. The cloud may be deployed locally or in other places. For example, the cloud of the Zhongshan public cloud may be deployed in Zhongshan or Shenzhen. 4. Hybrid cloud : A hybrid cloud consists of two or more different types of clouds (private, community, and public) that are independent, but combine them with standard or proprietary technologies that enable smooth data and application flow between clouds. A hybrid cloud consists of multiple clouds of the same type.\nThe hybrid cloud consisting of private and public clouds is the most popular one. When the demand for private cloud resources is too high for a short period (called cloud bursting), public cloud resources are automatically rented to suppress the peak demand for private cloud resources. For example, online stores have a huge number of clicks during holidays, which will temporarily use public cloud resources for emergency purposes. NIST defines four : public clouds, private clouds, community clouds, and A cloud  is defined according to where the infrastructure for the deployment resides and who has control over that infrastructure. Deciding which deployment model you will go with is one of the most important cloud deployment decisions you will make. Each  cloud deployment model satisfies different organizational needs, so its important that you choose a model that will satisfy the needs of your organization. Perhaps even more important is the fact that each cloud deployment model has a different value proposition and different costs associated with it. Therefore, in many cases, your choice of a cloud deployment model may simply come down to money. In any case, to be  able to make an informed decision, you need to be aware of the characteristics of each environment. Private cloud. The cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and  may exist on premise or off premise. Community cloud.\nThe cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns  (e.g., mission, security requirements, policy, and compliance considerations). It may be managed by the organizations or a third party  and may exist on premise or off premise. Public cloud. The cloud infrastructure is made available to the general public or a large industry group and is owned by an organization  selling cloud services. Hybrid cloud . The  cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together  by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between  clouds). These four  can see significant variation depending on other factors that we will discuss in the next section, but they serve to address the broad questions as to how one can deploy pooled cloud resources. Before we move on, it is important to make two points about the NIST Cloud Model: A customer or tenant  can have greater security control over more resources as one moves from  SaaS to PaaS and again from PaaS to the IaaS service model. A customer or tenant  can achieve greater security control over more resources when moving from a Public cloud to a community cloud and again from a community cloud to a Private cloud.\nHello all, This post is talking about VMware Infrastructure. After reading it you will learn all about its infrastructure. Legacy x86 computers are designed merely for running a single operating system or application program. Therefore, most of these computers are under-utilized. To address the under-utilization, virtualization technologies are adopted, enabling an x86 physical machine to host multiple virtual machines (VMs) and allowing the multiple VMs to run different operating systems and applications. That is, virtualization allows resources on this physical machine to be shared among multiple environments, thereby improving x86 hardware utilization. VMware virtualization technology (VMware Infrastructure) adds a condensed software layer on the computer hardware or in the host operating system. This software layer includes a VM monitor utility that allocates hardware resources in a dynamic and transparent way. Each operating system or application can access desired resources anytime as required. As an outstanding software solution for x86 virtualization, VMware Infrastructure enables users to manage their virtualized environments in an effective and easy manner. As Figure 1 shows, a typical VMware Infrastructure datacenter consists of basic physical building blocks such as x86 computing servers, storage networks and arrays, IP networks, a management server, and desktop clients. Figure 1 VMware Infrastructure datacenter In a VMware Infrastructure storage architecture (as shown in Figure 2 ): A Virtual Machine File System (VMFS) volume contains one or more LUNs which belong to one or more storage arrays. Multiple ESX servers share one VMFS volume; virtual disks are created on the VMFS volume for use by VMs.\nHi, guys! What is the Huawei Harmony OS? I want to develop Harmony OS applications, where can I download the development tools? Can you give me the development link? Thanks! Hi, Lisa! Have a nice day! 1. What is Huawei Harmony OS? Harmony OSis a future-proof distributed operating system open to you as part of the initiatives for the all-scenario strategy, adaptable to mobile office, fitness and health, social communication, and media entertainment, to name a few. Unlike a legacy operating system that runs on a standalone device,Harmony OSis built on a distributed architecture designed based on a set of system capabilities. It is able to run on a wide range of device forms, including smartphones, tablets, wearables, smart TVs, head units. 1. If you are an end user,Harmony OSintegrates your various smart devices to implement fast connection, capability collaboration, and resource sharing between them. This way, your services can be seamlessly transferred to a suitable device that delivers smooth all-scenario experience. 2. If you are an application developer,Harmony OSadopts distributed technologies to make your application development possible on different device forms. WithHarmony OS, you will have the choice to focus on upper-layer service logic and develop applications in a much easier and more efficient way. 3. If you are a device developer,Harmony OSuses a component-based software design to tailor itself to your particular device forms based on their respective resource capabilities and service characteristics. Harmony OSprovides multi-programming-language APIs for you to develop applications.\nwhen I come to connfigure oceanstor 2600 v5 file system in eDesinger, I saw one field called: file size: A) 64KB B) 128KB C)256KB D)512KB I want to undersand why we have these such sizes? and why it cannot go beyond 512KB ? are theses sized limited by the NAS system or by user? for examble: if iam an end user and I had this storage, and I want to upload one movie with 2GB to that NAS storage, what's will happen in uploading ? where theses sizes will play ? could you please describe it in details ? Hello, friend! The file size indicates the average size of a large file in the actual file size distribution range. If there are many large files in the actual application environment, for example, several GB large files, select a larger one based on the average size of the most common files. If the file size is 64 KB to 512 KB, the same is true. If the file size is generally large, choose the larger file. eDesigner is a capability evaluation tool. The preceding figure shows the service model. The file size corresponds to the block size on the storage device. The concept is the same. The configuration is based on the size of the file that appears most frequently. The storage system divides files into blocks. If the block size is proper, the number of blocks is small, which improves the performance.\nHyperMetro can be deployed between: Any two of the OceanStor 6800 V3, 6800 V5, 18500 V3, 18500 V5, 18800 V3, 18800 V5, 6800F V5, 18500F V5, and 18800F V5 Any two of the OceanStor 5500 V3, 5500 V5, 5600 V3, 5600 V5, 5800 V3, 5800 V5, 5500F V5, 5600F V5, and 5800F V5 Any two of the OceanStor 2600 V3, 5300 V3, 5300 V5, 5300F V5, 5110 V5, and 5110F V5 HyperMetro can be deployed between: V500R007C30 and V300R006C50 V500R007C50 and V300R006C60 V500R007C61 and V300R006C61 Only OceanStor 2600 V3 supports V300R006C61. For OceanStor V5 V500R007C00/C10/C20, the local and remote storage systems must be the same model.\nFor V500R007C30/C50/C60, HyperMetro can be deployed between: Any two of the OceanStor 6800 V5, 18500 V5, 18800 V5, 6800F V5, 18500F V5, and 18800F V5 Any two of the OceanStor 5500 V5, 5600 V5, 5800 V5, 5500F V5, 5600F V5, and 5800F V5 Any two of the OceanStor 5300 V5, 5300F V5, 5110 V5, and 5110F V5 HyperMetro can be deployed between: Any two of the OceanStor 6800 V5, 18500 V5, 18800 V5, 6800F V5, 18500F V5, 18800F V5, 6800 V5 Kunpeng, 18500 V5 Kunpeng, 18800 V5 Kunpeng, 6800F V5 Kunpeng, 18500F V5 Kunpeng, and 18800F V5 Kunpeng Any two of the OceanStor 5500 V5, 5600 V5, 5800 V5, 5500F V5, 5600F V5, 5800F V5, 5500 V5 Kunpeng, 5600 V5 Kunpeng, 5800 V5 Kunpeng, 5500F V5 Kunpeng, 5600F V5 Kunpeng, and 5800F V5 Kunpeng Any two of the OceanStor 5300 V5, 5300F V5, 5110 V5, 5110F V5, 5300 V5 Kunpeng, and 5300F V5 Kunpeng HyperMetro can be deployed between: V500R007C30 and V500R007C60 Kunpeng V500R007C60 and V500R007C60 Kunpeng V500R007C61 and V500R007C70 Kunpeng HyperMetro can be deployed between: Any two of the OceanStor 6800 V3, 18500 V3, 18800 V3, 6800 V5 Kunpeng, 18500 V5 Kunpeng, 18800 V5 Kunpeng, 6800F V5 Kunpeng, 18500F V5 Kunpeng, and 18800F V5 Kunpeng Any two of the OceanStor 5500 V3, 5600 V3, 5800 V3, 5500 V5 Kunpeng, 5600 V5 Kunpeng, 5800 V5 Kunpeng, 5500F V5 Kunpeng, 5600F V5 Kunpeng, and 5800F V5 Kunpeng Any two of the OceanStor 2600 V3, 5300 V3, 5300 V5 Kunpeng, and 5300F V5 Kunpeng HyperMetro can be deployed between: V300R006C50 and V500R007C60 Kunpeng V300R006C61 and V500R007C70 Kunpeng Only OceanStor 2600 V3 supports V300R006C61.\nHow Do I Query the Mapping Between Host Disks and LUNs When the UltraPath Software Is Not Installed? When the UltraPath is not installed on a host, perform the following operations to query the mapping between host disks and LUNs. On the storage system, obtain the WWN of the LUN mapped to the host. Log in to the command-line interface (CLI) of the storage system as a super administrator. Run show initiator initiator_type=? [ wwn=? | iscsi_iqn_name=? ] to show the host corresponding to the WWN or iSCSI IQN. In the preceding command output, the value of Host ID is the host corresponding to the WWN. Run show host lun host_id= command to view all LUNs mapped to the host. host_id= represents the ID of a host. The value of LUN ID is the ID of a LUN mapped to the host in the storage system. Run the show lun general lun_id= command to view the WWN of the LUN mapped to the host. On the host, obtain the WWN of the LUN corresponding to a disk. The serial number is an ASCII character. You can obtain the WWN of the LUN corresponding to the disk by seeing the ASCII table. Log in to the Windows application server as an administrator. Press Windows+R (if the operation is performed remotely, perform it in full screen mode) to open the Run dialog box. Enter diskmgmt.msc and press Enter . In the displayed Disk Management window, right-click the disk you want to query, and choose Properties .\nOn the Details tab page, set Property to Device Instance Path . Value below Property is the serial number of the disk. Check whether the WWN of the LUN mapped to the host and that of the host disk are the same. If they are, the LUN is just the one corresponding to the host disk. On the storage system, obtain the WWN of the LUN mapped to the host. Log in to the command-line interface (CLI) of the storage system as a super administrator. Run show initiator initiator_type=? [ wwn=? | iscsi_iqn_name=? ] to show the host corresponding to the WWN or iSCSI IQN. The value of Host ID is the ID of a host corresponding to the WWN. Run show host lun host_id= to view all LUNs mapped to the host. host_id= represents the ID of a host. The value of LUN ID is the ID of a LUN mapped to the host in the storage system. Run the show lun general lun_id= command to view the WWN of the LUN mapped to the host. Run the ls /dev/disk/by-id/ -l command to view the WWN of the LUN corresponding to the host disk. Check whether the WWN of the LUN mapped to the host and that of the host disk are the same. If they are, the LUN is just the one corresponding to the host disk. On the storage system, obtain the WWN of the LUN mapped to the host.\nLog in to the command-line interface (CLI) of the storage system as a super administrator. Run the show initiator initiator_type=? [ wwn=? | iscsi_iqn_name=? ] command to view the WWN or IQN of an initiator to query information about the corresponding host. The value of Host ID is the ID of a host corresponding to the WWN. Run show host lun host_id= to view all LUNs mapped to the host. host_id= represents the ID of a host. The value of LUN ID is the ID of a LUN mapped to the host in the storage system. On the host, obtain the ID of the LUN corresponding to a disk. Run the lsdev -Cc disk command to query scanned disk information. Run the lsattr -El hdiskX command to query the information about disk hidskx . In the command output, the value of lun_id is the ID of the LUN corresponding to the disk. Check whether the two IDs are the same. If they are the same, you can determine that the LUN is the one corresponding to the host disk. On the storage system, obtain the WWN of the LUN mapped to the host. Log in to the command-line interface (CLI) of the storage system as a super administrator. Run show initiator initiator_type=? [ wwn=? | iscsi_iqn_name=? ] to show the host corresponding to the WWN or iSCSI IQN. The value of Host ID is the ID of a host corresponding to the WWN. Run show host lun host_id= to view all LUNs mapped to the host.\nDear all, Today I will introduce you smart SAS and smart NVMe disk enclosures, they are both can be used in Huawei OceanStor Dorado V6 all-flash storage systems. A smart SAS or smart NVMe disk enclosure has the Kunpeng 920 CPU and DDR memory on its expansion modules, which provide computing capability for the smart disk enclosure to offload computing tasks from controllers. For example, the disk reconstruction task in the event of a disk failure can be offloaded to the smart SAS or smart NVMe disk enclosure, minimizing the impact of the reconstruction task on controller performance. Because a computer system supports a maximum of 128 PCIe devices (each I/O interface module or NVMe SSD is a PCIe device), the number of NVMe SSDs supported by a computer system cannot exceed 128. Huawei's smart NVMe disk enclosure is equipped with the Kunpeng 920 CPU and DDR memory, making it an independent computer system. The NVMe SSDs on a smart NVMe disk enclosure occupy the number of PCIe devices in this independent computer system, instead of that on the controller enclosure. In addition, smart NVMe disk enclosures connect to a controller enclosure through RDMA ports, allowing a system to support a large number of NVMe SSDs. A controller enclosure connects to smart SAS or smart NVMe disk enclosures through onboard 100 Gbit/s RDMA ports or back-end 100 Gbit/s RDMA interface modules, which provide large-bandwidth and low-latency transmission channels.\nHello everyone, This post is talking about the chips used inOceanStor Dorado V6. Huawei OceanStor Dorado V6 all-flash storage systems use Huawei-developed chips, including the front-end interface chip (Hi1822), Kunpeng 920 chip, Ascend AI chip (Ascend 310), SSD controller chip, and baseboard management controller (BMC) chip (Hi1710). Hi182x (IOC) is a Huawei-developed storage interface chip. It integrates multiple interface protocols including 8 Gbit/s, 16 Gbit/s, and 32 Gbit/s Fibre Channel, 10GE, 25GE, 40GE, and 100GE to achieve high interface density and flexible configuration. The Kunpeng 920 chip is independently developed by Huawei. It features high performance, throughput, and energy efficiency to meet diversified computing requirements of data centers. It can be widely used in big data and distributed storage applications. The Kunpeng 920 chip supports various protocols such as DDR4, PCIe 4.0, SAS 3.0, and 100 Gbit/s RDMA to meet the requirements of a wide range of scenarios. The Ascend chip is the first AI chip independently developed by Huawei to achieve ultra-efficient computing and low power consumption. It is the most powerful AI SoC for computing scenarios. The Ascend 310 chip is the first product of the Ascend series. This 12 nm chip provides a computing power up to 16TFLOPS. Huawei-developed SSDs use the latest-generation enterprise-class controller chip, which supports SAS 3.0 and PCIe 3.0 ports. The controller chip features high performance and low power consumption. It uses enhanced ECC and built-in RAID technologies to extend the SSD service life to meet enterprise-level reliability requirements.\nImagine that we have a 12-disk array, on which we have made a RAID 5 of four data disks and one parity disk, and left two disks in reserve. This is a \"classic\" array. If one of the disks fails, then the storage system will restore the data that was on it on the backup disk. Let's assume that our 5th disk is broken. In this case, one of the backup disks (for example, 11) will become active and data will be restored to it from the 5th disk. Until the data on this disk is recovered, it will be under high write load. Then he will play the role of the 5th disc. The recovery rate will be determined by the speed of that single drive. Now we'll go back to the OceanStor 5500 V5. It uses RAID 2.0+ technology, which writes data by dividing it not just between disks, but between blocks distributed between disks. The technology is very interesting, I recommend everyone to familiarize themselves with it. In the example, consider an array with the same backup level, 4 data blocks and one parity block. 1/1 1/2 2/1 2/2 3/1 3/2 4/1 4/2 5/1 5/2 6/1 6/2 7/1 7/2 8/1 8/2 9/1 9/2 10/1 10/2 11/1 11/2 12/1 12/2 As we can see, the spare capacity and parity blocks are distributed between different disks (except for the sixth, which was \"lucky\"). If there are more than two blocks on each disk, then the distribution will be more \"smeared\".\nNow let's imagine that the same 5th disk fails. There are two blocks on it. One of them is backup, you do not need to restore it, and the second is with data. It will be restored to the place of one of the backup units, for example, 7/1. If the first block was also with data, then it would be restored to the place of one of the backup blocks, for example, 12/2. Due to the fact that the spare blocks used for recovery are not located on one disk, but are evenly randomly distributed across all disks, the load is distributed across the entire array. Due to this, there is no \"bottleneck\" for the write speed of one disc, and the entire recovery takes less time. After the storage system has found a faulty disk and removed it from work, we have the task of replacing it. Just the notification comes to you. Consider two options again. First option. In the case of a simple storage system, you will most likely have a warranty for individual drives from the drive manufacturer. Or the disks themselves are in stock. In this case, you pull out the faulty disk, unscrew it from the sled, screw a new disk to them, and insert it into the storage. Next - software initialization of the disk. In a simple case, it will either become a backup, or data from the backup disk will be copied to it, and it will act as a primary disk.\nHi friend. An performs the same basic function as a hard drive, but data is instead stored on interconnected flash-memory chips that retain the data even when there's no power flowing through them. These flash chips (often dubbed \"NAND\") are of a different type than the kind used in USB thumb drives, and are typically faster and more reliable. SSDs are consequently more expensive than USB thumb drives of the same capacities. Like thumb drives, though, SSDs are often much smaller than HDDs and therefore offer manufacturers more flexibility in designing a PC. While they can take the place of traditional 2.5-inch or 3.5-inch hard drive bays, they can also be installed in a PCI Express expansion slot or even  be mounted directly on the motherboard, a configuration that's now common in high-end laptops and all-in-ones. Hard drives are still around in budget and older systems, but SSDs are now the rule in mainstream systems and high-end laptops like the Apple MacBook Pro, which does not offer a hard drive even as a configurable option. Desktops and cheaper laptops, on the other hand, will continue to offer HDDs, at least for the next few years. That said, both SSDs and hard drives do the same job: They boot your system, and store your applications and personal files. But each type of  storage has its own unique traits. SSDs are more expensive than hard drives in terms of dollar per gigabyte.\nCloud-based storage may be good for housing files you plan to share among your , and PC, but local storage is less expensive, and you have to buy it only once, not subscribe to it. This is where SSDs shine. An SSD-equipped PC will boot in far less than a  minute, often in just seconds. A hard drive requires time to speed up to operating specs, and it will continue to be slower than an SSD during  normal use. A PC or Mac with an SSD boots faster, launches and runs apps faster, and transfers files faster. Whether you're using your computer for fun, school, or business, the extra speed may be the difference between finishing on time and being late. A secondary issue to this: fragmentation. Because of their rotary recording surfaces, hard drives work best with larger files that are laid down in contiguous blocks. That way, the drive head can start and end its read in one continuous motion. When hard drives start to fill up, bits of large files end up scattered around the disk platter, causing the drive to suffer from what's called \"fragmentation.\" While read/write algorithms have improved to the point that the effect is minimized, hard drives can still become fragmented to the point of affecting performance. SSDs can't, however, because the lack of a physical read head means data can be stored anywhere without penalty. This contributes to SSDs' inherently faster nature.\nAn SSD has no moving parts, so it is more likely to keep your data safe in the event you drop your laptop bag or your system gets shaken while it's operating. Most hard drives park their read/write heads when the system is off, but when they are working, the heads are flying over the drive platter at a distance of a few nanometers. Besides, even parking brakes have limits. If you're rough on your equipment, an SSD is recommended. Because hard drives rely on spinning platters, there is a limit to how small they can be manufactured. Years back, there was an initiative to make smaller 1.8-inch spinning hard drives, but that stalled at about 320GB, and smartphone manufacturers only use flash memory for their primary storage. SSDs have no such limitation, so they can continue to shrink as time goes on. SSDs are available in 2.5-inch laptop-drive sizes, but that's only for convenience in fitting within established drive bays. They are increasingly moving, though, to the M.2 form factor discussed above, and  these drives come in 42mm, 60mm, 80mm, and 120mm lengths. Even the quietest hard drive will emit a bit of noise when it is in use. (The drive platters spin and the read arm ticks back and forth.) Faster hard drives will tend to make more noise than those that are slower. SSDs make no noise at all; they're non-mechanical. Plus, an SSD doesn't have to expend electricity spinning up a platter from a standstill.\nConsequently, none of the energy consumed by the SSD is wasted as friction or noise, rendering them more efficient. On a desktop  or in a server, that will lead to a lower energy bill. On a laptop or tablet, you'll be able to eke out more minutes (or hours) of battery life. Then there's the issue of longevity. While it is true that SSDs wear out over time (each cell in a flash-memory bank can be written  to and erased a limited number of times, measured by SSD makers as a \"terabytes written\" or TBW rating), thanks to that  dynamically optimizes these read/write cycles, you're more likely to discard the system for obsolescence before you start running into read/write errors with an SSD. If you're really worried, several tools can let you know if you're approaching the drive's rated end of life. Eventually, hard drives will wear out from constant use, as well, since they use physical recording methods. Longevity is a wash when it's separated from travel and ruggedness concerns. The  overall takeaway? Hard drives win on price and capacity. SSDs work best  if speed, ruggedness, form factor, noise, or fragmentation (technically, a subset of speed) are important factors to you. If it weren't for the price and capacity issues, SSDs would be the hands-down winner. Thanks you. Hello, SSD stands for Solid State Drive , in case you mean the comparison with Hard Disk Drive ( HDD) .\nThe SSD has the following features: Principles: Uses flash technology to store data. No mechanical structure with lower energy consumption, low heat signature and low noise. Summary: SSDs are faster but the HDD can store more capacity. More information about SSDs: Although traditional mechanical drives will not be disappearing from the market in a short period of time, but its replacement which is the Solid State Drive (SSD) has become more and more popular and widely used in the market. SSD does not use magnetic materials to store data but uses basic units of NAND Flash called cell as storage units to store data. NAND Flash is a non-volatile random access memory storing medium which has a unique feature of not losing any written data even if the power is cut off. This technology allows data to be quickly written to SSD drives much faster than compared to mechanical drives. Another benefit of SSD is that it does not produce loud noise or generate high heat signatures like traditional mechanical drives such as SATA/NL-SAS/SAS drives. SSD does not have moving mechanical parts which greatly lowers the risk of mechanical part failures, but this does not mean that its lifetime is unlimited. Due to NAND Flash is a non-volatile medium, each block of data needs to wiped before a new block of data can be written or else it may cause error data. This increases the write/erase cycles of the SSD drive.\nHowever, NAND Flash has a limited write/erase limit which means that each cell of the SSD has a certain limited number of times it can be erased and written into, once the limit is exceeded, that particular cell can no longer be used for data read/write. However, these kind of wear and tear can be easily monitored and predicted so that preparations can be made in advance for disk replacement in time. On the other hand, mechanical hard drives failures have no early symptoms which means that disk failure can happen at any time and replacement disk must be prepared at all times. All flash memory suffers from wear, which occurs because erasing or programming a cell subjects it to wear due to the voltage applied. Each time this happens, a charge is trapped in the transistors gate dielectric and causes a permanent shift in the cells characteristics, which, after a number of cycles, manifests as a failed cell. It is important to understand this before making the option between choosing which type of SSD that you want to invest in. The types of the SSD can be differentiated as following: SLC (Single Level Cell): Stores 1 bit of data in 1 cell. Lowest wear and tear. Longer lifespan. But expensive and available in small capacities only. MLC (Multi Level Cell): MLC memory is more complex and can interpret 2 bits from a signal stored in a single cell.\nHello team, What the product highlights of OceanStor 100D distributed storage? Thank you. Dear Phany, Here are 3 highlights: Cost-Effective Storage for Mass Data OceanStor 100D organizes storage media, including HDDs and SSDs, into large-scale resource pools using distributed technologies and provides industry standard interfaces for upper-layer applications and clients. This eliminates the bottlenecks of traditional data centers (DCs) and overcomes obstacles to system performance, such as unbalanced utilization of hardware resources by siloed storage systems. OceanStor 100D can start small and protect current investments with precise scale-out of thousands of nodes for linear performance growth as capacity expands. OceanStor 100D ensures data redundancy with elastic EC. EC technology nearly triples the disk space utilization of the traditional three-copy mode, reducing hardware investments and offering a variety of EC schemes for flexible and on-demand deployment. OceanStor 100D saves storage space with dynamic deduplication and compression on SSDs or HDDs used as main storage. It provides optimal data reduction effects and outstanding storage performance by intelligently choosing between inline and post-process deduplication based on front-end application loads. Maximized Efficiency for Diversified Data OceanStor 100D supports critical and emerging workloads using unique FlashLink performance acceleration, intelligent stripe aggregation, I/O priority scheduling, data identification and processing, and SSD cache acceleration. It delivers a consistent latency of 1 millisecond even during data reduction. OceanStor 100D is the first distributed storage system to feature extensive compatibility with Kunpeng processors.\nWhen NMS such as Zabbix accesses the storage array through SNMP, the generated performance data image shows a straight line. Zabbix monitors storage devices through SNMP. Zabbix image shows a straight line. 1. Immediate cause: The update policy of snmpget request is incorrect. As a result, the data is not updated. 2. Root cause: There are three types of SNMP requests: GET, WALK, and BULK. Each of the three types of requests has a specific update policy. It was inefficient to query from the underlying MIB modules each time as GET is to get the value of a single OID. GET has been optimized but with a bug to not query from the underlying MIB modules. Therefore, data in MIB tree remains undeleted and not updated. The WALK and BULK requests are normal. 1. Run the snmpget command to query data and check whether the data with a value greater than 10 seconds remains the same and not updated, as shown in the following figure. 2. Check Zabbix image that whether it shows a straight line. Check the values with data changes, such as CPU usage in node. In contrast, controller status may be always normal, and naturally data remains unchanged. If the storage system is an OceanStor Dorado storage system, upgrade it to V300R002C10SPC100SPH110. If the storage system is an OceanStor converged storage system, upgrade it from V500R007C10SPH017 or V300R006C20SPH017 to SPH019, or to V500R007C30SPH109 or V300R006C50SPH109. 1. Use snmpwalk instead of snmpget to deliver a query request. 2.\nHello, dear! Have a nice day! You can check the information about SmartDedupe and SmartCompression while services are running to obtain the storage space usage. 1. Go to the Storage Pool Properties dialog box. a. In the navigation tree, click Provisioning . b. In the Storage Configuration and Optimization area, click Storage Pool . Note: To check whether a LUN has SmartDedupe or SmartCompression enabled, select the LUN and click Properties. c. Select a storage pool and click Properties . The Properties of Storage Pool dialog box is displayed. 2. Click the General tab and view SmartDedupe and SmartCompression parameters. (Applicable to versions earlier than V300R001C30) The Deduplication Ratio parameter indicates a ratio of storage capacity occupied before data deduplication to that occupied after data deduplication. For example, if 100 MB of the storage capacity is occupied before data deduplication while 60 MB is occupied after data deduplication, the value of the Deduplication Ratio parameter is 1.67:1. (Applicable to versions earlier than V300R001C30) The Compression Ratio parameter indicates a ratio of storage capacity occupied before data compression to that occupied after data compression. For example, if 100 MB of the storage capacity is occupied before data compression while 60 MB is occupied after data compression, the value of the Compression Ratio parameter is 1.67:1. The Data Reduction Ratio parameter indicates the proportion of the raw data capacity to the used capacity in the storage pool after SmartDedupe and SmartCompression are enabled, for example, 1.7:1. Hope can help you! Hello, dear! Have a nice day!\ncan anyone suggest better way to add steps in the failover and failback procedure , i have made this below but customer is not happing and they want me to use more clarify on the steps. In storage disaster recovery, when the production site fails due to disasters or other faults, the disaster recovery site takes over services to ensure normal service running. ------------------------------------------------------------------------------------------------------------------------------------------- Prerequisites Exporting the Configuration File of a Shared Directory on the DC Storage Importing the Configuration File of a Shared Directory on the DR Storage Latest Synchronization is completed. Stop User service. NDMP backups are completed. The pair Status is Interrupted or spilt. ------------------------------------------------------------------------------------------------------------------------------------------- Steps for Failover-------------------------------------------------- . 1. Disable write protection for the secondary directory2. Wait until the status of the snapshot rollback of the secondary directory of the remote replication pair changes from No to Yes. 3. Modify the external DNS Server. 10.100.xxx 10.xxx 10.100.xxx 10.xxx 4. Re-access the User service. ------------------------------------------------------------------------------------------------------------------------------------------- Steps for DC-DR Switchover-(Optional) A primary/secondary switchover is a process of switching the roles of the primary and secondary directories in a remote replication pair. 1. Select the remote replication pair and perform a primary/secondary switchover. 2. Select the remote replication pair and enable write protection for the secondary directory. 3. Wait until the status of the snapshot rollback of the secondary directory of the remote replication pair changes from Yes to No. 4. Select the remote replication pair one by one and click Synchronize. 5.\nDear all, Apple has launched its newest chip M1. For the first time in nearly 15 years, Apples newest MacBooks and Mac mini dont come with Intel processors. Instead, they use the brand-new Apple M1 chip, unveiled on Tuesday as a powerful replacement for the many generations of Intel CPUs that have powered Apple computers since 2006. The M1 is the first appearance of the new paradigm that Apple has dubbed Apple Silicon. The new Macs available for pre-order now with the M1 include the MacBook Air, the 13-inch MacBook Pro, and the Mac mini. If youre contemplating buying one right out of the gate, youre entering uncharted waters, since the only information we have so far about how well they perform comes from Apple itself. Were looking forward to getting these new devices into PC Labs ASAP for some extensive testing and hands-on time. But in the meantime, lets take a look at everything we know so far about Apple Silicon in the Mac. Whether they run Windows, macOS, or Chrome, most of todays PCs come with an array of computing components inside that each handle different processing tasks. These include one processor (the CPU) for handling essential computations, including those used for browsing the internet and opening and closing apps, and another (the GPU) for processing graphics computations and outputting a signal to your monitor or laptop screen. On large, powerful machines like workstation desktops and gaming rigs, the CPU and GPU are completely separate items that are each connected to the motherboard.\nSmaller ultraportable laptops typically combine the CPU and GPU into a single component using whats known in the industry as integrated graphics processing, with the rest of the computers components, including cache, memory, and storage, located in different spots on the motherboard. As an evolution of the Apple A-series processors that have long powered the iPhone and the iPad, the M1 chip takes a different approach. Instead of a collection of separate processing parts, its a single system on a chip (SoC). The SoC handles all of the computations, including graphics output, which means that each software instruction can use the most efficient part of the M1. In part because it must do everything at once, the M1 has an eyebrow-raising maximum of 16 processor cores. That sounds like a ton compared with the six cores that are in the most powerful Intel laptop CPUs. In fact, the M1 has so many cores not so it can perform tasks more quickly, but so it can perform more tasks. Four of the cores are compute cores dedicated to complex calculations that require lots of processing power. Four more are dedicated to lighter tasks that dont require as much power, to ensure that the chip doesn't consume more energy than it needs to. Tasks get shunted to the appropriate core set on the fly. As many as eight additional cores are dedicated to graphics processing, similar to how Intels Iris integrated graphics work.\nOn the Mac mini, the MacBook Pro, and the MacBook Air, the graphics part of the M1 is capable of powering an external monitor at 60Hz and up to a 6K resolution, such as the Apple Pro Display XDR. Additionally, the M1 has a built-in storage controller to traffic data to and from Macs solid-state drive (SSD), as well as various other processors, controllers, and sensors that handle encryption, image processing from webcams, and other secondary tasks that are required for the computer to function. The M1 chip uses a 5-nanometer production process, similar to the latest A14 chip in the iPhone 12. Meanwhile, Intel's latest 11th-generation CPUs use a 10-nanometer production process, and Intel doesn't expect new chips based on 7-nanometer processes or lower until 2022 at the earliest. Intel said in a statement Tuesday that its CPUs \"provide global customers the best experience in the areas they value most, as well as the most open platform for developers, both today and into the future.\" But the big disparity in process technologies does speak for itself. Computer chip rival AMD's latest desktop and mobile chips are on a 7-nanometer process. Artificial intelligence (AI) and machine learning (ML) algorithms are essential to helping modern software run smoothly. Apple A-series and Intel Core processors have long had AI capabilities built in, and the M1 is no different. It has a dedicated neural engine with 16 processing cores of its own to handle AI tasks.\nEver touched up a photo using an automatic filter, or scrolled through a list of pre-populated search results? Both of these tasks and many others often rely at least in part on ML to run faster. Apple M1 chip diagram The M1 chips neural engine relies on instructions from software to work properly. Many third-party software developers integrate AI and ML algorithms into their apps, including many parts of the Adobe Creative Suite. Apple says that the new M1-powered MacBook Air can handle ML workloads up to nine times faster than the previous Intel-powered MacBook Air. No matter whether a computer uses an SoC or a collection of separate computing components, its still subject to certain unbreakable properties of computing and physics. The more cores a processor dedicates to a certain task, and the faster each of those cores run, the faster the task will complete. This process also generates a lot of heat, which is why most computers have fans, heat sinks, and other equipment. While Apple hasnt released many details yet, we do know that there will be slightly different versions of the M1 for each of the different Macs it powers. This is mainly due to the fact that each device handles heat differently. The new MacBook Air has no cooling fan, and we do know that the M1 chip in the base configuration of the Air will feature a slightly less-powerful graphics processor, with seven cores instead of eight.\nMeanwhile, the larger MacBook Pro and the Mac mini will both come with cooling fans, which enables their M1 chips to have full-powered GPUs. Its possible that the Mac mini and MacBook Pro M1 versions will also have higher clock speeds than the MacBook Airs chip, but Apple has not shared this information. According to Apple, the M1 chip is faster than the Intel processors in the Macs it replaces. In some cases, the company claims, its much faster. General performance of the M1 MacBook Air is 3.5 times faster than before, while graphics performance is five times faster, Apple says. On the M1 MacBook Pro, AI computation is 11 times faster than before, and Apple says that it can build code in the Xcode app up to 2.8 times faster. Apple also claims that M1-powered Macs are faster than their Windows counterparts. The company estimates that the new M1 in the MacBook Air will outperform 98 percent of PC laptops sold in the past year. These claims are based on performance tests that Apple performed in-house, though the company isnt providing more details about them except to say that they are industry-standard benchmarks. GPU performance versus power Benchmarks are easily manipulated, however, so its too soon to say definitely whether the M1 is as capable as Apple says it is. Were eager to try out the M1 using our own objective benchmarking process. If Apples claims ring true, the M1 is indeed poised to surpass the current crop of Intel-powered Macs.\nApple also claims that the M1 will offer excellent battery life in the MacBook Air and the MacBook Pro. In practice, previous models of the Air and Pro with Intel chips already offer excellent battery life, typically between 10 and 20 hours of light tasks like watching videos. A chip is only as good as the software that runs on it, and many macOS apps are designed to run on Intel processors, not the M1. Developers have had a bit of a head start, as Apple offered them a development kit this summer to help them translate their code to the M1. But the fact remains that while the macOS operating system itself will run natively on the M1, many third-party apps will not, at least initially. The lack of native M1 support doesnt mean third-party apps wont run at all, however. For apps that have yet to make the transition, Apple says the programs will still be able to run on the new Macs through the companys Rosetta 2 software, which can act as an emulator. But dont be surprised if the programs drag. The translation process takes time, so users might perceive that translated apps launch or run more slowly at times, Apple wrote in the developer documentation for Rosetta 2. Meanwhile, Apple confirmed on Tuesday that starting with macOS Big Sur, all of its Mac software runs natively on both Intel and M1 systems. As an added benefit, iPhone and iPad apps can now run directly on the M1-powered Macs.\nHey there, Where big data come from: Social Data & Machine data What big data is: Hope this help In a few words, I would say that is the capacity of analyzing large amounts of information in a short period of time. Combined with IA can optimize the results. Here is a wider concept and also we invite you to know the Big data solution from Huawei: As the amount of available data grows, the overabundance of information becomes increasingly difficult to manage. The information explosion phenomenon describes the rapid increase in the amount of published information or data and its consequences. Transposing Moores Law for the Information and Communications Technology (ICT) industry, the total amount of data generated by mankind is doubling every 18 months. The mobile Internet and the Internet of Things (IoT) are large contributors to levels of explosive growth that render conventional data processing technologies inadequate. To keep pace with this data explosion, the industry has been developing Big Data technologies that allow organizations to manage massive data and to extract value from it. Big Data technologies collectively refer to technologies and infrastructures that enable enterprises, institutions, governments, and other organizations to effectively manage Big Data and explore its potential economic benefits. Typical Big Data technologies include distributed-storage technologies, parallel-computing technologies that facilitate mass data queries and analytics, mass data mining algorithms, industry-specific mass data modeling methods, applications intended to monetize Big Data, and hardware infrastructure for carrying huge volumes of data.\nHello all, Today I will show you how does Huawei FusionCube help China Everbright Bank(CEB), in this case we will learn background, challenges, solution, and benefits. Recently, Huawei FusionCube Hyper-Converged Infrastructure won the bidding for China Everbright Banks (CEB) converged architecture level-2 resource pool construction project. CEB has its headquarters in Beijing and is a financial enterprise approved by both the State Council and by Peoples Bank of China. CEB ranked 57 in the 105th issue of English magazine, The Bankers Top 1000 World Banks. Due to the banks outstanding innovation capabilities and bright performance, CEB was named Most Innovative Bank of the Year and won CCTV Chinas Brand of the Year. This project involves the deployment of a new resource pool for all CEB branches, implementing a full-scale application of Huaweis FusionCube hyper-converged infrastructure and FusionSphere virtualization software. CEBs cloud construction core idea is to consult with the industrys mainstream cloud computing architects to build CEBs private cloud overall capabilities. With the construction of a private cloud, the bank plans to achieve the following key goals: 100 percent service support coverage 100 percent service provisioning and configuration coverage Accomplish mobility and interoperability within the private clouds internal resource pool. During the construction process, it is necessary to implement differentiated deployment, unified management, and heterogenous resources to achieve a resource pool for the entire bank. All HQ and branch resources are managed on the cloud management platform. Production and transaction applications of branches are migrated to the level-1 HQ resource pool.\nHello all, I read an article about Enterprise+ here is what I want to share with you. For enterprises, digitization is about connecting capital, workforce, assets, information, and regulations. Though connectivity is the lifeblood of digital enterprises, carriers still face huge challenges in providing enterprises with guaranteed connectivity. Intelligent connectivity systematically improves enterprise experience in three areas: coverage, architecture, and fusion, and will help enterprises go digital. Huawei has also come up with a formula that can help create greater digital value from enterprises: Enterprise digital value = Coverage x Architecture x Fusion, which is outlined below. Carriers need to develop a comprehensive service area plan and deploy optical private lines within 300 meters of enterprise users. In addition, carriers need to integrate multiple technologies such as 5G, OTN, IP, and PON into one optical network to intelligently meet enterprises' various needs with differentiated private line products. They can also make full use of their existing network resources, such as full-fiber cities, to deploy one network that serves multiple purposes and one private line that handles multiple services. Carriers need an elastic and intelligent architecture that is congestion-free, always-on, scalable, and simplified. This type of architecture can help achieve committed SLAs, including guaranteed bandwidth, latency, reliability, and service provisioning times. The architecture is essential for providing deterministic experiences and for monetizing SLAs. Huawei believes that network capabilities will become foundational someday. Network boundaries will disappear, and clouds and networks will fully merge.\nHello all, ICT Consulting firm DCIG has recognized Huaweis OceanStor Dorado in its latest 2020-21 Enterprise All-Flash Array Buyers Guide. The Buyers Guide provides an objective evaluation and ranking of all-flash storage features from an end users perspective. Huawei has been moving up the ranks in the Buyers Guide in recent years, achieving the highest growth rate for the global all-flash market over consecutive quarters. Huawei says this is a testament to its success. Now, the DCIG guide has recognized Huaweis OceanStor Dorado V6 and OceanStor F V5 series, which have both achieved Recommended ratings. Further, Huawei achieved the highest growth rate in the global all-flash market for several consecutive quarters as one of the fastest growing vendors in the industry. While most global storage vendors experienced a revenue decline in Q1 2020, Huawei storage continued to excel with a growth rate of 24.7%. Huawei all-flash storage also showed an exceptional market growth at 45.1%, surpassing average market growth by a wide margin, the company states. The OceanStor Dorado Series was launched as Huaweis next generation of all-flash products in 2019. The series capitalizes on the growth of E2E NVME to create data highway networking for unified data transmission and processing. The OceanStor Dorado series implements E2E NVMe with support for multiple protocols, such as NVMe over FC and NVMe over RoCE V2 for storage and host networks. It also implements a 100G RDMA interconnection between controller enclosures and NVMe disk enclosures for speed.\nDoes SmartCache need a dedicated SSD? Example Is it possible to destroy SSD used in SmartCache and use it for SmartTier? And vice versa, can you destroy the SmartTier and use SSD as SmartCache? Yes, SmartCache needs a dedicated SSD. SmartTier , an intelligent data storage tiering feature developed by Huawei manages data storage intelligently, calculates the activity level of data, and determines the appropriate tier for the data. It improves the performance of storage systems and minimizes TCO. The SmartTier relocates data with higher activity rates to high-performance tiers (such as SSDs), and relocates data with lower activity rates to more cost-effective tiers (such as NL-SAS disks). It provides hot data a fast response and high input/output operations per second (IOPS). The overall storage system performance is aggressively improved as a result. The data statistics, analysis, and relocation executed by the SmartTier are based on the SmartTier policy and performance requirements of applications. The SmartTier still keeps the required service continuity and data availability during the previous actions. SmartCache is an intelligent data cache feature developed by Huawei. This technology creates a SmartCache pool of SSDs and migrates frequently accessed random small I/Os from conventional hard disk drives (HDDs) to the high-speed cache pool. The data read speed of SSDs is much faster than that of HDDs, so SmartCache greatly shrinks the system response time and improves the system performance. SmartCache divides the cache pool into multiple partitions to provide fine-grained SSD cache resources. Different services can share one partition or use different partitions.\nHi team, What is the difference between full write and full write-ahead, and what are their corresponding policies in Oceanstor v3? Thanks. Dear Lucas, The reserved space for full write indicates the space threshold for triggering a system to enter the write protection mode. When the remaining space of a storage pool is less than or equal to the reserved space for full write, the storage pool is filled up. The reserved space for full write-ahead indicates the space threshold for triggering a system to enter the write through mode. When the remaining space of a storage pool is less than or equal to the reserved space for full write-ahead, the storage pool is going to be filled up. Full write Reserved space a. 20 GB b. 1 GB space is reserved for each thin LUN, thin FS, snapshot, and thick LUN with asynchronous remote replications in a storage pool. The total reserved space depends on the specific number of the thin LUNs, thin FSs, snapshots, and thick LUNs with asynchronous remote replications in the storage pool. c. Watermark for the total space of a storage pool: 5% The reserved space is subject to the minimum one among the preceding values. Criteria The remaining space of a storage pool is less than or equal to its reserved space for full write. Policy All LUNs and FSs in the storage pool are write-protected. Full write-ahead Reserved space The reserved space for full write-ahead is an integral multiple of the reserved space for full write.\nCurrently, the reserved space for full write-ahead is four times larger than the reserved space for full write. Criteria The remaining space of a storage pool is less than or equal to its reserved space for full write-ahead. Policy Thin LUNs, thin FSs, and thick LUNs with asynchronous remote replications in a storage pool are enabled with write through. Thanks. Dear Lucas, The reserved space for full write indicates the space threshold for triggering a system to enter the write protection mode. When the remaining space of a storage pool is less than or equal to the reserved space for full write, the storage pool is filled up. The reserved space for full write-ahead indicates the space threshold for triggering a system to enter the write through mode. When the remaining space of a storage pool is less than or equal to the reserved space for full write-ahead, the storage pool is going to be filled up. Full write Reserved space a. 20 GB b. 1 GB space is reserved for each thin LUN, thin FS, snapshot, and thick LUN with asynchronous remote replications in a storage pool. The total reserved space depends on the specific number of the thin LUNs, thin FSs, snapshots, and thick LUNs with asynchronous remote replications in the storage pool. c. Watermark for the total space of a storage pool: 5% The reserved space is subject to the minimum one among the preceding values.\nHello team, What does the disk enclosure ID DAE000 mean in Oceanstor v3? Thanks. Dear Geek, Have a good day. DAEXXX is used as an example. For 5300 V3, 5500 V3, 5600 V3, and 5800 V3, DAE indicates the disk enclosure, the first X indicates the cabinet ID, the second X indicates the loop ID, and the third X indicates the expansion depth (that is, location of the connected disk enclosure in the loop). For example, DAE000 indicates the disk enclosure that resides in location 0 of loop 0 in cabinet 0. For 6800 V3 and 6900 V3, DAE indicates the disk enclosure, the first X indicates the engine ID, the second X indicates the loop ID, and the third X needs to be converted to binary. Specifically, the third X consists of four binary bits. The most significant bit indicates the loop ID, and the least three significant bits indicate the expansion depth (if the most significant bit is 0, the loop ID is equal to the value of the second X ; if the most significant bit is 1, the loop ID is equal to 16 plus the value of the second X ). For example, DAE000 indicates the disk enclosure that resides in location 0 of loop 0 in cabinet 0. DAE008 indicates the disk enclosure that resides in location 0 of loop 16 in cabinet 0. DAE009 indicates the disk enclosure that resides in location 1 of loop 16 in cabinet 0.\nHello all, Today I will talk about4 factors that drive the digital transformation of banks. To ensure that customer needs are understood and analyzed. In today's ever-changing market trends, seamless delivery of services, high-end user experience, personalized product experience, transparency, and security are at the heart of customer satisfaction. In this competitive environment, organizations must adopt a \"customer first\" approach to success. Bring about changes in operating procedures, introducing digital platforms into service offerings, enhancing customer interactions, and so on. All of these activities not only improve the quality of your service, but also increase the level of customer interaction with your business, which is a key driver of any business success. The threats to traditional credit card payment systems, coupled with new technologies, have led to the development of various payment processing technologies that not only make transactions more convenient, but also make transactions more secure. EMV Credit Cards Also called Chip-and-PIN cards, these cards feature a chip that stores a cryptogram that detects modified transactions. It also requires a PIN for extra authentication. Contactless RFID Credit Cards This payment technology uses passive Radio Frequency Identification that allows cardholders to wave the cards in front of RF terminals to complete transactions. Mobile Wallets First launched in Japan in 2004, this technology works on NFC enabled smartphones, and has since been implemented by Google and Apple through their mobile platforms. New Payment Processing Architectures - three next-gen architectures designed to improve secure mobile payments. Encryption and tokenization Cloud-based PoS systems Secure Element systems.\nHello all, This post introduces the background and architecture of the STaaS solution. With the development of the mobile Internet, Internet of Things, and Internet of everything, data amount has been increasing exponentially. Users face the following challenges in data storage, optimization, use, and management of traditional data centers: Heterogeneous storage devices are purchased in several batches during data center construction, diversifying storage resources. The quantities and types of storage devices increase, increasing the complexity in storage resource management. Storage resources on different devices gradually form heterogeneous islands among which data cannot flow. Storage capacity fragments increase. Planning and allocating storage space become increasingly difficult. The rollout of medium- and large-sized data services involves service planning as well as procurement, configuration, and provisioning of storage resources. The rollout usually takes several months. Capacity expansion, reconstruction, and service provisioning across heterogeneous devices are difficult due to vendor lock-in. Data centers face problems related to resource consolidation and quick provisioning and cannot be transformed to cloud data centers. Huawei's STaaS solution consolidates resources from traditional enterprise storage, and private clouds, delivering storage services for upper-layer applications as services. Customers do not need to focus on complex storage details any more but transparently use storage resources through storage interfaces. This enables customers to use storage resources based on the service catalog and resource pool rather than each storage device. The STaaS solution implements servitization and intelligentization for management and usage of storage resources. The STaaS solution architecture is shown in Figure 1.\nHello all, Checking the host information includes checking the information about the configuration, multipathing software, and aggregated disks of a host. This post describes how to check the host information in different operating systems such as Windows, Linux, AIX, and HP-UX. You can check whether UltraPath is working properly by viewing information about physical paths, logical paths, virtual disk properties, performance statistics, and alarms on the CLI or GUI of UltraPath. 1. Check the physical path status. In the CMD window, run the upadm show path command to check the status of a specified or all physical paths, including the working status as well as the storage system, controller, and HBA to which the physical paths belong. 2. Check the virtual disk information. In the CMD window, run the upadm show vlun command to check the information about all LUNs or a specified virtual disk, including VLUN IDs, host LUN IDs, disk names, VLUN names, VLUN WWNs, VLUN status, capacities, storage device names, storage SNs, logical path IDs, and working status. 3. Check the performance statistics. In the CMD window, run the upadm show iostat command to check the performance statistics information about a storage system or VLUN, including the IOPS and bandwidth. 4. Check the UltraPath configuration. In the CMD window, run the upadm show upconfig command to check the UltraPath configuration. 1. Check whether the multipathing software is installed. Run the rpm -qa | grep UltraPath command to check whether the UltraPath is installed properly. If the UltraPath information is displayed, UltraPath is installed.\n2. Check the physical path status. Run the upadmin show path command to check the information about a specified or all physical paths, including physical path IDs, initiator WWNs, storage system name, controllers, target WWNs, physical path status, path detection type, path detection status, and port type. 3. Check the virtual disk information. Run the upadmin show vlun command to check the information about all LUNs or a specified virtual disk, including VLUN IDs, disk names, VLUN names, VLUN WWNs, VLUN status, capacities, storage device names, storage SNs, logical path IDs, and working status. 4. Check the logical path status. Run the upadmin show vlun id =? command to check the information about the logical path of a VLUN whose ID is specified, including logical path ID, SCSI address, and path status. 5. Check the UltraPath configuration. Run the upadmin show upconfig command to check the UltraPath configuration. 1. Check the physical path status. a. Run the upadm show phypath command to check the information about a specified or all physical paths, including physical path IDs, initiator WWNs, storage system name, controllers, target WWNs, physical path status, path detection type, path detection status, and port type. b. Run the upadm start phypathcheck id =? command to check the working status of a specified physical path. 2. Check the logical path status. Run the upadm show path command to check the information about all logical paths or a specified VLUN's logical paths.\nThe information includes VLUN IDs, logical path IDs, physical path IDs, initiator WWNs, storage system name, target WWNs, logical path status, and port type. 3. Check the virtual disk information. Run the upadm show vlun to view the information about a specified or all VLUNs mapped from the storage system to the application server. The information includes VLUN IDs, host LUN IDs, disk names, VLUN names, LUN WWNs, VLUN status, VLUN capacities, storage system name, and storage SN. 4. Check the performance statistics. Run the upadm show iostat command to view the IOPS and bandwidth information about the storage system or a VLUN. 5. Check the UltraPath configuration. Run the upadm show upconfig command to check the UltraPath configuration. HP-UX11.31 is delivered with the NMP multipathing software. The NMP multipathing software is installed upon the system installation. 1. Check the NMP status and ensure that it is enabled. Run the scsimgr get_attr -a leg_mpath_enable command to check the NMP status. 2. Check the disks that the system discovers and check the NMP status of LUNs that are mapped. a. Run the ioscan -funNC disk command to check the LUNs that are mapped. b. Run the scsimgr get_attr -D diskname -a leg_mpath_enable command. In the command, diskname indicates the name of the device to which the system allocates the LUNs. 3. Check the disk path information after the NMP takes over the disks. Run the scsimgr lun_map -D ? and scsimgr get_info -D ?\nHello team, Who can tell me about the meaning of FCV? Thank you for your help. Dear Lucas, FCV is a comprehensive system developed by Huawei for centralized monitoring and management of multiple sites in the remote offices and branch offices of enterprises, as well as service provisioning. This system is designed to manage sites in a centralized manner because many remote sites do not have O&M personnel. It can also simplify O&M at the central site so that IT skill requirements are lowered. FCV centrally monitors Layer 1 resources (energy infrastructure) and Layer 2 resources (servers, storage, and other IT resources) by site, covering all device types in typical solutions of FusionCube 1000. Operations such as site status query, basic configuration, power-off, and application deployment can be performed at the central site. For a large-scale site, deploy an upper-level FCV and multiple lower-level FCVs to manage NEs. The upper-level FCV manages multiple lower-level FCVs, is the unique global entry for users, and provides global information, device and resource reports, and alarm monitoring information. A lower-level FCV directly monitors and manages sites in the region, provides regional site information, device and resource reports, and alarm monitoring information, and allows you to maintain sites, and provision VMs, Docker resources, and applications to sites. To manage 500 to 20,000 sites, deploy FCV at two levels, where an upper-level FCV manages up to 80 lower-level FCVs and a lower-level FCV manages up to 500 sites. The lower-level FCV supports three deployment scales: 5000 NEs, and 20,000 NEs.\nHello all, This post is about the overview of heterogeneous data migration solution. The heterogeneous data migration solution migrates data from a source storage array to a target storage array by leveraging SmartVirtualization and SmartMigration functions. Takeover of the source storage: uses the heterogeneous virtualization function to take over the source storage (Huawei devices) online. Data migration: uses the SmartMigration function to migrate data online. After the migration is complete, remove the source storage from the live network online. Advantages 1. Data can be migrated between different SAN-based storage arrays. 2. Host resources are not occupied, and services are not interrupted during the migration. 3. The rollback solution is simple and easy to operate. Disadvantages 1. A value-added feature license is required. 2. The source storage must meet the heterogeneous compatibility requirements. Application Scenarios 1. When different types of SAN-based storage products are replaced, the heterogeneous migration solution is simpler than the application solution. 2. Applicable to scenarios where UltraPath or other built-in multipathing software is used. Process Step 1: Connect the source storage to the target storage and map the LUNs to be migrated to the target storage. Step 2: Create masqueraded eDevLUNs on the target storage and map them to the host. Step 3: The multipathing software aggregates the links between the source LUN and eDevLUNs and switches host I/Os to the links of the target storage. Step 4: Create a SmartMigration task on the target storage to migrate data from the eDevLUNs to the internal LUNs of the target storage.\nHello, team! We install eService, and I find alarm report status is abnormal, so what can I do if the alarm reporting status is abnormal when the eService client is running? Any solutions will be appreciated! Thanks in advance! Hello, dear! Have a nice day! When the eService Client is running, if the alarm reporting status of a device becomes \"Failed to receive the device alarms\" or \"Failed to connect the device\", perform the following steps to handle the abnormal states: Failed to receive device alarms 1. On the Devices page, select the device whose alarm reporting status is abnormal and click Stop Alarm Reporting . When alarm reporting is stopped, click Start Alarm Reporting . If alarm reporting is started successfully, go to the next step. If alarm reporting fails to be started, see and go to the next step. 2. Check whether the eService Client can receive the test alarms sent by the device. If yes, skip this step. If no, check whether port 10162 using UDP between the local host and device is enabled. If the port is not enabled, add an inbound rule for port 10162 on the local host running Windows. If the inbound rule is added successfully, check whether the eService Client can receive the test alarms sent by the device. Failed to connect the device 1. Check whether the network connection between the local host and the device is normal. If yes, go to the next step. If no, contact the equipment room administrator of the customer.\nHello, all! Our eReplication server failed to be started, the number of occupied ports and incorrect ports will be displayed. You must re-configure the ports that are displayed. So, how can I change the port of the eReplication server? Please help me, thanks in advance! Hello, dear! Some ports are occupied when the eReplication Server is being started. If those ports are occupied by other applications or the format of port numbers is incorrect, the eReplication Server cannot be started. You must re-configure ports of the eReplication Server. Changing ports of the eReplication Server in Linux: For example, when the eReplication Server fails to be started, a message is displayed indicating that the linux.trap.listening.port=10161 port is occupied and the linux.http.to.https.redirect.port=80ab port is with an incorrect format. 1. Use PuTTY to log in to the server where the eReplication Server is installed. In software package-based installation mode: Log in as user root . 2. Run the TMOUT=0 command to prevent PuTTY from exiting due to session timeout. 3. Run cd /opt/BCManager/Runtime/bin/config/conf to enter the path for saving the port configuration file. 4. Run the vi port.ini command to open the port.ini port configuration file. 5. Press i to go to the edit mode and edit the port.ini file. 6. Re-configure the port number as prompted or based on the port number configuration requirements. For example, the port numbers to be changed are linux.trap.listening.port=10162 and linux.http.to.https.redirect.port=8080 . 7. After you have edited the port configuration file, press Esc and enter :wq!\nHello all, Have a good day. Today I will introduce you Huawei Campus OptiX Solution. The emergence of cloud services, Internet of Things (IoT), Wi-Fi 6, and a multitude of video services, has overwhelmed traditional campus networks, now burdened with a complex architecture that has high maintenance and scaling costs. Huawei Campus OptiX Network Solution adopts cutting-edge technologies such as: Passive Optical Network (PON) access, a simplified PON architecture, multi-service transmission, passive long-haul transmission, and converged Ethernet to tackle the ever-growing requirements of a modern campus. The solution delivers high bandwidth and low latency, improves Operations and Maintenance (O&M) efficiency, supports future evolution, and protects existing investments for enterprises in the education, hotel, transportation, airport and healthcare sectors. University campus Service scenarios: classrooms, offices, libraries, canteens, dormitories, apartments, and security protection Huawei Campus OptiX Network Solution provides optical LANs for schools. It is passive and reliable, and has a smaller footprint and is more energy efficient. Hotel campus Service scenarios: hotel lobby, office, and guest room Huawei Campus OptiX Network Solution extends fiber to each room to attract more guests. The solution not only provides better internet for guests, but also support the access of hotel systems, such as surveillance, office, and intelligent systems. Airport campus Service scenarios: check-in, waiting area, and tax-free shop Huawei Campus OptiX Network Solution provides airports with reliable full-fiber networks to connect video devices, offices, and flight information display systems (FIDSs), and commercial buildings, accelerating the digital transformation of airports.\nHello, Community! Good day! What is InfoReplicator? What is InfoStamper? Could you introduce them briefly? Thanks in advance! Hello, dear! Have a nice day! InfoReplicator The InfoReplicator feature of theOceanStor 9000 V5storage system supports asynchronous remote replication specific to directories. Folders or files can be automatically and periodically or manually replicated between multipleOceanStor 9000 V5storage systems through IP links over a local area network (LAN) or wide area network (WAN). The remote replication-based data recovery goes through the following phases: Data of the primary storage system is periodically or manually replicated to the secondary storage system. When the primary storage system is faulty, access requests from upper-layer applications are failed over to the secondary storage system. When the primary storage system recovers, access requests are failed back to the primary storage system. If you want to know more information about InfoReplicator, please check the following post: InfoStamper Storage Networking Industry Association (SNIA) defines a snapshot as a point-in-time copy of a defined collection of data. Such a copy contains a static image of the source data at the copy point in time. Snapshots can be used for production testing, data backup, and data recovery. The InfoStamper is a directory-based snapshot feature provided byOceanStor 9000 V5. It can create snapshots for any directory (excluding the root directory) in a file system instantly, enabling users to back up critical data without affecting services. In addition, theOceanStor 9000 V5can periodically create snapshots for source data.\nWhat to do if the alarm rporting status is abnormal when eService is running? When eService is running, if the alarm reporting status of a device becomes \"Failed to receive the device alarms\" or \"Failed to connect the device\", perform the following steps to handle the abnormal states: Failed to receive device alarms 1. On the Devices page, select the device whose alarm reporting status is abnormal and click Stop Alarm Reporting . When alarm reporting is stopped, click Start Alarm Reporting . If alarm reporting is started successfully, go to the next step. If alarm reporting fails to be started, see and go to the next step. 2. Check whether eService can receive the test alarms sent by the device. If yes, skip this step. If no, check whether port 10162 using UDP between the local host and device is enabled. If the port is not enabled, add an inbound rule for port 10162 on the local host running Windows. If the inbound rule is added successfully, check whether eService can receive the test alarms sent by the device. Failed to connect the device 1. Check whether the network connection between the local host and the device is normal. If yes, go to the next step. If no, contact the equipment room administrator of the customer. After the network connection is restored, wait for 5 to 10 minutes. If the fault persists, go to the next step. 2.\nHello team, What is the difference between ext2, ext3, and ext4 file system in Linux? Thanks for your help. Dear Lucas, Ext stands for Extended file system, and is the first created specifically for Linux. It has major revisions. Ext is the first version of the file system, introduced in 1992. It is a major upgrade from the Minix file system used at the time, but it lacks important features. Many Linux distributions no longer support Ext. Ext2 is not a journaling file system. When introduced, it is the first file system to support extended file attributes and 2 terabyte drives. Ext2s lack of a journal means it writes to disk less, which makes it useful for flash memory like USB drives. However, file systems like exFAT and FAT32 also dont use journaling and are more compatible with different operating systems, so we recommend you avoid Ext2 unless you know you need it for some reason. Ext3 is basically just Ext2 with journaling. Ext3 was designed to be backwards compatible with Ext2, allowing partitions to be converted between Ext2 and Ext3 without any formatting required. Its been around longer than Ext4, but Ext4 has been around since 2008 and is widely tested. At this point, youre better off using Ext4. Ext4 is also designed to be backwards compatible. You can mount an Ext4 file system as Ext3, or mount an Ext2 or Ext3 file system as Ext4.\nHello all, Today I want to share with you Huawei BC&DR Geo-Redundant . The Business Continuity and Disaster Recovery Solution (Geo-Redundant Mode) provides two-level DR protection for customers' data centers to ensure service continuity. Solution Definition Business Continuity and Disaster Recovery Solution (Geo-Redundant Mode) is a three-site DR solution that consists of a production center, a same-city DR center, and a remote DR center. The continuity of core services can be ensured when any two data centers fail, remarkably improving the DR solution availability. Production center: Provides services for applications. Same-city DR center: It is dozens of kilometers away from the production center. It is directly connected to the production center over Fibre Channel networks to provide level-1 DR protection. Remote DR center: It is hundreds or thousands of kilometers away from the production center. It is established in case of natural disasters that may damage the previous two data centers simultaneously such as earthquake. It provides level-2 DR protection through periodic asynchronous replication. In the Disaster Recovery Data Center Solution (Geo-Redundant Mode) using HyperMetro and remote replication, HyperMetro and HyperVault, or DR Star (HyperMetro + asynchronous remote replication), two active-active data centers provide services simultaneously. Solution Architecture This figure shows the network of a DR Star solution based on synchronous remote replication + asynchronous remote replication. Figure Network of the DR Star solution The solution consists of four layers. Table 1 lists functions and components at each layer.\nHello, dear! As we know, there are many concepts for Storage. I am not very clear about some terms related to storage, such as file storage, block storage, and object storage. What do they mean? Can anyone explain their strengths and weaknesses? And how should I choose? Best Wishes! Freaks! Hello, Freaks! I can answer your question simply according to my experience, there are many experts in the forum, I believe you can get a satisfactory answer. File storage is pretty simple. You give a file a name, give them some attributes, and organize them in your favorite folder structure. Most of these files generally have a general naming convention that is identifiable within the file structure. Block Storage Block storages are files that are split into evenly sized blocks of data and each block has its own address but with no other information (metadata) on what that block of data actually is. Object-based storage stores data in containers known as objects. An object contains 3 parts (data, metadata, and a Unique Identifier).Object storage seeks to enable capabilities not addressed by other storage architectures such as block and file storage.Object-storage systems allow the retention of massive amounts of unstructured data. Object storage is used for purposes such as storing photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox. The following table briefly summarizes the advantages and disadvantages of each.\nHello all, Do you know what are the key factors for achieving failover in seconds in the event of a controller fault in dorado V6? Here are 3 key factors: 1. Fast controller fault detection OceanStor Dorado uses hardware acceleration to quickly detect controller faults. If a controller becomes faulty, the operating system captures the fault and initiates a reset process to which a quick detection technique has been added. Before the reset, the front-end, back-end, and expansion interface modules are quickly disabled, and the controller fault information is quickly sent to the other controllers in the enclosure over hardware. After receiving the reset interrupt information, the controllers in the enclosure disconnect the links and heartbeat from the faulty controller. In this way, the system can detect controller faults within 200 ms. 2. Quick service takeover Upon detecting a fault, the system immediately takes over services from the faulty controller. Because some metadata of the objects running on each VNode is saved on disks, and reading from disks could prolong the takeover time, the metadata is mirrored to another controller so the takeover can be completed as quickly as possible. When a fault occurs, the metadata mirroring controller takes precedence in taking over services. During the takeover, the metadata is directly restored from the memory, which reduces disk access to achieve quick takeover. 3.\nFront-end interconnect I/O module (FIM) On mission-critical storage systems equipped with FIMs, the FIMs forward requests to other controllers for processing in the event of a controller fault, and the host will not detect link interruption. The FIMs can detect a fault within 1 second. With these techniques, OceanStor Dorado can take over services from a faulty controller within seconds, ensuring service continuity. Testing Controller Failure In this test, the OceanStor Dorado 8000 V6 storage system is used. A controller is removed to verify that controller failover can be completed in seconds. Vdbench is used to deliver service loads. The following table lists the workload model. Table Workload model Test Tool I/O Size Read/Write Ratio IOPS Vdbench 8 KB random 8:2 240,000 Prerequisites 1. The OceanStor Dorado 8000 V6 storage system is running properly. 2. Vdbench is used to deliver 8 KB random I/Os, with 8:2 read/write ratio. The performance reaches 240,000 IOPS and the latency is 0.4 ms. Procedure 1. Check the load on Vdbench. Figure Performance on Vdbench 2. Confirm load balancing between controllers. Figure Performance of four controllers 3. Remove controller 0D from the storage system. Figure Device diagram 4. Verify that I/Os are not suspended and the performance restores to the original value within 10 seconds. Figure Performance on Vdbench 5. Query performance monitoring information on DeviceManager. Figure Performance charts 6. Confirm alarm information. A controller removal alarm is generated. Figure Alarm information 7. Restore the controller. Figure Device diagram 8. The performance does not drop during controller recovery.\nHello, expert! Do you know the advantage of the EC algorithm over traditional RAID 5? Can you describe it? Thanks in advance! Hello, dear! Have a nice day! The EC algorithm is a cross-node algorithm. The 4+2:1 redundancy allows a node failure without losing data. RAID 5 is often a group of disks on a node and is only immune to disk failures. The EC algorithm adopts the global hot spare mode, requiring no independent hot spare disks. All disks are involved in data access. Once the system has available space, data can be recovered. For RAID 5, at least one global hot spare disk needs to be configured for each node. The EC algorithm in 4+2:1 redundancy allows two disk failures without losing data. In RAID 5, each RAID group is immune to only one faulty disk. It is not practical to expect that all faulty disks are allocated in different RAID groups. If a disk is faulty, many disks are involved in data recovery in EC mode, while only one disk (hot spare disk) can be used to write data in RAID mode. For this reason, data recovery efficiency (1 TB/h) in EC mode is dozens of times faster than that in RAID mode. Disk failure potentials during data recovery dramatically drop (highly risky since capacity of a disk is at the TB level currently), improving system security. In EC mode, you can set protection levels for directories. For key data directories, you can set a high security level to ensure directory security.\nIn the era of cloud computing, the amount of data increases into a geometric form, and it is inevitable to consider increasing the storage capacity. However, the increase in storage capacity does not improve the storage performance, and there is no necessary connection between them; Storage capacity refers to the size of data that can be stored on the storage device. For example, if a disk array has 50T space, the storage capacity of the device is 50T (in general, it will not reach 50T); IOPS : (Input/Output Operations Per Second), which is the number of read and write IO operations per second. The IOPS performance of the storage side is different from that of the host side. IOPS refers to how many times the host can receive accesses per second. The host's IO needs to access the storage multiple times to complete. Generally speaking, the host writes a minimum data block, and also goes through three steps of \"send write request, write data, receive write confirmation\", that is, three storage accesses. When you need to consider the storage capacity upgrade, performance and capacity must meet the requirements. Just considering the capacity, you will fall into the trouble of insufficient performance.\nThen the FC plane serial port of the switch broad in slot 7 can be connected. 5. Alternatively, you can log in to the built-in FC plane system of the switch by entering the user name ( admin ) and the password ( password ). 2. Confirming the switch board IP address 1. Run the ipaddrshow command to view the IP address of the FC module. The default IP address of each FC switch module is 10.77.77.77, as shown in the following figure: 2. If you need to change the IP address of the switch, run the ipaddrset command. For example, if you need to change the IP address to 192.168.2.16, you only need to type 192.168.2.16 in the red part of the following figure and press Enter . You can change the IP address to another one onsite based on the actual conditions. For example, you can change the IP address to the address that is on the same network segment as that of the BASE network port on the server broad. However, you must ensure that the changed IP address is different from other network device IP addresses. 3. Collecting the switch broad information 1. Install an ATAE broad with Linux inside the enclosure and enable the FTP function, making the broad work as a server. Create a directory that is used to save the collected information, such as the creation of a / fcinfo directory.\nHello, I am fresh about Huawei enterprise storage. I have got OceanStor 2200 V3 with two redundant 2 port SmartIO module SFP+ 10Gb. What is want to achieve is create storage for virtual machines based on Hyper-V on Microsoft Server 2016. My question is what is the best practise for that, i mean: 1) Network configuration - OceanStor will be connected to two 10Gbit/s stacked switch. What is the best practise for that? Create bonding with 2 port on Controller A and bonding with 2 port on Controller B, connect them to 10Gbit/s switch (2 link from Controller A and 2 link from controller B) and create Link Aggregation on switch? How will failover work in this scenario? 2) Storage configuration - I have got 3 Hyper-V servers. There is possible to create one but shared LUN storage for all of them? I want to attach the same storage space on all server. Is that good practise or can impact on performance, etc.? Best Regards, Mark Hi, Mark Based on your question, I think you should be reminded of the following two points: 1. Huawei storage IP address floating applies only to NAS services, but SAN services do not support IP address floating pairs. For NAS services, assume that port 1 and port 2 on controller A are bonded to port A and port 3 and port 4 on controller B are bonded to port B. Services are switched to bond port B only when both port 1 and port 2 are faulty.\nSorry for the disturbance, let me try to ask several questions regarding 'how-to giudes' in file access via CIFS topic?The goal is spread across two controllers the access of a several thousands (but less 10000) Windows-based clients to CIFS share at OceanStor or Dorado system.  1. The document ' ' ( ) explains this for OceanStor 2600V3. At the Client configuration step #1 (both a and b ) it's visible the particular client is configured with only single DNS IP, which refers to first logical port. In which mechanism the different DNS IP should be distributed among clients in effective way? DHCP with configuration groups? Will it be more convenient (for organization) to set both primary and secondary DNS as first and second logical adapters addresses? Will it be possible to set both addresses as single name at corporate DNS to obtain the round-robin style of connections balancing? 2. The document ' ' ( ) shows the logical port creation panel (step 6) with 'Listen DNS query requests' option disabled. Should that option be set to 'enabled', if the DNS-based load balancing was configured at Step ' ' ( )? Does that procedure applicable not to Dorado only, but to OceanStor as well? Thank You! Dear polezhaevdmi, Multiple IP addresses can be mapped to one domain name, it's possible to set both addresses as a single name at corporate DNS to obtain the round-robin style of connections balancing. And you have many questions let me check it.\nHello team, Which networking modes do OceanStor 18000 series storage systems support? Hi Axe, The networking modes supported by S5000, T series, and OceanStor 18000 series storage systems are as follows: Internet Protocol storage area network (IP SAN): indicates a storage area network that uses Ethernet switches to interconnect application servers, application server clusters, and storage systems. The Internet Small Computer Systems Interface (iSCSI) is used as the communications protocol among application servers, application server clusters, and SAN storage devices. Fibre Channel storage area network (FC SAN): indicates a storage area network that uses FC switches to interconnect application servers, application server clusters, and storage systems. The FC protocol is used as the communications protocol among application servers, application server clusters, and storage devices. Seamless mixed networking of FC SAN and IP SAN: indicates a storage area network that uses FC switches and Ethernet switches to interconnect application servers, application server clusters, and storage systems. The iSCSI and FC protocols are used as the communications protocols among application servers, application server clusters, and storage devices. The networking modes supported by S2600 storage systems are as follows: IP SAN: indicates a storage area network that uses Ethernet switches to interconnect application servers, application server clusters, and storage systems. The iSCSI is used as the communications protocol between application servers, application server clusters, and SAN storage devices. FC SAN: indicates a storage area network that uses FC switches to interconnect application servers, application server clusters, and storage systems.\nHello all, This post describes the background, definition, and advantages of HyperSnap. Huawei HyperSnap creates a point-in-time consistent copy of original data (LUN) to which the user can roll back, if and when it is needed. It contains a static image of the source data at the data copy time point. In addition to creating snapshots for a source LUN, the OceanStor Dorado V6 series storage systems can also create a snapshot (child) for an existing snapshot (parent); these child and parent snapshots are called cascading snapshots. Once created, snapshots become accessible to hosts and serve as a data backup for the source data at the data copy time. Re-purposing of Backup Data Background With the rapid development of information technologies, enterprises' business data has exploded, making data backups more important than ever. Traditionally, mission-critical data is periodically backed up or replicated for data protection. However, traditional data backup approaches have the following issues: A large amount of time and system resources are consumed, leading to high backup costs. In addition, the recovery time objective (RTO) and recovery point objective (RPO) for data backup are long. The backup window and service suspension time are relatively long, unable to meet mission-critical service requirements. Facing exponential data growth, enterprises' system administrators must shorten the backup window. To address these backup issues, numerous data backup and protection technologies, characterized by a short or even zero backup window, have been developed. Snapshot is one of these data backup technologies.\nLike taking a photo, taking a snapshot is to instantaneously make a point-in-time copy of the target application state, enabling zero-backup-window data backup and thereby meeting enterprises' high business continuity and data reliability requirements. Snapshot can be implemented using the copy-on-write (COW) or redirect-on-write (ROW) technology: COW snapshots copy data during the initial data write. This data copy will affect the storage write performance. ROW snapshots do not copy data during a write operation, but frequently overwrite data, causing the data discretely scattered in the source LUN and consequently affecting the sequential read performance. Legacy storage systems use Hard Disk Drives (HDDs) and take COW snapshots, which introduce a data pre-copy process resulting in a storage write performance penalty. Comparatively, Huawei OceanStor Dorado V6 series all-flash storage systems use Solid State Drives (SSDs), take ROW snapshots, and offer high random read/write performance capabilities; that is, the OceanStor Dorado V6 series storage systems eliminate the necessity for both the data backup process and the sequential read operations done in legacy storage snapshots, thereby delivering lossless storage read/write performance. Definition Huawei HyperSnap creates a point-in-time consistent copy of original data (LUN) to which the user can roll back, if and when it is needed. It contains a static image of the source data at the data copy time point. HyperSnap provides the following advantages: Supports online backup, without the need to stop host services. Provides writable ROW snapshots with no performance compromise.\nHello all, This post mainly talks about how to plan storage pools for Dorado V6. Storage pools are containers of storage resources. The storage resources used by application servers are all from storage pools. Storage pools must be properly planned for better storage utilization. Planning Disk Types for a Disk Domain Disks can be classified into self-encrypting disks (SEDs) and non-encrypting disks. They cannot exist in the same storage pool. SEDs are not sold in the Chinese mainland. SED : When data is written into or read from the SED, the data is encrypted or decrypted using the hardware circuits and internal encryption key of the SED. Before using SEDs to create a disk domain, you must configure the key service. For details, see the disk encryption user guide specific to your product model and version. Non-encrypting disk : does not support encryption. Planning RAID Policies Dorado V6 storage systems use dynamic RAID for redundancy and provide different levels of protection based on the number of parity bits in a RAID group. Table 1 describes RAID 5, RAID 6, and RAID-TP provided by storage systems when hot spare space is not considered. Table 1 RAID Level Number of Parity Bits Redundancy and Data Recovery Capability Maximum Number of Allowed Faulty Disks RAID 5 1 Relatively high. Parity data is distributed on different chunks. In each chunk group, the parity data occupies the space of one chunk. RAID 5 is able to tolerate the failure on only one chunk.\nIf two or more chunks fail, RAID 5 protection can no longer be provided. 1 RAID 6 2 High. Parity data is distributed on different chunks. In each chunk group, the parity data occupies the space of two chunks. RAID 6 is able to tolerate simultaneous failures on two chunks. If three or more chunks fail, RAID 6 protection can no longer be provided. 2 RAID-TP 3 High. Parity data is distributed on different chunks. In each chunk group, the parity data occupies the space of three chunks. RAID-TP is able to tolerate simultaneous failures on three chunks. If four or more chunks fail, RAID-TP protection can no longer be provided. 3 Storage Pool Configuration Rules When a storage pool is created on DeviceManager, a disk domain is automatically created within the storage system but is not displayed on DeviceManager. By default, the capacity of a storage pool is equal to the available capacity of the corresponding disk domain. An OceanStor Dorado V6 storage system supports one or more storage pools. During the initial configuration of a storage system, you can configure that all disks constitute a unique storage pool. After this configuration applies, you do not need to manually create any storage pool. You can manually create one or more storage pools. DeviceManager automatically selects appropriate disks to create a storage pool. Alternatively, you can select desired disks to create a storage pool. A single storage pool requires at least eight normal member disks.\nHello team, What can I do If the new configuration of an external LUN on a heterogeneous storage system cannot be synchronized to the local storage system? Thanks. Dear Lucas, If information about the external LUN is not updated on the local storage system after operations on the heterogeneous storage system such as adding or deleting initiators or LUN mappings, or expanding external LUNs, manually scan the local storage system for LUNs. Manually scan for LUNs using one of the following methods: Start an automatic LUN scan in DeviceManager. In CLI, run the scan remote_lun command. The operations in the following procedure are based on DeviceManager. For details about how to use scan remote_lun , see OceanStor 5300 V3&5500 V3&5600 V3&5800 V3&6800 V3 Storage System V300R003 command reference. 1. Log in to the DeviceManager for the local storage system. 2. Go to the Automatic LUN Scan dialog box. On the navigation bar, click Data Protection . The Data Protection page is displayed. In the left function pane, click Remote Device . The Remote Device page is displayed. Based on Name and SN , select the remote device that corresponds to the heterogeneous storage system. In the lower function pane, click Automatic Scan . The OK dialog box is displayed. 3. Click OK to start an automatic LUN scan. DeviceManager then scans for LUNs, and synchronizes the configuration of the remote LUN with that of the external LUN and updates the number of remote LUNs. 4. Click Close . Thank you!\nHi team, I use InfoLocker to save data in OceanStor 9000 V100R001C30. How is the atime expiry time calculated? Thank you. Dear Geek, Each file has atime (In this feature, the file read or execution time is regarded as the expiration time of the file. Once the file enters the protected state, atime does not change with file access or execution and becomes the expiration time of the file), ctime (Time when the status of a file is last changed. The status of a file changes with the file's content, permission, owner, owning group, and link count.) and mtime (time when a file's content is last changed). The system determines the time when files are last modified based on mtime and records the time when WORM files expire in atime.\nHi all, I will share with you how to deal with the problem that an error message is reported for LUN capacity expansion, when OceanStor V5R7C10 and OceanStor V5R7C00 storage systems are interconnected. Symptom On DeviceManager, an error message is displayed after a LUN in a remote replication pair is selected for capacity expansion, as shown in the following figure. Fault Identification 1. On DeviceManager, query the version of the local storage system. 2. Log in to the DeviceManager of the remote storage system and query its version. If the remote storage system is OceanStor V300R006C10, V300R006C10SPC100, V500R007C00, or V500R007C00SPC100, this problem occurs. Cause OceanStor V300R006C10, V300R006C10SPC100, V500R007C00, and V500R007C00SPC100 do not support capacity expansion for LUNs in a remote replication pair, and no corresponding error code is available. Therefore, the \"The system is busy.\" message is reported when they are interconnected with OceanStor Dorado V300R001C20, OceanStor Dorado V300R001C21, OceanStor V300R006C20 and OceanStor V500R007C10 that supports the related capacity expansion function. This problem is not a functional problem. Solution 1. No solution is required. 2. Both local and remote storage systems must support the related capacity expansion function if the capacity of a LUN in a remote replication pair needs to be expanded. Fault Diagnosis For error code problems, check whether the remote storage system supports the involved functions if the problem cause cannot be located based on the error message.\nHello team, I have a question about OceanStor V3 Thin LUN. Does it support SmartErase, SmartDedupe and SmartCompression? Thank you! Dear Phany, For V300R006C30 and earlier versions, only the file systems configured with SmartThin support SmartDedupe and SmartCompression. SmartDedupe and SmartCompression features are developed by Huawei to provide smart data deduplication and data compression functions. SmartDedupe is a data downsizing technology that deletes duplicate data blocks in a storage system to save physical storage capacity, meeting growing data storage needs. The storage system supports inline deduplication, that is, only the data that is newly written is deduplicated. SmartCompression reorganizes data to reduce storage space consumption and improve the data transfer, processing, and storage efficiency without any data loss. The storage system supports inline compression, that is, only the data that is newly written is compressed. For more details, please click Thanks. Dear Phany, For V300R006C30 and earlier versions, only the file systems configured with SmartThin support SmartDedupe and SmartCompression. SmartDedupe and SmartCompression features are developed by Huawei to provide smart data deduplication and data compression functions. SmartDedupe is a data downsizing technology that deletes duplicate data blocks in a storage system to save physical storage capacity, meeting growing data storage needs. The storage system supports inline deduplication, that is, only the data that is newly written is deduplicated. SmartCompression reorganizes data to reduce storage space consumption and improve the data transfer, processing, and storage efficiency without any data loss. The storage system supports inline compression, that is, only the data that is newly written is compressed.\nDear all, Do you know 3Hs for OceanStor Dorado V6 storage system? Don't worry, I will tell you they are High Reliability, High Availability and High System Security. High Reliability OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems offer advanced data protection technologies to minimize risks of disk failures and data loss, and protect data against catastrophic disasters, ensuring continuous system running. Component failure protection Storage system components are in 1+1 redundancy and work in active-active mode. Normally, the two redundant components are working simultaneously and share loads. If one component fails or is offline, the other one takes over all loads without affecting ongoing services. RAID 2.0+ The RAID 2.0+ underlying virtualization technology is used to automatically balance loads across disks. If a disk encounters a fault, all the other disks in the same disk domain help reconstruct the faulty disk's service data, achieving a 20-fold faster reconstruction speed than traditional RAID and significantly reducing the possibility of multi-disk failure. RAID 2.0+ supports dynamic RAID and flexible data layout, accelerating SSD reconstruction. Power failure protection Built-in backup battery units (BBUs) supply power to controller enclosures in the event of unexpected power failures. This enables the storage system to write cache data to built-in disks of controllers to prevent data loss. Global wear leveling If data is unevenly distributed to SSDs, certain SSDs may be used more frequently and wear faster than others. As a result, they may fail much earlier than expected, increasing the maintenance costs.\nOceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems use global wear leveling that levels the wear degree among all SSDs, improving SSD reliability. Global anti-wear leveling When the wear degree of multiple SSDs is reaching the threshold, OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems preferentially write data to specific SSDs. In this way, these SSDs wear faster than the others. This prevents multiple SSDs from failing simultaneously. Disk data pre-copy The disk data pre-copy technology enables the storage system to routinely check hardware status and migrate data from any failing disk to minimize the risks of data loss. Advanced data protection HyperSnap supports writable snapshots. Snapshot creation and activation have no impact on performance. HyperReplication backs up local data to a remote storage system for disaster recovery. HyperClone creates physical backup copies for source LUNs in real time. It supports incremental synchronization, reverse synchronization, and consistency groups, and allows data copy between different controller enclosures or disk domains, ensuring local data availability. HyperCDP achieves continuous data protection at an interval of several seconds, generating more intensive recovery points on storage devices. HyperMetro enables real-time data synchronization and access between two storage systems. If data access fails in either storage system, HyperMetro implements seamless service switchover to ensure data security and service continuity. The storage system supports multiple cache copies, which means that two or three copies can be created for the write cache.\nThe three-copy mode ensures data integrity in the write cache and service continuity in the event that two controllers of the storage system fail simultaneously. For a storage system with two controller enclosures that house eight controllers, three cache copies are distributed to both controller enclosures to protect data integrity in the write cache and service continuity in the event that either controller enclosure becomes faulty. HyperMetro-Inner takes the advantage of continuous mirroring, back-end global sharing, and three cache copies to tolerate successive failure of seven out of eight controllers, simultaneous failure of two controllers, and failure of one controller enclosure on an eight-controller network. High Availability OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems use TurboModule, online capacity expansion, and disk roaming to ensure service continuity during routine maintenance. TurboModule enables hot swap of controllers, fans, power modules, interface modules, BBUs, and disks. Online capacity expansion allows you to add disks to the system online with ease. Disk roaming enables the storage system to automatically identify relocated disks and resume their services. OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems use multiple resource application technologies to flexibly manage resources and maximize customers' ROI. SmartVirtualization allows a local storage system to centrally manage resources of third-party storage systems, simplifying management and reducing maintenance costs. SmartMigration migrates LUNs in or between storage systems, adjusting and allocating resources along with business development.\nHello all, Today I'm going to introduce you OceanStor Dorado 8000 V6 and Dorado 18000 V6. They are Huawei's brand-new all-flash storage systems designed for medium- and large-size enterprise storage environments. The storage systems focus on the core services of enterprise data centers, virtual data centers, and cloud data centers to meet their requirements for robust reliability, excellent performance, and high efficiency. OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems leverage a SmartMatrix full-mesh architecture, which guarantees service continuity in the event that one out of two controller enclosures fails or seven out of eight controllers fail, meeting the reliability requirements of enterprises' core services. In addition, OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems incorporate AI chips, meeting the requirements of various service applications such as online transaction processing (OLTP), online analytical processing (OLAP), high-performance computing (HPC), digital media, Internet operations, centralized storage, backup, disaster recovery, and data migration. Figure 1 and Figure 2 show the appearance of the OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems. Figure 1 OceanStor Dorado 8000 V6 storage system Figure 2 OceanStor Dorado 18000 V6 storage system OceanStor Dorado 8000 V6 and Dorado 18000 V6 storage systems provide comprehensive data backup and disaster recovery solutions to ensure the smooth and secure running of data services. The storage systems also offer various methods for easy management and convenient local/remote maintenance, remarkably reducing management and maintenance costs.\nHi guys! We have purchased the OceanStor 9000 and plan to install it recently. I have a question about the OceanStor 9000. And my issue is: How does a front-end server access to the OceanStor 9000? How many IP addresses does a node present externally? This is my problem, can anyone help me? Thank you! Hello, dear zj5000! Have a nice day! It is recommended that a front-end server access an OceanStor 9000 through the domain name of the OceanStor 9000. IP address mounting is not recommended. Typically, one node externally presents two dynamic IP addresses and two static IP addresses. You can mount a dynamic IP address of a node so that if the node fails, the mounted IP address will be automatically floated to another node. The following table describes the OceanStor 9000's requirements for the IP addresses of the front-end network. Item Quantity Details Front-end service networks Node quantity x 4 + 1 Assign two IP addresses to each node. The two IP addresses act as static front-end IP addresses of the node. Assign ( = Number of nodes x 2) dynamic front-end IP addresses to the IP address pool of InfoEqualizer. Allocate one InfoEqualizer DNS IP address. InfoEqualizer is a load balancing feature used to manage connections between clients and the OceanStor 9000. The feature provides the following functions to improve availability, performance, and reliability of the OceanStor 9000: A client can use a domain name to access the OceanStor 9000, simplifying the client connection process.\nHello, everyone! Our engineer suggests we use Huawei active-active solution by the OceanStor 18000 V5, can you tell me the advantages of the active-active solution provided by the OceanStor 18000 V5? Thanks! Hello, dear! Have a nice day! Critical services (such as bank transaction settlement and telecom billing) of customers must be available around the clock. Once a fault or disaster occurs, the DR center must be able to take over services in real time. Active-active solutions naturally become an optimal choice to ensure the continuity of critical services. There are two active-active modes in the current storage industry: the active-passive (AP) mode and the active-active (AA) mode. In active-passive mode, some services run on DC A, with DC B as the hot backup, while other services run on DC B, with DC A as the hot backup. This achieves approximate active-active effects. In active-active mode, all I/O paths can access active-active LUNs to achieve load balancing and seamless failover. The OceanStor 18000 V5 employs the AA mode. Two sites simultaneously provide services, fully utilizing DR resources. However, the traditional active-active solution requires customers to deploy independent external gateways, largely complicating networking and driving up purchase and management costs. The deployment of external gateways adds more nodes to the IT system, prolonging system latency and compromising the overall system performance. Drawing strength from HyperMetro (gateway-free active-active solution), the OceanStor 18000 V5 uses its own storage software to implement active-active without requiring any extra gateways. Huawei active-active solution ensures service continuity for customers.\nThe advantages of this solution are as follows: 1. The active-active architecture supports concurrent read and write operations, fully utilizing DR resources. 2. No gateways are required, reducing fault points and improving system reliability. 3. The CAPEX is reduced, the I/O processing path is shortened, and performance bottlenecks caused by gateways are resolved. 4. Heterogeneous storage systems are supported, reusing existing devices. 5. FastWrite and network self-adaptation improve active-active performance. 6. This solution can be smoothly expanded to data protection solutions, such as 3DC and centralized DR. Any further questions, let us know! Hello, dear! Have a nice day! Critical services (such as bank transaction settlement and telecom billing) of customers must be available around the clock. Once a fault or disaster occurs, the DR center must be able to take over services in real time. Active-active solutions naturally become an optimal choice to ensure the continuity of critical services. There are two active-active modes in the current storage industry: the active-passive (AP) mode and the active-active (AA) mode. In active-passive mode, some services run on DC A, with DC B as the hot backup, while other services run on DC B, with DC A as the hot backup. This achieves approximate active-active effects. In active-active mode, all I/O paths can access active-active LUNs to achieve load balancing and seamless failover. The OceanStor 18000 V5 employs the AA mode. Two sites simultaneously provide services, fully utilizing DR resources.\nHi team, How manySmartTier policies doesOceanStor V3 have? How do I configure this? Thank you! Hi Phany, Would you please check this out forSmartTier policy? Set the SmartTier policy when you are creating a LUN or when the SmartTier is in use. The object of this parameter is a single LUN in a storage pool. This parameter value can be changed. Table Relocation Policy Description Application Scenario No relocation The no relocation policy does not relocate data blocks among storage tiers, and data can be relocated only when the SmartTier policy is changed. The no relocation policy is the default policy set upon the creation of a LUN. Automatic relocation It relocates data based on the hotness (access frequency) ranking order of data blocks. You must enable I/O monitoring for the configured automatic relocation policy to take effect. Service data experiences busy and idle periods, and hot and cold data coexists. To improve system overall performance, frequently-accessed data needs to be stored at the higher performance tier. Highest relocation This policy preferentially promotes data blocks to the high-performance tier and the performance tier. If this policy is configured and the amount of relocated data is large, the highest tier may be fully occupied and data cannot be relocated into or from the tier. The highest available policy is recommended for the applications (such as OLTP databases) sensitive to response time, IOPS, and bandwidth.\nDear all, This case mainly talks about how to analyzethe missing data on OceanStor 5800 V3 disk. [Problem Description]: Performancedata lost. [Problem Analysis] : Therelevant analysis of the missing data on performance data is as follows: From the feedback, data can be seen, and B control(SP1)'s performance data is lost for a period of time: Ascan be seen from the above figure, the performance data missing from the 0Bcontroller is from 0:00 on December 9 to around 0:38 on the 10th. As you cansee from the events and messages, the performance data during the period isnormally collected and generated: Theabove is the printing in the messages, and the relevant records of about 5o'clock on the 9th are taken. You can also see the relevant records of thecorresponding performance files on the 9th as follows: Fromthe feedback performance file, you can see that the corresponding B control(the main controller monitors more objects, the generated file is larger), eachfile is about 5M, because the space for storing performance files is limited,when the file space is reached. Or when the number of files reaches thethreshold, the old file will be deleted. The deletion strategy is as follows(if one of the conditions is met, the file will be deleted): Capacity: When the performance file reaches 768M*85%=652.8MB, delete the oldestperformance file, delete one at a time, if it is less than the upper limit,delete it, if it is still greater than the upper limit, continue to delete,delete up to 25 files. The cleanup mechanism cleans up once in 10 minutes.\nHello all, How do I determine the Optimal Networking Mode when I perform capacity expansion manually? This is my problem, can anyone help me? Thank you! Loops After Capacity Expansion Table 1 SAS Disk Enclosure High-Density Disk Enclosure Version Model Max. Number of Cascaded Disk Enclosures Recommended Number of Cascaded Disk Enclosures Max.\nHello team! I have a question, please help me. My problem is: How is metadata of oceanstor 9000 file system stored? Thanks! The OceanStor 9000 organizes metadata using dynamic subtrees. All metadata in a namespace are grouped based on name subtrees. Each name subtree is allocated to one MDS for processing. One MDS can manage multiple subtrees. The normal processing procedure is as follows: A CA sends a request to any MDS. The MDS queries the local dynamic subtree table. If the local name subtree is in charge of the request, the MDS processes the request. Otherwise, the MDS forwards the request to the MDS whose name subtree is in charge of the request. If such an MDS cannot be found, the MDS forwards the request to the MDS whose name subtree is most likely to be in charge of the request to find out the metadata. If one MDS fails, other MDSs will share the name subtrees managed by the failed MDS. If the number of MDS arrays increases or reduces or name subtrees managed by an MDS are frequently or seldom accessed, a load balancing policy is implemented to migrate some of the frequently accessed name subtrees to a seldom accessed node. workloads. In this manner, metadata of a file is processed by two MDSs at most. Hello zj5000! Have a nice day! The OceanStor 9000 stores metadata in the same way as data is stored. That is, metadata is distributed on multiple nodes.\nThe post will share a case fordata migration is slow when SmartTier is used on Oceanstor 2600 V3 device Problem Description It is found that the SmartTier feature is used to migrate data on Oceanstor 2600 V3 devices at a low speed. Problem Analysis The log shows that the CPU usage of the current storage is about 21%, and the reserved space of the storage pool is about 2 TB, meeting Smarttier migration conditions. A large amount of data needs to be migrated and the migration speed is set to a low value. Therefore, the migration is slow. You can set this parameter to high if required. (The feature occupies CPU resources of the controller. If the customer's service traffic is heavy, the rate increase may affect the performance.) The adjustment method is as follows (Note: If the adjustment is fast, the rate can be adjusted only by the system side.)     On the right navigation bar of DeviceManager, click . The Provisioning page is displayed.   Click Resource Performance Tuning . The Resource Performance Tuning page is displayed.   Click SmartTier . The SmartTier management page is displayed.   On the SmartTier page, set Data Migration Speed . Set Data Migration Speed to Low for migration during peak hours.  Set Data Migration Speed to Medium or High for migration during off-peak hours.  The Data Migration Speed of a single controller varies between values: Low : Data migration speed is 0 to 10 MB/s.\nScenario OceanStor 5300 V3 2 U controller enclosure with 12 disk slots was configured through DeviceManager. A disk domain is created containing all the 12 disks installed in the storage. Subsequently, storage pool, LUN and LUN Group were configured. A fully configured RH2288 V3 with Windows OS is connected to the OceanStor 5300 V3. Host was created after automatically scanning the connected server. The mapping was provisioned between the server and storage. A laptop is connected to the switch on which both the server and storage are connected. SmartKit was run to test the Parts Replacement function for storage, specifically Hard disk replacement. Since the SmartKit sees that all disks are normal, there is no replaceable disk. But a normal hard disk can be manually assigned as faulty. One of the disks configured in the storage was assigned as faulty so the SmartKit asked this disk to be replaced. After removing and reinserting the disk, the disk remained to be faulty eventhough it is normal. Tranferring it to other hard disk slots and changing its slots with other hard disks do not solve the issue and it remains faulty. Rebooting the storage does not also bring it back to normal. Solution There was an alarm generated in the OceanStor DeviceManager after the disk was assigned as faulty. The alarm says that the hot spare space is insufficient. This is because all of the storage resources under the 12 disks were all previously under used capacity.\nHello, team! What can I do if the logical bandwidth and the Replication link bandwidth are not the same in Asynchronous Remote Replication? Thank you! Hi Dear! Data replication involves following three conditions based on the types of primary and secondary LUNs: The primary LUN is a thin LUN not configured with SmartDedupe and SmartCompression and the secondary LUN is a thin LUN with no space allocated. During initial synchronization, data for which space is not allocated on the primary LUN will not be synchronized to the secondary LUN. The primary LUN is a thick LUN or a thin LUN configured with SmartDedupe and SmartCompression and the secondary LUN is a thin LUN with no space allocated. During initial synchronization, data in the primary LUN is read in sequence and the consecutive 64 KB all-zero data will not be synchronized to the secondary LUN. In other scenarios, during initial synchronization, the system reads data from the primary LUN in sequence. The 64 KB all-zero data is compressed to 4 KB (16:1), synchronized to the secondary LUN, decompressed to 64 KB, and then written to the secondary LUN. Other data is written to the secondary LUN in sequence. The amount of data to be transmitted is reduced to fully t*** bandwidth resources. As a result, the logical bandwidth and the replication link bandwidth are not the same. In addition, the logical bandwidth of asynchronous remote replication varies greatly from the replication link bandwidth after link compression is enabled and the performance of asynchronous remote replication drops sharply.\nPrepare a boot USB flash drive. If the available space is greater than or equal to 1 GB, perform the steps in sequence. If the available space is smaller than 1 GB, run a command to copy the EulerLinux.x86_64.iso file to a directory where the available space is greater than 3 GB, for example: cp /home/omuser/EulerLinux.x86_64.iso /var , and copy the EulerLinux.x86_64.iso file from the /home/omuser directory to /var . If the cluster software is not deployed, perform the following operations: If the cluster software is deployed, upload the file to the /home/omuser directory as user omuser using the management IP address of the cluster. Run the ifconfig NIC0 netmask command to configure a temporary IP address for port NIC0. Connect the installation PC to port NIC0. Assign an IP address that resides in the same network segment as port NIC0 to the installation PC. Upload EulerLinux.x86_64.iso to the /root directory as user root . Log in to a storage node running the Linux operating system. Upload EulerLinux.x86_64.iso to the storage node. For details about how to upload the EulerLinux.x86_64.iso file Insert the USB flash drive to the C72 USB port. Run the df -h command to view the available space of the directory where EulerLinux.x86_64.iso resides. Run the fdisk -l command as user root . Record the USB flash drive name, such as /dev/sdc . If the USB flash drive has been partitioned, partition names are displayed, that is, USB flash drive name and a serial number, for example, /dev/sdc1 and /dev/sdc2.\nplease explain what is that mean ? please provide a documentation explain what is Pre-Fail meaning ? how can i solve this issue ? [ disk_id= ] Parameter Description Value disk_id= To obtain the value, run \" \" without parameter. None. To query details on all hard disks, run the following command. The command output varies depending on a specific product. To query details on a specified hard disks, run the following command. The command output varies depending on a specific product. The following table describes the parameter meanings. Parameter Meaning ID ID of a hard disk. Health Status Health status. Running Status Running status. Type The type of a hard disk. Capacity The capacity of a hard disk (The capacity is calculated based on the sector size of 512 bytes. If the sector size employed by a disk is not 512 bytes, the value may be different from the actual capacity). Role The role of a hard disk. Disk Domain ID The disk domain ID of a hard disk. Speed(RPM) Speed(RPM). Health Mark Health mark. Interface Bandwidth(Mbps) Interface bandwidth(Mbps). Sector Size Sector size. Temperature(Celsius) Temperature(Celsius). Model Product model. Firmware Version Firmware version. Manufacturer Manufacturer. Serial Number Serial number. Light Status Light status. Disk Domain Name Disk domain name of a hard disk. Disk Domain Tier ID Disk domain tier ID of a hard disk. Coffer disk Whether this disk is coffer disk. Run Time(Day) Run time(day). Progress(%) Progress of reconstruction. Multipathing Information of path. Bad Time Bad time. Bad Type Bad type.\nNetwork disk I believe many people have used the \"network disk\" service launched by many large websites such as Tencent and MSN. The network disk is an online storage service. Users can upload and download files through WEB access, and realize the storage and network backup of important personal data. The advanced network disk can provide two kinds of access methods, such as web page and client software. I used the network disk software system of Xdisk in 2002, and it can create a virtual disk with the letter X as the X through the client software. Implement the storage and management of important files in the same way that you use a local disk. The capacity of a network disk generally depends on the service provider's service policy, or on how much the user wants the service provider to pay. Online document editing After rapid development in recent years, Google's services have been extended from a single search engine to Google Calendar, Google Docs, Google Scholar, Google Picasa and other online application services. Google generally refers to these online application services as cloud computing. Compared with the traditional document editing software, the emergence of Google Docs will make a huge change in our usage and usage habits. In the future, we will no longer need to install office and other software on the personal PC, just open the Google Docs webpage through Google.\nStorage layer The storage layer is the most basic part of cloud storage. The storage device can be a Fibre Channel storage device, which can be an IP storage device such as NAS and iSCSI, or a DAS storage device such as SCSI or SAS. Storage devices in cloud storage tend to be large and distributed in different regions, and are connected to each other through a wide area network, the Internet, or a FC Fibre Channel network. Above the storage device is a unified storage device management system, which can implement logical virtualization management of storage devices, multi-link redundancy management, and status monitoring and fault maintenance of hardware devices. Basic management The core management layer is the core part of cloud storage and the most difficult part of cloud storage. The basic management layer realizes the collaborative work between multiple storage devices in the cloud storage through technologies such as clusters, distributed file systems, and grid computing, so that multiple storage devices can provide the same service to the outside world and provide greater and stronger services. Better data access performance. CDN content distribution system, P2P data transmission technology and data compression technology can ensure that data in cloud storage can be stored more efficiently, use and occupy less space, occupy lower transmission bandwidth, and provide more efficient services to the outside. Data encryption technology ensures that data in cloud storage is not accessed by unauthorized users.\nA unified storage device can meet different types of storage requirements in a single system. Since there are different types of disk storage areas on the system, how to allocate resources in different disk areas becomes a problem. In addition, since the unified storage device can take up multiple roles, once the fault occurs, the impact is greater, so the reliability can not be ignored. In addition, the unified storage device must also have sufficient performance and capacity expansion to meet the storage tasks that were previously burdened by multiple different systems in a single system. The adjustment of storage resources can be divided into two parts: storage space and performance allocation. According to different tasks, the front-end server needs different capacity and storage performance, which will not be a problem for the independent systems. However, for a unified storage device, there will be a situation of resource competition, which will seriously impact the front-end application server that needs high storage performance when the system is fully loaded. In terms of capacity allocation, the key point is whether the system can flexibly schedule space and allocate to disk areas with different protocols, and the adjustment program can not affect the stored data or normal storage actions. Most of the current products have the ability to allocate capacity in different disk areas, and convert the created disk area between Fibre Channel or iSCSI transmission interface without affecting the data in the disk area. The performance adjustment is more complicated.\nFC+iSCSI+CIFS/NFS That is to support FC and iSCSI two block read and write protocols, as well as CIFS, NFS, the two most common file transfer protocols. Obviously, this type of storage device can support the widest range, whether it is in the form of SAN, NAS, etc., or different types of transmission channels such as Fibre Channel and Ethernet. Enterprises can meet all kinds of reading and writing requirements in a single system, which is quite convenient for management or construction. Such systems can be divided into NAS-based and extended to SAN, and some products are typical of extending from NAS to SAN; some products are extended by the opposite route, that is, based on FC SAN disk arrays. Support iSCSI and NAS read and write by adding boards that support different transport ports on the controller. FC+iSCSI These products can only support the block read and write environment of SAN, but can support both Fibre Channel and Ethernet transmission modes. Therefore, enterprises can enable front-end servers such as online databases or ERP mission-critical servers to read and write SANs through Fibre Channel. For better performance. For applications such as database test development that do not require special performance, iSCSI can be read and written to the SAN, so this combination can balance performance and cost.\nWhen a disk or riser card upgrade is performed using the CLI, the primary controller is switched to another controller during the upgrade (due to an unexpected reset of the primary controller or other exceptions). Afterwards, the upgrade result is not displayed on the CLI, as shown in the following figure. N/A The primary controller is unexpectedly reset (for example, an Oops or OOM error occurs). Network connections are normal before the upgrade, but the connection between the primary controller and external gateway becomes abnormal during the upgrade. Therefore, the storage array switches the primary controller to another controller that can connect to the gateway. Verify that the upgrade result is not displayed on the CLI. Log in to a controller using the CLI. On the CLI, go to the developer mode and then minisystem mode. Run showsysstatus to find the primary controller. For example, controller 0 is the primary controller, as shown in the following figure. Log in to the primary controller using the CLI. On the CLI, go to the developer mode and then minisystem mode. Run showsystrace to check whether a flow whose ID is 4 exists. If the flow exists, query the Date Time information. If the time is later than the upgrade start time, this fault occurs. Collect storage array logs to find out the reason why the primary controller is switched. If the controller switchover is irrelevant to the upgrade, run the upgrade command again. If the controller switchover is triggered by the upgrade, stop the upgrade.\nAfter a version is released, its formal patch is released. The path of the formal patch is not in the specified upgrade path. In normal cases, the version with the patch installed cannot be upgraded to a specified version released earlier than the patch. However, in special scenarios, such an upgrade is required. In this case, the cross-version check needs to be skipped so that the upgrade can be performed. A message is displayed on the upgrade interface, indicating that pre-upgrade processing fails and the upgrade cannot continue. In normal cases, a version with a patch installed cannot be upgraded to a specified version released earlier than the patch. However, there are special scenarios where a patch is installed on a device before delivery and the device needs to be upgraded to a specified version released earlier than the patch. A message is displayed on the upgrade tool interface, indicating that the current version cannot be upgraded to the target version and the upgrade cannot continue. After analysis, R&D engineers conclude that the cross-version check needs to be skipped so that the upgrade can continue. Create an empty filenamed ignoreCheckCrossVer.txt , and place it to a specified directory ( \\ tools\\ArrayUpgrade\\packages\\ArrayUpgrade\\product\\ \\). For example, to upgrade the system to V300R002C00, place the ignoreCheckCrossVer.txt file to \ools\\ArrayUpgrade\\packages\\ArrayUpgrade\\product\\V3V300R002C00\\ Restart the upgrade tool, and perform the upgrade as prompted. To avoid adverse impact on the next upgrade, check whether the ignoreCheckCrossVer.txt file exists in \ools\\ArrayUpgrade\\packages\\ArrayUpgrade\\product\\ \\ after the current upgrade is successful. If the file exists, delete it.\nScenario 1: A controller does not match its controller enclosure. Scenario 2: Coffer disks do not match the current storage device. The proper running of the storage system depends on the MAC address and SN of its controller enclosure. When the system is started, the system checks whether the MAC address and SN are the same as those recorded in coffer disks. If the MAC address and SN are different from those recorded in the coffer disks, the check is not passed and the system fails to be started. Method 1: Insert the controller enclosure and coffer disks into their original positions and power on the system again. Method 2: Clear the configuration data and power on the system again. The operation procedure is as follows: Use an SSH tool, such as Xshell 5, PuTTY 0.63, SecureCRT 6.7, or one of their later versions, to log in to the storage device via the management network port. Run the showsysstatus command to check whether the current node ID is the same as that of the master node in the cluster. If the current node ID is the same as that of the master node in the cluster, the current node is the master node. Run the cleardb command on the master node to clear the configuration data in the system. Run the rebootsys command on each controller to restart the system. After the system is restarted, check whether the error code is reported again. If yes, contact R&D engineers for help. If no, the fault is rectified.\nA storage system fails to be started because it has an engine that is not powered on. Check method: Run the showsysstatus command in minisystem mode to check the value of the node cfg field, which indicates the number of nodes configured in the storage system. The number of controllers listed in the ID column indicates the number of controllers running in the storage system. Scenario 1: The number of configured nodes is the same as that of running nodes. Cause 1: Some nodes are powered on relatively late. Cause 2: An engine is not powered on. Scenario 2: The number of configured nodes is different with that of running nodes. Cause 3: The number of nodes in the configuration file is changed after controller expansion. Cause 4: The number of nodes in the configuration file is incorrectly set before delivery. Causes 1 and 2: Power on the engine and run the rebootsys command in minisystem mode to reset each node. Causes 3 and 4: Modify the configuration file of the storage system in minisystem mode for each node. Modify the node configuration (regardless of the hardware type). BayId : physical engine ID (one physical engine per enclosure) ControllerTotal : correct number of nodes that are configured in the system For example: Physical engine 1 of four controller enclosures: os_write_bay_cfg.sh -i 1 -c 4 Synchronize the configuration to BMC. Run the rebootsys command on each node to restart all nodes. After all nodes are restarted, check whether the error code is reported again.\nI want to collect the performance logs of 5800V3, is there any simple method? thanks very much. If both the latency on the host side and that on the storage side are large and the difference between the latencies is small, the problem may reside in the storage system. Common reasons are as follows: The disk performance reaches the upper limit; the mirror bandwidth reaches the upper limit; a short-term performance deterioration occurs because LUN formatting is not complete. If the latency on the host side is much larger than that on the storage side, the configuration on the host side may be inappropriate, leading to a performance or network link problem. Common reasons are as follows: I/Os are stacked because the concurrency capability of the block device or HBA is insufficient; the CPU usage of a host reaches 100%; the bandwidth reaches a bottleneck; the switch configuration is inappropriate; the multipathing software selects paths incorrectly. After determining the location where a performance problem resides, such as on the host, storage, or network side, analyze and troubleshot the problem. Check the latency on the host side. On a Linux host, use different tools to query the host latency. Use the performance statistics function of service software, such as the AWR report function of Oracle, to query the host latency. Use iostat, a Linux disk I/O query tool, to query the latency.Run . In the command output, indicates the average time in processing each I/O request, namely, the I/O response time, expressed in milliseconds.\nEnterprise IT shops coping with the explosive growth of unstructured data need to consider their NASoptions and decide whether traditionalfixed-capacity NAS devices or the emerging scale-outNAS systems will better meet their file-storage needs. To help make your decision, heres a summary of the pros and cons for the two types of NAS systems.   One of the main attractions of traditional NAS is its simplicity. The systems are easy toinstall, configure, manage and operate, especially in environments of modest scale. Productupgrades in this scale-up category follow the traditional speeds and feeds pattern of replacing abox with faster processors and larger-capacity storage. Scale-up products are generally mature and have plenty of features and add-on software for data protection,business continuance and storage efficiency. Options include snapshots, one-to-manyand many-to-one replication, remote replication and remote snapshots, thin provisioning, deduplication and compression. Traditional NAS systems can be cost-effective and reliable, particularly for small- tomedium-sized businesses (SMBs). They help to consolidate file servers and centralize dataprotection.\nThey may also be tightly integrated with common business applications and theirnative management consoles.   The main downside of traditional NAS is its inability to scale beyond the limits of the system,forcing customers to purchase additional, separately managed boxes when they need to add capacity.Capacity on these NAS boxes may be underutilized if users arent able to add capacity becausetheyve run out of performance or bandwidth. As IT shops buy multiple products, NAS sprawl is a common problem, and the stove-piped systems have an adverse impact on floor space, power andcooling, and tend to complicate storage management. It becomes a nightmare managing 25, 30, 50, 100, 200 of these individual NAS boxes, said ArunTaneja, founder and consulting ***yst at Taneja Group in Hopkinton, Mass. Ninety percent of theadministrators time in typical, traditional NAS environments is moving files from one NAS box toanother in order to load balance these boxes. Scale-out NAS systems carry the advantage of scaling capacity and performance on an as-neededbasis, far beyond the limits of traditional scale-up NAS. They typically distribute data acrossmany storage controllers, and the systems clusteredarchitectures ensure high availability. Early scale-out NAS gained a reputation as being tough to set up and manage, but the difficultyvaries by product. To a large degree, several commercial products have eliminated thatdisadvantage.\nUsers report that adding a node to an EMC Corp. Isilon system, for instance, can beas easy as pushing a button on the front panel. Once the system is up and running, scale-out NAS brings the huge advantage of being able tomanage and move petabytes of data under a single distributedfile system and globalnamespace, and the systems generally support large volumes. Users, in turn, can maintain floorspace requirements, power and cooling costs, and management staff. Whether [the scale-out NAS systems] do that through namespace aggregation or having a trueglobal namespace, as long as the customer doesnt have any management headaches, thats whatcounts, said Terri McClure, a senior ***yst at Enterprise Strategy Group Inc. in Milford,Mass. Some scale-out NAS systems increase performance and capacity independently, while others requirethe scaling to take place simultaneously. In addition, some products allow their users to upgradeand service hardware without disrupting client access to data.   Some scale-out NAS systems carry fairly substantial licensing fees that are tacked onto theincremental costs associated with adding equipment. Plus, scale-out systems tend to lack thefeature/functionality of well-established scale-up NAS systems, at least at the moment. For instance, a scale-out NAS system might offer remote replication, but only the asynchronousvariety -- not one-to-many or many-to-one functionality.\n      There are a many misconceptions about fibre channel and iscsi san technologies. Several are perpetuated by the vested interest of specific storage vendors. Some of these san technology misconceptions have a grain of truth but are tainted by the past, not the present. Other misconceptions are correct on a technical basis but have no relevance in the real world. These descriptions clears up some of the most common misconceptions about fibre channel vs. Iscsi sans and also helps you choose which technology is best suited for your business environment.    Although it's true that 4 gbps fibre channel has a higher throughput than 1 gbps iscsi, it's also true that aggregating four 1 gbps ethernet ports gets the same bandwidth. And 8 gbps fc has slightly less bandwidth than 10 gbps iscsi. So from a throughput perspective, this misconception has a small kernel of truth based on the low-end bandwidth specs.    Intuitively, iscsi latency should be significantly greater than fibre channel, because of tcp latency. As latency increases so does response time. Higher latency generally means fewer iops. But vmware's fall 2009 testing of nfs, iscsi, and fc revealed some surprising results. The test results showed that iscsi latency is definitively higher than fibre channel, especially on initial load. But it also showed that i/o-ps were considerably higher on fibre channel than iscsi with the greatest differences coming at initial load. Oddly, as the load over time leveled off, the latency iops differences narrowed.\nThere was still a measurable difference, but it was much smaller than expected. So unless it is a very high transaction-oriented application, the latency differences will not matter very much for your small business application.     Iscsi sans are often seen to be less expensive than fibre channel sans. And when you compare 1 gbps iscsi vs. 4 gbps fibre channel, iscsi is less expensive in terms of acquisition and maintenance costs. The iscsi hardware, especially at the port level where no tcp / ip offload is required, is also less expensive than fibre channel. However, one misleading argument is that iscsi can also run on current infrastructure. Although iscsi can be run over existing switching and ip infrastructure, it is not recommended. Performance is likely to be severely degraded, unpredictable, and less secure if not operated on a dedicated network or subnet (lan or vlan). But comparing the costs on a per gigabits-per-second basis narrows the cost differences between iscsi and fibre channel considerably. Often, a 10 gbps iscsi san costs more than a 8gbps fiber chanel. Performance levels also make a difference in costs. At lower performance levels (1 gbps iscsi vs. 4 gbps fibre channel), iscsi requires considerably less capex than fibre channel. At higher performance levels (10 gbps iscsi vs. 8 gbps fibre channel), iscsi actually requires somewhat more capex. Opex differences are more pronounced. Furthermore, training costs differ between iscsi and fibre channel as well.\nFibre channel san technology is new for most storage administrators and therefore requires more training costs and a relatively long learning curve. It is not intuitive. Add this to the deterministic manually intensive nature of fibre channel and the opex costs are considerably higher than iscsi. Overall, the costs for fibre channel and iscsi sans differs for each company. An iscsi san can be more expensive than a fibre channel san and vice versa, but keep in mind that the difference in cost is often much narrower than expected.    Few dispute this fact that iscsi sans are easier to operate than fibre channel sans because of the non-deterministic, discoverability, and routing of tcp/ip ethernet networks. Also, network implementation, operations, management and change management is far more automated and forgiving on iscsi sans than fc sans. But this conventional wisdom is based on information from the past, not the present. Recent advances in small and medium fibre channel sans have made implementations, operations and management on an ease of use parity with iscsi. Larger environments are still far more complicated than iscsi, but even change management for fibre channel sans can be handled on a more simplified automated basis with products from aptare inc., netapp inc., sanpulse technologies inc. And tektools (now part of solarwinds). And boot from san is actually simpler on a fibre channel san vs. An iscsi san (which requires at least one separate dhcp server and usually two for ha, offering up a pxe or boot up image.\nHi Huawei Community, What is the minimum bandwidth requirement over WAN when implementing HyperMetro between two Oceanstor device? Thanks in advance. Network Description Storage-to-host network All of the hosts can be interconnected to form a cluster. Network type Support 8 Gbit/s Fibre Channel, 16 Gbit/s Fibre Channel, 10GE, and GE networks. Networking mode A fully interconnected network is used between hosts and storage systems, that is, each host is physically and logically connected to every controller on both storage systems. A host must connect to both storage systems using the same type of network. The HyperMetro replication network, storage-to-host network, and quorum network must be physically isolated and use different ports. HyperMetro replication network A network between the storage systems to synchronize heartbeat information and data. The storage system sets link priorities for transferring different types of data. Heartbeat information has a higher priority than data synchronization. Network type Supports 10GE, 8 Gbit/s Fibre Channel, and 16 Gbit/s Fibre Channel networks. If this is a 10GE network, you can use L2 or L3 devices. For better synchronization performance, L2 devices are recommended. Bandwidth: peak service bandwidth (total read and write bandwidth on both sides) The HyperMetro replication network, storage-to-host network, and quorum network must be physically isolated and use different ports. Networking mode Each controller on every storage system has at least two redundant physical links. Controllers are connected in a parallel way between the storage systems.\nTo be more specific, controller A of the local storage system connects to controller A of the remote storage system, and so do controllers B. The HyperMetro replication network does not support network address translation (NAT). Quorum network If communication between the storage systems is interrupted or a storage system malfunctions, the quorum server determines which storage system is accessible. The quorum server resides on a dedicated network that is linked to both storage systems. Network type Quorum links support GE and 10GE networks, but not a Fibre Channel network. Quorum ports on storage systems: independent service ports can be used as quorum ports, but management and maintenance ports cannot. Quorum links support IPv4 and IPv6 addresses. Network quality and bandwidth requirements: Latency: RTT 50 ms Bandwidth: 10 Mbit/s The HyperMetro replication network, storage-to-host network, and quorum network must be physically isolated and use different ports. Networking mode An independent quorum server must be deployed. A GE/10GE port on each controller of a storage system is connected to the quorum server and the service network ports on the quorum server are connected to two storage systems, ensuring that the quorum server is connected to all controllers of each storage system. The quorum ports on the storage systems are on different network segments to prevent arbitration failure caused by a network segment fault. The quorum server and storage systems can be connected by Layer 2 or Layer 3 networks, but do not support Virtual Router Redundancy Protocol (VRRP).\nIf the faulty device can be replaced only after the server is powered off, perform the required operation based on the deployment scenario ofFusionStorage. If partial SSD cards on the server that provides storage resources forFusionStoragebecome faulty and need to be replaced, place the server into maintenance mode before the replacement and remove the server out of maintenance mode after the replacement. For details, see and . If all SSD cards on the server that provides storage resources forFusionStoragebecome faulty, you do not need to place the server into maintenance mode. If a server is placed into maintenance mode, the timeout duration in which the server is removed will be prolonged by 45 minutes. Therefore, you will have 75 minutes to replace the faulty parts. If a server is removed from the storage pool for more than 75 minutes, the storage pool will reconstruct data. IfFusionStorageis deployed in the FusionSphere system, take service protection measures and then replace the faulty server by performing operations provided in in the FusionSphere product documentation. For example, if the target server is deployed in converged mode, you need to migrate the VMs on this server to other hosts before powering off the server. IfFusionStorageis deployed in other systems, take measures to protect running services, power off the faulty server, and replace it with a new one. If the replaced device is not an NVMe device and has been automatically added to the storage pool and runs properly, no further action is required.\nIf the replaced device is an NVMe device and has been automatically added to the storage pool and runs properly, go to . If the faulty device has been removed from the storage pool, perform the operation based on the faulty device type. After the operation is complete, if the device runs properly, no further action is required.If the replaced device is an NVMe device and hardware DIF was enabled before the fault occurs, you need to enable hardware DIF after the device is replaced but before it is added to the storage pool. For details, see the . If PCIe or NVMe SSDs are faulty, select the faulty device, click and select the new device as prompted.As shown in , indicates the faulty device, and indicates the new device. You can select and perform operations. If other disks are faulty, select the new device, click , and add the device to the storage pool as prompted. If the faulty device has not been removed from the storage pool, go to . In the CLI of the active FusionStorage Manager (FSM) node you have logged in to as user , run the required command based on the main storage type to restore storage resources: To restore a SATA or SAS disk, or an SSD, run the following command: To restore an SSD card, run the following command: If the replaced device is not an NVMe device, no further action is required.\nEnvironment: As shown in the following figure, a distributed switch is configured on the ESXi. Two vmkernel networks (vmk1 and vmk2) are added to the ESXi. UltraPath is installed on the ESXi and iSCSI Target on the four storage devices is added. After the configuration, load balancing takes effect only on P0 and P1 of SP/Controller A, the two ports in the SP/Controller B do not have data and do not work in load balancing mode. The following information is found: The multipathing mechanism of the front-end controller can be classified into the following types: 1. A/A: Symmetric Active/Acivie: For a specific LUN, the target ports of the two storage controllers in the path are in the active / optimized (active/optimized) state. The two controllers communicate with each other at a high speed. One I/O is sent to the control end, and the two controllers can participate in the processing at the same time. When a controller is busy, the system does not need the load balancing software on the host to implement load balancing. 2. ALUA: Asymmetric Active/Active: For a specific LUN, the target port of one controller in its path is in the active / optimized (active/optimized) state, and the target port of the other controller is in the active / non-optimized (active/unoptimized) state. At a certain time, a LUN belongs to only one controller. To implement load balancing on both sides, task A is thrown to controller A and task B is thrown to controller B.\nAbstractAfter the server configuration is modified and restarted, the resource status is not online or offline, and the status of faulted and state unknown occurs, causing the maintenance tool to fail to start Case Body Background The network management system installed this time is the local high availability system (Veritas) U2000, based on SUSE (Linux). Local dual-machine configuration, three directions: connect the core device in the south direction, connect the yi yang to the north through the DCN network, connect the heartbeat between the two machines, and configure BOND. The IP address needs to be configured with southbound, northbound, heartbeat, and floating IP. After the configuration, you need to restart to make the configuration take effect. Problem, event description After the restart, the status of APP BOND is FAULTED and the status of FloatIP is UNKNOW. In this case, even if the hares online command is used, online APP BOND cannot be enabled, and the network maintenance tool cannot be opened. Analysis and countermeasures Enter the following command to view the information hares display APPBOND | grep Device cat /etc/sysconfig/network/ifcfg-bond0 cat /etc/hosts cat /etc/hosts.YaST2save cat /etc/ICMR/netCfg/VCS/vcs_net_config.cfg cat /etc/OSSENGR/engrIPInfo.xml ifconfig a Copy the contents of bond1 to the configuration of bond2 ifcfg-bond0 cd /etc/sysconfig/network/ cp ifcfg-bond1 ifcfg-bond0 vi ifcfg-bond0 There is still no bond2 in the same directory, but there is eth38 cp ifcfg-eth38 ifcfg-eth2 Then execute rcnetwork restart At this point, both interfaces appear, and after restarting the device, the response is normal. The southbound bond has two interfaces, eth2 and eth38.\nHard disk domain configuration principle One or more hard disk domains can be configured in the storage system. The hard disk of each disk domain can be selected from one or more of SSD, SAS, and NL-SAS. Multiple storage pools can be created on one disk domain. Different disk domains are completely isolated, including fault domains, performance, and storage resources. Storage pool configuration principle A storage pool is a container for storage space resources. It is created in the disk domain and can dynamically allocate resources from the disk domain and define the RAID level of each storage layer and storage layer. LUN configuration principle Unlike a traditional RAID group that contains only a dozen hard disks, a RAID 2.0+ technology-based storage pool spans all hard disks in the disk domain, and the number of hard disks in the disk domain is tens or even hundreds. To maximize the performance of the hard disk, you are advised to configure the number and capacity of the LUN according to the following principles: The total number of LUNs in the disk domain: It is recommended to be no less than the number of hard disks 4/32 (4 is the reasonable number of concurrent disks, 32 is The default maximum queue depth of the LUN restricted by the HBA card. LUN capacity: Use the largest possible LUN to meet the above conditions to simplify management overhead. However, you need to pay attention to the limitations of the operating system and Oracle database when setting the capacity of the LUN.\nFirst of all a snapshot is a copy of the source data in a point time and it occupide small storage space. These are some Concepts you should know about the Snapshot : ROW : The LUNs created in the storage pool of the OceanStor Dorado V3 series storage systems consist of metadata volumes and data volumes. Metadata volume: records the data organization information (logical block address (LBA), version, and clone ID) and data attributes. A metadata volume is organized in a tree structure. LBA indicates the address of a logical block; the version corresponds to the snapshot time point; and the clone ID indicates the number of data copies. Data volume: stores user data written to a LUN Source Volume : Snapshot Volume : Snapshot Copy : Snapshot Consistency group : LUN consistency groups ensure data consistency between multiple associated LUNs. OceanStor Dorado V3 supports snapshots for LUN consistency groups. That is, snapshots are created for member LUNs in a consistency group at the same time. Snapshot consistency groups are mainly used by databases. Typically, databases store different types of data on different LUNs (such as the online redo log volume and data volume), and these LUNs are associated with each other. To back up the databases using snapshots, the snapshots must be created for these LUNs at the same time point, so that the data is complete and available for database recovery. ROW is a core technology used to implement snapshot.\nGo to the page for dumping operation logs. On the upper navigation bar, click System . In the left navigation tree, choose . Go to the page for dumping operation logs. Change Parameter Values. In the dump parameter area of the right function pane, click Modify . The Change Parameter Value dialog box is displayed. Change Parameter Values. describes the Values. Parameter Description Dump time Time when the NMS automatically starts dumping operation logs. The value is usually set to a point in time when the NMS is idle, for example, 02:00:00. Dump period (days) Period at which the operation logs are dumped. The value is an integer ranging from 7 to 120. Reserve recent data records (days) Days during which generated logs are reserved. The value is an integer ranging from 7 to 120. File format Format of the file saving operation log data. The format can be CSV or Excel . Language Language in which operation log data is saved in the file. Click OK . After setting operation log dump parameters, the NMS automatically dumps operation logs based on the settings. When the specified dump period and dump time are reached, the NMS automatically dumps operation logs to the SystemReporter installation path\\Runtime\\LegoRuntime\\datastorage\\sysoptlog path on the NMS server. Dumped logs will not be displayed in System Log List but stored in the specified folder. To view dumped logs, you can download dumped logs in a file on the client and open the file in the text editing tool.\nThe FSM can be deployed in a virtualized or physical manner. When deployed in a physical machine, the physical machine also supports other storage capabilities, but the overall storage performance may be reduced due to the management of the physical machine. The FSM is generally deployed on the control node of the FC or FSP. After the FSM is deployed, you can add storage nodes and compute nodes, create storage pools, and more through the fs portal. After each node is added, the installation program of the FSA agent process needs to be sent and received in the portal. After the installation is completed, the FSM can establish certain communication capabilities with each node. In the storage node or storage, the computing node has a cluster concept, and the corresponding control uni****C needs to select at least three storage nodes to deploy, so it is also a storage node, and three or more servers are deployed with MDC services. For a simple storage node, after adding it to the storage pool, an OSD process is created on it, and each hard disk corresponds to an OSD process. For the compute node, the VBS is automatically created after the block client is installed on the portal. Usually, the VBS needs to consume one process. Generally, there is only one VBS process on one server. If you want to improve the io capability, you need to improve the single-thread processing capability of the CPU. Or increase the number of vbs.\nThe management process provides operations such as alarms, monitoring, logs, and configuration. It is recommended to deploy the active and standby nodes. The management agent process is deployed on each node (server) to implement communication between each node and the FSM. The business control process implements state control of distributed clusters, as well as control data distribution rules and data reconstruction rules. When creating a control cluster, the metadata management service (zookeeper) is deployed on three, five, or seven nodes, and one MDC process is deployed on each metadata management service node to form a control cluster. The system can create up to 96 MDCs. If the number of MDCs in the system is less than 96, the system will automatically select one storage node to deploy MDC in the created storage pool. If the number of MDCs in the system has reached 96, the system will no longer continue to deploy MDC. The service IO process is responsible for the management of volume metadata and provides distributed cluster access point services, enabling computing resources to access distributed storage resources through VBS. A VBS process is deployed on each server to form a VBS cluster. A business IO process that performs specific I/O operations. Multiple OSD processes are deployed on each server, and one OS corresponds to one OSD process. According to the deployment form of each process, fs can be divided into computing storage convergence deployment and separation deployment.\nIn the case of a converged deployment, the node deploys FSA, OSD, VBS, (MDC), and the virtual machine can be issued on the node. In the case of separate deployment, the storage node deploys FSA, OSD, (MDC), and the compute node deploys FSA and VBS. For the fusionstorage disk, in addition to the system disk needs to do raid1 to improve stability, there is no need to do raid, add a separate disk to the storage pool on the fusionstorage portal (3108 2208 1708raid card needs to be given to each disk) Raid0 alone can be recognized). In addition, fusionstorage provides two links: scsi and iscsi. For Huawei's virtualization platform, fusionstorage provides storage capacity through the scsi link, which eliminates the need to configure bootloaders and the like; for vmware, or physical machines, Mount the disk for it by iscsi, so you need to configure the initiator. In addition, fusionstorage also supports the use of iscsi for third-party openstack, but some advanced features may not be possible due to platform reasons. For the security of fusionstorage, it is divided into server level and rack level. Because fusionstorage usually uses three copies of the form, the original data is done as two integrity backups, its security is not the highest, but at the same time, its disk usage rate is one-third. In addition, when server-level security is selected, the replicas will be stored on different servers, and when the rack level is selected, the replicas will exist on different racks.\nWhat to do if the alarm reporting function failed to be enabled on a device? After the SNMP parameters of the device are configured and the alarm reporting function is enabled for the device, a message indicating that the operation fails is displayed. The possible causes are as follows: OceanStor V3series V300R002 products are used as examples to describe how to locate and rectify the fault. The read and write community of SNMPv2c is incorrect or the security parameters of SNMPv3 are incorrect. Check whether the SNMP parameters are correctly configured by referring to SNMP parameters in section \" .\" The number of alarm dump servers reaches the upper limit. A maximum of four alarm dump servers can be added. You can run the show notification trap command to view the added servers. If four alarm dump servers have been added, check with the device administrator whether there is a Trap server that is not used. If there is a Trap server that is not used, run the delete notification trap server_id=? command to delete the Trap server. SNMPv2c of the device is not enabled. (If the device uses SNMPv2c.) SNMPv2c is disabled on devices by default. If you want to use SNMPv2, run the change snmp version v1v2c_switch=On command to enable it. Incorrect SNMP parameters are used. As a result, the IP address of the device is locked (Try again later). Run the show event command to check whether an IP address is locked recently.\nThe switch port rate is 1/2/4/8/16Gbps. The optical module has long-wavelength short-wavelength. The fiber-optic cable has single-mode and multi-mode, so what is the relationship between them and the transmission distance? First, at the same rate, the transmission distance of single-mode fiber is farther than that of multimode fiber, and the long-wavelength optical module is farther than the short-wavelength optical module. Of course, single-mode fiber lines and multimode fibers must be used in conjunction with corresponding optical modules. Secondly, in the case of configuration determination, such as 8Gbps multimode optical module with multimode fiber optic cable, the lower the rate, the farther the transmission distance. As mentioned above, the farthest transmission distance specification of an 8Gbps optical module is 500m, which means that it runs at 2Gbps and the optical fiber line is the longest distance that OM3 can reach. At 8Gbps, it can only transmit 150m. . Finally, there is a case where the optical module and the optical fiber line can support a certain distance, such as 25km, but the actual bandwidth value may not reach the minimum rate supported by the optical module. Why? This involves another concept, Credit Buffer. By default, each port of the switch is assigned a certain number of Buffers. When the sender sends a frame in data to the peer, it counts a number of times for each frame.\nA port is a basic module for building a fiber-optic network. Ports in a Fibre Channel network include device-side ports, switch-side ports, and configuration ports. Device-side port type: The device-side port mainly refers to the port of the terminal device connected to the switch. The port type includes N_Port and NL_Port. N_Port: Port in point-to-point mode, device direct mode port. NL_Port: Arbitrated loop mode port. Switch port type: There are many port types on the switch, and the port types supported by different vendors are different. The port types supported by Brocade Fibre Channel are listed below: U_Port: Universal port mode. Strictly speaking, U_Port is not a port mode. It is only a state when the port is idle. It waits for the port to connect to the device and then transitions to the final port mode. F_Port: Fabric port mode, F_Port and N_Port can establish a connection. FL_Port: Fabric loop port mode, FL_Port and NL_Port can establish a connection. This port type is no longer supported on the Brocade Condor3 ASIC platform. G_Port: Generic port (similar to G_Port and U_Port). When the port mode is displayed as G_Port, it is not the final state of the port. It is to be converted to the final F_Port or E_Port mode. E_Port: Expansion port. Port used to establish interconnection with other switches D_Port: Diagnostic port. The port in this mode cannot be connected to the fabric network and cannot communicate with other devices. It is only used for diagnostic analysis.\nFC-0 Physical Interface This is the bottom layer, FC-0 defines the physical connection. For Fibre Channel, its primary use is to select different physical media and data rates for protocol operation. This approach ensures maximum flexibility in the system, allowing existing cables and different technologies to be used to meet the needs of different systems. Commonly used cables are copper and fiber optic cables. FC-1 Byte Coding This layer records the 8b/10b transmission code used to balance the transmission bit stream. In addition, encoding can also be used as a data transmission and accept error detection mechanism. The 8b/10b encoding was chosen because of its excellent transmission characteristics. This extremely stable encoding reduces component design costs while ensuring good transmission density for easy clock recovery. Note: The 8b/10b design is also used in IBM ESCON. FC-2 Data Distribution This layer contains the basic rules for sending data within the network. include: 1) How to split the data into small frames, 2) How much data can be sent at a time (flow control)? 3) Where should the frame be sent? It also includes defining service levels based on the application. FC-3 General Services This layer definition includes advanced features such as striping (transferring data over multiple channels), multicasting (sending one message to multiple destinations), and hunt groups (multiple ports for one node). Thus, when FC-2 defines a function for a single port, the FC-3 layer can define functions across ports.\nThe CPU of Switchboard includes 4 kernelsCPU0~CPU3The CPU3 usage of the 2X switchboard is kept at 100% from 2018-03 -22 07:46CPU usage%= User%+ Kernel%. Here is log screenshot: The thread of the stacking module is bound to CPU3. the CPU3 usage of the 2X switchboard frequently appears to 100%, it causes the stack protocol communication between the active and standby stack members becomes abnormal. As a result, the stack splits, triggering the restart of lower priority switchboard (3X CX912 has the lower priority than 2X, so 3X switchboard restart). The priority of the 2X switchboard is 150, 3X switchboard uses the default priority 100, so the priority of the 2X switchboard is higher than 3X switchboard (the default priority 100 of 3X switchboard will not be shown in log), as shown in the following figure Current switchboard software version is 3.10 which is too low, and it has the issue of frequently writing logs. When logs are frequently written, the log files are compressed, replaced, and deleted. These operations occupy a large number of CPU resources. CPU resources are occupied for a long time, causing stack protocol packets to fail to be processed. As a result, the switchboard restart mechanism with a lower priority is triggered. Why a Stack Fail to Be Established After 3X Switchboard Restart and cause service affect The 2X switchboard frequently writes logs, after 3X switchboard restarting, the stack communication is still in abnormal state.\nInformation similar to the following is displayed: To enable Keystone V3 authentication, enter 1 , press Enter , and enter the password of OS_USERNAME as instructed. The default password is FusionSphere123 . The environment variables are successfully imported if the command outputs of both and are automatically displayed. After the environment variables are imported, the system uses Keystone V3 to authenticate requests, and you can run CPS and OpenStack commands. To use Keystone authentication, ensure that Keystone is successfully connected. Therefore, Keystone authentication is available only after is successfully deployed. Keystone V3 authentication is recommended after is installed and deployed. To enable CPS authentication, enter 2 , press Enter , and enter the password of CPS_USERNAME as instructed. The default password of user cps_admin is FusionSphere123 . The environment variables are successfully imported if the command output of is automatically displayed. After the environment variables are imported, the system uses CPS to authenticate requests, and you can run only CPS commands. The system automatically runs the OpenStack command after environment variables are imported. Therefore, the \"ERROR...\" message is displayed under the output. Ignore this message. CPS authentication is recommended only when Keystone authentication is not available, for example when deployment is not complete, or an exception has occurred in Keystone authentication. To enable Keystone V2 authentication, enter 3 , press Enter , and enter the password of OS_USERNAME as instructed. The default password is FusionSphere123 . The environment variables are successfully imported if the command outputs of both and are automatically displayed.\nBefore VAAI, migrations of virtual machines, and their associated disks, between datastores was done by a component called the software Data Mover, a loadable VMkernel module accessible via special library calls. A data move request, in the form of a data structure, is submitted to the Data Mover for asynchronous completion. The Data Mover takes this data structure and divides it into smaller I/O requests that it sends to the appropriate back end. The Data Mover relies on a constantly filled queue of outstanding I/O requests to achieve maximum throughput. Incoming Data Mover requests are broken down into smaller chunks, and asynchronous I/Os are simultaneously issued for each chunk until the configured Data Mover queue depth is filled. Then the asynchronous Data Mover request returns to the caller who would normally proceed with what it was doing or wait for I/O completion. Meanwhile, as and when an asynchronous request completes, the next logical request for the overall Data Mover task is issued, whether it is for writing the data that was just read or to handle the next chunk. In the following example, a clone of a 64GB VMDK file is initiated. The Data Mover is asked to move data in terms of 32MB transfers. The 32MB is then transferred in PARALLEL as a single delive****ut is divided up into a much smaller I/O size of 64KB, using 32 work items that are all queued in parallel (because they execute asynchronously).\nTo transfer this 32MB, a total of 512 I/Os of size 64KB are issued by the Data Mover. By comparison, a similar 32MB transfer via VAAI uses eight work items, because XCOPY uses 4MB transfer sizes. VMkernel can choose from the following three Data Mover types:  fsdm This is the legacy Data Mover, the most basic version. It is the slowest because the data moves all the way up the stack and then down again.  fs3dm (software) This is the software Data Mover, which was introduced with vSphere 4.0 and contained some substantial optimizations whereby data does not travel through all stacks.  fs3dm (hardware) This is the hardware offload Data Mover, introduced with vSphere 4.1. It still is the fs3dm, but VAAI hardware offload is leveraged with this version. fs3dm is used in software mode when hardware offload (VAAI) capabilities are not available. The following diagram shows the various paths taken by the various Data Movers: The decision to transfer using the software Data Mover or using hardware offloading to the array with VAAI is taken upfront by looking at storage array hardware acceleration state. If we decide to transfer using VAAI and then FAILa scenario called degraded modethe VMkernel will try to accomplish the transfer using the software Data Mover.\nChoose the optimal path In order to ensure the stability and continuity of business operations, the storage system generally configures two or more controls.Implement component redundancy. Each LUN in the storage system has a one-to-one corresponding home controller. The home controller is a specific controller specified by the storage system for the LUN to prevent two or more controllers from operating on the same LUN at the same time. data. During the daily operation, if the application server accesses the LUN through the non-home controller, it needs to be transferred to the home controller of the LUN for processing. Therefore, the fastest I/O rate can be obtained by directly accessing the corresponding LUN through the home controller. In a multi-path networking environment, the home controller of the LUN on the storage system corresponding to the virtual LUN on the application server is called the preferred controller of the LUN. Therefore, the application server on which the UltraPath is installed preferentially selects the LUN on the storage system through the preferred controller (home controller) to obtain the fastest I/O rate. Therefore, the path to the preferred controller is the optimal path. UltraPath can obtain the preferred controller information of the LUN, so it can automatically select one or more paths on the preferred controller for the data stream to obtain the optimal I/O rate. As shown in the following figure, the networking of the LUN is A control, that is, the A control is the preferred controller.\nShield redundant LUNs In a multi-path networking storage network, an application server that does not have multipathing software installed will pass each path.A LUN is discovered, so the same LUN is considered to be two or more different LUNs.Remaining LUN. A redundant LUN is generated by directly reporting a LUN to the application server. Take the dual-link direct connection to the storage system networking as an example, as shown on the left side of the following figure. As shown in the figure, the storage system is mapped to only one LUN of the application server. Since the application server has two paths to the storage system and the multi-path software is not installed, the application server discovers two LUNs of LUN0 and LUN1 at the same time. LUN. These two LUNs are actually the same LUN on the storage system. Due to the identification error of the application server, when the application server performs the write operation, different applications repeatedly write different data to the same location of the same LUN, and finally the data written on the LUN is destroyed. In this case, the application server needs to identify which one is actually available. UltraPath can query the configuration of the storage system, so it can identify which LUNs can actually be used by the application server.\nA new VAAI primitive, using the SCSI UNMAP command, enables an ESXi host to inform the storage array that space can be reclaimed that previously had been occupied by a virtual machine that has been migrated to another datastore or deleted. This enables an array to accurately report space consumption of a thin-provisioned datastore and enables users to monitor and correctly forecast new storage requirements. The mechanism for conducting a space reclamation operation has changed somewhat since the primitive was introduced in vSphere 5.0. Then, the operation was automatic: When a virtual machine was migrated from a datastore or deleted, the UNMAP was called immediately and space was reclaimed on the array. There were some issues with this approach, however, primarily regarding performance and an arrays ability to reclaim the space in an optimal time frame. For this reason, the UNMAP operation is now a manual process. Because it is the only manually initiated primitive, the process to reclaim dead space will be discussed in detail. To reclaim dead space from a thin-provisioned datastore in vSphere 5.1, first verify that the storage array is capable of processing the SCSI UNMAP command. With esxcli, users can display device-specific details regarding Thin Provisioning and VAAI. The device is indeed thin provisioned and supports VAAI. Next, users can display the VAAI primitives supported by the array for that device, as well as whether the array supports the UNMAP primitive for dead space reclamation (referred to as the Delete Status).\nAnother esxcli command is used for this step, as is shown in the following: The device displays Delete Status: supported to indicate that it is capable of sending SCSI UNMAP commands to the array when a space reclamation operation is requested. If a Storage vMotion operation is initiated and a virtual machine is moved from a source datastore to a destination datastore, the following actions will reclaim that space: When the Storage vMotion operation has completed, the vSphere Client will report that the VMFS5 volume now has much more free space. The issue is that when the amount of free space is checked on the thin-provisioned LUN backing this VMFS5 volume on the storage array, there is unused and stranded space. In this example, a NetApp FAS array is providing the back-end storage. Using a lun show CLI command on this NetApp array, we see in the following that 8.8GB of space is still being consumed, even though there are now no virtual machines or any other files on this datastore. This is the crux of the issue that we are trying to solve with the VAAI UNMAP primitive. To run the UNMAP command, change the directory to the root of the VMFS volume that you want to reclaim space from. The following command is run: In this example, we attempted a reclamation of 60 percent of free space.\nVAAI Tuning and Monitoring This section describes some of the options available for fine-tuning and monitoring the VAAI primitives. This will help administrators determine whether VAAI is working correctly in their respective environments. We first will discuss XCOPY. The default XCOPY size is 4MB. With a 32MB I/O, the expectation would be to see this counter in esxtop incrementing in batches of eight, the number of work items that will be created to deliver a 32MB I/O. The default XCOPY size can be incremented to a maximum value of 16MB if required, but that should be done only on the advice of your storage array vendor. The following command shows how to query the current transfer size and how to change it: # esxcfg-advcfg -g /DataMover/MaxHWTransferSize Value of MaxHWTransferSize is 4096 # esxcfg-advcfg -s 16384 /DataMover/MaxHWTransferSize Value of MaxHWTransferSize is 16384 NOTE: Please use extreme caution when changing this advanced parameter. This parameter is a global parameter, so it will impact ALL storage arrays attached to the host. While a storage array vendor might suggest making a change to this parameter for improved performance on their particular array, it may lead to issues on other arrays which do not work well with the new setting, including degraded performance. The data-out buffer of the WRITE_SAME command contains all zeroes. A single zero operation has a default zeroing size of 1MB.\nThere are several components required in the VMkernel to make VAAI work correctly. The first of these is thePluggable Storage Architecture (PSA) device filter framework. This is a feature that is intended to facilitatedevice-level value-add into the PSA I/O stack. It enables VMware to introduce new value-add into the PSA stackasynchronously, decoupling it from the vSphere release cycle. The primary motivation is to support VAAI.Without the PSA device filter framework, VAAI would have to be implemented in the storage array type policy(SATP) on a per-array basis. Implementation in the PSA device filter framework reduces duplicate code andsaves memory space in the VMkernel. Also, if VAAI were implemented in the SATP, it would prevent otherthird-party plug-ins, such as EMC PowerPath, from using the offload primitives.The term filter is somewhat misleading because the PSA device filter framework enables specific value-addedsoftware to be inserted between VMFS and the PSA device layer.The second required component can be referred to as a VAAI plug-in specific to the VAAI filter. It implementsvendor-specific VAAI functions such as ATS, XCOPY and WRITE_SAME. There were different implementationsof the VAAI block primitives in vSphere 4.1, but all of the primitives in vSphere 5.0 have been ratified by T10, soany array that is T10 compliant should be able to use VAAI./huaweiconnect/data/attachment/forum/201811/27/142923lclsl***d4xtqsss.png To leverage VAAI functionality on a LUN, you must have both a PSA device filter and a vendor-specific VAAIplug-in for each device. VMware has a single PSA device filter plug-in called VAAI_FILTER.\nIf a device supportsVAAI offloads, it first will be claimed by the VAAI filter, VAAI_FILTER. After it has been claimed by the filter, avendor-specific VAAI plug-in will claim and then manage the device. The following commands display the claim rules that are associated with device filters in the PSA. By using the following command, you can check whether an individual device has been claimed by the VAAIfilter.\nA storage infrastructure network is a network used to connect computer hosts, servers, and storage devices for data access and communication over a local or wide area. It includes a dedicated SAN and a common IP network infrastructure for applications. The system accesses the distributed storage devices on the network to provide data transmission channels and basic network services. With the increasing dependence of enterprise development on information technology, information systems have become an important tool for enterprise management. A virtualized desktop office system that integrates desktop, application and data into three functions. It can be combined with suitable clients to centralize all data and applications on the servers in the data center, effectively separating data, applications and clients. System security, manageability, and availability. Office system use problem The enterprise's application information system is formulated in the face of different employees and sub-role. Ordinary employees generally only have the right to enjoy one user module according to their job responsibilities. If employee A's position in the enterprise is the leader of the material department, he can only see the information within his role's authority after entering the system through password authentication. If in an emergency, he needs to work on other PCs in the system, he is eager to log in to his desktop in a different location without reconfiguration, which is difficult to achieve in the old PC management mode. In addition, due to the wide variety of PC underlying hardware, different departments and different systems have different desktop environments and hardware configurations.\nTherefore, PC desktop standardization has become a problem. The distributed nature of PCs makes it difficult for enterprises to pool resources, increase utilization, and reduce costs. Enterprises spend a lot of manpower and material resources in daily PC management, hardware maintenance, software deployment, updates and patches. In order to better solve the above problems, the introduction of virtual office systems has become a new breakthrough. Cloud computing and virtualization technology Cloud computing is a kind of distributed computing technology. The most basic concept is to automatically split a huge computing processor into a myriad of smaller subroutines through a network, and then it is composed of multiple servers. The huge system, after the search, calculation and analysis, the processing results are returned to the user. Through this technology, network service providers can process tens of millions or even billions of information in a matter of seconds. Virtualization is a broad term. In the computer field, it usually means that computing components run on a virtual basis rather than on a real basis. Virtualization technology can expand hardware capacity and simplify the software reconfiguration process. The CPU virtualization technology enables a single CPU to emulate multiple CPUs in parallel, allowing one platform to run multiple operating systems simultaneously, and the applications can run in separate spaces without affecting each other, significantly improving computer productivity.\nConception of virtual office system The overall architecture of the virtual office system is divided into three levels: virtual storage layer (storage pool layer), server cluster layer (CPU, memory resource pool layer), management and application layer. The entire virtual storage system can be seen as a whole, and a server cluster is built on the basis of the shared storage system. The system automatically detects faults while using fewer redundant components. The virtual office system utilizes the hardware resource pool provided by the virtual storage layer and the server cluster layer to establish virtual machine resources, and establishes layer 3 (management and application layer) based on the virtual machine resources, and the layer has management services and user authentication management. Features. Through the application of a series of virtual office technologies, office data has been transferred from the previous scattered storage to centralized storage in a more secure all-weather machine room environment, and is managed by professional and technical personnel. This management method first makes the security of traditional data effectively improved; Secondly, it realizes the absolute control of the data, and achieves the access of the terminal data stream, fully ensuring that the information and data are not leaked; Once again, the unified management of the operating system is realized, so that system maintenance personnel do not have to shuttle to various scattered office locations, and the software system can be centrally maintained after communication by telephone. If the user applies for application system initialization, the preset template is used only.\nWhen the host attempts to access LUN 1, controller A directly delivers the access requests to LUN 1. Such a LUN access mode is called local access. When the host attempts to access LUN 2, the access requests are first delivered to controller A. Then, controller A forwards them to controller B through the mirror channel between controllers A and B. Finally, controller B delivers the access requests to LUN 2. Such a LUN access mode is called peer access. The peer access scenario involves the mirror channel between controllers. The channel limitations affect LUN read/write performance. To prevent peer access, you must ensure that a host has a physical connection to both controllers A and B. If a host is physically connected to only one controller, set the owning controller of the LUN to the one connected to the host. Owner Controller : 0A Work Controller : 0A If a link connected to application server 1 fails as shown in , the UltraPath running on application server 1 switches the working controller of the LUN to controller B. If the two links connected to application server 2 work properly, both controllers of the LUN are the same. The UltraPath of application server 2 will attempt to switch the working controller of the LUN to controller A, and then UltraPath of application server 1 will switch it back to B. As a result, the switching between the two application servers will continue to recur. Disable the automatic LUN switchover function of UltraPath.\nI need to configure the SNMP to the SCOM collects the informations about the Storage System. I am using a Storage OceanStor 5500V3 and i am following the steps bellow: 1. Configure the SNMP Trap Server Address Management 1.1 - Add the Ip and Port from the Server Address - Agent of SCOM 1.2 - When I click in test connection, i receive the message that is Ok with the connection. After this step, I used the CLI and change the user mode to developer and I used the minisystem for doing the ping test between Storage Server and Agent SCOM, and I receive the messagem that is Ok with the connection. 2. Install the Plugin SCOM of Huawei into SCOM Server 3. Configure the Discovery into SCOM Server, but the SCOM Server is not collect the data and informations on the Storage System. for this environment we need have the AD domain server,SCOM Server,Management Terminal and storage device. note: 1SCOM server have join the AD domain and the server have configured Windows System Center Operations Manager service. 2Ensure that the management network port of the storage system can communicate with the network port of the SCOM server. the following is the configure details. For SCOM server: A.install the Operation System B.join the AD server. C.configure the firewall D.install Report Viewer E.install the Net Framework 4.0 F.install the web server (IIS) member. G.install SQL service H.install the SCOM software. OceanStor Microsoft System Center Plug-in.\n(After obtaining the installation package, you need to use the sha256sum tool to verify the integrity of the installation package.) configuration steps: Step 1 In the navigation bar at the bottom left of the SCOM main interface, click Administration. Step 2 In the upper left navigation bar, click Discovery Wizard. Step 3 The Computer and Device Management Wizard dialog box is displayed. Step 4 On the Discovery Type page, select Network devices. Step 5 Click Next to set the basic properties on the General Properties page. Step 6 Click Next. Set the discovery type to Explicit discovery on the Discovery Method page. The different parameters are described below. Explicit discovery(Discover the specified network device.) Recursive discovery(Discover the specified network device and all devices connected to the specified network device.) Step 7 Click Next. On the Default Accounts page, select SNMP v1 or SNMP v2 to run the account as the default account for discovering network devices. If there is no optional running account in the account list, click \"Create Account\" and follow the wizard to create a new account. The community word of the running account needs to be the same as the community word of the network device to be discovered, otherwise the network device cannot be discovered. For the Huawei OceanStor series storage system, the default community words are as follows: Storage_private(A community word with read and write capabilities.) Storage_public(Read-only functional community word.) Step 8 Click Next. On the Devices page, specify the network devices that need to be discovered and managed. A.Click \"Add\".\n Read-Only Watermark The default read-only watermark indicates that the used disk capacity occupies 90% of the valid disk capacity. When the used capacity of a node pool reaches the read-only watermark, the system reports a read-only alarm, and refuses data to be written to the node pool. By default, the recovery threshold of the read-only watermark is 5% lower than the alarm threshold of the read-only watermark. That is, when the used capacity is recovered to lower than 85%, the read-only alarm is cleared, and the system allows data to be written. The alarm threshold and recovery threshold of the read-only watermark are both configurable. The system combines nodes of the same type into node pools, which can then be combined into one tier. Different tier types deliver different performance.       describes the related parameters.      Name Defines the name of a node pool to facilitate the management and maintenance of the node pool. [Description] A node pool name contains 1 to 127 characters among letters, digits, hyphens (-), underscores (_), and periods (.).  It must start with a letter and cannot end with a hyphen (-), underscore (_), or period (.).  [Example] data-pool Owning tier Defines the storage tier to which a node pool belongs. [Example] tier1 Read-only watermark alarm threshold(%) After the node pool capacity usage exceeds the read-only watermark threshold, the system reports an alarm and no data can be written to the node pool.\nIf data is being written to the node pool, the system returns a failure message. [Description] By default, means that 90% of the total disk capacity of a node pool is used. Administrators can modify . The value ranges from 85% to 100%. When the node pool capacity usage is recovered from to , the system clears the alarm generated when the read-only watermark threshold is reached and the node pool is not in the read-only state. However, no new data can be written to the node pool. [Default value] 90% High watermark alarm threshold(%) After the node pool capacity usage exceeds the high watermark threshold, the system reports an alarm. [Description] By default, means that 85% of the total disk capacity of a node pool is used. Administrators can modify . The value ranges from 80% to 95%. must be lower than . When the node pool capacity usage is recovered from to , the system clears the alarm generated when the high-watermark threshold is reached and new data can be written to the node pool. [Default value] 85% Recovery read-only watermark recovery threshold(%) After the node pool capacity usage exceeds the recovery read-only watermark threshold, the system clears the alarm generated when the read-only watermark threshold is reached and the node pool is not in the read-only state. The new data can be written to the node pool. [Description] By default, means that 85% of the total disk capacity of a node pool is used. Administrators can modify .\nThe value ranges from 80% to 95%. must be lower than . [Default value] 85% High Watermark Recovery threshold(%) After the node pool capacity usage exceeds the recovery watermark threshold, the system clears the alarm generated when the high-watermark threshold is reached. [Description] By default, means that 80% of the total disk capacity of a node pool is used. Administrators can modify . The value ranges from 75% to 90%. must be lower than and . [Default value] 80% Enable Spillover Specifies whether files can be stored to a migration node pool when the capacity usage of the node pool specified for storing the files exceeds . [Description] Disabled Spillover: When new files need to be stored but the capacity usage of the node pool specified for storing the files exceeds , the files fail to be stored.  Enabled Spillover: When new files need to be stored but the capacity usage of the node pool specified for storing the files exceeds , the files will be stored in a migration node pool.  Multi-tier Spillover is not allowed. If node pool A is configured to spill over to node pool B, node pool B cannot be configured to spill over to node pool C. [Default value] Disabled Target Node Pool After Spillover is enabled, a node pool must be selected for storing migrated files. Click . In the dialog box that is displayed, select a node pool for storing migrated files. Only one node pool can be selected for storing migrated files.\nThe following is an example: Command output before heterogeneous disks are deleted: linux:~ # upadmin show vlun type=migration ------------------------------------------------------------------------------------------------------------------------------------- Vlun ID Disk Name Lun WWN Status Capacity Ctrl(Own/Work) Array Name Dev Lun ID 0 sdc hy1 600188210062f2579636991a00000006 Normal 1023.00MB 0A/0A storage219_70 102 0 sdc eDevLUN022_001 600188210062f2579636991a00000006 Normal 1023.00MB 0A/0A storage219_73 21 2 sdd hy2 600188210062f2579636e59500000007 Normal 1022.50MB 0A/0A storage219_70 103 2 sdd eDevLUN022_002 600188210062f2579636e59500000007 Normal 1022.50MB 0B/0B storage219_73 22 ------------------------------------------------------------------------------------------------------------------------------------- Command output after heterogeneous disks are deleted: linux:~ # upadmin show vlun type=migration ------------------------------------------------------------------------------------------------------------------------------------- Vlun ID Disk Name Lun WWN Status Capacity Ctrl(Own/Work) Array Name Dev Lun ID 0 sdc eDevLUN022_001 600188210062f2579636991a00000006 Normal 1023.00MB 0A/0A storage219_73 21 2 sdd eDevLUN022_002 600188210062f2579636e59500000007 Normal 1022.50MB 0B/0B storage219_73 22 ------------------------------------------------------------------------------------------------------------------------------------- Run the upadm show vlun command to query the names of the heterogeneous disks to be deleted. In this example, the disk name is , which represents both the local and heterogeneous disks. -bash-3.00# upadm show vlun type=migration --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Vlun ID Host Lun ID Disk Name Vlun Name Vlun WWN Status In Use Capacity Controller(Own/Work) Array Name Array SN Dev Lun ID No. of Paths(Available/Total) 0 1 hdisk0 Lun_008 632373610031304180C259F600000150 Fault No 8.00GB 0B/0B array_test_218_185 210235G6EDZ0AB201103 -- 0/4 0 0 hdisk0 eDevLUN155_002 632373610031304180C259F600000150 Available No 8.00GB 0A/0A storage219_70 2102350SHY10G6000014 155 2/2 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Run the lspath command to query all logical paths of . The result includes the logical paths of both the local and heterogeneous disks.\n-bash-3.00# lspath -l hdisk0 -F\"name:parent:connection:status\" hdisk0:fscsi2:2400323736313041,0:Failed hdisk0:fscsi2:2210323736313041,0:Failed hdisk0:fscsi3:2401323736313041,0:Failed hdisk0:fscsi3:2211323736313041,0:Failed hdisk0:fscsi2:201248435a775148,1000000000000:Enabled hdisk0:fscsi2:200348435a775148,1000000000000:Enabled NOTEIn the preceding command, represents the disk name that is obtained by the upadm show vlun = command. Run the upadm show phypath command to query the path of the heterogeneous storage system in Target Port. -bash-3.00# upadm show phypath ---------------------------------------------------------------------------------------------------------------------------------------------- PhyPath ID Initiator Port Array Name Controller Target Port PhyPath State Check State Port Type Port ID 0 10000090fa1c6c32 array_test_218_185 0A 2400323736313041 Normal -- FC -- 1 10000090fa1c6c33 array_test_218_185 0B 2211323736313041 Normal -- FC -- 2 10000090fa1c6c33 array_test_218_185 0A 2401323736313041 Normal -- FC -- 3 10000090fa1c6c32 array_test_218_185 0B 2210323736313041 Normal -- FC -- 6 10000090fa1c6c32 storage219_70 0B 201248435a775148 Normal -- FC CTE0.B.IOM0.P2 7 10000090fa1c6c32 storage219_70 0A 200348435a775148 Normal -- FC CTE0.A.IOM0.P3 ---------------------------------------------------------------------------------------------------------------------------------------------- NOTE indicates the WWN of the port on a storage system. In this example, the paths whose is , , , and belong to the heterogeneous storage system. According to the results in and , the logical paths of on the heterogeneous storage system are the following: hdisk0:fscsi2:2400323736313041,0:Failed hdisk0:fscsi2:2210323736313041,0:Failed hdisk0:fscsi3:2401323736313041,0:Failed hdisk0:fscsi3:2211323736313041,0:Failed Run the rmpath command to delete the logical paths obtained in . NOTE represents the path to be deleted. represents the disk name. represents the logical device name of the parent device. indicates deletion.\nBigdata and data mining are two different things. Both of them relate to the use of large data sets to handle the collection or reporting of data that serves businesses or other recipients. However, the two terms are used for two different elements of this kind of operation. Big data is a term for a large data set. Big data sets are those that outgrow the simple kind of database and data handling architectures that were used in earlier times, when big data was more expensive and less feasible. For example, sets of data that are too large to be easily handled in a Microsoft excelspreadsheet could be referred to as big data sets. Data mining refers to the activity of going through big data sets to look for relevant or pertinent information. This type of activity is really a good example of the old axiom \"looking for a needle in a haystack.\" The idea is that businesses collect massive sets of data that may be homogeneous or automatically collected. Decision-makers need access to smaller, more specific pieces of data from those large sets. They use data mining to uncover the pieces of information that will inform leadership and help chart the course for a business. Data mining can involve the use of different kinds of software packages such as analytics tools. It can be automated, or it can be largely labor-intensive, where individual workers send specific queries for information to an archive or database.\n [Hannover, Germany, June 15, 2018] A recent report by international IT and data analysis consultancy IDC, titled IDC MarketScape: Worldwide IoT Platforms (Device and Network Connectivity Providers) 2018 Vendor Assessment, ranks Huawei OceanConnect as a leader in the IoT market segment. Depicted in the following chart, their analysis finds that Huawei's platform excels in three dimensions: capabilities (vertical axis), strategies (horizontal axis), and market performance (bubble size). IDC MarketScape Worldwide IoT Platforms (Device and Network Connectivity Providers) Source IDC, 2018 The report notes that Huawei's OceanConnect IoT Platform provides full-stack platform services, including connectivity management, device management, and application enablement. In large commercial deployments of their technology, Huawei collaborates with operators to offer cloud-network synergy and leverage the advantages of NB-IoT networks. Unlike other IoT players, Huawei provides enterprises with IoT cloud services accessible anywhere in the world. In addition, HUAWEI CLOUD strategies lay a solid foundation for IoT development. Customers praised Huawei for its secure and reliable services, and for forming a genuine partnership. Huawei remains committed to bringing digital transformation to every person, home, and organization to create a fully connected, intelligent world. To this end, back in 2015, Huawei focused its strategic development on a bringing together three elements: the OceanConnect IoT Platform, two access modes (wired and wireless), and the lightweight but powerful LiteOS. Taking an integrative approach to innovation and development, Huawei endeavors to build a smart IoT platform, develop multiple connection modes, and foster an intricate IoT ecosystem.\n [Hannover, Germany, June 14, 2018] At CEBIT 2018, Huawei exhibits technological innovations for distributed cloud storage, which will enable more enterprises to benefit from cloudification, and accelerate digital transformation for all people everywhere. Hu Yuhai (left), Senior Director of Huawei European Marketing & Product Management Department and Markus Leberecht (right), Senior Cloud Specialist of Intel Deutschland GmbH exhibit technological innovations for distributed cloud storage Huawei FusionStorage, a software-defined distributed cloud storage product, provides both traditional applications and emerging cloud applications with storage services featuring large-scale scale-out capabilities, best-in-class efficiency, and on-demand service provisioning. FusionStorage helps enterprises build cloud infrastructure platforms and supports 30% more cloud applications than its peer products with the same hardware configurations. \"Huawei FusionStorage helps you eliminate data silos, optimize resource configuration, and simplify storage management regardless of whether the enterprise data center is experiencing storage consolidation or evolving toward private clouds or hybrid clouds.\" Hu Yuhai, Senior Director of Huawei European Marketing & Product Management Department, said, \"The scale-out and on-demand service provisioning capabilities allow customers to purchase, deploy, and refresh hardware in batches within a flexible budget. FusionStorage storage resource provisioning efficiency in a cloud environment is 10 times faster compared with that in a traditional storage architecture. This cooperation is a new chapter in making digital transformation accessible to more and more people. Huawei will work with Intel to build a converged and efficient cloud infrastructure platform, helping enterprises achieve data-driven service innovation.\" The cloud transformation of each industry is in full swing.\nThe biggest challenge for enterprises is cloud-enabled storage. Markus Leberecht, Intel's senior cloud expert, gave a speech and showed how Intel Xeon Scalable platform enhances capability of storage systems, to improve performance of storage system, to strengthen storage expansion capability, to eliminate data transmission bottlenecks, at the same time, Intel Xeon Scalable platform as the foundation of Data Center Innovation, delivers 1.65x average performance boost over prior Generation. Based on Intel's technical innovations, the latest-generation FusionStorage increases IOPS from 100,000 to 200,000 on a single node. With comprehensive product capabilities, Huawei is dedicated to providing full service capabilities for each type of enterprise applications. By constructing a data service platform oriented to cloud data centers, Huaweis storage offerings help enterprises leverage storage resources on cloud and achieve data collaboration in on-premises and cloud-based scenarios. Since the first distributed storage product was launched in 2009, Huawei distributed storage has been widely adopted by more than 1000 industry clients worldwide. With the wide application of cloud computing in enterprise data centers, Huawei will further cooperation with industry partners, including Intel, to build a more comprehensive cloud storage ecosystem. Huawei together with its partners and customers is showcasing new digital transformation solutions to orchestrate a digital symphony at CEBIT from June 11-15 in Germany, the home of classical music.\n [Hannover, Germany, June 13, 2018] At CEBIT 2018, Huawei release the SAP HANA Tailored Datacenter Integration (TDI) solution. This elastic, reliable, and efficient solution fully satisfies the data center and business consolidation needs of large enterprises. Meng Guangbin (left), President of Huaweis Storage Product Line and Marco Ciavarella (right), Director of SAP EMEA South Region release the SAP HANA TDI solution Based on Huawei's high-end all-flash storage OceanStor Dorado18000 V3 and KunLun Mission Critical Server, the highly reliable high-performance HANA TDI solution is the most powerful SAP HANA infrastructure solution in the industry. It supports up to 200 SAP HANA nodes, and each node can be expanded to 32 TB, which is the highest among all SAP certified infrastructures. Utilizing the RAS2.0 technology, Huawei KunLun Mission Critical Server provides industry-leading reliability. Huawei OceanStor Dorado18000 V3 ensures end-to-end high availability of SAP services. Together, they lay the foundation for the most reliable high-performance SAP HANA TDI solution in the industry. Huawei's SAP HANA TDI solution adopts the OceanStor Dorado V3 and OceanStor 5000F V5 series all-flash storage systems, enables SAP HANA databases to be seamlessly integrated into existing data center architecture. This allows the databases to offer robust flexibility, unwavering reliability, and efficient management and maintenance. In addition, existing data center IT investments and processes can be reused to reduce the TCO. With superb elasticity, reliability, and efficiency, this newly released solution helps enterprises build converged data centers that support on-demand resource expansion.\nThe solution provides shared storage resource pools for multiple services to improve resource utilization, and offers the industry's best scalability of up to 200 nodes for enterprise storage, certified by SAP, to support the rapid growth of enterprise business. In addition, the solution uses dedicated management plug-ins to work with the SAP Landscape Management (LaMa) management platform, automatically manages SAP applications, databases, networks, and IT infrastructures from end to end, and takes advantage of Huawei storage snapshots to increase the setup speed of the development and testing environment by 10 times compared with the traditional mode. Focusing on enterprise analysis applications, SAP HANA, launched by SAP in 2011, is a product based on in-memory computing, which supports real-time application and analysis operations. This speeds up both business intelligence and complex computing. To provide robust flexibility, SAP defines the TDI mode for HANA deployment. Therefore, enterprises can choose to run HANA hardware components based on their needs and deploy the SAP HANA system using the TDI solution. Meng Guangbin, President of Huaweis Storage Product Line, states, \"Huawei is the storage vendor to put NVMe into commercial use. Specifically designed for the heavily-loaded critical applications, Huawei's all-flash storage meets the storage requirements of enterprise-class applications such as databases, virtual desktops, and server virtualization. This allows it to effectively meet solid-reliability and high-performance standards in SAP HANA scenarios.\nVersion information: V300R003C20SPC200 Test cases: 1.Check the configuration of the 4 LUNs, they are working in AA mode, 2of them are belong to controller A and the others belong to controllerB. At VMware side, customer create VMFS and striped, then create VM ondatastore. Normally, controller A and controller B should get equal performancetest result. And the LUNs should get equal perfomrance test result. 2. Analyze the storage log, we can find a lot of multipath failover events, as below: These failover commands were sent by multipath software of ESXi. So,we colllected vmsupport to analyze why VMware ESXi need to failoverpaths. 3. Analyze VMware kernel log(var\un\\log\\vmkernel.log), we found below error messages: It means storage IO latency two high causes multipath software set the path status as degrade status and failover to other paths. 4. Analyze storage performance log again, we do find large write IO response on controller, like below: But on disk domain layer, the write response time is normal. 5. So, this performance issue is concentrate on software, replicationlink and mirror link between controllers. We analyze the performancelog on remote site and find the similar result. 6. First, we analyze the replication link between the HyperMetro sites, Check replication link in \"Other\ss_info\". As below: 1. The speed of replication link is 10000bps, which meansmaximum 1200MB/s bandwidth. 2. Since the storage is all flash array, the maximum write bandwidthon each LUN is around 800MB/s. Theoretically, each controller can reach1600MB/s bandwidth, but the replication link can only reach 1200MB/sbandwidth.\nThe never-ending flood of unstructured data -- from documents and spreadsheets to photos andvideos -- is driving many IT shops to pay closer attention to their file-basedstorage infrastructure. And, increasingly, the decision will come down to traditional scale-upvs. scale-outnetwork-attached storage (NAS) boxes. Traditional NAS boxes have fixed capacity while scale-outNAS systems can expand to store and manage multiple petabytes of data. However, there aretradeoffs for that added scalability. Traditional NAS comprises one or two controllers, or NAS heads, and a pre-setamount of CPU, memory and drive slots. Once the NAS device reaches its limits, the user needs tobuy a new, separately managed system to boost capacity and performance. Traditional NAS issometimes known as scale-upNAS because it's upgraded by adding improved performance and capacity (speeds and feeds) to anexisting architecture. In contrast, scale-out NAS grows by adding clustered nodes. These are often x86 servers with aspecial operating system and storage connected through an external network. Users administer thecluster as a single system and manage the data through a global namespace or distributed filesystem, so they dont have to worry about the actual physical location of the data. Both traditional and scale-out NAS are growing, though use cases are evolving, said RickVillars, vice president of storage systems and executive strategies at IDC in Framingham, Mass.,via email. Traditional NAS is playing a greater role in virtualized server environments.\nScale-outis the foundation for many cloud and large archive environments, and will come to dominate in termsof capacity shipped. IDC predicts that more than 83% of the shipping capacity for enterprise storage systems willaccommodate file-based data within three years, and the growth rate for file storage will be 2.5times greater than the rate for block-based storage capacity. Enterprise Strategy Group (ESG) Inc. in Milford, Mass., forecasts that by 2015, scale-outstorage will make up 80% of all net-new networked storage shipments from a revenue standpointand 75% of all networked storage capacity. ESG doesnt distinguish between NAS and SAN because itassumes that all scale-outsystems will ultimately support file and block storage. Traditional NAS systems increasingly have become multiprotocol.For instance, earlier this year EMC Corp. unveiled its VNX unified storage family, which converges its Celerra NAS and Clariion SAN systems. NetApp Inc.s FAS and V-Series products also support unified connectivity for file and block workloads. In this tutorial, we'll focus on file-storage capabilities. Here are some of the differentiatorsto consider when evaluating traditional NAS vs. scale-out NAS.    File-storage covers a broad range of workloads, from office productivityand collaboration applications to specialized systems in financial services, manufacturing andhealth care. Vendors generally optimize scale-up NAS devices for the random access of small files, and theseproducts work especially well with predictable performance and capacity requirements.\nTraditionalNAS also can serve as an alternative to tape-based backup and handle limited data archiving. More recently, traditional NAS has seen an uptick in usage with virtual servers, especiallythose based on technology from VMware Inc., and databases, including those from Oracle Corp. Greg Schulz, founder and senior ***yst at Stillwater, Minn.-based StorageIO Group, said thetrend will likely accelerate now that VMware vSphere5.0 has added greater feature/function parity between NAS and SAN with its vStorageAPIs for Array Integration (VAAI) and vStorageAPIs for Storage Awareness (VASA). Plus, NAS tends to be flexible and easy to use withvirtualization technology, he added. Jason Blosil, a product marketing manager at NetApp, attributed the increasing interest in NASfor databases to faster 10Gigabit Ethernet (10 GbE) networks, the lower cost of Ethernet in comparison to Fibre Channel(FC), and the ease with which file protocols configure and scale.  High-performance computing (HPC) was the original sweet spot for scale-outNAS, as select industries craved the high throughput the systems offered for exceptionally largefiles and data sets. Early scale-out systems were especially popular in scientific and academicresearch, biotechnology, oil and gas, engineering, design and media production.   Mike Davis, director of NAS marketing at Dell HPC applications need multiple processors, memory modules and data paths. Parallel dataservices, which break up single files and deliver them in pieces in parallel, are an absolute must,said Terri McClure, a senior ***yst at ESG. She likened the services to the checkout process at agrocery store.\nScale-out storage is rising in popularity in the storage industry. Earlyadopters of the technology were primarily companies that had big capacity requirements, but nowsmall firms and large enterprises are implementing the technology in their data storageenvironments. The technology allows organizations to start out with a small amount of data storageand expand it to fit their needs without dishing out a lot of upfront costs. In addition, specifictypes of scale-out storage, such as scale-outnetwork-attached storage (NAS), can help boost performance and sustainability. This tutorial discusses how and why scale-out storage and scale-out NAS are gaining traction intodays storage environments. Take a look at why cloud gateways, scale-out NAS and object storage are replacing file storage.Read our case study on photo/video company Industrial Color Inc. and why Isilon scale-out NAS was abetter fit for its demanding workloads than traditional NAS. Finally, listen to our podcast onscale-out NAS in which Enterprise Strategy Group senior ***yst Terri McClure predicts that in thenext five years scale-out storage will make up the majority of data storage systems.   As more people become dissatisfied with their file-based primary storage, three technologies arebecoming more prominent in the storage industry: scale-out NAS, object-based storage and using the cloud as a NAS tier. See outhow these three NAS technologies can help with your primary file storage and decide which one isright for your storage environment.   Industrial Color Inc., a New York-based creative production company, installed Isilon scale-outNAS five years ago to handle its large quantities of photos and videos.\nThe storage industry is in a period of rapid innovation that is creating new options for storagebuyers. However, since the lifespan of a storage system typically ranges from three to five years,many storage professionals need to dust off their request-for-proposal(RFP) requirements to capitalize on these new developments. Here are some tips and resourcesfor crafting an updated storage RFP to address the three most commonly cited reasons for buying newstorage systems: the need for more capacity, the desire for a technology refresh, and the need forincreased performance.   In a recent Enterprise Strategy Group (ESG) survey, a quarter of respondents selected need additionalcapacity and cant expand existing system(s) as the top reason for buying and deploying a newstorage system. This highlights a drawback of many legacy storage arrays: When theyre full,theres nowhere to grow. In the best-case scenario, this means planned downtime and an in-placeupgrade. In the worst case, it could mean a forklift upgrade and a time-consuming data migrationproject. A new generation of storage systems addresses this challenge with near-infinite scalability fororganizations of all sizes. Every major storage supplier offers scale-outNAS and SAN systems, including unified scale-out systems that provide the ability to run multiple storageprotocols on a single platform. If you are among the 25% of storage buyers who have run out ofheadroom to grow your legacy system, you should add a requirement for scale-out storage to yournext RFP.    According to IDC, the adoption of converged systems will change, over time, how storage isprocured, deployed and utilized.\nVirtualization, private clouds and software-defineddata centers are driving rapid adoption of a new model for architecting infrastructure. Likescale-out storage, integratedstacks are already available from every leading storage vendor and range in size fromentry-level to petabyte-scale. For any technology refresh that requires the support of virtualized applications or a privatecloud, you should consider a converged storage infrastructure solution as an option for meetingthose needs.  Perhaps the hottest development in the storage industry is the widespread use of flashtechnology to accelerate performance. The question is not whether to use flash technology, but howto best employ flash to optimize performance at a reasonable cost. Your next RFP should provideenough detail on your dataaccess patterns so vendors can tailor a solution to best meet your needs, using server-sideflash, hybrid arrays, all-flash arrays or a combination of approaches. A closely related consideration is whether to leverage tiering software and capacity-optimizeddrives to reduce the overall cost for your flash-optimized system. While every major vendor offersa tiering solution, there can be significant differences in results based on how frequently thesesolutions move data and the granularity at which they operate.  An industry leader, NetApp has introduced a number of innovations in the latest release of its clusteredData ONTAP architecture that enable Infinite scale, Immortal operations and Intelligent datamanagement, including support for unified scale-out, integrated stacks and multiple options fordeploying flash.\n Looking at the key storage trends for 2013, we predict it will be another big year for solid-statetechnologies. Flash will be applied broadly to a wide range of workloads,from virtualizedservers and desktops to online transaction processing (OLTP) to file services. Organizations willcontinue to integrate flash into every area of the storage architecture, from cache at the hostlevel and in-storage arrays to all-flasharrays. However, as flash increases its enterprisepenetration, performance alone wont be a differentiator. Questions about product- andcompany-readiness for enterprise deployments will dominate. Softw***e integration, currently anafterthought, will bubble to the forefront of storage trends, and implementation decision criteriawill focus on both performance gains and data management.  More than 30% of organizations will move at least one enterprise data center workload to thecloud. Cloud services will be deployed even more broadly for privatestorage, including disaster recovery, backup and archiving. Partnerships between enterprisestorage and hyperscalar cloud service providers will increase. Another important trend to watch isthe convergence of on-premises and cloud storage, whereby enterprises will no longer need tocompromise between the security, resiliency and availability of an on-premises implementation andthe scalability and performance of cloud services.  In most cases, customers will operate more than one hypervisor, with Microsoft Hyper-V gaining a stronger presence. The emergence of Hyper-V will be driven by theanticipated penetration of WindowsServer 2012 in private cloud implementations.\nThe coming year will also see emerging storage trends like wideruse of open source alternatives, including CloudStack and OpenStack, for large-scale cloudorchestration.  Few technologies are poised to have as dramatic an impact on IT as in-memory computing, drivenby growing data requirements and the push to make real-time business decisions. SAP HANA will generate high interest as an example of a new class of combined OLTP/***ytics platforms.And interest in NoSQL technologies will increase as customers seek lower-cost alternatives toconsolidate application platforms.  Unplanned downtime has never been tolerated, but planned downtime has been an unavoidablereality. As clusteredstorage gains a foothold and more organizations embrace the notion of an agile datainfrastructure, the idea of 100% uptime and the elimination of planned downtime will becomerealities.  More enterprises will turn to the flexibility of converged infrastructure models to enablerapid innovation. Vendor-exclusive stacks will give way to best-in-class converged infrastructureswith prevalidated components and clear deployment guidance. This will be a breakout year for the FlexPod data center platform from Cisco and NetApp as organizations look to spend less energy integratingtheir infrastructure.  Alternatives will emerge to offer in-place access to enterprise data, which will slowadoption of offerings from enterprise box startups. In-place access solutions will provideenterprises with security and control of data while meeting mobile access and collaborationneeds.  Large-scale growth in object storage will begin in 2013, fueled by the massive growth in theInternet of things (smart devices, remote sensors) and mobile devices.\n        With the increased use of computer technologies, data backup becomes increasingly important. Traditional data backup approaches, however, fail to meet a demanding recovery time objective (RTO) or recovery point objective (RPO). Traditional data backup approaches can also reduce service performance or even cause service interruptions. Moreover, facing exponential data growth, these approaches need long backup windows. To address these backup issues, a variety of data backup and protection technologies, including the snapshot technology, have come into being.   A virtual snapshot is a point-in-time copy of source data. A virtual snapshot serves as a data backup and is accessible to hosts. The virtual snapshot technology has the following characteristics:  : A T series storage system can generate a virtual snapshot within several seconds.  : A virtual snapshot is not a full physical data copy. Even if the amount of source data is large, a virtual snapshot occupies only a small amount of storage space. The T series storage systems also support policy-driven periodic creation of virtual snapshots. This sub feature is called periodic snapshot.\nA periodic snapshot policy enables T series storage system to create data copies that preserve the state of source data at multiple points in time, providing continuous protection for the source data.   Table 1-1          This section describes the availability of the virtual snapshot feature in terms of license requirement and applicable versions.   Virtual snapshot is a value-added feature that requires a license.                                     Table 1-2.                Figure 1-1  Figure 1-1 Virtual snapshot access process         Figure 1-2 shows   Figure 1-2 Virtual snapshot rollback process           During the virtual snapshot rollback process, if an application server attempts to write data to the source LUN, the storage system performs the following processing:  If rollback is implemented for the location where the new data is to be written, the storage system directly writes the new data to the location.  If rollback is not implemented for the location where the new data is to be written, the storage system saves the data to the cache.\nIndustrial Color Inc. installed its first of five Isilon Systems Inc. scale-out network-attached storage (NAS) clusters fiveyears ago to cope with a deluge of digital photos uploaded through its homegrown GLOBALedit Webapplication. Aaron Holm, the companys vice president of development, said he cant imagine using anythingother than scale-out NAS to handle the digital photos and videos of customers such asKohls, NBC Universal, Old Navy, Victorias Secret and Warner Brothers. New York-based Industrial Color had approximately 25 TB of data in 2006 when it purchased itsfirst clustered NAS system from Isilon, long before EMC Corp. acquired the vendor inlate 2010. The company currently stores about 250 TB of data across its five Isilon scale-out NAS clusters. I dont think wed be able to do what we do as a business unless we had that kind of storage,Holm said. Its fundamental. Industrial Color has two businesses: software and creative photo/video production. The companybuilds and hosts two Web-based software-as-a-service (SaaS) applications: GLOBALedit for work-in-progressphoto and video management, review and approval; and File Society for high-speed file transfer.\nTheIT team also manages data storage of digital photos and videos that the companys technicians shootfor the Capture and Motion businesses. Although Holm didnt specifically seek out clustered NAS or scale-out NAS prior to the initial Isilon purchase, he waslooking for a system that would let the company scale as easily and inexpensively as possible tomeet escalating data storage needs. We just wanted really high availability and high-performance storage, he said. Before switching to Isilon scale-out NAS, GLOBALedit used a traditional NAS system from NetApp Inc. When the NetApp NAS system reached its limit, theIT team faced the prospect of buying new NAS heads and disk trays or perhaps an entire newdevice. Holm said with Isilons OneFS operating system running across all the hardware, adding storagenodes is as easy as plugging in a new Isilon blade and typing a command to add it to thecluster. With Isilon, you add the storage, but you also add the additional network interfaces, cache andprocessing because of the new hardware youve added to the infrastructure, Holm said. Becauseevery node in an Isilon cluster has the same components, when you scale it, youre actually addingperformance [in addition to capacity]. The concept of failover in scale-out vs. traditional NAS also appeals to Holm. With NetApp,the failover is based on the idea that I have two of everything, he said.\nWith Isilon, thefailover is based on the operating system being striped across the entire cluster. But because Isilon wasnt designed to handle mainstream business applications, Industrial Colorkept its NetApp system with approximately 5 TB for database servers. Industrial Colors second Isilon cluster coincided with its move from Manhattan to White Plains,N.Y., to a new disaster recovery (DR) facility. The project spanned four months because theteam decided to shift its production data from GLOBALedits legacy Isilon iSeries 6000 to its newX-Series 9000. Holm said the iSeries system is used for DR now. Holm said hes impressed by Isilons continually evolving roadmap, which tends to keep pace withhis needs. For example, Industrial Color recently needed to roll out a 10 Gigabit Ethernet (10 GbE) network to stream high-definition videosimultaneously to four desktops. Isilons latest S-Series supports 10 GbE. A year ago, Isilon didnt have a product that would have met what we wanted to do, hesaid. Industrial Color had been using Apple Inc.s Xsan storage with its Capture and Motion businesses when itrecognized the need for a system that could scale faster and require less time to manage. At that point, the metadata implementation on the Xsan had hurt us. Wed had situations wherewed had total storage failure, Holm said. But the three-month move from the Xsan to the more sophisticated Isilon cluster was not withoutpain; the new system required more administrative expertise as well as engineering work on thenetwork to get it to perform as desired.\n  Jon Toigo: Well, I think you're seeing two different trends here. One is that Hadoop is sort ofjoined at the hip with the concept of big data. We're talking about Hadoop-styleclustering. The industry is basically dissing shared storage at this point -- SAN [storage areanetwork] and NAS [network-attached storage] -- and preferring direct-attached storage [DAS],particularly DASthat uses flash. IBM has gone up on stage -- the director of their storage group -- and they'vespecifically said, 'We see flash as the future of everything.' So they're trying to push flash-basedstorage, direct-attached to clustered units as the modality for storing data that will be usedfor big data ***ytics. I don't know [if] that's the best approach to do this stuff, and I thinkwe're going to find out and we're going to spend a ton of money on it. It sets the clock back topre-1999 in terms of storage architecture and it re-introduces issues that we had two decades agoand currently have forgotten about regarding howyou protect your data in a world of isolated islands of storage. You're going to have to have awhole lot of replication going on from one node to another in order to find out how much bandwidththat's going to utilize to provide protection to isolated storage components. It does, however,sell more gear, which is something the industry likes to see given the general trendstoward slowing storage sales. The other side of the house is trying to look at this from a holistic perspective.\nThey'resaying, 'We spent the last ten years deploying shared storage, deploying Fibre Channel fabrics. Wehad the potential to grow that technology going forward in significant directions, whether it was usingInfiniBand, or SAS, or whatever the next generation of that is going to be. But why would wewant to aggregate all our stuff for ten years and then segregate it again?' It doesn't make anysense. So you look at companies like DataCoreSoftware, or to some extent IBM with SAN Volume Controller, and a few others who are trying tovirtualize that storage infrastructure so they can present virtual volumes up to the servers thatpretend or behave as though they're direct-attached storage for those servers. That makes a heck ofa lot more sense and also gives you the ability to manage holistically all the infrastructurethat's associated with storage. And I think a lot more work needs to be done in that space ratherthan segregating all the storage and doing a bunch of directattachment on a bunch of servers. And I think we're eventually going to get to that lattermodel.   Toigo: You know, I have a mixed mind as far as clouds go. I'm not a big advocate of cloudtechnology generally -- of the public variety. However, I did think that maybe one of the bettermodels for cloud going forward -- a sustainablebusiness model for cloud, would be cloud that is specialized in holding huge repositories ofcertain kinds of data. I asked experts about this.\nJeff Jonas at IBM, I asked him, would it makesense for a cloudservice provider to stand up big data so I don't have to buy the infrastructure myself? And Ithought that might make sense for a company that doesn't want to spend big bucks on theinfrastructure to supportHadoop for a business analysis project that's only going to be used once, or very infrequently,like voter registration analysis. Why would you want to stand up a multi-million dollarinfrastructure to ***yze one aspect of data and then go home and basically turn it off? It doesn'tmake any sense to me. [Jonas] didn't think much about that idea though, and I was kind of scratching my head aboutthat, but he explained his view. He said the amount of time it takes to positiondata in a cloud, and then the bandwidth that you have to pay for to get access to the data inthe cloud, and then the initial security issues, resiliency issues associated with data and thecloud, and many other aspects of cloud operations are not necessarily the best places to hostdata for big data ***ytics. Now, I think that, assuming some of these problems can be worked out, and that's a bigassumption, you might find a cloud provider that says, 'We handle all the data for the nationalinstitute that has all the treatment data for oncology for cancer treatment. We've properly takenout all references to the patients themselves, and the original data is all here.'\nNow if JohnsHopkins wants to runa big data analysis on a new drug trial they're doing, they should be able to, as a service,plug into that data set and include it in their ***ytical model. And that would make sense,because then you've got multiple customers in need of that kind of data. Would I put my own data up in a cloud? Probably not. I don't today, and I wouldn't going forwardbecause a cloud service provider is hampered by the fact that he doesn't own the network thatprovides the connectivity to my shop. So how can he with a straight face promiseme a service level? He doesn't control the mechanism by which I access the server. If my phonesystem goes up and down several times a month, it doesn't matter if I have a super stable cloudservice or not, I'm not going to be able to access it. So I don't understand why I believe anythinga cloud service provider tells me. I have a hard time believing my information is secure if it's inthe cloud. Now that may not be a big issue if I adopt some sort of one-way hash like [IBM's Jonashas suggested] and I depersonalized all the data that's up there and I don't have anything to worryabout. But for my businessprocesses that are mission-critical and for my business transactions, my financial information,credit card information, whatever, I'm sure as heck not going to put it up there.\nBottom line: Ihave issues with cloud and I'm not sure it's everything it's made out to be. Also, [on] the numberson cloud -- I read an article recently that said there was a 340% increase in cloud adoption, butit was 19 people they had in the survey.   Toigo: If you follow the Hadoopmodel, which is basically to break up your shared storage and deploy it on individual nodes anddirect-attached storage modalities, you run into a huge, huge problem with how you're going toreplicate and safeguard that data. That's a major issue. We're already encountering that in shopsthat have adopted VMware, because VMware doesn't perform well at all with traditionalshared storage modalities. What VMware is asking you to do is break up your SAN and deploy yourstorage in direct-attached configurations right next to each VMwareserver in the cluster. That creates an issue where you have to rely on back-end replication andmirroring from one box to another, and the problem with mirrors is nobody ever checks them. It's apain to shut a mirror down, quiesce the application, flush the data out of the cache onto the disk,copy that disk over to the secondary mirror, and then shut the whole operation down and do afile-by-file compare, and then cross your fingers and restart everything and pray to God that youdon't have a career-limiting event where things won't synchronize again. For that reason, nobodyever checks mirrors.\nThe insatiable need for file-based primary data storage is propelling three technologies -- scale-outnetwork-attached storage (NAS), object-basedstorage and the cloud as a NAS tier -- to the forefront as potential lifelines for IT shops overwhelmed by unstructureddata. Scale-outNAS systems can boost capacity, performance and availability with the addition of storage nodesor x86 servers equipped with a special operating system and storage. The most scalable of the clusteredstorage systems have the potential to manage petabytes of data across more than 100 nodes, buttheyre accessed and managed as a single system through the use of a distributed file system orglobal namespace. Object-basedstorage systems are another promising alternative to traditional NAS. Object storage foregoestraditional file systems, which have capacity and management shortcomings. Instead, these systemsassign a unique identifier, or digital fingerprint, to each file plus its metadata. This identifierrenders the physical location immaterial and provides massive scalability. Using the cloudas a NAS tier is another option for IT shops coping with a flood of unstructured data. Inparticular, a lot of attention is gravitating toward a new wave of file-based gatewayappliances that move data to a cloud service provider. These can be hardware or virtualappliances, and they can solve security and data access issues that make IT shops hesitant to usethe public cloud. Heres what you need to know about these three NAS technologies as you plot out your filestorage:   A traditional scale-up NAS box has a fixed amount of CPU, cache and drive slots. When it fillsup, the customer needs to buy another device.\nScale-outNAS systems appeal to organizations with huge files because of their potential for seeminglylimitless expansion while still being managed as a single storage resource. Also known as clusteredNAS, scale-out NAS originally took aim at applications requiring high throughput and highbandwidth, such as those in media and entertainment, high-performance computing, bio-informatics,and oil and gas. But these scale-out systems often werent tuned to perform well with the typical enterpriseapplication, where EMC Corp. and NetApp Inc. held sway with their traditional NAS devices. Terri McClure, a senior ***yst at Enterprise Strategy Group (ESG) in Milford, Mass, saidscale-out NAS tended to excel in environments with fewer numbers of unusually large files ratherthan the large number of small files the typical enterprise has. That made them a good choice forapplications such as video streaming. But as scale-out vendors tune their systems to perform betterwith more I/O-intensive enterprise applications, their systems are starting to show up in moreenterprise IT shops. Scale-out NAS got a major shot in the arm late last year when EMC acquired IsilonSystems. Isilon offers three options: its S-Series aimed at I/O-intensive smaller files, itsX-Series for fewer number of large files and its NL-Series for bulk high-capacity andlow-performance storage. Isilons 72000X has a maximum capacity of 10.4 PB in a single file system from a 144-nodecluster.\nThe companys solid-state drive (SSD)-equipped S200 has a lower maximum capacity at 2 PB,but offers 85 Gbps of aggregate throughput and 1.2 million NFS IOPS in a single file system/volumefrom a 144-node cluster. Isilon claims its distributed file system-centric system was built from the ground up for scale-outstorage, whereas systems that make use of a global namespace require a software layer forscale-out NAS. But Jeff Boles, a senior ***yst and director, validation services at Hopkinton, Mass.-basedTaneja Group, said the nuances of the architecture matter less to end users than the ease withwhich the system scales and whether multiple storage nodes can be managed as a single storagesystem. Scale out is still very new and innovative and proprietary, Boles said.\nBecause its not assimple of an operation as building a controller head on an array, youre not going to see aconvergence of technologies around one best architecture. In addition to Isilons offering, other scale-out products include BlueArc Corp.s Mercury and Titan Series Servers (which Hitachi Data Systems resells as the Hitachi NAS platform), DellInc.s PowerVaultNX3500 with a clustered file system acquired from Exanet, Hewlett-Packard (HP) Co.s X9000family (based on technology acquired from Ibrix), and IBMs SONAS.NetApp has a clustermode version of its Data Ontap 8 operating system (but not a clustered file system) and QuantumCorp.s StorNext and Symantec Corp.s FileStor are clustered file systems that run on hardware appliances. Greg Schulz, founder and senior advisor at StorageIO Group in Stillwater, Minn., said some scale-outNAS products increase the number of nodes for parallel performance or large sequentialstreaming, while others optimize for concurrent access of multiple small random file or page views.Some focus on data storage capacity, and others emphasize clustered file systems or clusterednodes, he said. More scale-out options are on the way.\nDell, for instance, plans to use Exanet technology to addscale-out capabilities to its EqualLogic and Compellent SAN systems, according to Scott Sinclair,senior manager of Dell enterprise storage. NetApps Brendon Howe, vice president and general manager of the NAS business unit, added viaemail that the companys next-generation Ontap 8 Cluster-Mode is designed as a scale-out version ofits unified architecture that extends to enterprise applications and virtualized data centers. We find that segmenting the scale-out discussion to just NAS isn't that meaningful tocustomers, Howe said. Randy Kerns, a senior strategist at Evaluator Group Inc. in Broomfield, Colo., said althoughthere are situations where scale-out NAS makes sense, there are also plenty of use cases wherecustomers will prefer simpler traditional NAS. It may boil down to theres a place for both, Kerns said. I think scale-out NAS andtraditional NAS will both be around a long time.   Object-based storage is hardly new. EMC pushed it into the forefront in 2002 with its Centeraline in an attempt to stake out a new market known as content-addressablestorage (CAS).\nBut performance issues generally relegated the use of CAS products to archivesof information that rarely if ever changed, such as medical images. A new wave of objectstorage makes use of such protocols as Representational State Transfer (REST), and is gaining asecond look for near-line and primary data storage -- especially in the cloud. Theres no technical barrier that says you cant use object [storage] for primary storage,said Andrew Reichman, a principal ***yst at Cambridge, Mass.-based Forrester Research Inc. Someprimary storage is not that performance sensitive, especially with files. EMC now promotes Atmos for that purpose. Other object offerings include Caringo Inc.s CAStor, DataDirectNetworks Web Object Scaler (WOS), Dells DXObject Storage (which uses Caringos technology), NetApps (formerly Bycast) StorageGrid,and products from startups such as Amplidata, CleversafeInc., MezeoSoftware and Scality. In the long run, we could see object[as] a replacement for file storage -- just a better way to do file storage, Reichmansaid. Object storage is attractive to cloud storage providers because of its massive scalability andshared tenancy features, especially in comparison to ordinary file- or block-based storage. You have so much metadata for each chunk of data, you can lock it down more easily and move itaround based on policies and change the redundancy based on policies, Reichman said, explainingthe draw for cloud providers.   Using the publiccloud as a NAS tier for primary storage is a much tougher sell for most IT shops than forbackups or archives.\nBut one of the emerging technologies that could start to make that prospectmore palatable is the gateway that acts as a hybridcloud storage appliance. The appliances supply an on-premises cache that can provide access to the most active orfrequently accessed data, so latency or network or cloud outages wont prevent users from gettingneeded files. Algorithms determine which data to store in the cache. Many of the appliances also offer data reduction technologies such as deduplication orcompression to reduce bandwidth consumption and lower the fees associated with transferring data toand from the cloud. They also encrypt the data before sending it off-premises and offer extrafeatures such as snapshots to lighten the load on backup systems. Several startups currently rule the roost in the NAS hybrid cloud space and typically partnerwith prominent cloud storage providers. They include CteraNetworks Ltd., NasuniCorp. and StorSimpleInc. Nasuni makes a software-based virtual NAS appliance that installs on a virtual machine(VM). Another option is Nirvanix Inc.s CloudNAS product, which can transform Linux or Windows servers into a virtual NAS gateway to the companysStorage Delivery Network (SDN) encrypted off-site storage. Nirvanix uses standard protocols such asNFS, CIFS and FTP for access to its service. Rick Villars, vice president of storage systems and executive strategies at Framingham,Mass.-based IDC, predicted that major NAS vendors such as EMC or NetApp will eventually provide theprotocol support for a cloud tier in addition to their SSDs and SATA and SAS drives. We think that day is coming. It may not be this year.\nAs a result, a digital library requires a storage system that is high capacity, scalable, and efficient. When SOU decided to move to a new site, an IT team from the university proposed constructing a brand-new data storage system, instead of just adding to the existing infrastructure. The team turned to Huawei for help, hoping that the company could build a basic architecture that would be user-friendly, reliable, and scalable enough to meet the needs of the rapidly growing academic community. The SOU IT team understood that in addition to meeting the needs of the digital library, the library's existing storage system would soon need to accommodate a new high-performance computing environment. As Ke Lixin, director of network systems for SOU put it, \"It is inevitable that we will need more space for a new supercomputing center soon.\" To ensure future storage scalability and security, the IT team chose the OceanStor N8000 as its platform, and supplemented the system with data protection software to create a complete data storage and protection solution. With the OceanStor N8000, the team can expand the storage capacity up to a maximum of eight petabytes and consolidate backup and recovery management. \"The OceanStor N8000 offers us the required scalability and flexibility.\nSince the data is moved dynamically to different tiers based on the policies, the team can reclaim tier-one storage space without expending extra IT resources. The OceanStor N8000 supports this thriving academic environment 24/7 with its high usability and the performance needed for the library's collection of 100 terabytes of unstructured data, such as e-books, images, audios, and video. The OceanStor N8000 is expandable, up to a maximum of 16 engines with a multi-node active cluster design, and its fully redundant modular design helps prevent single points of failure as well. Because all engine nodes in a cluster work together and can access a single file at the same time, the response for users accessing the same file is greatly reduced. This technique enhances the system's performance without degrading usability and flexibility. Such flexibility offers the IT team a comprehensive fail-over solution to minimize planned and unplanned downtime across its systems. The OceanStor N8000 also enables the team to expand, shrink, or reconfigure the system without downtime. The amount of data stored in SOU's digital library will continue to grow. Not only is higher storage performance and capacity a necessity, so is better resource utilization and cost management.\nThe value in unified storage lies in its ability to meet different needs for storage devices, support various protocols and applications, and share and manage one set of hardware to reduce procurement and maintenance costs. The OceanStor N8000 can simultaneously support multiple protocols, such as the Network File System (NFS), Common Internet File System (CIFS), Fiber Channel (FC), and the Internet Small Computer Systems Interface (iSCSI). This means that one set of hardware can support both file-level and blocklevel applications, which is a compelling advantage for many customers. The OceanStor N8000 also provides a user-friendly central management tool the N8000 Integrated Storage Manager (ISM) which offers an easy-to-use graphical user interface to configure, control, and monitor the system itself. System management is simplified, and the amount of manpower needed for installation and maintenance is reduced. The OceanStor N8000 and Net-Backup software are also cost-efficient. Because the incorporated NetBackup software can directly run on the OceanStor N8000, data can be protected in a unified manner under one solution without affecting application server performance.\nA file system snapshot is a permanent image of theN8500at a specified point in time. The file system snapshot provides an online backup mechanism. The file system snapshot can track the changes of the file system blocks since the last snapshot. The backup and replication applications only need to retrieve the changed data, which reduces the data motion activities. Meanwhile, data availability and integrity can be improved by increasing the backup and replication frequency.   The file system snapshot can be instantly generated for backup without affecting host services.   Snapshots can server as data sources for backup and archiving, thus improving data security and reliability.  File system snapshots can enable storage units to create flexible and frequent recover points, thus recovering data quickly.  You can create the backup policy as required. According to the policy, the stable files system snapshots are created. File system snapshots exert little impact on system performance and serve as an effective backup and recovery solution for the user. Purchase an extra license for file system snapshot.  The file system is in the stable state and all data is written on the hard disk.  The snapshot tool freezes the file system (source file system) and prevents all I/O requests instantly. Then, the file system snapshot without any data is created.  Initialize the file system snapshot.\nThis snapshot points to the block mirror of the source file system.  Retract the prevention of I/O requests.  The source file system snapshot initially includes the pointer that points to the file system block mirror but not any actual data. The block mirror points to the file system data. The snapshot displays the precise image by searching for data from the source file system. New data is written into the source file system after the source data is replicated to the snapshot. Before new data is written into the source file system, the old data on the source file system is replicated to the snapshot first. Then, refresh the file system. After the refreshment, new data can be written into the specified database of the source file system. In this way, old data will not be replicated to the snapshot when the write operation is performed, because old data only needs to be stored once. As the data block in the source file system keeps updated, the source data blocks are accumulated to the snapshot.  TheN8500supports the periodical snapshot function and the snapshots can be automatically created according to the customized policy. A snapshot is created at intervals of 15 minutes.  A single file system supports up to 512 snapshots, including 128 periodical snapshots.  Manual snapshot recovery is supported.    Online backup  Through the snapshot function, the copy of the source file system can be instantly created.\n  Backup is a key component of any good data protection strategy. But does the advent of virtualization mean traditional backup can be replaced by new data protection methods, such as replication, snapshots and live migrations? Why backup is still vital and how replication, snapshots and so forth can be used with it to build a comprehensive data protection strategy.    Backup is the process of making a secondary copy of data that can be restored to use if the primary copy becomes lost or unusable. Backups usually comprise a point-in-time copy of primary data taken on a repeated cycle daily, monthly or weekly. Sometimes backups can be used to roll back a virtual server environment to a previous point in time as part of the maintenance or upgrade process.\nBackups can also be used as a virtual machine cloning tool.  Backup may be required in the following scenarios:   Data can become corrupted through application software bugs, storage software bugs or hardware failure, such as a server crash.  An end user may delete a file or directory, a set of emails or even records from an application and subsequently need the data again.  Failure scenarios can include hard disk drive (HDD) or flash drive failure (multiple failures can cause data loss even when RAID is used), server failure or storage array failure.  Possibly the worst scenario is an event such as fire that renders hardware inoperable and permanently unrecoverable.    Service levels for backup are described using two metrics recovery point objective and recovery time objective   defines the historical point in time to where the backup will be restored. Some applications require an RPO of zero, but a period of minutes or hours is acceptable in many other scenarios.\nOccasionally, it is necessary to restore data from much further back in time, maybe days or weeks previously.  defines the amount of time required to restore backed-up data to the primary system, which could be measured in minutes or hours but is typically required to be as fast as possible.       Remote data replication is sometimes assumed to be equivalent to backup, but this is not the case. Replication solutions can be either synchronous or asynchronous; meaning transfer of data to a remote copy is achieved either immediately or with a short time delay. Both methods create a secondary copy of data identical to the primary copy, with synchronous solutions achieving this in real time. This means that any data corruption or user file deletion is immediately (or very quickly) replicated to the secondary copy, therefore making it ineffective as a backup method. Another point to remember with replication is that only one copy of the data is kept at the secondary location. This means that the replicated copy doesnt include historical versions of data from preceding days, weeks and months, unlike a backup.    A snapshot is a point-in-time copy of data created from a set of markers pointing to stored data and is effectively a backup.\nSnapshots provide a variety of approaches that can supplement backup and provide rapidly accessible copies to which is it possible to roll back.  So what are the key snapshot variants?  They include:  Most snapshot implementations use a technique called copy-on-write, which makes an initial snapshot then further updates as data is changed. Restoration to a specific point in time is possible as long as all iterations of the data have been kept. For that reason, snapshots can protect against data corruption, unlike replication.  Another common snapshot variant is the split-mirror, where reference pointers are made to the entire contents of a mirrored set of drives, file system or LUN every time a snapshot is made. Clones take longer to create than copy-on-write snapshots because all data is physically copied when the clone is created. There is also the risk of some impact to production performance when the clone is created because the copy process has to access primary data at the same time as the host.    .  is a method of snapshoting that tracks and store all updates to data as they occur. Theoretically, this means CDP solutions can roll back to any point in time, down to the smallest granularity of update. But there is a price to pay with CDP in terms of the cost of storage needed to keep every changed block copy and the performance impact of storing the data.\nAs a result, some vendors implement what they call near-CDP, taking snapshots of changed data at set times and consolidating changes over a longer time period. This means heavily updated data doesnt overwhelm the capacity of the CDP system. In virtual environments, APIs such as spheres VADP enable CDP solutions to be implemented by third-party software vendors.  The live migration functionality that comes with virtualization hypervisor platforms is undoubtedly an incredibly useful thing. It allows users to move virtual machines and data stored between physical locations without disruption. But despite the claims of some in the IT industry, it isnt really a backup solution, as the primary data is simply moved to another location. The ability to transparently move a VM to another location provides for some degree of disaster recovery, although the actual process of migrating really needs to happen before disaster strikes, making it more of a disaster-avoidance solution.       A good data protection strategy combines a number of aspects that can include all of the above features. Short-term snapshots are great for dealing with user errors and some data corruption scenarios. More importantly, they are very fast (data can be reverted or restored in seconds), usually very space-efficient and in many cases, restores can be performed by the user, taking the workload off the backup administrator. CDP takes things a step further with more flexible recovery scenarios that trade off backup capacity and performance against restore granularity.\nAfter logging in to the , configure the storage system using the wizard. The configurations include configuring basic device information, device time, alarm notification, and license management.       The dialog box is displayed.      Name Description Device Name  The value can contain only letters, digits, periods (. ), underscores (_), and hyphens (-).  The value contains 1 to 23 characters.  Device Location The value contains 1 to 255 characters. Password for super administrator  In the area, click . Using passwords strengths storage system security. Therefore, you are recommended to change the password during the initial configuration.   Enter the old password in the text box.  Enter a new password in the text box and repeat it in the text box.         Configure the device time using one of the following methods. Name Description Client Time Synchronization  Click . The dialog box is displayed.  Read and confirm the information, select the check box for .  Click . The window is displayed.  Set NTP Automatic Synchronizing  Enter the IP address of the NTP server in the text box. Both IPv4 and IPv6 IP addresses are supported.   Set the period of synchronizing the NTP server time in the text box. The minimum period is one minute, and the maximum is 10 days.   Click .\nThe dialog box is displayed.  Read and confirm the information, select the check box for .  Click . The window is displayed.  Manual  Click . The dialog box is displayed.  Modify the date displayed on the device in the group box.  Modify the device time in the group box.  Select the time zone of the location where the device is located in the drop-down list box.  Click . The dialog box is displayed.  Read and confirm the information, select the check box for .  Click . The dialog box is displayed, stating .  Click .  Click . The window is displayed.  Do not change the current device time Not to synchronize the device time. Click . The window is displayed.          Select and click . The dialog box is displayed, indicating that the disk domain configuration has succeeded. Click . The window is displayed. Click , you can view the disk number which can be used into the default disk domain.   Select and click .\nThe window is displayed.           Set .  Choose tab.  Select .  Enter the sender email address, SMTP server IP address, and SMTP port number in , , and text box. By using the SMTP server, emails with alarm information can be sent to the recipient's email address.   If the SMTP server needs to authenticate users, select . Then enter the user name and password in and text box to ensure the SMTP server security.  If you need to encrypt network connections at the transport layer, select .  Click . The dialog box is displayed.  In text box, enter the recipient email address. In , select the severity of alarms for which alarm notifications are sent to the recipient.  When is configured as , the system sends the alarms whose severity is critical, major, or warning to the recipient's email address.  When is configured as , the system sends the alarms whose severity is either critical or major to the recipient's email address.  When is configured as , the system sends only the alarms whose severity is critical to the recipient's email address.    Click .  Select the email that you have added, Click to check whether the settings are correct.  If the settings are correct, the mailboxes corresponding to Email Addresses will be able to receive the test email.  If the settings are incorrect, those mailboxes cannot receive the test mail.\nThe controller enclosure interconnects with maintenance terminals through its management network ports. Also, you can log in to the DeviceManager and initialize the storage system through management network ports. The IP addresses of the management network ports must be on the same network segment as the IP address of a maintenance terminal. If this is false, change the IP addresses of the management network ports through serial ports for rectification. The maintenance terminal has been connected to the serial port on the storage device through a serial cable.  The maintenance terminal and storage device must be interconnected through a serial port.  The default IP address of the management network port is 192.168.128.101 or 192.168.128.102 . The default subnet mask is 255.255.0.0 .  The IP addresses of both management network ports and those of heartbeat network ports or iSCSI host ports must be on different network segments. Otherwise, route conflicts may occur. The default IP addresses of heartbeat network ports are 127.127.127.10 and 127.127.127.11 , and the subnet mask is 255.255.255.0 . In addition, for a dual-controller system, you cannot use IP addresses that belong to the 127.127.127.XXX network segment. For a four-controller storage system, you cannot set IP addresses that belong to the 127.127.127.XXX , 172.16.126.XXX , 172.16.127.XXX , and 172.16.128.XXX network segments.  Make sure that the IP addresses of the management network ports on both controllers are in the same network segment.  The terminal management tools for Windows XP include ttermpro.exe and HyperTerminal which is a native software in Windows.\n   Domain management means the management of adding a computer into a group. TheN8500provides a unified logical view, allowing all users and programs to access all resources on the network. TheN8500supports multiple domain management modes, such as Network Information Service (NIS), Light Weight Directory Protocol (LDAP), and Active Directory Services (ADS). Domain management improves the enterprise-level manageability and achieves centralized management, such as account management, application management, and network management. With the directory information from the domain management, the user can efficiently and quickly find and read all information required online, which facilitates information share and circulation. You do not need to purchase an extra license for domain management. The directory in the domain management resembles a database, covering some descriptive information about properties, such as the the login user's email address. Because reading information from the directory is more frequent than writing information into the directory, the directory only needs to modify or query information but not process complicated services like the database. Directory service is a tool facing the end user as well as an information management tool, similar to a software hub. The NIS, LDAP, and ADS define the standard method of accessing and updating the information about the common directory. They are open industrial standards dominant in the market. TheN8500supports multiple domain management modes, such as NIS, LDAP, and ADS.\nAs the client in the domain, theN8500can share different resources with other clients according to their permissions, achieving unified management.    Introduction to the ADS   The ADS, the core of Windows 2000 networking system, is evolved from the Windows NT4.0 directory service.  Besides the common features like security and distribution, the ADS has some other features that facilitate information management and query.     Introduction to the NIS   The NIS is developed by SUN and used in the UNIX operating system for centralized management. It has become an industry standard. All mainstream UNIX-like operating systems, such as Solaris, HP-UX, AIX, Linux, FreeBSD, support the NIS.  The NIS belongs to TCP/IP and is used to query network information. Based on the RPC and portmap, the NIS server can enable important files to be shared with other servers on the network. Although the NIS and domain in Windows are implemented in different ways, yet they play the same role.     Introduction to the LDAP   The LDAP is a protocol that is based on X.500 and supports TCP/IP. The LDAP server is used to maintain a table that contains the directory information about current usage of storage resources, such as information about resource allocation and usage of each user or user group, and permission information. The server also converts logical file name to the physical file location.\nIn addition, with current directory information, the LDAP server provides directory services at a higher level, for example, search and query functions based on resource properties.  The LDAP directories are organized in tree structure and consist of entries. The entries equal to records in the database and integrate properties that have distinguished names. The DN is equal to the key word in the records. Entries in the LDAP are clearly organized according to geographical location and relationships. At present, based on the LDAP, many mature resource management systems and tools are developed, such as the Openldap, Novell Directory Service (NDS), and ADS.    The client sends query or operation requests to the LDAP server as required.  The server handles the request entry in the directory tree.  The server sends a reply to the client. The reply may be the query result, operation error information, or a reference. Reference is a redirection mechanism, indicating that the request entry cannot be carried out on this server.\nThe disk spin-down is to set the disks that have not undertaken write/read operations for a long time to spin down. The drive motor and the magnetic head of a spin-down disk stop running. The disk spins up when new I/O requests are delivered. The purposes of the disk spin-down is to lower the consumption of the storage system without effecting the write/read performance of the system and improve the energy efficiency of the storage system. The disk spin-down greatly reduces the power consumption, lowers the operating cost, and shortens the operating time of free disks, which prolongs disks' service life. You do not need to purchase an extra license for disk spin-down. The disk spin-down technology of theN8500is in the units of RAID groups. After confirming that the data in the RAID group is the near-line or offline data, you can set the RAID group to allow its hard disks to spin or wake up.\nAll the hard disks in a RAID group can be spun down or up as required; however, the disk spin-down technology does not apply for a single hard disk.  If no I/O is accessed in the RAID group, all the hard disks in the RAID group are spun down, that is, all the hard disks stop rotating.   If I/O is still accessed to any hard disk in the RAID group, the RAID group cannot be spun down.   After a RAID group is spun down, to read data from or write data to any of its hard disk, the RAID group needs to be spun up first. You can spin up a spun-down RAID group through either of the following methods:  Automatic I/O spin-up For a spun-down RAID group, if new I/O applications are sent to any hard disk in the RAID group, the applications queue for the storage system to spin up all the hard disks in the RAID group, and after that, the applications can be sent. There is a restart latency of 10 to 15 seconds during the hard disk is changed from spun down to running.   Control command spin-up As the automatic I/O spin-up method delays new I/O processing for 10 to 15 seconds, it is not suitable for applications that allow little latency.\nFor latency-sensitive applications, use control commands to immediately spin up the RAID group.     Backup and Archiving Applications  WhenN8500mainly stores backup data when it serves as a backup array. Generally, backup data is seldom accessed; for example, non-hot data on the backup array might not be accessed for up to a year. The disks storing such data can be set to spin down and then to spin up when data access is requested. This mechanism reduces power consumption and greatly prolongs the service life of backup disks.    Streaming Media and Video Surveillance Applications  WhenN8500is used for video-on-demand (VoD) data storage applications, VoD files that are seldom accessed can be stored in the RAID group that is set to spin down. Hot files are stored in another RAID group for a quicker response.  For video surveillance applications, monitoring data can be deleted only when it is stored for a certain period and outdated data can be kept as required. If outdated data is seldom accessed, the related RAID group can be set to spin down to save power and protect the disks.     Special Industries and Scenarios  For example, the storage unit can be used in the health sector's picture archiving and communication systems (PACS). PACS mainly stores high volumes of medical images and videos of operations. Data is saved in the storage system for archiving and recording purposes, case studies and statistics, follow-up consultations, and appraisals.\nLUN Bad Sector Tag is a technology used to tag the uncorrectable bad sector. By examining the sector fault, it reduces the granularity of the damaged area to sector level, thus avoiding service interruption and data loss caused by the fault of the disk. This feature can avoid disk fault or RAID group fault during the reconstruction after the RAID group is degraded and prevent service interruption and data loss. By using the LUN bad sector tag feature, the granularity of the damaged area is reduced and the service reliability is enhanced. You do not need to purchase an extra license for LUN bad sector tag. If uncorrectable bad track is examined during the reconstruction or ordinary I/O operation when the RAID group is degraded, the LUN bad sector tag feature tags the bad sectors positioned in the bad track. When the host read data in this area afterward, it returns Medium error to the host. When the host writes data to this area, it triggers bad track repair which is performed on this area and nearby position. This document takes RAID 5 as an example. As shown inFigure 1, under the condition that a member disk is faulty, the RAID group loses redundancy function, and the I/O requests in degraded mode or reconstruction is performed. In this process, when an uncorrectable track on another member disk is examined during I/O operation, if disk fault is reported, the entire RAID group gets faulty and the data on the entire RAID group is lost.\n HyperThin allows users to develop capacity plans specific to each host and allocate virtual storage space to the host as planned.   Growing requirements for large amounts of storage space Adding storage devices for capacity scaling requires a service interruption and cannot ensure service continuity.   Reduced storage performance Explosive data growth causes different applications to fiercely compete for storage resources, impairing the overall system performance.     Forward-looking storage allocation The allocated storage space must fulfill both the current and future storage requirements. Allocating a relatively large storage space is recommended for ensured service continuity.   Appropriate storage space allocation Appropriate storage space allocation can avoid overusing or underusing storage resources and control upfront construction costs and operation and maintenance costs.    HyperThin, an innovative storage space allocation mechanism, helps increase storage utilization, reduce upfront investments, postpone capacity scaling and update, and lower overall costs by allocating and consolidating physical storage space on demand. Additionally, HyperThin supports effortless capacity scaling to adapt to growing business requirements. Capacity scaling is transparent to upper-layer applications and imposes no impact on ongoing services   Ease-of-use HyperThin simplifies data layout by using the default wide striping mechanism that delivers optimal performance and requires less planning and manual intervention than the conventional allocation mode. Using HyperThin to add additional capacity to the application server is much easier than using the conventional allocation mode.   Superior storage utilization HyperThin allows multiple applications to share one storage pool.\nThe physical storage space is allocated only when there are I/Os requests, which greatly increases storage utilization.        HyperThin is a license-enabled feature and compatible with other features supported by the storage system. A license file is required for HyperThin. The license for HyperThin has been enabled and not expired on the Integrated Storage Manager (ISM). You can contact Huawei Technologies Co., Ltd. for learning about or purchasing the license for HyperThin. Product Name Version OceanStor S2600T/S5500T/S5600T/S5800T/S6800T V100R005 HyperThin implements data backup and protection by partnering other features such as snapshot, LUN copy, remote replication, and split mirror.   HyperThin uses the capacity-on-write technology to implement on-demand storage allocation.\nThis section describes the concepts and I/O processing procedures related to HyperThin. HyperThin uses the on-demand storage allocation mechanism to adapt to increasing storage requirements and facilitate storage space maintenance. Storage space allocation is a process of allocating storage resources to meet different application demands on capacity and performance.  Conventional storage space allocation   HyperThin-based allocation   Allocation Mode Management Mode Application Scenario Conventional storage space allocation  The storage space identified by an application server equals the actually allocated storage space.  There is the storage space that has been allocated but not used.  Capacity scaling is implemented by adding disks.   Performance-intensive applications such as ms-based applications  Performance-predictable applications  Applications that require data to be precisely located on the physical disks and logical data objects  Applications that require physical data separation  HyperThin-based allocation  The storage space identified by an application server is larger than the actually allocated storage space.  The storage space is allocated as required.  When an I/O request comes, the system automatically allocates storage space to meet the storage requirement.   Applications that have moderate performance requirements  Applications that require ease-of-use and ease-of-management  Applications that require optimal storage utilization  Applications that require lowered energy consumption and costs  Applications whose storage storage consumption is unpredictable.  The following list some critical concepts associated with HyperThin.  Thin pool   Thin LUN  For the application server, common LUNs and thin LUNs are the same.  For storage systems, the capacities of common LUNs are the same as the capacities allocated to the application server, whereas the capacities of thin LUNs are only virtually presented to the application server, and the actual physical capacities are allocated through the capacity-on-write technology only when actual I/O requests are issued from the application server to the thin LUN.    Mapping table A mapping table is used to record where the date on a thin LUN is actually stored in a thin pool.   Capacity-on-write Capacity-on-write is a technology that enables physical storage space to be allocated when the application server is accessing a thin LUN.\nThe minimum allocation unit is a 32 KB of chunk. When the thin LUN is insufficient, the system will apply to the thin pool for additional storage space by zone.   Redirect  Read direct-on-time: A read request triggers a query in the mapping table. If physical space is already allocated to meet this request, data is directly read from the corresponding location in the thin pool. If physical space is not allocated, a string of zeros is returned.  Write direct-on-time: A write request triggers a query in the mapping table. If physical space is already allocated to meet this request, data is directly written to the corresponding location in the thin pool. If physical space is not allocated, allocate space for the request and then write data to the allocated space.     Procedure for reading a thin pool   An application server sends a read request to a storage system.  The storage system queries a mapping table.  Either of the following operations is performed according to the query result:  If the storage system finds that there is physical space allocated for the desired data in the thin pool, it will read the data and return an acknowledgement to the application server.\nFor example, when an application server sends a read request to a storage system for the data in location 1 on a thin LUN, the storage system will pinpoint the data's actual location (location a for example) in a thin pool according to the mapping table and return an acknowledgement to the application server.  If the storage system finds that there is no physical space allocated for the desired data, it will return a string of zeros to the application server.\n This section describes the definition, purposes, and benefits of the DST feature.  DST is a technology that automatically migrates data among multiple storage tiers based on data access frequency. The frequently accessed data is dynamically migrated to high-performance storage tiers, and infrequently accessed data is dynamically migrated to low-performance storage tiers. This technology optimizes the overall storage performance for applications.   Optimizing storage performance Infrequently accessed data is automatically migrated to a low-speed storage array to release space of a high-speed storage array. This ensures fast access to hotspot data, improving the system's storage performance.   Reducing the total cost of storage Infrequently accessed data is stored on a storage array of a low cost and low performance, leveraging cost advantages of storage arrays that offer different performance levels.   Enhancing data availability Data is stored on storage arrays that offer different performance levels. The system gives priority to data accesses that require high I/O performance.\nThis improves the storage system's data access mode, accelerates the process of saving data, and enhances availability of hotspot data.   Implementing automatic data migration Based on factors such as data access frequency and data retention period, a user can determine the optimal storage policy to control data migration rules, eliminating costly and complex manual management.      Table 1describes the benefits provided by the DST feature for customers.       Benefits of the DST feature  Item Benefit Data access mode Data is stored on storage arrays that offer different performance levels. The system gives priority to data accesses that require high I/O performance, improving the storage system's data access mode. Load balancing Frequently accessed data and infrequently accessed data are stored on storage arrays that offer different performance levels, preventing system resource contention caused by concurrent data accesses. Storage cost  Instead of storing all data on a high-performance storage array, non-hotspot data is stored on a low-cost storage array, leveraging cost advantages of storage arrays that offer different performance levels.  Disks of low-speed storage array that remain idle for a long time can be spun down. When there are new I/O requests, the system wakes up the disks in the spin-down state.\nIn this way, the system power consumption is reduced and the disk service life is prolonged, cutting investments.  Automatic migration Based on factors such as data access frequency and data retention period, a user can determine the optimal storage policy to control data migration rules.     This section describes the specifications of the DST feature. Table 1describes the specifications of the DST feature.      Specifications of the DST feature  Parameter Description Maximum file system capacity 255 TB Maximum number of files  It is recommended that a maximum of 300,000,000 files be created per file system.  It is recommended that a maximum of 100,000 files be created per directory.  Maximum number of LUNs supported by a storage unit  S2600T: 2048  S5500T/S5600T/S5800T: 4096  Maximum number of tiers 2 a: LUN is short for Logical Unit Number.         This section describes how to obtain the DST feature. To use the DST feature, you need to purchase a license separately. Product Applicable Version OceanStor S2600T/S5500T/S5600T/S5800T V100R005         This section describes the implementation principle of the DST feature.   Virtual storage volume Virtual storage volumes are virtual disks that represent the range of addressable disk blocks used by a file system.\nA collection of volumes is called a volume set.   Single-volume file system A single-volume file system is a file system created on a separate virtual storage volume.   Multi-volume file system On the storage system, a multi-volume file system is a file system that occupies two or more virtual storage volumes, that is, a volume set. A multi-volume file system provides a single namespace that makes volumes transparent to applications and users.   Placement level To facilitate management and control of the storage location of a single file, each volume of the file system on the storage system has a unique volume label, namely, placement level. A placement level is a DST property of a specific volume in a volume set of the multi-volume file system. The property represents a storage tier, namely, tier-1 storage or tier-2 storage.\nOn the storage system, you can specify the initial storage tier to determine whether data written to the file system is by default stored on tier-1 storage or tier-2 storage.  As shown inFigure 1, a volume set of a multi-volume file system contains two placement levels, and , where represents tier-1 storage created on a high-speed disk array (such as SSDs, Fibre Channel disks, and SAS disks) for storing hotspot data, and represents tier-2 storage typically created on a low-performance disk array (such as SATA disks) for storing non-hotspot data.    DST policy A DST policy includes a file placement policy and a DST schedule.   A file placement policy is a file placement rule defined by a user on the storage system. It determines the condition of file migration and placement. Based on a file placement policy, files can be stored on and migrated between any volumes of a volume set. A file placement policy specifies the initial storage position of a file and defines a file migration rule based on the access time limit, average access frequency, and access period.  A DST schedule is defined by a user for carrying out a file placement policy automatically.    Access time limit A file's access time limit is the interval between the time when a file placement policy is forcibly carried out and the time when an application program accesses the file last time.\nThe access time limit is expressed in days.   Access period A file's access period is the interval at which the file is accessed by an application program. The access period is expressed in days.   Average access frequency A file's average access frequency is the frequency at which a file is accessed within the specified period. On the storage system, the average access frequency is the number of file read or write requests handled within the specified period divided by the number of specified period.   File change log A file change log (FCL) records information about the changes made to a file, such as creating, deleting, and extending a file and the amount of a file's I/O activities (number of bytes being read or written and number of I/O reads or writes).    The working principle of DST is based on access to partial data.\nInfrequently accessed data is automatically migrated to a low-level storage tier so that high-cost storage space is released for frequently accessed data, achieving better cost-effectiveness.  As indicated by 1 inFigure 2, after tier-2 storage is configured for tier-1 storage of the storage system, the single-volume file system is automatically upgraded to a multi-volume file system that includes tier-1 storage (typically created on a high-performance storage array equipped with SSDs, Fibre Channel disks, or/and SAS disks) and tier-2 storage (typically created on a low-performance storage array equipped with SATA disks).  When a client accesses a shared file on the storage system through a file access protocol such as NFS and CIFS, the FCL on the storage system automatically records the amount of I/O activities. As indicated by 2 inFigure 2, when the file placement policy is carried out manually or automatically, the storage system migrates data that is not accessed for a long time to low-performance tier-2 storage according to the file placement policy.  As indicated by 3 inFigure 2, when data on tier-2 storage becomes hotspot data again, the storage system migrates the data to tier-1 storage according to the DST policy.  Data migrations are transparent to applications and users. That is, regardless of whether data is migrated to tier-1 storage or tier-2 storage, all files are a part of the same namespace. These files are accessed and processed as if they occupy a single volume without affecting users' viewing the directory structure of the source file system.\nIn addition, there is not function or latency effect. For example, when an application program accesses a migrated file, the latency in accessing the first byte does not occur.       http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/DST/figure/dstfea/os_dstfea_prcp_fig02.png        A single-volume file system is upgraded to a multi-volume file system. The file system created on the storage system is a single-volume file system. As shown inFigure 3, after tier-2 storage is created for the file system, the storage system automatically creates a volume set, and the original single-volume file system becomes a part of the volume set and its placement level is . Data disks allocated to the volume set become a newly added part of the volume set and the placement level of this part is .     http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/DST/figure/dstfea/os_dstfea_prcp_fig03.png   A file placement policy is set. When the storage system has the DST feature enabled, the storage position of a file changes depending on its access frequency.\nOn a storage system, data on tier-1 storage can be migrated to tier-2 storage only when the data's access time limit exceeds the specified access time limit; data on tier-2 storage can be migrated to tier-1 storage only when the data's access frequency exceeds the specified average access frequency within the specified access period.   Based on the access time limit, non-hotspot files are migrated from tier-1 storage to tier-2 storage. A file's access time limit is the interval between the time when a file placement policy is forcibly carried out and the time when an application program accesses the file last time. The access time limit is expressed in days. In the example shown inFigure 3, the specified access time limit is three days. If a file on tier-1 storage of the file system is never accessed within three days, the file is migrated to tier-2 storage after the file placement policy is carried out.   Based on the average access frequency and access period, hotspot files are migrated from tier-2 storage to tier-1 storage. A file's access frequency is the number of file read or write requests handled within the specified period divided by the number of specified period. In the example shown inFigure 3, the specified access frequency is five within two days.\nCIFS is a protocol used for sharing network files. CIFS allows Windows clients on the Internet and intranet to access shared files and other resources. The CIFS share is mainly applicable to the file sharing. Server Message Block (SMB) is a protocol used for network file access and CIFS is a public version of SMB. The SMB protocol allows a local PC to access files and request services on PCs over the local area network (LAN). With the continuous expansion of enterprises, more and more users need to access the share service in enterprises. Restricted by the server where shared files reside, the access speed decreases and system response slows down when a large number of users access shared files. Therefore, improving the performance of accessing shared files becomes an urgent need for enterprises. The CIFS feature allows Windows clients to identify and access shared resources provided by the S2600T/S5500T/S5600T/S5800T storage system. With CIFS, clients can quickly read, write, and create files in the storage system as on local PCs. The storage system delivers high performance, addressing the problems of decreased access speed and slow response.  High concurrency CIFS supports the file sharing and file locking mechanisms, allowing multiple clients to access a file. Multiple clients can access a file at the same time, but only one client is allowed to update the file each time.   High performance Access requests sent by a client for a shared file are cached locally but not delivered to the S2600T/S5500T/S5600T/S5800T storage system.\nWhen the client sends access requests for shared files again, the system directly reads shared files in the cache, improving access performance.   Data integrity CIFS provides the cache, pre-read, and write back functions to ensure data integrity. Access requests sent by a client for a shared file are cached locally but not delivered to the S2600T/S5500T/S5600T/S5800T storage system. If other clients want to access the shared file, the cached data is written to the S2600T/S5500T/S5600T/S5800T storage system. Only one copy file is activated each time to prevent data conflicts.   Robust security CIFS supports anonymous file transfer and share access authentication.\nThe authentication management function controls users' access permissions, ensuring data confidentiality and security.   Wide application Any client that supports the CIFS protocol can access the CIFS share space.   Unified coding standard. CIFS supports various types of character sets, applicable to different language systems.    This chapter describes the CIFS share specifications.  Table 1lists specifications of the CIFS share based on one S2600T/S5500T/S5600T/S5800T storage system.     CIFS share specifications Item Specifications Number of file systems shared in CIFS Normal mode  60 Number of file systems shared in CIFS Homedir mode  16 Number of links in the CIFS Homedir share  3000  Number of active links in the CIFS Homedir share   800  a: The number of local users and domain users in one storage system cannot exceed 3000, and the number of user groups cannot exceed 3000. b: The number of active links refers to the number of online users. c: A storage system supports a maximum of 800 active links.     This section describes the CIFS share availability from the aspects of the license support, version support, network requirements, feature dependency, and system performance. A license is required to use the CIFS share feature. Product Name Product Version OceanStor S2600T/S5500T/S5600T/S5800T Storage System V100R005 The CIFS share feature supports the Internet Protocol version 4 (IPv4), but does not support Internet Protocol version 6 (IPv6).      Configuration items for the domain controller  Item Description AD  Storing information related to network objects, the AD enables the administrator and users to easily find and use the information. Kerberos or NTLM authentication Authenticates users to protect files in the system. DNS server Storing host names and IP addresses on the network, the DNS server converts host names to corresponding IP addresses. NTP server Synchronizes the time of devices on the network. a: Active Directory (AD) b: NT LAN Manager (NTLM) c: Domain Name Server (DNS) d: Network Time Protocol (NTP)  Table 2describes the dependency between the CIFS share feature and other features.      Dependency between the CIFS share feature and other features  Feature Dependency NFS /FTP /HTTP share The S2600T/S5500T/S5600T/S5800T storage system can share file systems using multiple protocols.\nHowever, clients cannot write one file in a file system at the same time if the file system is shared through multiple protocols. If multiple protocols are used to share file systems, you are advised to configure read-write share for only one protocol and read-only share for other protocols. Archiving After a CIFS share is created, the data archiving feature of the file system is unavailable. File system snapshot Before accessing the file system snapshot, clients must create the file share for it. a: Network File Server (NFS) b: File Transfer Protocol (FTP) c: Hypertext Transfer Protocol (HTTP)   The system supports file system sharing through CIFS, NFS, FTP and HTTP. When the file system is accessed by clients using different protocols, the overall system performance is slightly degraded.  Different file systems provide services through the CIFS share or NFS share. If one share feature is in use, the performance of another share feature deteriorates.  The system performance deteriorates if clients access a large amount of small files in the CIFS share after the full_acl permission is enabled, because the S2600T/S5500T/S5600T/S5800T storage system spends a large amount of time in authentication. Therefore, you are advised not to enable the full_acl permission in the CIFS Normal share.  The system performance deteriorates if file system snapshots are created after the CIFS Normal share has been created for a file system.     The CIFS share is mainly applicable to the file sharing by Windows clients.\nThe CIFS share is classified into CIFS Normal share and CIFS Homedir share. This section describes the CIFS Normal share and CIFS Homedir share in a non-domain and an AD domain environment. With the continuous expansion of enterprises, more and more data needs to be shared in enterprises. Therefore, enterprises require large shared space for users to store shared data and simplified shared space management. The S2600T/S5500T/S5600T/S5800T storage system shares the file system to all the users of enterprises in CIFS Normal mode. The shared file system appears as a directory. All the users can access the shared directory. Besides, permission management based on local group allows enterprises to control users' permissions for the shared directory. Meanwhile, the storage system allows enterprises to set shared space quotas for departments and users. User quota management ensures that all the users can perform read and write operations in the file system by preventing some users from occupying too much shared space.       http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/CIFS_feagud/figure/cifs_fea/os_cifsfea_appscen_fig01.png With the continuous expansion of enterprises, the number of users increases. The need for private storage space emerges. The private space of a user cannot be viewed or accessed by other users. The S2600T/S5500T/S5600T/S5800T storage system shares the file system to a user in CIFS Homedir mode. The shared file system appears as a directory. The directory name is the same as the user name. This user can only access the shared directory of his/her own.\nMeanwhile, the storage system allows enterprises to set shared space quotas for departments and users. User quota management ensures that all the users can access the file system and files by preventing some users from occupying too much shared space.       http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/CIFS_feagud/figure/cifs_fea/os_cifsfea_appscen_fig02.png With the expansion of LAN and wide area network (WAN), many enterprises use the AD domain to manage networks on Windows. The AD domain make network management simple and flexible. The S2600T/S5500T/S5600T/S5800T storage system can be added to an AD domain as a client, namely, it can be seamlessly integrated with the AD domain. The AD domain controller saves information about all the users and groups in the domain. All the users in the AD domain can access the CIFS Normal share provided by the storage system. Before the access, they need to be authenticated by the AD domain controller. The AD domain administrator can implement file-specific permission management. Different users have different permissions for each shared folder.       http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/CIFS_feagud/figure/cifs_fea/os_cifsfea_appscen_fig03.png With the expansion of LAN and WAN, many enterprises use the AD domain to manage networks on Windows. The AD domain make network management simple and flexible. The S2600T/S5500T/S5600T/S5800T storage system can be added to an AD domain as a client, namely, it can be seamlessly integrated with the AD domain. The AD domain controller saves information about all the users and groups in the domain.\n OceanStor S2600T/S5500T/S5600T/S5800T storage system supports mixed-protocol data access. This document focuses on NFS-based data access. NFS is a mainstream share protocol originally developed by Sun Microsystems. This protocol provides a client or server distributed file service, allowing file systems to be shared across heterogeneous platforms.  As the computer industry rapidly evolves, the large-capacity storage system remains expensive although the CPU is much cheaper than before. The emergence of NFS helps eliminate this conflict by enabling data sharing among clients while maintaining the optimal processor performance.  NFS works in a client-server model. The server program presents the client program with the file system share service by using NFS to enable clients running different operating systems to share each other's files.  OceanStor S2600T/S5500T/S5600T/S5800T storage system utilizes NFS to present users with a flexible and easy-to-use configuration and application environment. Serving as an NFS server, the storage system allows any client that uses NFS version 2 or 3 to access the shared file system.\nAfter data is stored onto the storage system, a user on a client computer can access files over a network in a manner similar to how local storage is accessed, which significantly saves local storage space.    This section introduces the documents that describe different versions of NFS. The documents are listed as follows: Document Description RFC1094   RFC1813        The prerequisites for the NFS feature must be verified from the perspectives of license requirements, applicable versions, network requirements, feature dependency, and system impact.  The NFS feature requires license permission; therefore, you need to purchase a license to use this feature.  Product Name Version OceanStor S2600T/S5500T/S5600T/S5800T V100R005  The NFS feature only supports the use of Internal Protocol Version 4 (IPv4).  Feature Dependency File system snapshot Creating a CIFS share or NFS share is required prior to accessing a file system snapshot. CIFS/FTP /HTTP  In a multi-protocol share environment, to avoid overwriting or losing data and ensure consistency of shared data, it is inadvisable to perform multiple writes onto the same file of a file system. If multiple protocols are used to share file systems, you are advised to configure read-write share for only one protocol and read-only share for other protocols. a: Common Internet File System b: File Transfer Protocol c: Hypertext Transfer Protocol  OceanStor S2600T/S5500T/S5600T/S5800T storage system supports multiple share modes such as CIFS, NFS, FTP, and HTTP.\nThe simultaneous access to the same file system using multiple protocols compromises the overall system performance.       Over the network, NFS enables clients running different operating systems to share each other's files in different application scenarios such as the non-domain environment, Lightweight Directory Access Protocol (LDAP) domain environment, and NIS domain environment. Serving as a server, the clustered NAS storage system makes an NFS shared file system accessible to a client by mounting the NFS shared file system to the local computer. This allows the user on the client to access the NFS shared file system on the server so rapidly as accessing local files. Only the clients that have been specified on the server are allowed to access the NFS shared file system, as shown inFigure 1.       http://localhost:7890/pages/3118G2D8/07/3118G2D8/07/resources/dita/nas/NFS_feagud/figure/nfs_feagud/os_nfsfea_appscen_fig01.png The current authentication mode requires users to enter their user names and passwords for each specific application. As applications proliferate and become complicated, a user has to set different user names and passwords for different applications. This creates significant obstacles for user management. Directory services provided by Lightweight Directory Access Protocol (LDAP) are designed to remove these obstacles. LDAP is an open, flexible network protocol. Benefiting from its user-friendly, secure, and powerful information query feature as well as its cross-platform data access capability, LDAP is becoming a major tool for network management. The principal purpose of LDAP-based authentication is to set up a directory-oriented user authentication system, specifically an LDAP domain.\nSmartCache leverages the advantage of solid state disks (SSDs) in their fast access to random small I/Os. It combines one or more SSDs into a high-speed cache pool to store hotspot random small I/Os in the storage system. In this way, the read performance and access efficiency of random small I/Os are improved. The development of the multi-core CPU technology greatly improves the CPU processing capability, which is much stronger than the processing capability of hard disks. The performance gap between front-end application servers and back-end storage systems is widened. The response time of storage systems becomes the bottleneck of service processing. Therefore, the performance of a service system cannot be optimized only by improving application servers' CPU processing capability. The early workaround to shorten the previous performance gap was to add expensive cache resources. However, as the capacity of back-end storage systems constantly increases, this workaround is not effective any more. SSDs deliver a short response time, and their capacity is much larger than ordinary cache resources. Because of these highlights, SSDs gradually become a mainstream data cache medium for storage systems. The cache resources made of SSDs deliver a shorter response time and a higher data access efficiency. SmartCache is a new caching technology that uses SSDs as cache resources. The storage system makes statistics on the access frequencies of data blocks, and promotes the hotspot read data that are frequently accessed random small I/Os from traditional hard disk drives (HDDs) to the high-speed cache pool of SSDs.\nThe data read speed of SSDs is much faster than that of HDDs, so SmartCache considerably shrinks the system response time and improves the system performance. SmartCache does not interrupt existing services or compromise data reliability. SmartCache is a licensed feature. It is compatible with all the other features in the storage system. A license file defines the permission to use SmartCache. Before using SmartCache, log in to the Integrated Storage Manager (ISM) to confirm that the activated license already permits SmartCache, and the permission has not expired. If you want to purchase a license file of SmartCache, contact Huawei Technologies Co., Ltd. Product Applicable Version S2600T/S5500T/S5600T/S5800T/S6800T V100R005 SmartCache improves the read performance of random small I/Os on a per-LUN basis. Therefore, the LUN capacity must meet the requirement for using SmartCache. sts the upper limits on the total capacity of LUNs under which SmartCache can be enabled. Table 1 Upper limits on the total capacity of LUNs Product Total Capacity of LUNs (TB) S2600T 16 S5500T 32 S5600T 32 S5800T 64 S6800T 64 SmartCache also imposes an upper limit on the total capacity of SSDs in the Smart the upper limits on the total capacity of the SmartCache pools. Table 2 Upper limits on the total capacity of the SmartCache pools Product Total Capacity of the SmartCache Pools (GB) S2600T 1200 S5500T 1200 S5600T 2400 S5800T 3600 S6800T 4800 This section introduces SmartCache implementation principle and data read processes.\nPerformance statistics The storage system arranges data blocks on LUNs based on their sizes, and monitors the data blocks within a specific period in real time. In this way, the storage system finds out the access frequency to each data block within that period. Performance analysis The storage system ranks data blocks based on their access frequencies, and regards the data blocks with high access frequencies as hotspot data. Data copy The source data of hotspot data is still stored on hard disks in the storage system. The data stored in the SmartCache pools is only backup copies. Therefore, even if one solid state disk (SSD) in a SmartCache pool is faulty, the data reliability is not compromised. In the next statistics period, the storage system starts another statistics on the access frequencies to data blocks, and updates the rank of data blocks. This ensures that the data in the SmartCache pools is constantly hot. SmartCache uses SSDs to enhance ordinary cache resources and store hotspot data, greatly improving the read performance of the storage system. compare the data read processes before and after SmartCache is enabled. Figure 2 After SmartCache is enabled, hot data is mixed with cold data on hard disk drives (HDDs). After the storage system receives a read request from an application server, it forwards the request to HDDs for processing, and returns the data read from HDDs to the application server. HDDs require seek time to read data, which adversely affects the data read performance.\nIf you forget the user name or password of the super administrator account, contact Huawei technical engineers. If you enter incorrect passwords a specified number of times (equal to the value specified in wrong times on the Password Policy Management page), the account is automatically locked for the period of time specified in Lock Time . You are advised to change the default login password immediately after you have logged in to the storage system for the first time and periodically change your login password in the future. This reduces the password leakage risks. For details about how to change the password, see Modifying Login Password . Click Log In .  The DeviceManager main window is displayed.   Logging In to the DeviceManager (Through Web)  The communication between the maintenance terminal and the SVP is normal. Before logging in to the DeviceManager as a Lightweight Directory Access Protocol (LDAP) domain user, first configure the LDAP domain server, and then configure LDAP server parameters on the storage device accordingly, at last create an LDAP domain user.  This document exemplifies how to log in to the DeviceManager in Windows using Internet Explorer. For other operating systems, revise the login procedure accordingly.  Open Internet Explorer on the maintenance terminal. In the address box, type https://xxx.xxx.xxx.xxx:8088 or https://xxx.xxx.xxx.xxx:443 and press Enter , indicates the IP address of the SVP management network port.  Your web browser may display that the website has a security certificate error.\nTo make their data storage systems run efficiently as possible, storage managers need to think carefully about how they can keep from transferring their old problems to newer, more expensive systems, according to Jon Toigo, CEO and managing principal of Toigo Partners International. During his keynote on cutting storage costs at Storage Decisions in New York City earlier this month, Toigo said storage managers need to enforce storage resource management (SRM) basics to create more efficient data storage. \"Select and deploy an SRM package today,\" he told the hundreds of storage pros who attended his presentation. According to Toigo, users should choose hardware-agnostic storage services to help address what he described as one of the biggest challenges facing storage pros today. \"We have set up an infrastructure characterized by isolated storage islands policed by rogue processes,\" Toigo said. \"Storage is fast becoming the most expensive component of IT hardware spending.\" Data growth rates make current storage practices unsustainable, he explained. By now, most IT pros are tired of hearing the numbers associated with data growth, but most are acutely aware of how fast storage capacity is growing at their companies. One of the biggest problems, Toigo said, is that companies are buying too much tier-one storage when their overall tiering strategy should involve more tier-three data and archived data. \"I think archiving is really the untold story,\" he said. \"Archive is what makes tiering purposeful.\" Tiering remains a \"partial plan,\" while archiving adds a data-management dimension.\nArchiving can include tape, Toigo added, who advised storage pros to consider the newest tape technology as part of their overall data storage efficiency and cost-cutting measures. Toigo also offered several tips on storage efficiency:  . In Toigo's words, \"previously loved\" gear might provide an affordable option to shiny new storage. He recommended the Association of Service and Computer Dealers International and the North American Association of Telecommunications Dealers to users trying to save money on new investments. However, Toigo acknowledged, the \"real problem is that these are no longer that [energy] efficient,\" as they were often built during the pre-green era of IT computing.  Toigo reminded users that most services do not need to be implemented near the disk, and are hosted on array controllers only because that is where the vendors want them to be. \"Storage is a resource delivered through a combination of hardware and plumbing augmented by software services,\" he said, encouraging users to think about storage outside of traditional arrays. \"The hardware is the commodity stuff. The software is typically a mechanism for adding value -- in the form of specialized services to the storage hardware -- and for adding digits to the price sticker,\" Toigo explained. But virtualizing your existing block storage can be a good first step toward cutting costs.  \"I know that everyone wants their new application to shine,\" Toigo said. But not every application requires performance storage. \"Why are we doing tier one for everything?\" he asked.\n(y/n) y command operatessuccessfully.     Next is the command :    In the above screen I created 3different LUNs (TESTLUN005,TESTLUN006,TESTLUN007) which are assigned to RG 3 ,controlled by controller A , with the size of 2 GB and with the stripe depth of 256 KB.To checkthat the LUN are correctly displayed you can check with the command      Because after creatingthe LUNs , your job administrating the Storage is far from over you must keepin mind that the LUNs must be managed.So if you manage a storage system, ormaybe more, for a big company , you will not have just one or two LUNs tomanage but dozens.So just as a little trick I will show you how to managerelated LUNs all at once.For this thing we must group LUNs in resource pools. Aresource pool is used to store temporary data and comprised of one or moreLUNs. The capacity of a resource pool equals capacities of all LUNs in theresource pool. Resource LUNs can be dynamically added to the resource pool toexpand its total capacity. Resource pools are required when you use manyfunctions such as snapshot, LUN copy, and remote replication.\nI know you are interested in the technical issues, but if you can address issues with managementand create a roadmap, it would alleviate at least half of the dependency on the Oracle Hyperionpartner. I would break things down to three areas to consider: technical, management androadmap. You need to be aware of a handful to things from a technical standpoint, especially for a largeenterprise solution. Usually large enterprise solutions are complex. The complexity resides on twofronts: multipleintegration points and a larger database with complex calculations. Multiple integration points are designed to merge data and metadata from multiple sources. Toolssuch as Hyperion Data Relationship Management (DRM) and Oracle Data Integrator (ODI) are used tointegrate metadata and data into Hyperion Planning and Essbase applications. Relational databasescan also be used to store the data and metadata from DRM, therefore making the integration designmore elaborate. You need a lot of coordination between the DRM, ODI and DBA developers to ensuresuccess. Another metadata and data integration tool to use is Oracle Enterprise PerformanceManagement Architect (EPMA), which does not require levels of coordination as high as DRM. But EPMAhas had issues in the past with deploying applications in Planning and Essbase. Using EPMA mayrequire more patience and investigation on application deployment. The newest release of EPMA may be more stable than previous releases. Database size and calculations are other potential technical issues for a large enterprise.Usually large companies have many dimensions in their database because they want several ways totrack their data. This helps establish their metadata.\nNext, large companies sometimes require moreelaborate calculations. But developing an elaborate database can hamper system performance. Runninga series of stress tests helps with server and databasetuning, as well as hardware sizing. Other technical considerations for a Hyperion project would be backup and disaster recovery.Large enterprises usually already have an established requirement for backups and DR. If therequirement exists at the client, then use this methodology. However, the Oracle partner cansuggest a schedule for backups and a DRstrategy. Some management issues reside around whether internal resources have enough technical knowledgefor system maintenance. If the client is implementing just a planning application, then a resourcewill minimally need experience developing Essbase calc scripts or business rules and planning application development. If DRM and ODI are used, you'll need a full-time resource to maintain both applications. So thecomplexity of the project dictates the resources needed. This issue is often overlooked and isusually why there is such a large dependency on Oracle partners. If a company puts in place aninternal team while development is ongoing, it can learn from Oracle partners and participate indevelopment in a controlled manner. This will help the team establish credibility, gain experiencewith the tools, and foster an easy transition from an Oracle partner to internal support teams. When it comes to managing the business process, many clients depend on Oracle partners to designthe process of the planning cycle. Although this can be done, it's better to have the internal teamdrive the discussion.\nAs Dave Wittwer climbed the ranks at TDS Telecommunications from internal auditor to CFO to COOand now CEO, he learned to appreciate the competitive advantage that IT integration with thebusiness can deliver. In SearchCIO's Business POV piece on ITas competitive advantage, Features Writer Karen Goulart recapped Wittwer's advice from a talkat FusionCEO-CIO Symposium in Madison, Wis., highlighting his views on how IT adds value to thebusiness. To follow up, Goulart asked readers, \"Is IT well-versed in your company's business, and does itprovide competitiveadvantage?\" Our respondents were split 50-50, with half saying, \"Yes, we're a team\" and theother half saying, \"No, we're siloed.\" So, how do you do business-IT integration correctly? Ourreaders shared some of their wisdom in the comments section:  \"Yes, our topmanagement does involve IT in all the strategic decisions so that IT can identify the scope ofimprovement and contribution right from the thought-inception phase.\"  \"Those days are history when IT was a business background player. Now IT walks with the leadteam and, if not, the business lives in dark ages. Managements realizing these facts are enjoyingthe fruit of automation.\"  \"My ITS department fills many roles, and yet must be flexible enough to adjust to a simple taskin providing one-on-one customer service. My motto is, 'Do it right with the right equipment thefirst time, and you do not have to do it again.'\nFrom that vantage point, you can then look to growyour business with advanced changes that reduce cost and keep you competitive.\"  The main objective in IT integration with the business is aligning IT with organizationalgoals and objectives in order to generate value across departments:  \"It is important that we align IT with our business goals, adding value to the business.\"  \"IT is the most important [device] in our daily business transaction. So, I can say IT is asource of our business deal.\"  \"We focus our technology investments specifically on productivity and value generation forindividuals, teams and business units. One example is offering individuals the freedom to choosetheir preferred computationalenvironment and supporting their choices. Others are constantly asking questions about how wecan help teams and organizations to do their work more productively.\"  Not everyone has hopped on the business-IT integration bandwagon.Some readers shared their organizations' siloed approach to IT departments:  \"There is no front-to-back integration of our processes.\"  \"Many times IT is viewed as an outsider.\"  Simply put, the traditional outsider view of IT prevents some organizations from properlyintegrating IT into business processes. Readers expanded on this, pointing out problems such as themisdefined roleof IT in business matters:  'The main problem in the organization, and most organizations I've seen, is they consider IT asjust a tool for something. It is a headache for many of them. ... But it is important to understandIT as a part of the main strategy and as way of living in business.\nWith unemployment at all-time high, salary cuts becoming commonplace and seemingly no end tolayoffs in sight, IT jobs are just as vulnerable as other positions in this recession. Here is someadvice from John A. Challenger, CEO of outplacement firm Challenger, Gray & Christmas Inc., onhow to make yourself indispensable and increase the chances of keeping your IT job.  Get and maintain a firm grasp of the company's business, industry, consumers and vendors. Youneed to know where the company is going and where it has been. (\"Maybe a proposal you're suggestinghas already been attempted.\") You need to read the periodicals that pertain to your profession andthe stuff the company leadership reads: , ,    Become the go-to person on the technical aspects of your IT job, but be flexible enough tosolve any problem you might be thrown.    Before you open your mouth about a proposal, make sure you understand the company culture. Howquickly does your company adapt to change? Does it work by committee, or are decisions made lessformally? Does it maintain high or low transparency?  \"If you are unable to convey in language that resonates with them, your value is diminished. Fromyour first conversation with your direct supervisor and every level you move your proposal up theladder, it is important that you phrase the problem and the solution in language they can relateto,\" Challenger said.\n   Value proposition is key to getting management's attention, so you need a clear statement oftangible results that corporate can expect to see from implementing your proposal. \"You want toavoid vague generalities like, 'This will save us a lot of money,'\" he said.    As you put the projects in place, let people know about them. \"One of the keys in anenvironment like this is boasting in a way that isn't claiming you can do things for your company,but telling people about what you have done. Make sure you talk about concrete results, and notjust to your boss but to people around the company.\" This is hard because \"most of us have beentaught not to toot our horn, but in this environment, you are fighting for your job even in yourown company, and need to make sure people know what you've done.\"    Be known as someone who can get things done. Bosses want to know that if they turn things overto you, they don't have to watch you closely because you are going to do it just like theywould.    But don't do this by any means possible. Driving your staff to the ground to get something donecauses low morale and turnover, which causes more problems for the company.    Identify the areas where you are really the only person in the company who can answer thespecific and technical questions regarding some aspect of your field, so that your absence wouldleave a big gap.\nThe CIO job has gotten its share of bad press, including the snide joke that the acronym standsfor . As businesses of all stripes become more information- and digital-centric,however, CIOs are increasingly being asked to use their IT smarts to generate revenue. Consider the recent experience of Larry Bonfante, CIO of the United States Tennis AssociationInc. (USTA). The USTA, the world's largest tennis association and sponsor of the U.S. Open, has accumulatedabout 25,000 hours of historical video footage from its 129-year-old marquee tournament and othertennis matches. The footage has now been digitized. In fact, snippets of it showed up in the USTA's\"It Must Be Love\" commercials for the just-finished 2010 games. A DVD of the U.S. Open'sgreatest hits just went on sale, and the organization is working with a third-party company tolicense the footage so fans can buy it by the slice online. \"We drove that,\" Bonfante said. Three years ago, the USTA's digital asset management andarchival teams were rolled together under his management. \"At that point, it was a back-end processand a back-end function. We brought it to the forefront by working with our marketing and broadcastpartners to create videos and commercials, and help drive revenue for the organization,\" hesaid.   Bonfante is not alone in his newrole as a revenue generator -- according to recruiters at top search firms and ***ysts whofollow the CIO role, the job is transcending its traditional function of automating and enablingother aspects of the business. \"That's not to say, that role is no longer really important.\nIt is,\" said headhunter ShawnBanerji, a member of the technology sector practice at executive search firm Russell ReynoldsAssociates Inc. in New York. And the \"best CIOs,\" he is quick to add, possess both strategicbusiness smarts and superb operational ability. But as more businesses become aggregators of data, the ability to monetize those assets \"is abig deal.\" Banerji said. \"The companies that get it are looking at their CIOs and saying, 'Gee, Bobor Sally, you're the one who owns this stuff. You're the business information officer. Help usfigure out how we can make money off this -- or make more money off it,'\" he said. Indeed, the digitization of business assets-- in concert with the rise of socialmedia and networking as the vehicles of choice for reaching customers -- increasingly puts ITat the heart -- or in the maelstrom -- of the business' marketing effort. \"It's to the point nowwhere in the marketing functions of certain organizations, you can't tell the difference betweenthe IT guy and the marketing guy,\" Banerji said. In the past year, Russell Reynolds has worked with a number of companies in media, entertainmentand publishing that are looking for just such IT leaders. A major educational testing companywanted a CIO who could use technology to turn its information assets into products, and extend itsfranchise to the Web and mobile devices.\nA large book publisher, scrambling to figure out how tostay afloat in a business co-opted by the Amazons of the world, ended up hiring acommunications-savvy CIO from the radio industry. Some companies aren't accustomed to looking to their CIO for ideas on driving revenue. In thosecases, as the CIO of a metropolitan newspaper on the West Coast recently told SearchCIO.com,gadgets can help. As the head of the paper's digital business content group, as well as head of IT,this CIO knows that figuring out how to use mobile platforms to deliver content to customers is animportant initiative for the paper. In an effort to drum up enthusiasm and brainstorm ideas on thebusiness side, he purchased iPads for a handful of top executives and loaded the devices with newscontent. Two months in, \"the response has been overwhelming,\" he said.   The search for the CIO as product visionary is not confined to the content industries,however. Gartner Inc. ***yst Mark McDonald, head of research for executive programs at the Stamford,Conn.-based consultancy, said his group is fielding requests from CIOs in all kinds of industriesto help them make products from their digitalassets and monetize their intellectual property. The most recent request came from the CIO at aGlobal 100 optical engineering firm; his only question was how he could sell his company's opticaldesigns to other manufacturers down the supply chain. Shawn Banerji, technology sector practice, Russell Reynolds Associates Inc. \"This is not just limited to publishing or entertainment.\nIt is starting with a fundamentalquestion: 'We have intellectual property -- how do we get more value from it? '\" McDonald said,citing Qualcomm Inc. as anearly example of a company that made intellectual property its core business. The power of socialmedia to add value to intellectual property is driving some of these efforts. But in many cases,the impetus is more basic. \"Companies, in this current economic climate are looking for everysource of revenue they can get,\" he said. Monetizing intellectual property is not confined to the company's traditional products, McDonaldsaid. CIOs at companies with large, sophisticated IT departments are looking for ways to leveragetheir IT assets, sometimes by adding scale to the already large departments, so they can selltheir product or service to other companies. Nor is the effort limited to giant companies. When USTA's Bonfante could not find a softwaresystem that could handle the myriad functions of the USTA's command center for big events, his teambuilt one. The USTA's homegrown event management system developed workflows for handling calamitiesranging from a burned-out light bulb to a suspicious package in the athletes' village. Positivefeedback from people who work at other events has led to Bonfante shopping the program around toother sports and entertainment leagues. \"We've gotten some bites,\" he said. A word of caution: Don't neglect your day job for the product business, Bonfante said. And avoidacting like you know more than you do. \"I consider myself a good marketing person for a CIO, butthat doesn't mean I am a good marketing person.\nWhen Atrion NetworkingCorp. first opened its doors in 1987, the IT services company routinely included clauses in itscustomer contracts that discourage them from hiring away its engineers. Often, there was afinancial penalty when it happened. But the company's philosophy about this has changed dramatically over time because it can sendthe wrong signals to both customers and IT engineering staff, said Tim Hebert, CEO of the Warwick,R.I.-based company.   Robby Hill, president and CEO of HillSouth Inc. \"We realized it could be bad for relations and that we have to focus more on making sureemployees didn't want to leave in the first place,\" he said. This dilemma is faced by many VARs, managed service providers (MSP) and services firms thatplace engineers on-site at a customer for extended periods of time. Having a person embedded within an account can be incredibly useful for helping identify futureproject needs or business practice opportunities. But when an engineer or technician spends moretime with a customer's team rather than his or her actual employer, it's easy for the lines ofloyalty to blur. So how are technology solution providers coping with this challenge? While there is no rightrecipe, here are some strategies for staff retention being used to good effect. While it may be tempting and good for the bottom line to make sure a service technician's timeis almost fully booked, it's beneficial to pull them into meetings back at home base, Hebertsaid.\nAtrion Networking actually has a person dedicated to maintaining an ongoing connection withservices personnel attached to long-term engagements in order to keep the human connection. Atleast once every six weeks, they are involved in either training or strategy sessions at theheadquarters office. While this was initially a culture shock -- some employees were leery of being pulled out of thefield -- this initiative not only offers them a better sense of career development, it also helpsreduce tensions if there is a situation where someone wants to leave because Atrion Networking'smanagers have a better sense of that person's career interests. The company's voluntary turnover rate (where people leave for other employment opportunities) is5% to 6%, Hebert said. One staff retention strategy used by Heartland Technology Solutions of Joplin, Mo., involves assigning teams toprojects, so that more than one person is responsible for keeping an account happy. This serves several purposes. It can ensure that a client's needs are met promptly, whileexposing more than one person to the account's ongoing needs, said Jane Cage, former chiefoperating officer and now a consultant to Heartland, which wasacquired by WesTel Systems in early January. In addition, it encourages Heartland engineers to align themselves in teams, which forges atighter connection back to the IT services firm. \"It changes the approach. Very seldom do we send people out always for the same specific work,\"Cage said.\n\"Especially in SMB accounts, we don't have people at places long enough that they becomeattached.\" Crafting managed services that are delivered remotely, something that most technology solutionproviders are already doing, can help make this more practical. Although it sounds like common sense, creating a very specific, well-managed career path for ITengineering staff can be beneficial for staff retention, said Robby Hill, president and CEO of HillSouth Inc., a 21-person firm in Florence,S.C. His company recently adopted a program that gives cash incentives to technicians to increasecertification levels. They literally receive a check every time they pass another certification.But to keep the money, the engineers have to stay with the company another six months. HillSouth also avoids assigning any of its on-staff engineers to long-term on-site technicalsupport contracts. Instead, it keeps developing a steady stream of candidates that it can refer forthese positions, recognizing that many of these assignments can wind up becoming full time, hesaid. \"The customer is simply using you as a placement service,\" Hill said. \"At the same time, inanything that we do as a solution provider, we don't want our customer to look elsewhere.\" HillSouth also actively spends time giving employees a chance to interact outside of work hours,with casino nights and lake weekends. It also encourages ongoing connections with the local ITcommunity through technology user groups, knowing that one day, some of these people may eventuallyjoin its staff. \"In my experience, the formal things don't breed loyalty.\nTodays storage managers are facing exponential data growth and the number of NAStechnologies and products offered to c*** data sprawl seem to be growing just as quickly.More and more, data storage managers are looking at the differences between traditional scale-upchoices and newer scale-out products. Heres a quick overview of the evolving NAS market. As 2012 nears, dominant players positionthemselves with aggressive acquisitions and new scale-out offerings.  EMC and NetApp, a NAS pioneer that released its first system in1993, continue to dominate the NAS market. Through the first quarter of this year, EMC had a 48.8%revenue share and NetApp was next at 30.8%, according to IDC. Scale-up vs. scale-outnetwork-attached storage  NAS technologies: Vendor landscape NASoptions: Pros and cons of scale-out and scale-up Both EMC and NetApp have proprietary NAS operating systems, but many of their compe***s --including Dell, HP and IBM -- leverage Microsoft Corp.s Windows Storage Server in traditional NAS.IBM also rebrands NetApp products as its enterprise NAS platform. Traditional NAS products tend to differentiate largely based on the built-in or add-on softwarethat vendors make available, rather than performance or minimum/maximum starting points, accordingto Arun Taneja, founder and consulting ***yst at Taneja Group in Hopkinton, Mass. People buy NetApp because NetApp has the best surrounding software: SnapMirror, SnapVault andthe list just goes on, Taneja said. That's the No. 1 distinguishing factor on the traditional NASside. Smaller vendors, such as BuffaloTechnology Inc., D-LinkCorp., DroboInc., IomegaCorp. (a wholly owned subsidiary of EMC), LaCie Group S.A., NetgearInc.\nBut the fruits of the merger didnt start to takeshape until the 2006 release of NetApp's Ontap GX and, more significantly, with the 2009 release ofits DataOntap 8 operating system. NetApp still isnt a true scale-out architecture, but Data Ontap 8.1,expected by years end, aims to integrate more of the enterprise-class features and functions fromNetApps traditional NAS with the cluster mode. HPs mid-2009 dealfor Ibrix produced its X9000 family of scale-out NAS products, and that same year, OracleCorp.s purchase of Sun Microsystems Inc. led to the ZFS OpenStorage Appliance that Sunmarketed as its 7000 series. Dells February 2010 purchase of Exanet Ltd.s assets contributed to its PowerVaultNX3500, which launched in April 2011. Dell also added scale-out NAS capabilities to its EqualLogicFS7500 and is working on integrating its scalable file system into the SAN platform acquiredfrom Compellent Technologies Inc. IBM based its ScaleOut Network Attached Storage (SONAS) product, launched last year, on its General Parallel FileSystem (GPFS) and the Scale-Out File Services (SOFS) offered through IBM Global TechnologyServices. PanasasInc. heads the list of vendors focused on HPC. Other HPC specialists include Terascala Inc. andXyratex International Inc. DataDirectNetworks Inc. also plays in the space when its arrays are bundled with the open-source Lustredistributed file system or IBMs GPFS. Additional vendors with targeted offerings include Facilis Technology Inc., which focuses onmedia and entertainment; Gridstore Inc., Reldata Inc. and Scale Computing Inc., which cater tosmall- and medium-sized enterprises (SMEs); and Nexenta Systems Inc. NexentaStor, Quantum Corp.StorNext and Gluster Inc.\nVirtual Storage Appliance that have file systems that serve as theunderlying technology for scale-out NAS systems. One of the main distinctions between scale-out NAS products is whether they tilt towardthroughput to better handle large files or IOPS for large numbers of small files. The instant you know the answer to that question, Taneja Group's Taneja said, you know whichapplications it will do very well in and which applications it will not be a good product for. Taneja noted that Isilon started with throughput-centric systems that performed well with largefiles such as rich media. But, more recently, Isilon worked to balance its architecture and improveits IOPS performance, he said. The company now offers three product lines, each geared toward adifferent type of workload. Another thing to look for in scale-out products is their support for the CommonInternet File System (CIFS), the file-sharing protocol in Windows-based systems, noted RandyKerns, a senior strategist at Evaluator Group in Broomfield, Colo. Kerns said that scale-out network-attached storage systems in HPC were predominantly Unix- orLinux-based, and may lack a native CIFS implementation. Without native CIFS, users might experienceproblems with permissions handling, security and Active Directory integration, he said.  Some NAS vendors, such as Dell and HDS BlueArc, claim tosupport both traditional/scale-up capacity and scale-out performance configurations, with a singlenamespace to ease management. Scale-up systems cant typically scale out, but scale-out systems can scale up, said ESGsMcClure. She said the transactional file I/O capabilities in EMC's IsilonS-Series qualify those products as scale-up and scale-out.\n    One reason DASsolutions continue to live on is that SAN and NAS have largely underdelivered on theirpromises. SANs were supposed to make it easy to create a global pool of storage that could bedynamically divvied up among servers so that only the capacity actually needed at the time wasassigned to a server. For the first eight years or so of the technology's existence, thiscapability was largely unavailable, and SAN storage had to be hard-partitioned to individualservers. When a server needed more capacity, a new partition had to be allocated to that server andthen concatenated into the existing storage pool on the server or, worse, managed separately. Theprocess of adding storage to a server on a SAN was very similar to the prior DAS methodology. Data protection was also supposed to get a lot easier. The goal was to back up the SAN directlyand not have to back up the individual servers. While a few software applications were able toaccomplish that feat, all suffered from blindly backing up data and not understanding what thatdata was. Users quickly realized they needed \"applicationawareness\" to back up active applications and then perform intelligent restores. As a result,some form of backupsoftware was required on the servers. Finally, the price of SAN or NAS technology is still significantly higher than DAS. Many usershave decided it's less expensive to inefficiently directly attach storage than to efficiently shareit. To be fair, modern SAN and NASimplementations have addressed the early storage allocation shortcomings with technologies suchas thin provisioning.\nHowever, the time it took to deliver on the allocation promise allowed DAS tobuild on its foothold in the data center. But the other challenges remain, for the most part. The primary driver for SAN/NAS adoption has been the advent of server and desktopvirtualization, since the ability to move virtual server images between physical hosts requires sharedstorage. Virtualization also makes application-aware, off-host backup viable due to the entireserver being a file that can be backed up without interacting with the original physical host. Butdespite this new and important use case for shared storage, DAS continues to live on in the datacenter. And its value is increasing. One of the key reasons for the continued popularityof DAS solutions in the data center is the need for a local boot drive. While most SANs supportsome form of booting methodology, it still requires specialized host bus adapters (HBAs) and specificsupport on the SAN storage system. As a result, most physical servers still boot from DASstorage. Thanks to solid-state drives (SSDs), booting from thelocal server offers some specific advantages over booting from the SAN. First, servers can now bebooted or rebooted in seconds from a local SSD. And the SSD can be used as a virtual memory pagingarea, which is incredibly important in virtual environments. As hosts in these environments getloaded up with virtualmachines (VMs), they can quickly run out of RAM and begin to use local storage as a memorypaging area. If this local storage is hard disk, performance can degrade substantially.\nWhen thislocal storage is memory-based, like flash SSD, the drop in performance is negligible. SSD as a bootdrive allows for more virtual machines without the need to purchase expensive RAM. Solid-state storage also plays another role in the resurgence of DASadoption: as an extension to the SAN. Leveraging even higher performing PCIe-based solid-statestorage, architectures are now developing that allow the tiering or caching of data directly to theserver that needs it. PCIeSSDs can communicate directly with the CPU and don't get bogged down by SAS or SATA protocolslike typical SSDs. This again makes an ideal virtual memory paging area for RAM-constrainedsystems, but it's the tiering or caching use case that's becoming increasingly interesting. With this architecture, storage systems can intelligently pre-stage the most active data withinthe PCIe SSD. Then, when a request for data is made by an application or user, it will be availablefor high-speed delivery on the PCIe SSD. This means the application or user doesn't have to waitfor the request to travel across the storage network, be accepted and processed by the storagecontrollers, wait for hard drives to rotate into position, and then send the requested data orwrite acknowledgment all the way back up that infrastructure. If successful, this model of storage architecture design would turn the SAN world upside down.Storage on the SAN would become the central repository of information that's growing cold, and thelocal PCIe SSD DAS would be used for the most active data.\nThe SAN would be used for long-termretention or backup, and the server would be used for active processing. This would lead to SANstorage system designs where capacity is the focus and performance is less important. But the onedownside to native PCIe SSDs is that you can't boot from them, so a local SAS hard drive, or evenan SSD in a drive form factor, would still be required. Other key drivers for the revivalof DAS solutions are the designs of massive storage environments like those of Facebook, Googleand others. These systems combine compute and storage on a single server that's highly networkedfor communication with the other servers. These systems often have locally attached storage and theability to access data on other servers. They can even leverage a combination of PCIe SSD and harddisk drive for booting. These online providers and Internet technology companies chose this designso they could get incredibly cost-efficient architectures with the ability to scale easily as newservers were added. This model of DAS converged with compute was thought to be a limited use case -- one that onlycompanies with large online apps would deploy. Now, however, thanks again to server virtualization,there's often a need to build scalable compute and storage infrastructure simultaneously. Vendorslike Nutanix offer products that are clusters of servers with internal storage to provide a turnkeycloud compute-type of infrastructure suitable for more traditional data centers. Server virtualization still needs shared storage to move virtual machine images and provide highavailability.\nThese converged architectures automatically copy data to the other nodes in thecluster so that virtual machine images are available to any node in the cluster. This \"shared DAS\"model provides the simplicity and cost-effectiveness of local storage while providing many of thebenefits of a SAN. DASsolutions are thriving. There are many storage experts who believe the data center is movingtoward a \"DAS mostly\" environment (described above), where the SAN would become the long-termrepository, while truly active data gets stored locally on the server that needs it. The softwareto manage this movement of data is maturing quickly and will be used to keep active data locally.It will also be able to acknowledge the writing of new data locally, and then sync that data to thecapacity SAN in the background. The drivers for a potential shift to this \"DAS mostly\" model are the performance demands of thevirtual environment and the performance capabilities of solid-state storage. One driver has a needfor data locally, and the other has the ability to leverage local data by avoiding the latencycaused by the storage network. As always, there are a lot of potential options for a storage administrator when dealing withstorage challenges. The first step is to invest in a performanceanalysis tool that can help fine-tune the current environment.\nEMC Corp. today made its long-awaited launch of its second-generation VNX unifiedstorage platform, upgrading the platform with new controllers and software designed to takebetter advantage of flash. The EMC VNX2 rollout consists of six hybridarrays that can combine solid-state drives (SSDs) and hard disks, as well as an all-flashmodel. EMC claims its flash optimization improves speeds to as much as four times greater thanthose of the first-generation VNX. While EMC included SSDsas an option in VNX since 2011, it never designed controllers and software for flash inprevious versions. \"In previous versions we assumed the bulk of the array was going to be a disk drive and a littlebit of flash,\" said Eric Herzog, senior vice president of product management for EMC's unifiedstorage. \"In this pass, we've completely optimized the box for flash. Now the controller isengineered to take advantage of flash no matter what.\" Herzog said the new controllers removebottlenecks that impede flash by increasing lanes from the CPU to the PCI Express bus from 60to 160. The original VNX came about as a merger of EMC'sClariion SAN and Celera's NAS platforms. All of the software was single-threaded and could nottake full advantage of multiple-core CPUs.\nWith VNX2, EMC developed dynamic multicore optimizationsoftware that sits on top of the block and file operating systems to take advantage of Intel'smulticore technology and distributes all VNXdata across up to 32 cores to improve performance. \"A lot of the magic fairy dust here is around the software,\" Herzog said. EMC claims that the highest-end new system -- the VNX8000 -- can handle up to 1.1 milliontransactions, 30 GB per second of bandwidth and 6 petabytes of capacity. The following are the new hybrid models:  VNX5200, with 125 drives, 600 GB of FAST Cache and one or two X-Blades (NAS heads)  VNX5400, with 250 drives, 1 TB of FAST Cache and one or two X-Blades  VNX5600, with 500 drives, 2 TB of FAST Cache and one or two X-Blades  VNX5800, with 750 drives, 3 TB of FAST Cache and two or three X-Blades  VNX7600, with 1,000 drives, 4.2 TB of FAST Cache and two to four X-Blades  VNX8000, with 1,000 drives (with plans to expand to 1,500 drives), 4.2 TB of FAST Cache and twoto eight X-Blades.  Each model includes two storage processors that range in memory per processor from 16 GB on thelower-end systems to 126 GB on the VNX8000. CPU cores range from four on lower models to 16 on theVNX8000.\nThe VNX2 arrays support CIFS, NFS and pNFS for files and 8 Gbps Fibre Channel, iSCSI and FibreChannel over Ethernet for block storage. Customers can choose from among 300 GB, 600 GB and 900 GB SAS drives; 1 TB, 2 TB and 3 TB (NL)SAS hard drives; and 100 TB, 200 TB and 400 TB enterprisemulti-level cell SSDs in storage pools. The EMC VNX2systems also use 100 TB and 200 TB single-levelcell SSDs for FAST Cache. The VNX7600-F is an all-flasharray that scales to 400 TB of capacity. EMC claims that the VNX7600-F can handle 500,000 8KIOPS, but Herzog pointed out that EMC's XtremIOall-flash system is twice as fast, with half the latency of the VNX7600-F. \"XtremIO is stillour top dog for flash,\" he said. The new VNX boxes also includepost-process, fixed block datadeduplication for primary storage. VNX will dedupe at 8K blocks and at the LUN level. Like theoriginal VNX systems, the VNX2 arrays support single-instance storage to reduce file storagecapacity. EMC also said that the VNX5400, 5600 and 5800 will be included in new Vspexreference architectures, and it will also add recently launched Avamar7 backup software and DataDomain DD2500 and DD4200 disk backup systems to Vspex. The new VNX systems will be included in Vblockconfigurations sold by VCE, a joint venture formed by EMC, Cisco and VMware.\nIt allows us to focus on applicationoptimization. While some vendors are selling converged infrastructure systems, its still nowhere near amajority of the enterprise, said Greg Schulz, founder of the Stillwater, Minn.-based StorageIOGroup, a technology advisory and consulting firm. One NetApp partner said that while FlexPods have fared well among large customers, the systemsprobably are not selling as quickly as vendors would like. SMBs are just getting the hang of being virtualized, said Tory Skyers, a solutions architectfor a NetApp partner.Its still a challenge getting into smaller customers [convergedinfrastructure] has been a little scary for folks who understand it, much less for folks where ITis not their business.  Some enterprise IT customers are skeptical about the value of a pre-integrated infrastructure. I do think converged infrastructures are great for certain businesses, [but] I just dont seehow it provides the flexibility and cost per VM or server I can currently get in a more traditionalenterprise setting, wrote Chris Rima, supervisor of infrastructure systems for a utility in theSouthwest, in an email. Other users see the appeal of converged infrastructure offerings in theory, according to ThomasPeacock, systems architect with SYS/TOMS Technology Partners Inc. in Tewksbury, Mass. He saidseveral of his clients have evaluated products like the Vblock and FlexPod. IT departments are becoming very lean with little in their staffing budgets, and some people Italk with are interested in soup to nuts, preconfigured systems, Peacock said. While his customers like the idea of a converged infrastructure system, Peacock said none ofthem have purchased one.\n Architecting the enterprise for the future     [There are] a couple of ways I would look at that.One is you want to empower your employee base. So, we just talked about businessintelligence and ***ytics tools; if IT departments can provide great tools to provide insighttoday and then get out of the way, they're doing their workforce a great service, because the ITdepartment cannot fit between users and information. They have to provide tools so users can nimblyand agilely access the data they need to, when they need to, and look at it in ways they want to.So, I think that's very important for IT departments going forward -- and that produces agility.The architectures that will deliver that will be the ones that are successful.  You can think of theenterprise at two levels. You can think of the enterprise of the actual business and how thebusiness will operate. And then you can think of the enterprise as what that business brings to themarket -- IT will play a role in both, so we need to prepare with skills. If you think of the enterprise of the future as how the business will operate, organizationswill become virtual. Business will need to be able to collaborate and exchange ideas and shareintellectual property well beyond the walls of the company. As IT, we need to make that happen.Collaboration, mobility and sharingof information become critical, and [becoming virtual] is a completely different approach [to]how you actually engineer your networks and how you implement security.\nSo, you prepare by bringingthe right skills. You prepare by keeping the team constantly connected to what happens in thebusiness, what happens in technology and the industry. It's a multifaceted problem, but that iswhere IT is going.  It used to be, [when] people came to work, that's wherethe technology was; at home you didn't have it. Now it's the opposite: A lot of people havebetter technology at home than they do at work. And to me, there's a chasm, because you havethe kind of older generation who run these companies, who are really not that technology-literate.Then you get the new kids out of school who are just the opposite. And I think, 'Wow when they getinto the management, things are really going to change.' But how do we do it? I have a core team of people that have worked with me for quite some time. I have some peoplethat have worked for me for almost 15, 20 years, and they've worked at four different retailerswith me. These are my core management team [members], so they know me [and] I know them. And, tome, to be successful, it's not just about technicalities, it's about having domain experiencebecause, in retail, they are slow to adapt technology. But now, everyone realizes you can'tseparate technology from the enterprise. But because my team knows retail inside out -- becausewe're looked at as trusted advisers -- we end up bringing most projects to them. It's not the casethat they're going to argue, 'Oh my God! You're too slow!'\nA lot of the innovation is actuallycoming from my group, and it's because of the close relationship that we have with thebusiness.  For me, the most important thingin a CIO's role is to understand what the business strategy is, [and] then have [an] ITroadmap that actually supports that strategy. I never try to think of a roadmap that's five or10 years long, because I think in this changing world, it's no question that neither the business,nor IT as a result of that, can really have a plan that can be that long in terms of takingeverything into account. And so it constantly changes. As things change, the most important thing is to really have the agility to change your planwith it and how you operate and how your team operates with it. I say the second one is moreorganizational, and truly for IT to be part of the business and very close to the business, [youmust make] sure, first of all, that your employees are well-versed and understand the business thatthey're trying to support through the technology. So, it's really understanding and being close tothe business and making sure that you are their confidant from a technology perspective and[understand] what we need to do from a business perspective.\n